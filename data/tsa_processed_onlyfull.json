[{"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 0, "text": "Extracting information about entities and their relationships from unstructured text is an important problem in NLP. Conventional datasets and methods for information extraction (IE) focus on within-sentence relations from general Newswire text\u00a0[26]. However, recent work started studying the development of\r\nfull IE models and datasets for short paragraphs\r\n(e.g., information extraction from abstracts of scientific articles as in SciERC \u00a0[13]),\r\nor only extracting relations (given ground truth entities)\r\non long documents (e.g.\u00a0[10]).\r\nWhile these tasks provide a reasonable testbed for developing IE models, a significant amount of information can only be gleaned from analyzing the full document. To this end, not much work has been done on developing full IE datasets and model\r\nfor long documents.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 1, "text": "Creating datasets for information extraction at the document level is challenging because it requires domain expertise and considerable annotation effort to comprehensively annotate a full document for multiple IE tasks.\r\nIn addition to local relationships between entities,\r\nit requires identifying document-level relationships that go beyond sentences and even sections.\r\nFigure\u00a0REF  shows an example\r\nof such document level relation  (Dataset: SQuAD, Metric: EM, Method: BiDAF, Task:machine comprehension).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 3, "text": "To overcome the annotation challenges for large documents, we perform both automatic and manual annotations, leveraging external scientific knowledge bases. An automatic annotation stage identifies candidate mentions of entities with high recall,\r\nthen an expert annotator corrects these extracted mentions by referring to\r\nthe text of the article and an external knowledge base.Papers with Code: paperswithcode.com\r\nThis strategy significantly reduces the time necessary to fully annotate large documents for multiple IE tasks.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 6, "text": "we introduce SciREX, a dataset that evaluates\r\na comprehensive list of IE tasks, including \\(N\\) -ary relations that span long documents.\r\nThis is a unique setting compared to prior work that focuses on short paragraphs or a single IE task.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 7, "text": "In recent years, there has been multiple attempts to automatically extract structured information from scientific articles. These types of extractions include\r\ncitation analysis\u00a0[12], [5],\r\nidentifying entities and relations\u00a0[2], [15], [14],\r\nand unsupervised detection of entities and their coreference information\u00a0[22].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 8, "text": "Most structured extraction tasks from among these have revolved around extraction from sentences or abstracts of the articles. A recent example is SciERC \u00a0[13],\r\na dataset of 500 richly annotated scientific abstracts containing mention spans and their types, coreference information between mentions, and binary relations annotations.\r\nWe use SciERC to bootstrap our data annotation procedure (Section\u00a0REF ).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 9, "text": "There has been a lack of comprehensive IE datasets annotated at the document level.\r\nRecent work by\u00a0[9], [10] tried to rectify this by using distant supervision annotations to build datasets for document-level\r\nrelation extraction.\r\nIn both datasets, the task of relation extraction is formulated as a binary classification to check if a triplet of ground-truth entities is expressed in the document or not.\r\nInstead, our work focuses on a comprehensive list of information extraction tasks \u201cfrom scratch\u201d, where the input is the raw document.\r\nThis makes the IE model\r\nmore interesting as it requires to perform entity extraction, coreference resolution, saliency detection\r\nin addition to the relation extraction.Another approach is to perform entity extraction then use the binary classification approach with a list of all possible combinations of relation tuples. This might work for short documents, but it is intractable for long documents because of the large number of entities.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 10, "text": "Most work in general domain IE focus on sentence-level information extraction\u00a0[21], [17], [11].\r\nRecently, however, [25] introduced DocRED, a dataset\r\nof cross-sentence relation extractions on Wikipedia paragraphs. The paragraphs are of a comparable length to that of SciERC, which is significantly shorter than documents in our dataset.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 14, "text": "Our entities are abstract objects of type Method, Task, Metric, or Dataset that appear as text in a scientific article. We define \u201cmentions\u201d (or spans) as a specific instantiation of the entity in the text \u2013 this could be the actual name of the entity, its abbreviation, etc. The entity recognition task is to identify \u201centity mentions\u201d and classify them with their types.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 15, "text": "Entities appear in a scientific article are not equally important. For example, a task mentioned in the related work section is less important than the main task of the article.\r\nIn our case, salient entity identification refers to finding if\r\nan entity is taking part in the article evaluation.\r\nSalient Datasets, Metrics, Tasks, and Methods are those needed to describe the article's results.\r\nFor the rest of this paper, we will use the term salient to refer to\r\nentities that belong to a result relation tuple.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 17, "text": "is the task of extracting \\(N\\) -ary relations between entities in a scientific article. We are interested in discovering binary, 3-ary, and 4-ary relations between a collection of entities of type (Dataset, Method, Metric, and Task). It is important to note that this 4-ary relation can't be split into multiple binary relations because, e.g., a dataset might have multiple tasks, and each one has its own metric, so the metric cannot be decided solely based on the dataset or the task.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 19, "text": "Papers with Code (PwC)https://github.com/paperswithcode/paperswithcode-data is a publicly available corpus of 1,170 articles published in ML conferences annotated with result five-tuples of (Dataset, Metric, Method, Task, Score).\r\nThe PwC curators collected this data from public leaderboards,\r\npreviously curated results by other people, manual annotations,\r\nand from authors submitting results of their work.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 20, "text": "This dataset provides us with distant supervision signal for\r\na task that requires document-level\r\nunderstanding - extracting result tuples.\r\nThe signal is \u201cdistant\u201d\u00a0[19] because,\r\nwhile we know that the PwC result tuple exists in the article, we don't know where exactly it is mentioned (PwC does not provide entity spans,\r\nand PwC entity names may or may not appear exactly in the document).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 22, "text": "Given the length of the document is on the order of 5K tokens, we simplify the\r\nhuman annotation task by automatically labeling the data with noisy labels,\r\nthen an expert annotator only needs to fix the labeling mistakes.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 23, "text": "One possible way to augment the distant supervision provided by PwC is\r\nfinding mention spans of PwC entities. Initial experiments\r\nshowed that this did not work well\r\nbecause it does not provide enough span-level annotations that the model\r\ncan use to learn to recognize mention spans.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 26, "text": "Given this noisily labeled data, we ask our annotator to perform necessary corrections to generate high-quality annotations. Annotators are provided with a list of papers-with-code entities that they need to find in the document, making their annotations deliberate (as opposed to not knowing which entities to annotate). Our annotator deleted and modified types of spans for salient entities (belong to PwC result tuple) and non-salient entities, while only adding missed spans for salient ones. Also, if a mention was linked to a wrong PwC entity, then our annotator was also asked to correct it. Full annotation instructions are provided in Appendix\u00a0.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 28, "text": "Table\u00a0REF  provides information about the average number of changes made during the human annotation. It shows that 83% (sum of diagonal) are correct automatic labels, 15% (sum of bottom row) are newly added spans, 2% are type changes, and a negligible percentage is deleted entities (sum of the last column). Also, on average, 12% (not in the table) of the final mentions in the document had the wrong PwC links and needed to be corrected, with a majority of changes being removing links from Method spans.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 29, "text": "We also asked four experts (Ph.D. students in ML/NLP field) to annotate five documents to compute the inter-annotator agreement. For mention classification, we achieve 95% average cohen-\\(\\kappa \\)  scores between each pair of experts and our main annotator.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 30, "text": "To measure if automatic labeling is making the human annotation faster, we also asked our annotator to perform annotations on five documents without automatic labeling. We compute the difference in time between these two forms of annotation per entity annotated. Note that here, we only ask our annotator to annotate salient mentions. With the automatic labeling, annotation speed is 1.34 sec per entity time vs. 2.48 sec per entity time on documents without automatic labeling (a 1.85x speedup).\r\nWe also observe 24% improvement in recall of salient mentions by including non-salient mentions, further showing the utility of this approach.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 31, "text": "We develop a neural model that performs document-level IE tasks jointly in an end-to-end fashion.with the exception of coreference resolution\r\nThis section details our model design (also summarized in Figure\u00a0REF ).\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 35, "text": "Each mention \\(m_j\\)  is classified as being salient or not (i.e., should it belong in a relation tuple) by passing its span embedding \\(me_j\\)  through a feedforward layer. Because saliency is a property of entities, not mentions, this mention saliency score is just an input to the salient entity cluster identifications.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 36, "text": "The coreference step is given a list of all pairs of identified mentions, and it decides which pair is coreferring. This component is separate from the end-to-end model. It concatenates the \u201csurface forms\u201d of two spans \\(m_i\\)  and \\(m_j\\) , embed them using SciBERT, then use a linear classification layer on top of [CLS] embedding to compute the pairwise coreference score \\(c_{ij}\\) . We also tried integrating it into our model, where we classify pairs of \u201cspan embeddings\u201d (not the surface form) but found the separate model that uses surface forms to work much better.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 37, "text": "Given a list of span pairs \\(m_i\\)  and \\(m_j\\) , and their pairwise coreference scores \\(c_{ij}\\) , they are grouped into clusters that can be thought of as representing a single entity. We generate a coreference score matrix for all pairs and perform agglomerative hierarchical clustering\u00a0[24] on top of it to get actual clusters. The number of clusters is selected based on the silhouette score\u00a0[20]\r\nwhich optimizes for the cohesion and separation of clusters and does not depend on having gold standard cluster labels.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 38, "text": "This step filters out clusters from the previous step, and only keep salient clusters for the final relation task.\r\nTo do so, we take a simple approach that identifies a salient cluster as the one in which there is at least one salient mention (as determined previously). The output of this step is a set of clusters \\(C_1, ..., C_L\\)  where each cluster \\(C_i\\)  is a set of mentions \\(\\lbrace m_{i_1}, ..., m_{i_j}\\rbrace \\)  of the same type.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 39, "text": "Given all the clusters of mentions identified in a document from the previous step, our task now is to determine which of these belong together in a relation. To that end, we follow [10] methodology. We consider all candidate binary and 4-tuples of clusters and classify them as expressed or not expressed in the document. Here we describe the classification of 4-ary relations. For binary relation, the method is similar.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 41, "text": "For each cluster \\(C_i \\in R\\) , we construct its section embedding \\(E_i^{s}\\)  by max-pooling span embeddings of the mentions of \\(C_i\\)  that occur in section \\(s\\)  (along with a learned bias vector \\(b\\)  in case no mentions of \\(C_i\\)  appear in section \\(s\\) ). Then the section \\(s\\)  embedding of tuple \\(R\\)  is \\(E_R^s = \\text{FFN}([E_1^s; E_2^s; E_3^s; E_4^s])\\)  where \\(;\\)  denotes concatenation and FFN is a feedforward network.\r\nWe then construct a document level embedding of \\(R\\) , \\(E_R\\)  as mean of section embeddings \\(\\frac{1}{|S|}\\sum _{s=1}^{|S|} E_R^s\\) . The final classification for relationship is done by passing the \\(E_R\\)  through another FFN, which returns a probability of this tuple expressing a relation in this document.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 42, "text": "While mention identification, span saliency classification, and relation extraction share the base document and span representation from BERT + BiLSTM and trained jointly, each of these subparts is trained on ground truth input. Note that we require the saliency classification and relation extraction to be independent of mention identification task since the output of this task (essentially the span of mention text) is non-differentiable. It is conceivable that mixing the gold mention spans with predicted mention spans might give an improvement in performance; therefore, we leave this as future work.\r\nThe model jointly optimizes three losses,\r\nnegative log-likelihood for mention identification,\r\nbinary cross-entropy for saliency classification,\r\nand binary cross-entropy for relation extraction,\r\nwith all three losses weighted equally.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 44, "text": "Mention Identification is a sequence labeling task, which we evaluate using the standard macro average F1 score of exact matches of all mention types.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 47, "text": "Salient Entity Clustering evaluation relies on some mapping between the set of predicted clusters and gold clusters.\r\nGiven a predicted cluster \\(\\mathcal {P}\\)  and a gold cluster \\(\\mathcal {G}\\) , we consider \\(\\mathcal {P}\\)  to match \\(\\mathcal {G}\\)  if more than 50% of \\(\\mathcal {P}\\) 's mentions belong to \\(\\mathcal {G}\\) ,We consider two mention spans to be a match if their Jaccard similarity is greater than 0.5. that is \\(\\frac{|\\mathcal {P} \\cap \\mathcal {G}|}{|\\mathcal {P}|} > 0.5\\) . The 0.5 threshold enjoys the property that, assuming all predicted clusters are disjoint from each other (which is the case by construction) and gold clusters are disjoint from each other (which is the case for 98.5% of them), a single predicted cluster can be assigned to atmost one gold cluster. This maps the set of predicted clusters to gold clusters, and given the mapping, it is straightforward to use the F1 score to evaluate predictions. This procedure optimizes for identifying all gold clusters even if they are broken into multiple predicted clusters.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 48, "text": "Relation Extraction evaluation relies on the same mapping used in the evaluation of salient entity clustering. Under such mapping, each predicted \\(N\\) -ary relation can be compared with gold relations, and decide if they match or not. This becomes a binary classification task that we evaluate with positive class F1 score.\r\nWe report F1 scores for binary and 4-ary relation tuples. We get binary relations by splitting each 4-ary relation into six binary ones.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 50, "text": "\u00a0[23] is an end-to-end model for entity and binary relation extraction (check Section\u00a0 for details).\r\nBeing a span enumeration type model, DyGIE++ only works on paragraph level texts and extracts relations between mentions in the same sentence only. Therefore, we subdivide SciREX documents into sections and formulate each section as a single training example. We assume all entities in relations returned by DyGIE++ are salient. We map each binary mention-level relation returned to entity-level by mapping the span to its gold cluster label if it appears in one.\r\nWe consider 3 training configurations of DyGIE++,\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 54, "text": "Results in Table\u00a0REF  show that we perform generally better than DyGIE++. The performance on end-to-end binary relations shows the utility of incorporating a document level model for cross-section relations, rather than predicting on individual sections. Specifically, We observe a large difference in recall, which agrees with the fact that 55% of binary relation occur across sentence level.\r\nDyGIE++ (All sections) were not able to identify any binary relations\r\nbecause 80% of training examples have no sentence level binary relations, pushing\r\nthe model towards predicting very few relations.\r\nIn contrast, training on SciERC (and evaluating on SciREX) gives better results\r\nbecause it is still able to find the few sentence-level relations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 61, "text": "Overall, these results indicate that identifying the saliency of entities in a scientific document is a challenging task. It requires careful document-level analysis, and getting it right is crucial for the performance of an end-to-end document-level IE model. Also, the difference between results in the third block of the results and the component-wise results indicate that the whole model can benefit from incremental improvements to each component.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 64, "text": "This research was supported by the ONR MURI\r\nN00014-18-1-2670, ONR N00014-18-1-2826,\r\nDARPA N66001-19-2-4031, and Allen Distinguished Investigator Award. We thank the Semantic Scholar team at AI2, UW NLP, and anonymous reviewers for their insightful comments. We are especially grateful to Kyle Lo for help with Grobid parser, the complete Papers With Code team for making their data publicly available, Dan Weld and Robert Stojnic for helpful discussion and feedback.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 66, "text": "For the BERT coreference model, we use SciBERT-base embeddings with two mentions encoded as [CLS] mention 1 [SEP] mention 2 [SEP]. We use a linear layer on top of [CLS] token embedding to compute the mention pair's coreference score.\r\nAll our models were implemented in AllenNLP library[7].\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 0, "text": "Natural Language Processing (NLP) and Computer Vision (CV) are among the most important Machine Learning fields. The last breakthroughs resulted in models that share similar concepts, such as representation learning or transfer learning\u00a0[0], [1]. It is sufficient to mention ResNet\u00a0[2] or DenseNet\u00a0[3] in the CV and BERT\u00a0[4] or RoBERTa\u00a0[5] from the NLP field. These models are preliminarily pre-trained on the massive amount of general data and can be fine-tuned to suit the downstream task such as classification or entailment. In the field of CV, there is a very lively discussion about using Metric Learning methods for fine-tuning pre-trained models. Metric Learning tries to shape the embedding space so that similar data are close to each other while dissimilar are far from each other\u00a0[6].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 1, "text": "In NLP, the cross-entropy loss has been widely used for supervised fine-tuning language models. However, it turned out that it is not always the most optimal objective function since it is shown to have relatively high variations across multiple runs with different seeds, even though only a few training components can relate directly to the seeds\u00a0[7]. It is even more unstable in fine-tuning models where the number of observations is relatively limited\u00a0[8].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 2, "text": "The DML methods address some of the CE problems. Instead of learning to distinguish observations from different classes, the DML losses are meant to minimize the distances between representations of observations from the same class and maximize distance if the classes are different. In this way, it learns not separable features of the representations as in the case of CE loss but how the general representation of the class looks\u00a0[9]. The difference between the CE loss and DML is especially visible when the number of observations is small.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 3, "text": "Therefore, this research aims to create a benchmark that shows how different DML losses can influence supervised fine-tuning language models in the few-shot learning settings in NLP. Previous work of Gunel and team\u00a0[10] first examined how to combine Cross-Entropy and Supervised Contrast Learning losses for the RoBERTa-large fine-tuning. Still, it has certain limitations, e.g. it only considered one of many possible DML losses and did not use the entire language model output, only the vector associated with the token [CLS]. We extend these experiments by introducing more advanced proxy-based DML losses and applying them to the entire RoBERTa output, which has been comprehensively analyzed based on the 40-fold cross-validation. We have also developed an inference method incorporating proxy-based DML losses into the inference/prediction process.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 5, "text": "\r\nWe have described the language model fine-tuning process with DML loss functions and added modifications that improve the method for applying proxy-based DML losses.\r\n\r\nWe have elaborated an inference method that relies on proxies derived from proxy-based DML methods and can further improve performance on downstream tasks.\r\n\r\nWe have conducted systematic and thorough experiments based on 40-fold cross-validation over several datasets to investigate how different DML losses perform in the few-shot learning settings for the supervised fine-tuning of the RoBERTa-base and RoBERTa-large language models.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 6, "text": "The following Section\u00a0 is a brief overview of the DML methods. Section\u00a0 outlines our approach \u2013 a generalization of DML in training, modifications in inference and experimental procedure. The next Section\u00a0 provides a performance analysis of the models and investigates their behaviour. Finally, a summary of the experiments is described in the last Section\u00a0.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 7, "text": "As already stated, Distance Metric Learning involves transforming the embedding space so that representations of observations from the same class are close together, while those from different classes are far apart\u00a0[11]. Most often, the DML is used when the task is related to the information retrieval, such as retrieving data that are most similar to a query\u00a0[12], during self-supervised visual representation training\u00a0[13], k-nearest neighbours classification\u00a0[14] or clustering[15].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 8, "text": "Throughout this paper, we use the following terms. Observations, which we refer to as \\(x_{i}\\)  are represented by embeddings \\(z_{i}\\) , where \\(i \\in I \\equiv \\lbrace 1, \\ldots , N\\rbrace \\)  indicates the index of the anchor observation \\(x_{i}\\) . \\(x^{a}\\)  denotes an anchor, and \\(z^{a}\\)  stands for its vector representation. \\(x^{p}\\)  belongs to the same class as the anchor (positive observation) and \\(x^{n}\\)  - the negative observation, is from different than the anchor class.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 9, "text": "One of the most popular DML methods is Triplet Loss, which is the extension of the Contrastive Loss [16]. It calculates loss from representations triplets of the sample \\({x^{a}}, {x^{p}}, {x^{n}}\\)  The main idea is to minimize the distance between the representations of the anchor and positive observations while maximizing between an anchor and a negative one. The Triplet Loss is given in Equation\u00a0REF .\r\n\\(\\ell _{Triplet}=\\sum _{i=1}^{N}\\left[\\left\\Vert z_{i}^{a}-z_{i}^{p}\\right\\Vert _{2}^{2}-\\left\\Vert z_{i}^{a}-z_{i}^{n}\\right\\Vert _{2}^{2}+m\\right]_{+},\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 11, "text": "Another important loss DML is the N-Pairs loss\u00a0[17]. It represents a minor evolution of the Triplet Loss in that it computes the loss not only from a triplet containing the anchor, single positive and single negative sample representations but pairs the anchor with the single positive and all negative observation representations. The N-Pair Loss is given in the Equation\u00a0REF .\r\n\\(\\leavevmode \\xbox {resizebox}{\\XMLaddatt {width}{213.5pt}\\ell _{NPairs}=\\frac{-1}{N} \\sum \\limits _{i=1}^{N} \\log \\frac{\\exp \\left(z_{i}^{a} \\cdot z_{i}^{p}\\right)}{\\exp \\left(z_{i}^{a} \\cdot z_{i}^{p}\\right)+\\sum \\limits _{j \\ne i} \\exp \\left(z_{i}^{a} \\cdot z_{j}^{n}\\right)}}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 12, "text": "Supervised Contrastive Learning (SupCon) is one of the modern non-proxy DML approaches; it most often outperforms the other non-proxy DML methods such as Triplet Loss or Contrastive Loss. The SupCon introduces temperature regularization\u00a0[18] and batch processing which means that the loss is not calculated just for a single triplet but is the average of all possible triplets from a given batch.\r\nThe SupCon loss extends the self-supervised batch DML approach to the fully-supervised setting, which provides the ability to use the label information\u00a0[19]. Instead of contrasting one positive example for an anchor with all other observations from the batch, SupCon contrasts all examples from the same class (as positives) with all other observations from the batch as negatives. The most critical issue with this approach is that as the number of observations in the batch grows, the number of triplets grows cubically.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 13, "text": "The SupCon loss has been previously applied in the supervised fine-tuning of the RoBERTa-large language model\u00a0[10] and its formula is as follows:\r\n\\( \\leavevmode \\xbox {resizebox}{\\XMLaddatt {width}{213.5pt}\\ell _{SupCon}=\\sum \\limits _{i \\in I} \\frac{-1}{|P(i)|} \\sum \\limits _{p \\in P(i)} \\log \\frac{\\exp \\left(z_{i} \\cdot z_{p} / \\tau \\right)}{\\sum \\limits _{k \\in K(i)} \\exp \\left(z_{i} \\cdot z_{k} / \\tau \\right)},}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 15, "text": "ProxyNCA Loss is the first loss we have used that is proxy-based\u00a0[20]. Proxies are artificial embeddings that can be learned in the training process and are designed to represent data. In supervised settings, there are as many proxies as the number of classes, so each class is well approximated by one artificial proxy. The loss is calculated based on possible triples formed from observations representing the anchor class, a positive proxy (a proxy from the anchor class) and a negative proxy (a proxy from a class different from the anchor). The advantage of the proxy-based method over previous approaches is that the number of all possible triples grows linearly to the cardinality of the dataset, whereas it grows cubically in the case of non-proxy-based loss. In addition, the proxies are more robust to outliers and thus have better performance in terms of speed and convergence. The ProxyNCA Loss is given by the Equation\u00a0REF :\r\n\\(\\ell _{ProxyNCA}=-\\frac{1}{N} \\sum _{i=1}^{N} \\log \\left(\\frac{e^{-d\\left(z_{i}^{a}, p_{i}\\right)}}{\\sum \\limits _{j=1, j \\ne i}^{M} e^{-d\\left(z_{i}^{a}, p_{j}\\right)}}\\right)\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 17, "text": "SoftTriple is based on a similar idea as ProxyNCA Loss but provides the ability to introduce more than one proxy per class, so it can better reflect the distribution of features across classes\u00a0[11].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 18, "text": "The following formulas define the SoftTriple loss:\r\n\\(\\leavevmode \\xbox {resizebox}{\\XMLaddatt {width}{192.1487pt}\\ell _{SoftTriple}=-\\frac{1}{N} \\sum \\limits _{i \\in I}{\\log \\frac{\\exp \\left(\\lambda \\left(\\mathcal {S}_{i, y_{i}}^{\\prime }-\\delta \\right)\\right)}{\\exp \\left(\\lambda \\left(\\mathcal {S}_{i, y_{i}}^{\\prime }-\\delta \\right)\\right)+\\sum \\limits _{j \\ne y_{i}} \\exp \\left(\\lambda \\mathcal {S}_{i, j}^{\\prime }\\right)}}}\\) \r\n\\( \\mathcal {S}_{i, c}^{\\prime }=\\sum _{k \\in K} \\frac{\\exp \\left(\\frac{1}{\\gamma } {{z}_{i}}^{\\top } {w}_{c}^{k}\\right)}{\\sum \\limits _{k \\in K} \\exp \\left(\\frac{1}{\\gamma } {{z}_{i}}^{\\top } {w}_{c}^{k}\\right)} {{z}_{i}}^{\\top } {w}_{c}^{k},\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 20, "text": "ProxyAnchor Loss, unlike the ProxyNCA loss, represents the anchor as an approximation, while positive observations and negative classes are represented as embeddings\u00a0[21].\r\nEquation\u00a0REF  defines the AnchorProxy Loss.\r\n\\(\\leavevmode \\xbox {resizebox}{\\XMLaddatt {width}{192.1487pt}\\begin{aligned} \\ell _{\\text{AnchorProxy }} &=\\frac{1}{\\left|P_{+}\\right|} \\sum _{p \\in P_{+}} \\log \\left(1+\\sum _{z \\in Z_{p}^{+}} e^{-\\alpha (s(z, p)-\\delta )}\\right) \\\\ &+\\frac{1}{|P|} \\sum _{p \\in P} \\log \\left(1+\\sum _{z \\in Z_{p}^{-}} e^{\\alpha (s(z, p)+\\delta )}\\right) \\end{aligned}}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 22, "text": "We study the influence of the DML loss function on fine-tuning a pre-trained language model by extending the categorical cross-entropy (CCE) loss with different DML loss functions. We also conduct experiments on the effect of reusing trained proxies from proxy-based DML methods in the inference process.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 23, "text": "In our experiments with fine-tuning models, we utilize a loss function that combines both the categorical cross-entropy loss and the DML, as given in Equation\u00a0REF .\r\n\\(\\mathcal {L}=(\\beta ) \\mathcal {\\ell }_{CCE}+(1 - \\beta ) \\mathcal {\\ell }_{DML},\\) \r\n\\(\\mathcal {\\ell }_{CCE}=-\\frac{1}{N} \\sum _{i=1}^{N} \\sum _{c=1}^{C} y_{i, c} \\cdot \\log \\hat{y}_{i, c}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 24, "text": "where\r\n\\(N\\)  denotes the observations number,\r\n\\(C\\)  is the class number,\r\n\\(y_{i c}\\)  represents the label of the \\(i\\) th observation from the \\(cth\\)  class,\r\n\\(p_{i c}\\)  represents the model prediction for the \\(i\\) th observation from the \\(c\\) th class,\r\n\\(\\beta \\)  denotes the scaling factor that tunes the influence of both parts of the loss,\r\n\\(\\ell _{{DML }}\\)  denotes DML losses described above such as \\(\\ell _{Triplet}\\) , \\(\\ell _{NPairs}\\) , \\(\\ell _{SupCon}\\) , \\(\\ell _{ProxyNCA}\\) , \\(\\ell _{SofTriple}\\)  or \\(\\ell _{AnchorProxy}\\) .\r\n\\(\\ell _{{CCE }}\\)  stands for the categorical cross-entropy loss.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 25, "text": "The first case is not possible for non proxy-based DML functions (Triplet Loss, N-Pairs Loss and SupCon Loss) due to the high resource intensity. Figure\u00a0REF  sketches the whole training procedure.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 26, "text": "In inference experiments, we explore the effect of reusing trained proxies from the proxy-based DML methods on the inference process. We extend the original vector of logitsThe logits are defined as the vector of non-normalized predictions that a classification layer generates derived from the dense classifier layer with additional logits computed as the cosine distance between the observation representation outputted by the encoder to the proxy from the DML loss \u2013 see Equation\u00a0REF .\r\n\\({p_{i_c}}=\\beta {\\sigma (dense(z_{i}))}+(1 - \\beta ){s(z_{i}, pr_{c})},\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 28, "text": "As the language model we use the current state-of-the-art RoBERTa\u00a0[22] encoder provided by the huggingface library as the pre-trained model in the large and base form: respectively roberta-large and roberta-base. The standard procedure for supervised fine-tuning of the RoBERTa-based language models starts by tokenizing the input text \\(x\\)  to the array of numbers with the special tokens such as [CLS] the first token, [EOS] the end token and [SEP] as the separating sentences token. The tokenized text is then fed to the RoBERTa encoder, which outputs the embedding array \\(z\\) . It is then passed to the dense layer \\(dense()\\) , which returns the logits array \\(l\\) , which cardinality equals the number of classes. The output is passed to the softmax function \\(\\sigma ()\\) , and then to the categorical cross-entropy function that calculates the loss.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 29, "text": "The proxy-based DML methods are fed with the entire output of RoBERTa models, while non-proxy-based methods are much more resource intensive as they need to compare all observations from the training sets and are therefore fed with embeddings associated with the token [CLS], as proposed in the paper\u00a0[10].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 30, "text": "We test our models in the 40-fold cross-validation settings. It implies that each result is an average F1 score of 40 runs. In line with the conclusions of the paper\u00a0[10], we restricted our study to the few-shot learning settings limiting the datasets to 20, 100 and 1,000 observations. For each dataset, we generated 40 folds with the same seed for different test models. Each of the 40 folds consisted of training and test sets, from which we sampled the training set with the same seed so that it was limited to 20, 100, or 1,000 observations in different experiments. It ensured that each test model was trained and tested on the same data. The best hyperparameters were chosen based on the model with the best average F1 score.\r\nAlthough the 40-fold cross-validation is very time-consuming, we decided to apply it to tackle the problem of high variance, which is common when the amount of training data is limited\u00a0[23].\r\n", "annotation": {"entities": {"v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#498f4d2e-17db-4f25-ab8c-0bc8fa363e83", "surface_form": "40", "start": 26, "end": 28}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#7e8514ec-e6d0-4f99-9ba6-26733bfb36c8", "surface_form": "fold", "start": 29, "end": 33}, {"id": "#03360f42-afb0-4e8a-9ccf-713ad201463c", "surface_form": "fold", "start": 749, "end": 753}]}, "a17": {"id": "a17", "type": "a", "subtype": null, "surface_forms": [{"id": "#087f080b-5408-4d5f-a42b-9bf0b2c51a30", "surface_form": "cross-validation", "start": 34, "end": 50}, {"id": "#c21c940b-1e61-44ec-bb0e-7cef7bac271c", "surface_form": "cross-validation", "start": 754, "end": 770}]}}, "relations": {"r3": {"id": "r3", "source": "v1", "target": "p1", "evidences": [{"id": "#7d3dd206-1a3d-435a-8d07-f6396a7e3b7c", "source_surface_form": "#498f4d2e-17db-4f25-ab8c-0bc8fa363e83", "target_surface_form": "#7e8514ec-e6d0-4f99-9ba6-26733bfb36c8", "evidence_sentence": "We test our models in the 40-fold cross-validation settings.", "start": 0, "end": 60}, {"id": "#eb603716-b3b7-49eb-a9b0-45d739d4c40e", "source_surface_form": "#116fc7a7-c50b-4b24-b7ab-8b5665825d55", "target_surface_form": "#03360f42-afb0-4e8a-9ccf-713ad201463c", "evidence_sentence": "Although the 40-fold cross-validation is very time-consuming, we decided to apply it to tackle the problem of high variance, which is common when the amount of training data is limited\u00a0[23].", "start": 734, "end": 924}]}, "r0": {"id": "r0", "source": "p1", "target": "a17", "evidences": [{"id": "#f04d2e3c-16db-4044-ae2c-99e3af900179", "source_surface_form": "#7e8514ec-e6d0-4f99-9ba6-26733bfb36c8", "target_surface_form": "#087f080b-5408-4d5f-a42b-9bf0b2c51a30", "evidence_sentence": "We test our models in the 40-fold cross-validation settings.", "start": 0, "end": 60}]}, "r2": {"id": "r2", "source": "p1", "target": "a17", "evidences": [{"id": "#e240e90b-4664-47da-8d8e-ff3122689bfe", "source_surface_form": "#03360f42-afb0-4e8a-9ccf-713ad201463c", "target_surface_form": "#c21c940b-1e61-44ec-bb0e-7cef7bac271c", "evidence_sentence": "Although the 40-fold cross-validation is very time-consuming, we decided to apply it to tackle the problem of high variance, which is common when the amount of training data is limited\u00a0[23].", "start": 734, "end": 924}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 31, "text": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .\r\nThe final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.\r\n", "annotation": {"entities": {"c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#dc6597af-8f33-42c7-b926-70f1c101484b", "surface_form": "hyperparameter search", "start": 111, "end": 132}]}, "a18": {"id": "a18", "type": "a", "subtype": null, "surface_forms": [{"id": "#9bf99390-8cd9-494e-847f-7f17172896c9", "surface_form": "linear warmup", "start": 278, "end": 291}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ec590506-a241-41b2-8b85-db94b6f42386", "surface_form": "6%", "start": 306, "end": 308}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#5de35b8f-3dd4-4c3e-ba34-cdde419c1f7d", "surface_form": "steps", "start": 312, "end": 317}]}}, "relations": {"r7": {"id": "r7", "source": "c1", "target": "v4", "evidences": [{"id": "#47e19318-a524-499d-a72d-adbc3760cedd", "source_surface_form": "#dc6597af-8f33-42c7-b926-70f1c101484b", "target_surface_form": "#ec590506-a241-41b2-8b85-db94b6f42386", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}]}, "r8": {"id": "r8", "source": "v4", "target": "p5", "evidences": [{"id": "#cadd14a8-f448-4115-bdcb-3949d5e8a229", "source_surface_form": "#ec590506-a241-41b2-8b85-db94b6f42386", "target_surface_form": "#5de35b8f-3dd4-4c3e-ba34-cdde419c1f7d", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}]}, "r9": {"id": "r9", "source": "p5", "target": "a18", "evidences": [{"id": "#867febf1-a996-421d-aad9-dd13da5d95e4", "source_surface_form": "#5de35b8f-3dd4-4c3e-ba34-cdde419c1f7d", "target_surface_form": "#9bf99390-8cd9-494e-847f-7f17172896c9", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 33, "text": "We conducted experiments on the SentEval Transfer Task datasets from the classification and textual entailment tasks\u00a0[24], as described in\r\nTable\u00a0REF .\r\n{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 34, "text": "tab:rb-20-examples-dataset,tab:rb-100-examples-dataset,tab:rb-1000-examples-dataset,tab:rl-20-examples-dataset,tab:rl-100-examples-dataset,tab:rl-1000-examples-dataset present our results separately for the RoBERTa-base and RoBERTa-large encoders trained on 20, 100 and \\(1,000\\)  observations. In the results, we included the baseline performance, where the models were trained with CCE loss as well as models trained with the different DML losses. The results also include the performance of the models with the modified inference process denoted with +inf in tables, as described in Equation\u00a0REF . We also calculated p-values for the results, and placed a \\(*\\)  sign next to the results for which the p-value was less than 0.05.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 32, "text": "Each DML loss has its own hyperparameter search. The search space of hyperparameter \\(\\beta \\)  was the same for all methods: \\(\\beta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . Apart from that, there are parameters depending on the method.\r\nFor the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) . The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .\r\nThe ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .\r\n", "annotation": {"entities": {"a8": {"id": "a8", "type": "a", "subtype": null, "surface_forms": [{"id": "#e7fea104-b107-4596-8fcf-3076e77cd822", "surface_form": "Triplet Loss", "start": 253, "end": 265}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#b2b203f6-fa81-4314-bb5a-1b2fb8c2b26b", "surface_form": "grid search", "start": 270, "end": 281}]}, "p8": {"id": "p8", "type": "p", "subtype": null, "surface_forms": [{"id": "#de22dbd1-a690-4c0e-8cc1-4ca89347e42c", "surface_form": "m", "start": 319, "end": 320}]}, "v2": {"id": "v2", "type": "v", "subtype": "s", "surface_forms": [{"id": "#804d17da-bdd3-4b57-8872-11e88d0748f1", "surface_form": "\\lbrace 1, 3 ,5, 7, 9\\rbrace", "start": 325, "end": 353}]}, "a12": {"id": "a12", "type": "a", "subtype": null, "surface_forms": [{"id": "#8902d870-d816-4740-9bca-7ec1fe6d91a6", "surface_form": "SupCon Loss", "start": 378, "end": 389}]}, "p9": {"id": "p9", "type": "p", "subtype": null, "surface_forms": [{"id": "#96a366c6-20cd-4d71-85e8-5eae6b77fe89", "surface_form": "\\tau", "start": 415, "end": 419}, {"id": "#04cea9a8-288c-48f7-b72c-df2c202f261a", "surface_form": "k", "start": 540, "end": 541}]}, "v3": {"id": "v3", "type": "v", "subtype": "s", "surface_forms": [{"id": "#a2378ab8-b008-4c00-a764-6c00703b76a4", "surface_form": "\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace", "start": 424, "end": 462}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#2c91357f-b459-4e02-bf3b-9f1f63aa2047", "surface_form": "SoftTriple loss,", "start": 483, "end": 499}]}, "v4": {"id": "v4", "type": "v", "subtype": "s", "surface_forms": [{"id": "#5442c00c-4855-4c9b-bf0e-65f937ead631", "surface_form": "\\lbrace 5, 25, 1,000, 2000\\rbrace", "start": 546, "end": 579}]}, "p10": {"id": "p10", "type": "p", "subtype": null, "surface_forms": [{"id": "#ebb7c44a-a329-413f-b8bd-fac1acc9e211", "surface_form": "\\gamma", "start": 587, "end": 593}]}, "v5": {"id": "v5", "type": "v", "subtype": "s", "surface_forms": [{"id": "#036f92ac-a48f-4cae-b5b2-c214be31cd7a", "surface_form": "\\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace", "start": 598, "end": 640}]}, "p11": {"id": "p11", "type": "p", "subtype": null, "surface_forms": [{"id": "#9b0f8b69-1ad7-49ad-85f7-92946941b46b", "surface_form": "\\lambda", "start": 648, "end": 655}]}, "v6": {"id": "v6", "type": "v", "subtype": "s", "surface_forms": [{"id": "#8f67ba66-34aa-4366-a137-73607b115fe5", "surface_form": "\\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace", "start": 660, "end": 693}]}, "p12": {"id": "p12", "type": "p", "subtype": null, "surface_forms": [{"id": "#3645faaf-040d-49d8-9e9a-e2d60bfa8566", "surface_form": "\\delta", "start": 705, "end": 711}, {"id": "#5c30eb7a-fbe5-4969-a824-293eff7a42a3", "surface_form": "softmax scale", "start": 840, "end": 853}]}, "v7": {"id": "v7", "type": "v", "subtype": "s", "surface_forms": [{"id": "#385a8b3d-6dcd-4935-a418-eff3050a964b", "surface_form": "\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace", "start": 716, "end": 757}]}, "a13": {"id": "a13", "type": "a", "subtype": null, "surface_forms": [{"id": "#fa4c8cba-1fd0-4181-99f7-bb78031f1a2f", "surface_form": "ProxyNCA Loss", "start": 767, "end": 780}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#fdfc7f26-23b1-4832-b02b-9623efe3b6fa", "surface_form": "grid search", "start": 812, "end": 823}]}, "v8": {"id": "v8", "type": "v", "subtype": "s", "surface_forms": [{"id": "#1179b259-b968-4a68-87a7-1a4917b9d9a9", "surface_form": "\\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace", "start": 858, "end": 918}]}, "a15": {"id": "a15", "type": "a", "subtype": null, "surface_forms": [{"id": "#9da89b93-4e7e-487b-bcf9-6c8d4e60570d", "surface_form": "ProxyAnchor Loss", "start": 928, "end": 944}]}, "p13": {"id": "p13", "type": "p", "subtype": null, "surface_forms": [{"id": "#f256b126-e8cd-4c78-9d13-bad846997db2", "surface_form": "\\alpha", "start": 1012, "end": 1018}]}, "v9": {"id": "v9", "type": "v", "subtype": "s", "surface_forms": [{"id": "#b58f8e42-9981-4d83-b1f1-fc8e024ab9dd", "surface_form": "\\lbrace 16, 32, 64, 128\\rbrace", "start": 1023, "end": 1053}]}, "p14": {"id": "p14", "type": "p", "subtype": null, "surface_forms": [{"id": "#455ac1dc-e358-4ae9-891f-68e228413904", "surface_form": "\\delta", "start": 1064, "end": 1070}]}, "v10": {"id": "v10", "type": "v", "subtype": "s", "surface_forms": [{"id": "#da9ba332-c002-426d-9c4c-5d1dc436b789", "surface_form": "\\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace", "start": 1075, "end": 1116}]}}, "relations": {"r1": {"id": "r1", "source": "v2", "target": "p8", "evidences": [{"id": "#1191866b-a978-4aad-94d9-978fdaad8c5b", "source_surface_form": "#804d17da-bdd3-4b57-8872-11e88d0748f1", "target_surface_form": "#de22dbd1-a690-4c0e-8cc1-4ca89347e42c", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) .", "start": 246, "end": 359}]}, "r2": {"id": "r2", "source": "p8", "target": "a8", "evidences": [{"id": "#65301f38-6c2d-4b16-a3a9-625a1a320fa1", "source_surface_form": "#de22dbd1-a690-4c0e-8cc1-4ca89347e42c", "target_surface_form": "#e7fea104-b107-4596-8fcf-3076e77cd822", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) .", "start": 246, "end": 359}]}, "r3": {"id": "r3", "source": "v3", "target": "p9", "evidences": [{"id": "#745927ff-d162-456b-83a6-5a8f68924978", "source_surface_form": "#a2378ab8-b008-4c00-a764-6c00703b76a4", "target_surface_form": "#96a366c6-20cd-4d71-85e8-5eae6b77fe89", "evidence_sentence": "The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 360, "end": 468}]}, "r4": {"id": "r4", "source": "p9", "target": "a12", "evidences": [{"id": "#6927f811-0e1f-4814-81f5-089996fd6118", "source_surface_form": "#96a366c6-20cd-4d71-85e8-5eae6b77fe89", "target_surface_form": "#8902d870-d816-4740-9bca-7ec1fe6d91a6", "evidence_sentence": "The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 360, "end": 468}]}, "r5": {"id": "r5", "source": "v4", "target": "p9", "evidences": [{"id": "#3931a6ab-115c-4576-a030-06ba40c9af12", "source_surface_form": "#5442c00c-4855-4c9b-bf0e-65f937ead631", "target_surface_form": "#04cea9a8-288c-48f7-b72c-df2c202f261a", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}]}, "r6": {"id": "r6", "source": "p9", "target": "a14", "evidences": [{"id": "#cfaace1b-5cff-4a84-9fd3-ab677cb25fbd", "source_surface_form": "#04cea9a8-288c-48f7-b72c-df2c202f261a", "target_surface_form": "#2c91357f-b459-4e02-bf3b-9f1f63aa2047", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}]}, "r7": {"id": "r7", "source": "v5", "target": "p10", "evidences": [{"id": "#9fac43fd-3341-4ca3-9987-5db2df07e803", "source_surface_form": "#036f92ac-a48f-4cae-b5b2-c214be31cd7a", "target_surface_form": "#ebb7c44a-a329-413f-b8bd-fac1acc9e211", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}]}, "r8": {"id": "r8", "source": "p10", "target": "a14", "evidences": [{"id": "#d21b5871-9606-4a43-900e-f12317166423", "source_surface_form": "#ebb7c44a-a329-413f-b8bd-fac1acc9e211", "target_surface_form": "#2c91357f-b459-4e02-bf3b-9f1f63aa2047", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}]}, "r9": {"id": "r9", "source": "v6", "target": "p11", "evidences": [{"id": "#6d0bf432-8041-4372-b43e-772ca0aeb5c6", "source_surface_form": "#8f67ba66-34aa-4366-a137-73607b115fe5", "target_surface_form": "#9b0f8b69-1ad7-49ad-85f7-92946941b46b", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}]}, "r10": {"id": "r10", "source": "p11", "target": "a14", "evidences": [{"id": "#fba0b4eb-f0a5-4de9-88a1-49ea38b87d10", "source_surface_form": "#9b0f8b69-1ad7-49ad-85f7-92946941b46b", "target_surface_form": "#2c91357f-b459-4e02-bf3b-9f1f63aa2047", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}]}, "r11": {"id": "r11", "source": "v7", "target": "p12", "evidences": [{"id": "#631300cb-575b-497c-b228-14dfa10d754a", "source_surface_form": "#385a8b3d-6dcd-4935-a418-eff3050a964b", "target_surface_form": "#3645faaf-040d-49d8-9e9a-e2d60bfa8566", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}]}, "r12": {"id": "r12", "source": "p12", "target": "a14", "evidences": [{"id": "#f8ec9aaf-c962-4261-822f-3e37b365c93e", "source_surface_form": "#3645faaf-040d-49d8-9e9a-e2d60bfa8566", "target_surface_form": "#2c91357f-b459-4e02-bf3b-9f1f63aa2047", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}]}, "r13": {"id": "r13", "source": "c2", "target": "v2", "evidences": [{"id": "#984c7168-36fb-42fd-ba0a-1dfae10e0310", "source_surface_form": "#b2b203f6-fa81-4314-bb5a-1b2fb8c2b26b", "target_surface_form": "#804d17da-bdd3-4b57-8872-11e88d0748f1", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) .", "start": 246, "end": 359}]}, "r14": {"id": "r14", "source": "c2", "target": "v3", "evidences": [{"id": "#e96e9617-984f-45d4-8ce2-96fdbaa7b768", "source_surface_form": "#b2b203f6-fa81-4314-bb5a-1b2fb8c2b26b", "target_surface_form": "#a2378ab8-b008-4c00-a764-6c00703b76a4", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 246, "end": 468}]}, "r15": {"id": "r15", "source": "c2", "target": "v4", "evidences": [{"id": "#f6ebe47b-ddce-4e0e-8935-9e98a4e74b2c", "source_surface_form": "#b2b203f6-fa81-4314-bb5a-1b2fb8c2b26b", "target_surface_form": "#5442c00c-4855-4c9b-bf0e-65f937ead631", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 246, "end": 763}]}, "r16": {"id": "r16", "source": "c2", "target": "v5", "evidences": [{"id": "#fe88ff63-eced-4900-8075-796562a41b50", "source_surface_form": "#b2b203f6-fa81-4314-bb5a-1b2fb8c2b26b", "target_surface_form": "#036f92ac-a48f-4cae-b5b2-c214be31cd7a", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 246, "end": 763}]}, "r17": {"id": "r17", "source": "c2", "target": "v6", "evidences": [{"id": "#024d8e97-22e1-4276-ada8-e7ac329e0ffb", "source_surface_form": "#b2b203f6-fa81-4314-bb5a-1b2fb8c2b26b", "target_surface_form": "#8f67ba66-34aa-4366-a137-73607b115fe5", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 246, "end": 763}]}, "r18": {"id": "r18", "source": "c2", "target": "v7", "evidences": [{"id": "#a8fc1c24-f14e-40de-81ac-014da2805dca", "source_surface_form": "#b2b203f6-fa81-4314-bb5a-1b2fb8c2b26b", "target_surface_form": "#385a8b3d-6dcd-4935-a418-eff3050a964b", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 246, "end": 763}]}, "r19": {"id": "r19", "source": "c3", "target": "v8", "evidences": [{"id": "#add34fe3-e3f9-4fa3-aea9-c0265d9fc8fe", "source_surface_form": "#fdfc7f26-23b1-4832-b02b-9623efe3b6fa", "target_surface_form": "#1179b259-b968-4a68-87a7-1a4917b9d9a9", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .", "start": 764, "end": 924}]}, "r20": {"id": "r20", "source": "v8", "target": "p12", "evidences": [{"id": "#8c634374-1f5e-4a7e-9ff4-a69e7f16590c", "source_surface_form": "#1179b259-b968-4a68-87a7-1a4917b9d9a9", "target_surface_form": "#5c30eb7a-fbe5-4969-a824-293eff7a42a3", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .", "start": 764, "end": 924}]}, "r21": {"id": "r21", "source": "p12", "target": "a13", "evidences": [{"id": "#135a96f0-48b2-488e-b791-eb28960fac01", "source_surface_form": "#5c30eb7a-fbe5-4969-a824-293eff7a42a3", "target_surface_form": "#fa4c8cba-1fd0-4181-99f7-bb78031f1a2f", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .", "start": 764, "end": 924}]}, "r22": {"id": "r22", "source": "c3", "target": "v9", "evidences": [{"id": "#60b9d144-cb7d-4757-983c-650b391f4456", "source_surface_form": "#fdfc7f26-23b1-4832-b02b-9623efe3b6fa", "target_surface_form": "#b58f8e42-9981-4d83-b1f1-fc8e024ab9dd", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .\r\nThe ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 764, "end": 1123}]}, "r23": {"id": "r23", "source": "v9", "target": "p13", "evidences": [{"id": "#6c98b8c8-1df2-4da0-99ce-2b1bbec46a28", "source_surface_form": "#b58f8e42-9981-4d83-b1f1-fc8e024ab9dd", "target_surface_form": "#f256b126-e8cd-4c78-9d13-bad846997db2", "evidence_sentence": "The ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 926, "end": 1123}]}, "r24": {"id": "r24", "source": "p13", "target": "a15", "evidences": [{"id": "#082865c0-cb07-4b9f-947d-cf0a833bd478", "source_surface_form": "#f256b126-e8cd-4c78-9d13-bad846997db2", "target_surface_form": "#9da89b93-4e7e-487b-bcf9-6c8d4e60570d", "evidence_sentence": "The ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 926, "end": 1123}]}, "r25": {"id": "r25", "source": "c3", "target": "v10", "evidences": [{"id": "#8a332a13-3ba5-42fb-b07f-7699652de925", "source_surface_form": "#fdfc7f26-23b1-4832-b02b-9623efe3b6fa", "target_surface_form": "#da9ba332-c002-426d-9c4c-5d1dc436b789", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .\r\nThe ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 764, "end": 1123}]}, "r26": {"id": "r26", "source": "v10", "target": "p14", "evidences": [{"id": "#a2f4f0b0-7fdb-461f-8b3d-af98bb9c4e5e", "source_surface_form": "#da9ba332-c002-426d-9c4c-5d1dc436b789", "target_surface_form": "#455ac1dc-e358-4ae9-891f-68e228413904", "evidence_sentence": "The ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 926, "end": 1123}]}, "r27": {"id": "r27", "source": "p14", "target": "a15", "evidences": [{"id": "#e2bb2a0e-7749-44ab-9aa4-1a89d0469736", "source_surface_form": "#455ac1dc-e358-4ae9-891f-68e228413904", "target_surface_form": "#9da89b93-4e7e-487b-bcf9-6c8d4e60570d", "evidence_sentence": "The ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 926, "end": 1123}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 35, "text": "Models trained with the non-proxy DML losses, i.e. Triplet Loss N-Pairs Loss and SupCon Loss, on average outperform the baseline by 0.97 percentage points but are worst than the proxy-based DML losses and proxy-based DML with inference modification by 0.98 and 1.57 percentage points, respectively. The best non-proxy DML loss is the SupCon Loss. It is better on average than the baseline by 1.58 percentage points. The N-Pair Loss is better on average than the baseline by 0.42 percentage points and the Triplet Loss by 0.90 percentage points. As we can see, surprisingly, the worst non-proxy-based DML loss and worst of all analysed DML losses is the N-Pairs Loss. Moreover, considering the average F1 score for RoBERTa-base and 100-element datasets, the SupCon Loss obtained the highest score among all losses, is marginally better than the ProxyAnchor + inf. Also, we note that the p-value is greater than 0.05 for most of the results in the non-proxy group.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 36, "text": "Models trained with proxy-based DML losses, i.e. SoftTriple Loss ProxyNCA Loss and ProxyAnchor Loss, are on average better than the baseline by 1.94. The SoftTriple Loss had the highest average performance increase from baseline among all proxy-based DML losses, at 2.24 percentage points. The ProxyAnchor Loss increased performance by 1.91 percentage points, and ProxyNCA Loss noted the performance gain at 1.68 percentage points. Furthermore, the p-value is less than 0.05 for most of the losses in this group when the dataset size is 20 or 100. Furthermore, if we consider only SofTriple Loss, 63% of the results from data sets with 20 and 100 observations have a p-value below 0.05.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 37, "text": "The language models fine-tuned with the proxy-based DML losses and inference modification yielded the best performance gain, averaging 2.54 percentage points over the baseline model, of all DML losses tested.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 38, "text": "Modification of the inference yielded further performance gains, with models outperforming the baseline by 3.27, 2.00, and 2.35 percentage points for ProxyAnchor Loss + inf, ProxyNCA Loss + inf, and SoftTriple Loss + inf, respectively. That is, the inference modification increased the performance of all proxy-based DML losses, with the highest gain reported for ProxyAnchor Loss at 1.36 percentage points, followed by 0.33 percentage point for ProxyNCA Loss and 0.11 percentage points for SoftTriple Loss.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 39, "text": "Based on the results, we found that, on average, the best DML loss was ProxyAnchor + inf, which increased the performance of the fine-tuned models by 3.27 percentage points. The second loss was SoftTriple + inf resulting in the performance gain of 2.35 percentage points, and the third SoftTriple increased the performance by 2.24 percentage points.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 40, "text": "As in the previous case, the p-value is less than 0.05 in most cases when the dataset size is 20 or 100. However, when considering only ProxyAnchor + inf, about 64% of all the results have p-value smaller than 0.05. If we consider only datasets of size 20 and 100, 75% of the results have p-value smaller than 0.05.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 41, "text": "The previous analysis shows that the models fine-tuned with DML losses on average perform better than the baseline models. Still, this increase is not uniform across different dataset sizes. The average performance increase over the baseline model for datasets of size 1,000 for all DML losses is about 0.22 percentage points, with the most significant increase for the ProxyAnchor + inf at about 0.78 percentage points. For a 100-element dataset, the average gain is 0.73 percentage points, with the largest performance increase for ProxyAnchor + inf at 1.89 percentage points. For the 20-element dataset, there was an average increase over baseline of about 3.95 percentage points, with the most significant growth for ProxyAnchor + inf at about 7.14 percentage points. The performance overview of different DML family methods throughout dataset size is shown in Figure\u00a0REF .\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 42, "text": "Our results show that the smaller the dataset, the more performance gains can be obtained using DML losses. We also note that the highest performance increase can be gained using the proxy-based DML loss function with the inference modification.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 43, "text": "In this paper, we investigated the influence of DML loss functions' performance during supervised fine-tuning of RoBERTa-base and RoBERTa-large language models compared with baselines (fine-tuned with the use of CCE loss only). We also studied the impact of modifying the inference procedure on the models' performance. The analysis was performed based on the 40-fold cross-validation over several datasets from the SentEval Transfer Tasks in the few-shot learning settings. We found that each DML loss function, on average, increases the performance of the RoBERTa base and large encoders. The non-proxy-based DML losses improve on average the performance by 0.97 percentage points, with the highest increase for SupCon Loss at 1.58 percentage points. The proxy-based DML losses increase the model's performance by 1.94 percentage points, with the highest performance gain for the SoftTriple Loss at 2.24 percentage points. In addition, applying inference modifications to models fine-tuned with proxy-based DML losses steadily improves the performance by an average of 0.6 percentage points, with the most significant gain for ProxyAnchor + inf being an increase of 3.27 percentage points over the baseline, making this loss the best of all tested.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 0, "text": "The area of computer vision [0] has excelled in terms of innovation and performance delivered by leveraging Deep Learning [1]. The various tasks of computer vision are classification, object detection [2], object counting [3], image segmentation [4], [5] and many more. Classification [6] can be termed as one of the most primordial tasks in computer vision. The task of classification is identification of an entity or object by prognosticating its appropriate label is done effectively using Convolutional Neural Networks [7] abbreviated as CNN. The standard CNN gone under massive improvements in the latter period when ImageNet [8] Large Scale Visual Recognition Challenge (ILSVRC) [9] came into existence yielding many models that are currently state-of-the-art deep learning models for image classification. These models were later refined to a greater extent and turned out to be a base for many new models effectively. One sub-category in the area of classification is image forgery [10], where the deep learning can be used with binary classification [11], [12], [13] problem. The task will be simple as classifying whether the image is original or fake, hence leading to binary classification problem.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 1, "text": "The image forgery can be termed as a sub-category of image forensics [14] where there are many methods where Error Level Analysis [15] is one of the commonly used techniques. The image when tampered performs compression which is used as the main point for Error Level Analysis abbreviated as ELA. The compression is obtained at 95% of the input image and saved, which is later evaluated with the original image for differences. Images have grid squares which are thoroughly evaluated by ELA which yield errors due to successive saving of the image after editing. The forged section of the image is bright after ELA operation whereas non-tampered area is darker than all the parts of the image. The section highlights with lighter shades where tampering is done specifically. This data can be labelled and given to CNN for classification where the ELA filters over an image are input. Standard CNN application [16] is very obvious and lack the necessary performance where the transfer learning [17] can be performed with state-of-the-art models. The varied popular models can be used, viz. VGG [18], ResNet [19], [20], InceptionNet [21], XceptionNet [22] and EfficientNet [23]. The appropriate data for such an application is CASIA dataset, where the CASIA ground-truth [24] is the first dataset that has distinct 8 classes of images. The tampering over this dataset was performed which yielded CASIA ITDE v.2 [25] dataset having 2 distinct classes, authentic images and forged images. This paper is compendious comparison of state-of-the-art models using transfer learning over the CASIA ITDE v.2, which will be intricately explained in further sections of this paper.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 2, "text": "The VGG [18] is one of the primarily used deep learning computer vision classification based algorithm which has 2 variants, 16 layered known as VGG-16 and 19 layered known as VGG-19. The network consists of blocks, where each block has 2-dimensional convolutional layers and 2-dimensional max-pooling [26] layers. The VGG-19 has total 5 blocks, where first 2 blocks have 2 convolutional layers followed by one max-pooling layer. The remaining 3 blocks have 4 convolutional layers followed by max-pooling layer. Since the model was originally trained on imagenet [8] data which has 1000 classes, we are using the same trained weights of the imagenet for transfer learning. We ignore the output layer of model and append 2-dimensional global average pooling [27] layer followed by 2 layers. The first layer has 1024 hidden neurons with ReLU [28] activation function, followed by the output layer that has 2 hidden neurons with softmax [29] activation function. Since we are dealing with binary classification problem, we could use 1 hidden neuron with sigmoid [30] activation function, but we wanted the output probabilities for inference examination in a through manner. The network for backward propagation [31] uses binary cross-entropy loss function [32] and Adam [33] optimizer for loss optimization. The network has learned 20,551,746 total parameters which are approximately, 20.55 million parameters. If we remove the personally initialized layers the VGG-19 itself consists of roughly 20.02 million parameters.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 3, "text": "The computationally efficient network was definitely the InceptionNet [21] consisting of 22 layers. The inception module was the main point of focus for the network that later made it a state-of-the-art model. The inception modules force the network to train many different filters in parallel fashion with concatenation for depth-wise aspect. This network over the period of time had many improvements, where batch normalization layers were used which is known as Inception-V2 [34] network. Later the network was made deeper with 42 layers which still turned out to be computationally efficient as compared to VGG. This modified network was Inception-V3 [35] that we have used, where we stripped of the output layers and applied 2-dimensional global average pooling [27] followed by one 1024 hidden neuron ReLU [28] activation layer and one 2 hidden neuron softmax [29] activation output layer. The loss function is binary cross-entropy and loss optimizer is Adam. The network has total 23,903,010 parameters, which approximately are around 23.90 million. The trainable parameters are 23,868,578 and non-trainable parameters are 34,432. 21,802,784 are the sole parameters of the Inception-V3 which are roughly 21.80 million respectively.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 4, "text": "The ResNet [19] is termed as one of the most prominently used network for image classification tasks. The main problem that ResNet emphasized on was the degradation problem with convergence of the network. This is basically saturation of the accuracy with increasing depth of the network. For this the residual block[20] was developed which uses a concept known as skip connection which abruptly skips the layers in between while back-propagation [31] to avoid the saturation in loss optimization. The network we have used is ResNet with 152 layers. Now it has 2 versions, where version 1 uses non-linearity in the last layer and version 2 has removed it. We have used the ResNet 152 v2 [36], where we have removed the output layer and added 2-dimensional global average pooling [27] with 1024 hidden neuron ReLU [28] activation layer followed by 2 hidden neuron softmax [29] activation output layer. The loss function and optimizer is same as the earlier networks and it learns 60,431,874 parameters which are approximately 60.4 million parameters. After removing the custom added layers, the ResNet alone has 58,331,648 parameters.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 5, "text": "The XceptionNet [22] is 71 layers deep extension network of Inception-V3 [35] that replaces the inception modules with depth-wise separable convolutions. The depth of the input is not only covered in this network but also spatial dimensions which makes it special which also covers the kernels that cannot be separated into further sections. We remove the output layer and add custom layers where first 2-dimensional global average pooling is applied. Followed by it is one layer with 1024 hidden neurons and ReLU activation function. Finally one output layer with 2 hidden neurons and softmax activation function is used. The loss function used is binary cross-entropy and loss optimizer used is Adam. The network learns total of 22,961,706 parameters which are approximately 22.96 million. The non-trainable parameters are 54,528 only and 22,907,178 are trainable. The parameters excluding the custom defined layers are 20,861,480, which roughly are around 20.86 million respectively.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 6, "text": "The need of efficiency in training and computation of the networks has risen in a broader perspective for which EfficientNet [23] drives as a good example. The scaling of the base features of the input like width, height and depth is done arbitrarily which has been made uniform with EfficientNet. The compound scaling is the main feature and many variation of it were made where advancement also reached the version 2. We have used EfficientNet-V2L [37] which is large network with over 110 million parameters. The network we have trained has 119,060,642 parameters which are around 119 million roughly where the non-trainable parameters are around 500k and rest are trainable. The network without our layers is 117,746,848 parameters which is approximately 117.74 million parameters. The last layer of the EfficientNet-V2L is replaced and 2-dimensional global average pooling is applied. Followed by it is one dense layer with 1024 hidden neurons and ReLU activation function. The final output layer is applied with 2 hidden neurons and softmax activation function. The loss function used is binary cross-entropy and adam loss optimizer for loss optimization.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 7, "text": "The below figure REF  gives a general overview of the training and validation accuracy for all the models used in the implementation. The highest accuracy in the last epoch of training accuracy for VGG-19 is 95.46% and validation accuracy is 93.51% which is very effectively generalized. Similarly the training accuracy for Inception-V3 is 97.35% whereas the validation accuracy is 89.67% which makes model less generalized but a better training accuracy than VGG-19. The training accuracy for ResNet152-V2 is 99.6% and validation accuracy is 90.75%. The training accuracy for XceptionNet is 97.71% and validation accuracy is 90.87%. Finally the training accuracy for EfficientNet-V2L is 96.36% and validation accuracy is 93.15% making it generalized and effective as compared to other networks yet this is not sufficient metrics for stating an inference.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 8, "text": "The figure REF  gives a graphical depiction of the models converging throughout the epochs. The training loss for VGG-19 is 12.25% in the last epochs whereas the validation loss is 18.72%. The training loss for Inception-V3 is 8.8% and validation loss is 28.91%. The training loss for ResNet-152V2 is 2.39% while the validation loss is 30.03% effectively. The training loss for XceptionNet is 5.98% whereas the validation loss is 28.48%. Finally the training loss for EfficientNet-V2L is 9.67% and validation loss is 21.05% respectively. The generalization can be seen better with VGG-19 but highest training loss convergence can be seen with ResNet-152V2. EfficientNet-V2L attains a middle ground effectively.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 9, "text": "The precision [38] in simple terms is positive sample for classification in model accuracy. The better explanation can be given as number of positive samples classified with respect to total number of samples. The table REF  below gives an explanation of precision scores for all the respective models.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 10, "text": "The precision obtained for VGG-19 is highest which is an indication of higher number of positive samples classified as per the total samples effectively. The precision alone though is not the perfect metric for deriving the final inference.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 11, "text": "The recall [38] is number of positive samples classified appropriately with respect to total number of positive samples. The table REF  gives the recall scores for all the algorithms used.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 12, "text": "The Recall score is highest for EfficientNet-V2L which is was directly lowest in terms of precision. This is the reason recall also cannot be used for final inference judgement.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 13, "text": "The Receiver operating characteristic [39] curve is specifically for binary classification that gives the performance of classification at all the thresholds ranging from 0 to 1. It uses false and true positive rates for evaluation of the RoC curve which the abbreviation basically. The area under curve is also its crucial aspect where the entire area covering the RoC and signifies the separability of classes. It is difficult to make a judgement from figure REF  since the predictions are merging, for which zooming into the depth can help derive some intuition.\r\n{FIGURE}{FIGURE}{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 14, "text": "The figure REF  shows the pivots every algorithm made after a stagnant set of predictions. All the algorithms eventually reach the peak that is 1 value. The EfficientNet-V2L has it around 96% as seen in figure REF  clearly. Big leap for Inception-V3 is at 95% approximately. For ResNet-152-V2 it is somewhere above 95% approximately. The VGG-19 turns just before 96% and XceptionNet clearly at 96%. These values are an indication of the learning procedures adopted for every classification threshold and area under their curve.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 15, "text": "All the metrics are very contradictory for making a judgement for the best algorithm. The precision indicates that VGG-19 is better whereas the EfficientNet-V2L is highest for recall. These both inferences are very far away from each other and proper metric is required. The averaging system for this is required where the F Measure also known as F-Score [38] is used.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 16, "text": "The F measure has 1 as the \\(\\beta \\)  value where the precision and recall are in the balanced state for metric measuring. If the \\(\\beta \\)  value is 0.5 or 2, the metric inclines towards precision and recall respectively. These values can be represented with a table for comparison. The table REF  is the depiction of F measure for all the models used where XceptionNet outperforms all the models. Now this is most different observation and least expected. Since one relies on half measures, the results can turn out to be very varied but the least expected always turns out to be correct.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.15196", "paragraph_index": 17, "text": "The area of image forgery is a potential application which in this paper has been leveraged using state-of-the-art deep learning image classification models with transfer learning. The dataset we used was CASIA ITDE v.2 which is a binary classification dataset that differentiates between authentic and tampered images. After performing the error level analysis from image forensics, the variety of famous models were used. Making an inference for the best model was a difficult task indeed due to results section variability yet the right judgement was attained. This paper does open potential thought for transfer learning models and wide variety of applications in the area of deep learning for computer vision. The limitations of the paper are only bounded and entitled to subtle differences between various state-of-the-art models that are very well explained throughout the result section of the paper. The distinction and clarity of reaching an inference in the paper is bounded to the methods of proving a point and we hope with our best belief that some more extensions with more better results can be obtained in future work.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 0, "text": "Graphs are a useful data format that occurs frequently in real-world applications, e.g., computer vision and graphics\u00a0[38], recommender systems\u00a0[56], molecular chemistry inference\u00a0[26], traffic forecasting\u00a0[12], drug discovery\u00a0[23], and so forth. With the rise of graph-based data, graph neural networks (GNNs) are attracting much attention these days. However, there have been fierce debates on the neural network architecture of GNNs\u00a0[31], [49], [15], [51], [9], [11], [60].\r\n{TABLE}{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 1, "text": "For the past couple of years, many proposed methods have been designed based on the diffusion concept. Many recent GNN methods that rely on low-pass filters fall into this categoryDiffusion processes are typically carried out by applying low-pass filters to graphs, which also corresponds to image blurring processes in computer vision.. Although they have shown non-trivial successes in many tasks, it is still unclear whether it is an optimal direction of designing GNNs.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 2, "text": "In Table\u00a0REF , we compare recent methods. Most of them rely on diffusion processes while two of them (i.e., FA-GCN and GPR-GNN) partially utilize reaction processes (although the authors of the two methods were not aware of it). Those two methods, however, utilize limited forms of the reaction processes. This is because those methods were designed without considering reaction processes but by chance, they correspond to certain reaction processes. In this regard, there do not exist any methods that fully consider reaction processes.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 3, "text": "To this end, we propose the concept of graph neural reaction-diffusion equation (GREAD), which is one of the most generalized architectures since we consider both the diffusion and the reaction processes. Reaction-diffusion equations are physical models that can be used when i) substances are diffused over space and time, and ii) they can sometimes react to each other. Whereas diffusion processes smooth node features on a graph out, reaction-diffusion processes lead to many local clusters that are also known as Turing patterns\u00a0[47], [27] (see Fig.\u00a0REF ). Since it is natural that nodes on a graph also constitute local clusters, we conjecture that reaction-diffusion equations are suitable for designing GNNs.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 5, "text": "For our experiments, we consider 6 heterophilic and 3 homophilic datasets \u2014 heterophilic (resp. homophilic) means that neighboring nodes tend to have different (resp. similar) classes. We also compare our method with a comprehensive set of 17 baselines, which covers early to recent GNNs. Our contributions can be summarized as follows:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 7, "text": "Let \\(\\mathcal {G}=\\lbrace \\mathcal {V}, \\mathcal {E}\\rbrace \\)  be a graph with node set \\(\\mathcal {V}\\)  and edge set \\(\\mathcal {E}\\) . The nodes are associated with a feature matrix \\(\\mathbf {X}\\in \\mathbb {R}^{|\\mathcal {V}| \\times F}\\) , where \\(|\\mathcal {V}|\\)  denotes the number of nodes and \\(F\\)  denotes the number of input features. \\(\\mathbf {A}^{raw}\\in \\lbrace 0,1\\rbrace ^{|\\mathcal {V}| \\times |\\mathcal {V}|}\\)  is the adjacency matrix, where \\(\\mathbf {A}^{raw}_{[i,j]}\\)  means the \\((i,j)\\) -th element. The nodes are labelled by the index \\(i\\in \\mathcal {V}\\) , and one-hop neighborhood of each node is denoted as \\(\\mathcal {N}_i\\) .\r\nThe symmetric normalized Laplacian matrix, a commonly used feature aggregation matrix in GNNs, is defined as \\(\\mathbf {L}=\\mathbf {I}-\\mathbf {D}^{-1/2}\\mathbf {A}^{raw}\\mathbf {D}^{-1/2}=\\mathbf {I}-\\mathbf {A}\\) , where the diagonal degree matrix of \\(\\mathbf {A}^{raw}\\)  is \\(\\mathbf {D}\\) , and \\(\\mathbf {A} := \\mathbf {D}^{-1/2}\\mathbf {A}^{raw}\\mathbf {D}^{-1/2}\\)  is the symmetric normalized adjacency matrix \u2014 note that \\(\\mathbf {A} \\in [0,1]^{|\\mathcal {V}| \\times |\\mathcal {V}|}\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 8, "text": "GNNs have many variants and applications. We focus on a brief introduction of representation learning for nodes in supervised or semi-supervised classification tasks. Most existing approaches follow a message-passing framework and use a permutation-invariant local aggregation scheme to update each node's representation, e.g., spatial GNNs. For example, GCN\u00a0[31] averages features of each node's neighbors, including the node's self feature, to update its representation. GAT\u00a0[49] introduces an attention mechanism to learn aggregation weights over all neighbors. For fast and scalable GNN training, sampling-based methods have been developed, such as GraphSAGE\u00a0[28] and FastGCN\u00a0[7]. The simplifying approaches\u00a0[51], [46], [59] also make the GNN methods more efficient.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 9, "text": "Recently, Balcilar et al.\u00a0[1] bridge the gap between spectral and spatial GNNs and unify them into a single framework. ChebNet\u00a0[15] uses the Chebyshev polynomial to approximate filters. GraphHeat\u00a0[53] uses the heat kernel to design graph filters. APPNP\u00a0[32] utilizes the personalized PageRank to set the filter weights. FA-GCN\u00a0[4] can adaptively integrate different signals in the message passing process, and GPR-GNN\u00a0[11] learns the polynomial filters via gradient descent on the polynomial coefficients.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 10, "text": "The diffusion on graphs has recently been actively used in various applications\u00a0[22], [21], including data clustering and dimension reduction\u00a0[2], [14], image processing\u00a0[17], [19], [24], and semi-supervised graph node classification\u00a0[61], [58]. The relationship between GNNs and diffusion on graphs has received much attention. For example, GNNs have been interpreted as a graph diffusion process that performs low-pass filtering on the input features\u00a0[39]. Furthermore, insights from the graph diffusion process have been used to improve the performance of GNNs\u00a0[13], [30], [50], [6], [5].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 11, "text": "Several methods have proposed continuous-depth GNNs leveraging the neural ordinary differential equation (NODE) technology\u00a0[10]. One recent work is GRAND\u00a0[6], which parameterizes the diffusion equation on graphs with a neural network. BLEND\u00a0[5] used a non-euclidean diffusion equation (known as Beltrami flow) to solve a joint positional-feature space problem, resulting in a scheme with adaptive spatial derivatives (known as graph rewiring). These approaches contribute to non-trivial improvements in graph machine learning. We extend the diffusion equation to the reaction-diffusion equation in this work.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 12, "text": "Neural ordinary differential equations (NODEs) solve the initial value problem (IVP), which involves a Riemann integral problem, to calculate \\(\\mathbf {h}(t_{i+1})\\)  from \\(\\mathbf {h}(t_i)\\) :\r\n\\(\\mathbf {h}(t_{i+1}) = \\mathbf {h}(t_i) + \\int _{t_i}^{t_{i+1}} f(\\mathbf {h}(t_i), t;\\mathbf {\\theta }_f) dt,\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 13, "text": "where the neural network parameterized by \\(\\mathbf {\\theta }_f\\)  approximates the time-derivative of \\(\\mathbf {h}\\) , i.e., \\(\\dot{\\mathbf {h}} \\stackrel{\\text{def}}{=}\\frac{d\\mathbf {h}(t)}{dt}\\) . We rely on various ODE solvers to solve the integral problem, from the explicit Euler method to the 4th order Runge\u2013Kutta (RK4) method and the Dormand\u2013Prince (DOPRI) method \u00a0[18]. The Euler method is written as follows:\r\n\\(\\mathbf {h}(t + h) = \\mathbf {h}(t) + \\tau \\cdot f(\\mathbf {h}(t)),\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 14, "text": "where \\(\\tau \\) , which is usually smaller than 1, is a pre-configured step size. Eq.\u00a0(REF ) is identical to a residual connection when \\(h=1\\)  and therefore, NODEs are a continuous generalization of residual networks.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 15, "text": "Other ODE solvers use more complicated methods to update \\(\\mathbf {h}(t + \\tau )\\)  from \\(\\mathbf {h}(t)\\) . For instance, the fourth-order Runge\u2013Kutta (RK4) method uses the following method:\r\n\\(\\mathbf {h}(t + \\tau ) = \\mathbf {h}(t) + \\frac{s}{6}\\Big (f_1 + 2f_2 + 2f_3 + f_4\\Big ),\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 17, "text": "In order to solve the above integral problem, therefore, we need to iterate one of the fixed-step ODE solvers \\(\\lceil T/\\tau \\rceil \\)  times since each iteration updates \\(\\mathbf {h}(t)\\)  to \\(\\mathbf {h}(t+\\tau )\\) . The DOPRI method, on the other hand, is an adaptive solver that dynamically adjusts the step size based on estimated potential errors. As a result, in the case of DOPRI, the number of iterations is not deterministic. DOPRI is widely regarded as one of the most advanced solvers.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 19, "text": "Given a graph \\(\\mathcal {G}\\)  with its node feature matrix \\(\\mathbf {X}\\)  and its symmetric normalized Laplacian (resp. symmetric normalized adjacency) matrix \\(\\mathbf {L}\\)  (resp. \\(\\mathbf {A}\\) ), GREAD can be written as follows:\r\n\\(\\mathbf {H}(0) &= \\mathbf {e}(\\mathbf {X}),\\\\\\mathbf {H}(T) &= \\mathbf {H}(0) + \\int _0^T \\mathbf {f}(\\mathbf {H}(t))dt,\\\\\\hat{\\mathbf {y}} &= \\mathbf {o}(\\mathbf {H}(T)),\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 20, "text": "where \\(\\mathbf {f}(\\mathbf {H}(t)) := \\frac{d \\mathbf {H}(t)}{dt} = -\\alpha \\mathbf {L}\\mathbf {H}(t) + \\beta \\mathbf {r}(\\mathbf {H}(t), \\mathbf {A})\\)  is in the reaction-diffusion form. \\(\\mathbf {r}(\\mathbf {H})\\)  is a reaction term, and \\(\\alpha \\)  and \\(\\beta \\)  are trainable parameters to (de-)emphasize each term. \\(\\mathbf {e}\\)  is an encoder embeds the node feature matrix \\(\\mathbf {X}\\) , an initial hidden state \\(\\mathbf {H}(0)\\) . We then evolve the initial hidden state to \\(\\mathbf {H}(T)\\)  via the reaction-diffusion equation of \\(\\mathbf {f}\\) . The function \\(\\mathbf {o}\\)  is an output layer for a downstream task, e.g., node classification. In particular, \\(\\beta \\)  can be either a scalar or a vector parameter, where the scalar setting means that we apply the same reaction process to all nodes and in the vector setting, we apply different reaction processes with different coefficients to nodes.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 21, "text": "The encoder \\(\\mathbf {e}\\)  has a couple of fully-connected layers with rectified linear unit (ReLU) activations. The output layer \\(\\mathbf {o}\\)  is typically a fully-connected layer, followed by a softmax activation for classification in our experiments.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 22, "text": "In particular, we consider almost all existing reaction terms for \\(\\mathbf {r}\\) , which is different from existing works that do not consider them in a thorough manner. In this perspective, our work is the most comprehensive study on reaction-diffusion GNNs to our knowledge. In the following subsection, we also show that some choices of the reaction term correspond to other famous models \u2014 in other words, some other famous models are special cases of GREAD.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 24, "text": "In order to generate such soft adjacency matrices, we use the scaled dot product method\u00a0[48]:\r\n\\(\\mathbf {A}_{[i,j]} = softmax\\Big (\\frac{(\\mathbf {W}_K\\mathbf {H}_i)^T\\mathbf {W}_Q\\mathbf {H}_j}{d_K}\\Big ),\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 29, "text": "The first three reaction terms, i.e., F, AC, and Z are widely used in various domains. For instance, F is used to describe the spreading of biological populations\u00a0[20], and AC is used for describing the phase separation process in multi-component alloy systems, which includes order-disorder transitions\u00a0[0]. Z is a generalized equation that describes the phenomena that occur in combustion theory\u00a0[25]. The last BS is specially designed by us for GNNs, which we will describe shortly.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 31, "text": "We then proposed to apply the following high-pass graph convolutional filtering or sharpening process to \\(\\mathbf {B}(t+h)\\) . In other words, there is a sharpening process following the above blurring process in a layer:\r\n\\(\\begin{split}\\mathbf {H}(t+h) &= \\mathbf {B}(t+h) + \\mathbf {L}(\\mathbf {B}(t+h)),\\\\&\\Rightarrow \\mathbf {A}\\mathbf {H}(t) + \\mathbf {L}(\\mathbf {A}\\mathbf {H}(t)),\\\\&\\Rightarrow \\mathbf {A}\\mathbf {H}(t) + (\\mathbf {I}-\\mathbf {A})\\mathbf {A}\\mathbf {H}(t),\\\\&\\Rightarrow 2\\mathbf {A}\\mathbf {H}(t) - \\mathbf {A}^2\\mathbf {H}(t),\\\\&\\Rightarrow (2\\mathbf {I}-\\mathbf {A})\\mathbf {A}\\mathbf {H}(t),\\\\&\\Rightarrow (\\mathbf {I}+\\mathbf {L})(\\mathbf {I}-\\mathbf {L})\\mathbf {H}(t),\\\\&\\Rightarrow \\mathbf {H}(t) - \\mathbf {L}^2\\mathbf {H}(t),\\\\&\\Rightarrow \\mathbf {H}(t) - (\\mathbf {I}-\\mathbf {A})^2\\mathbf {H}(t),\\\\&\\Rightarrow \\mathbf {H}(t) - (\\mathbf {I}-\\mathbf {A})\\mathbf {H}(t) + (\\mathbf {A}-\\mathbf {A}^2)\\mathbf {H}(t),\\\\&\\Rightarrow \\mathbf {H}(t) - \\mathbf {L}\\mathbf {H}(t) + (\\mathbf {A}-\\mathbf {A}^2)\\mathbf {H}(t).\\end{split}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 32, "text": "Therefore, we can derive the following difference equation:\r\n\\(\\mathbf {H}(t+h) - \\mathbf {H}(t) = - \\mathbf {L}\\mathbf {H}(t) + (\\mathbf {A}-\\mathbf {A}^2)\\mathbf {H}(t).\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 35, "text": "[t]\r\nHow to train our proposed GREAD\r\nTraining data \\(D_{train}\\) , Validating data \\(D_{val}\\) , Maximum iteration number \\(max\\_iter\\) \r\nInitialize model parameters \\(\\mathbf {\\theta }\\) ;\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 43, "text": "The well-posednessA well-posed problem means i) its solution uniquely exists, and ii) its solution continuously changes as input data changes. of NODEs was already proved in [36] under the mild condition of the Lipschitz continuity. We show that training our NODE layers is also a well-posed problem. Almost all activations, such as ReLU, Leaky ReLU, SoftPlus, Tanh, Sigmoid, ArcTan, and Softsign, have a Lipschitz constant of 1. Other common neural network layers, such as dropout, batch normalization, and other pooling methods, have explicit Lipschitz constant values. Therefore, the Lipschitz continuity of \\(f\\) , \\(m\\)  and \\(j_i\\)  for all \\(i\\)  can be fulfilled in our case, making it a well-posed training problem. Our training algorithm solves a well-posed problem so its training process is stable in practice.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 46, "text": "We use a comprehensive set of baselines classified into the following four groups:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 47, "text": "\r\nIn the first group of baselines, we consider classical GNN methods: ChebNet\u00a0[15], GCN\u00a0[31], GAT\u00a0[49], GraphSAGE\u00a0[28], and SGC\u00a0[51].\r\n\r\nThe next group includes the GNN methods designed for heterophilic settings: Geom-GCN\u00a0[41], H2GCN\u00a0[60], FA-GCN\u00a0[4], and GPR-GNN\u00a0[11].\r\n\r\nThe third group has GNN methods tackling the oversmoothing problem: PairNorm\u00a0[57], JKNet\u00a0[54] and GCNII\u00a0[9].\r\n\r\nThe last group contains continuous-time GNN methods: GDE\u00a0[42], CGNN\u00a0[52], GRAND\u00a0[6], and BLEND\u00a0[5].\r\n\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 49, "text": "Table\u00a0REF  shows the average ranking and accuracy of all real-world datasets. GREAD-BS is ranked at the top with the average ranking of 1.44, followed by GREAD-AC and GREAD-Z. The fourth-ranked method, GREAD-F, shows a clearly higher ranking in comparison with GCNII and others. In Fig.\u00a0REF , we visualize the hidden node features at each ODE time step of Eq.\u00a0(), and the reaction-diffusion processes of GREAD lead to local clusters after several steps.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 51, "text": "We conduct ablation studies about the soft adjacency matrix generation. GREAD can use both the original symmetric normalized adjacency matrix, denoted as OA, and the soft adjacency matrix, denoted as SA. We compare both options. As reported in Table\u00a0REF , SA increases the model accuracy in Cornell except for GREAD-F.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 52, "text": "Next, we also perform the ablation study on \\(\\beta \\) . \\(\\beta \\)  can be either a scalar parameter (denoted as SC) or a learnable vector parameter (denoted as VC). We compare them in Table\u00a0REF . VC shows effectiveness for GREAD-BS and GREAD-F. The VC setting creates rich reaction-diffusion process enough to isolate nodes as shown in Fig.\u00a0REF .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 53, "text": "By varying \\(T\\) , we investigate how the model accuracy changes. The detailed results are in Fig.\u00a0REF . In Chameleon, GREAD-BS achieves the highest mean test accuracy at \\(T=1.7\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 54, "text": "Fig.\u00a0REF  shows the mean test accuracy by varying the step size \\(\\tau \\)  of RK4. In Chameleon, GREAD-BS shows stable test accuracy at all the step sizes, while the others tend to show higher accuracy with larger step sizes.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 55, "text": "We can define the oversmoothing\u00a0[39], [40] from the perspective of the Dirichlet energy. The Dirichlet energy \\(E(\\mathbf {H}, \\mathbf {A})\\)  on the node hidden feature \\(\\mathbf {H}\\)  of an undirected graph \\(\\mathcal {G}\\)  is defined as follow:\r\n\\(\\begin{small}E(\\mathbf {H}, \\mathbf {A})=\\frac{1}{|\\mathcal {V}|}\\sum _{i \\in \\mathcal {V}}\\sum _{j \\in \\mathcal {N}_i} \\mathbf {A}_{[i,j]}||\\mathbf {H}_i - \\mathbf {H}_j ||^2,\\end{small}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 58, "text": "We use the synthetic dataset, called cSBMs\u00a0[16], to demonstrate the mitigation of oversmoothing. This synthetic data is an undirected graph representing 100 nodes in a two-dimensional space with two classes randomly connected with a probability of \\(p=0.9\\) . We report the layer-wise Dirichlet energy given a GNN of 40 layers.\r\n", "annotation": {"entities": {"c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#bdeb7658-07f5-4622-9466-0ef019bec09b", "surface_form": "demonstrate the mitigation of oversmoothing", "start": 52, "end": 95}]}, "a1": {"id": "a1", "type": "a", "subtype": null, "surface_forms": [{"id": "#5bc6ba3e-5d23-4f07-a6d8-cfac43c00f83", "surface_form": "GNN", "start": 310, "end": 313}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#f2d09225-e3b5-485c-99a7-258b282d99c8", "surface_form": "40", "start": 317, "end": 319}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#d65c1c12-b83b-4e54-8e5c-d09318022871", "surface_form": "layers", "start": 320, "end": 326}]}}, "relations": {"r2": {"id": "r2", "source": "c1", "target": "v4", "evidences": [{"id": "#7ba29bde-0be1-4512-9461-0a8d6414c8e1", "source_surface_form": "#bdeb7658-07f5-4622-9466-0ef019bec09b", "target_surface_form": "#f2d09225-e3b5-485c-99a7-258b282d99c8", "evidence_sentence": "We use the synthetic dataset, called cSBMs\u00a0[16], to demonstrate the mitigation of oversmoothing. This synthetic data is an undirected graph representing 100 nodes in a two-dimensional space with two classes randomly connected with a probability of \\(p=0.9\\) . We report the layer-wise Dirichlet energy given a GNN of 40 layers.", "start": 0, "end": 327}]}, "r0": {"id": "r0", "source": "v4", "target": "p6", "evidences": [{"id": "#82189298-e992-4047-9aa6-d49ce96ee9a4", "source_surface_form": "#f2d09225-e3b5-485c-99a7-258b282d99c8", "target_surface_form": "#d65c1c12-b83b-4e54-8e5c-d09318022871", "evidence_sentence": "We report the layer-wise Dirichlet energy given a GNN of 40 layers.", "start": 260, "end": 327}]}, "r1": {"id": "r1", "source": "p6", "target": "a1", "evidences": [{"id": "#8d2df259-dc93-482f-9acf-1a2376fd8b9a", "source_surface_form": "#d65c1c12-b83b-4e54-8e5c-d09318022871", "target_surface_form": "#5bc6ba3e-5d23-4f07-a6d8-cfac43c00f83", "evidence_sentence": "We report the layer-wise Dirichlet energy given a GNN of 40 layers.", "start": 260, "end": 327}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 59, "text": "Fig.\u00a0REF  demonstrates traditional GNNs, such as GCN, and GAT, suffer from oversmoothing because the Dirichlet energy decays exponentially to zero in the first five layers. Converging to zero indicates that the node features become constant, while GREAD has no such behaviors. The Dirichlet energy of GREAD can be bounded in time thanks to the reaction term. GRAND only has a diffusion term with learned diffusivity, so that it can delay the oversmoothing. In the case of H2GCN, it is impossible to report on deeper layers due to memory limitations.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 60, "text": "To test the classification capability of GNNs, we use the synthetic Cora generator\u00a0[60], [34]. We generate synthetic graphs with various homophily ratios and report the test accuracy.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 61, "text": "Fig.\u00a0REF  shows the mean test accuracy on all random splits of the synthetic Cora datasets. MLP, which does not consider the connectivity of nodes, maintains its test accuracy for all homophily rates, which is obvious. GCN, GAT, and GRAND, which consider only diffusion, perform poorly at low homophily settings. H2GCN shows reasonable performance on low homophily rates, but its accuracy suddenly decreases at some homophily settings. All GREAD models have the best trend overall without sudden drops. The reaction terms of GREAD contribute to their stable accuracy for both homophily and heterophily settings compared with other models that rely on only diffusion processes, such as GCN and GRAND.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 64, "text": "The space complexity of GREAD is dominated by evaluating the soft adjacency matrix in Eq. (7), which is \\(\\mathcal {O}(|\\mathcal {E}|\\text{dim}(\\mathbf {H}))\\) , where \\(|\\mathcal {E}|\\)  is the number of edges and \\(\\text{dim}(\\mathbf {H})\\)  is the size of hidden dimension.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 66, "text": "If we set the adjacency matrix and \\(\\beta \\)  to OA and SC respectively, the time complexity of the one-step GREAD-BS computation becomes \\(\\mathcal {O}(n_{\\tau }(|\\mathcal {E}|+|\\mathcal {E}_2|)\\text{dim}(\\mathbf {H}) + |\\mathcal {E}|d_{\\text{max}})\\) , where \\(n_{\\tau }\\) , \\(|\\mathcal {V}|\\)  and \\(d_{\\text{max}}\\)  are the number of steps in \\([0,T]\\) , the number of nodes, and the maximum degree of all nodes respectively. Given that \\(\\mathbf {A}\\)  is sparse, we can calculate \\(\\mathbf {A}^2\\)  in \\(\\mathcal {O}(|\\mathcal {E}|d_{\\text{max}})\\)  because \\(d_{\\text{max}}\\)  is equal to the maximum number of non-zeroes in any row of \\(\\mathbf {A}\\) . The sparse matrix multiplication of \\(\\mathbf {A}^2 \\mathbf {H}(t)\\)  takes \\(\\mathcal {O}(|\\mathcal {E}_2|d_{\\text{max}})\\) , where \\(|\\mathcal {E}_2|=\\frac{1}{2}\\sum _{v\\in \\mathcal {V}}|\\mathcal {N}_2|(v)\\) .\r\nThe computational complexity of the one-step GREAD-F computation is \\(\\mathcal {O}(n_{\\tau }(|\\mathcal {E}|+\\text{dim}(\\mathbf {H})^k))\\text{dim}(\\mathbf {H}))\\) , where \\(k=1\\) . In the case of GREAD-AC and GREAD-Z, their \\(k\\)  values are 2 and 3, respectively.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 67, "text": "For the experiment with real-world datasets in Table\u00a0REF , we consider both the heterophilic and homophilic datasets. They can be distinguished based on the homophily level. We employ the homophily ratio, defined by\u00a0[41], to distinguish high or low homophily/heterophily graphs:\r\n\\(\\text{Homophily ratio}=\\frac{1}{|\\mathcal {V}|}\\sum _{v\\in \\mathcal {V}}\\frac{\\sum _{u \\in \\mathcal {N}_v}(y_u=y_v)}{|\\mathcal {N}_v|}.\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 68, "text": "A high homophily ratio means that neighbors tend to be in an identical class. Some dataset statistics are given in Table\u00a0REF . The 9 real-world datasets we consider are as follows:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 45, "text": "We now evaluate the performance of GREAD and existing GNNs on a variety of real-world datasets. We consider 6 heterophilic datasets with low homophily ratios used in\u00a0[41]: i,ii) Chameleon, Squirrel\u00a0[43], iii) Film\u00a0[45], iv, v, vi) Texas, Wisconsin and Cornell from WebKB\u00a0http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb. We also test on 3 homophilic graphs with high homophily ratios: i) Cora\u00a0[37], ii) CiteSeer\u00a0[44], iii) PubMed\u00a0[55]. Table\u00a0REF  summarizes the number/size of nodes, edges, classes, features, and the homophily ratio. We use the dataset splits taken from\u00a0[41]. We report the mean and standard deviation accuracy after running each experiment with 10 fixed train/val/test splits.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 69, "text": "\r\nChameleon and Squirrel are subgraphs of web pages in Wikipedia\u00a0[43]. The node in Wikipedia graphs represent web pages, the edge mean mutual links between pages, and the node feature corresponds to several informative nouns in the Wikipedia page. All nodes are classified into 5 categories based on the average monthly traffic.\r\n\r\nFilm is a subgraph of the film-director-actor-writer network\u00a0[45]. Each node corresponds to an actor, an edge between two nodes denotes the co-occurrence relationship in a Wikipedia page, and the node feature corresponds to some keywords in the Wikipedia page. All nodes are classified into 5 categories according to the type of actors.\r\n\r\nCornell, Texas, and Wisconsin are three subsets of the WebKB dataset collected by CMU, having many links between web pages of the universities. In these networks, nodes represent web pages, edges are hyperlinks between them, and node features are the bag-of-words representation of web pages. All nodes are classified into 5 categories: student, project, course, staff, and faculty.\r\n\r\nCora\u00a0[37], Citeseer\u00a0[44], and Pubmed\u00a0[55] are among the most widely used benchmark datasets for the semi-supervised node classification. These are citation networks, where nodes, edges, features, and labels respectively correspond to papers, undirected paper citations, the bag-of-words representations of papers, and the academic topics of papers.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 71, "text": "For Fig.\u00a0REF , we use cSBM\u00a0[16] to generate synthetic networks. cSBM generates Gaussian random vectors as node features on top of the classical SBM. The synthetic graph has 100 nodes with 2 classes and two-dimensional features sampled from a normal distribution with \\(\\sigma =2\\) , \\(\\mu _1=-0.5\\) , and \\(\\mu _2=0.5\\) . The nodes are randomly connected with a probability of \\(p=0.9\\)  if they are in the same class and \\(p=0.1\\)  otherwise.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 72, "text": "We use the random graphs generated by cSBM to show the capability of GREAD to alleviate oversmoothing. In the case of GREAD, we run without any hyperparamerter search but list the full hyperparameter list we used in Table\u00a0REF .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 73, "text": "To compare the performance in various homophily rates, we use the synthetic Cora network. We run the experiment with 3 fixed train/valid/test splits and report the mean and the standard deviation of accuracy accordingly. In Table\u00a0REF , we list the hyperparameter range we consider.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 70, "text": "The synthetic Cora dataset is provided by \u00a0[60]. They generate graphs for a target homophily level using a modified preferential attachment process. Nodes, edges, and features are sampled from Cora to create a synthetic graph with a desired homogeneity and feature/label distribution. In Table\u00a0REF , we summarize the properties of the synthetic Cora networks we used.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 74, "text": "We performed 10 repetitions on the train/valid/test splits taken from\u00a0[41] and strictly followed their evaluation protocol. For all data sets, we used the largest connectivity component (LCC) except for Citeseer. We use the dropout only in the encoder network and the output layer. We refer to the dropout in the encoder as `input dropout' and the dropout in the output layer as `dropout'.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 77, "text": "Tables\u00a0REF  to\u00a0REF  show the results of our additional ablation studies in the remaining datasets that are not reported in our main paper. In TablesREF , SA outperforms OA in all the datasets except for Texas and Wisconsin. In the case of Texas and Wisconsin, SA performs worse than OA from time to time. In Table\u00a0REF , we compare two types of \\(\\beta \\) . \\(\\beta \\)  can be either a scalar parameter (SC) or a learnable vector parameter (VC). In almost cases, it shows better performance when the type of \\(\\beta \\)  is VC.\r\n{TABLE}{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 78, "text": "In Figs.\u00a0REF  and\u00a0REF , we show the findings of our sensitivity studies in the remaining datasets, that are not disclosed in our main manuscript. GREAD-BS maintains performance even when \\(T\\)  is increased, but GREAD-Z tends to show low performance in Texas, Cornell, and Film.\r\n{FIGURE}{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 0, "text": "Academic research is always in development and never in isolation, as it has to build upon the existing knowledge to create new knowledge, which then becomes the foundation for future research. Citations in the end product of research, i.e. the research papers, are the embodiment of this connection between the current research and its predecessors. Without this connection, there will be no creation and accumulation of knowledge. Moreover, citations also put one's work into an academic context. On one hand, it bolsters readers' understanding of the current work by preparing readers with adequate information on a certain field. Further, it strikes a common ground between the author and the reader. On the other, citations can verify the credibility of the current work. Proper citations show that the author is well-informed of this particular field. Others' work can also be evidence to support the author's arguments. Therefore, the importance of appropriate citing can not be overestimated, but citing accurately and fully is a difficult task\u00a0[25].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 1, "text": "In the first place, with academic publications in overwhelming abundance, it is hard to decide which one is the right source to cite. The publishing industry has been digitized in general and academic publishing is no exception. It has become much more easier for knowledge to propagate when stored at a digital platform. As stated by the Dimensions database,https://app.dimensions.ai/discover/publication. the last decade, from 2011 to 2020, witnesses a continuous growing trend in academic publishing, with a 74.9% growth rate.Data was obtained in November of 2021. Apart from official publications, digital repositories for e-prints, like arXiv\u00a0[19], also register an upward movement in submissions, with 77.1% submissions of computer science on arXiv becoming peer-reviewed publications eventually\u00a0[35]. The rapid increase in the amount of published information creates enormous difficulties for researchers to manage data available to them, and thus causes the problem of information overload\u00a0[21], [49]. For any researcher, time and energy are limited resources that need to be managed efficiently. At the same time, keeping up to date with newly published research in a certain field is crucial as it prevents one's work from being repetitive or derivative. With the sheer number of online services and platforms available to researchers, it becomes unprecedentedly difficult to do a thorough literature search among the overloaded academic data. Worse still, academic information overload also causes inappropriate citation practices. Researchers can make mistakes in citing as they do not have enough time to learn more about the citing source, or they might just rely on secondary sources without reading the source papers.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 2, "text": "With new knowledge come new published scientific entities, such as PageRank\u00a0[43], MapReduce\u00a0[13] and Transformer\u00a0[56], which serve as symbols for research. These entities, newly coined by the researchers to represent their ideas and findings, are generally introduced in papers. Citing these entities consistently and accurately is important in academic writing, as it makes academic connections and maintains academic integrity. However, it is not uncommon for inappropriate citations of these entities to occur. Among the published scientific entities with inappropriate citation practices, software and frameworks are highly inclined to be used in academic papers without proper citations. Some software and frameworks are introduced officially in published papers, but many researchers only conveniently cite their web pages. TensorFlow is a typical example. With its first version released in 2015, TensorFlow has become a prestigious and widely used framework for deep learning. Some researchers only cite its official websitehttps://www.tensorflow.org/. or its GitHub pagehttps://github.com/tensorflow. when using TensorFlow in their papers. In fact, the TensorFlow team has published a paper \u201cTensorFlow: A System for Large-Scale Machine Learning\u201d\u00a0[0] to introduce this framework in 2016. With the publication of this paper, the proper citation of TensorFlow in academic papers shall be this paper rather than the web pages. Yet this paper has been ignored by some researchers, in part because they fail to catch up with its publication.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 3, "text": "Furthermore, some researchers are unaware of missing citations. Citations build up the edifice of knowledge by turning factual claims into what eventually being accepted as established knowledge over time. The existence of some published scientific entities has been regarded as usual by some researchers, after being cited over and over again in the publications. For these entities, it is more and more common that no citations are provided, which does not follow the academic norms.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 4, "text": "We can use BLEU\u00a0[45] and SciBERT\u00a0[3] in comparison as a distinct case in point. BLEU is a method for automatic evaluation of machine translation proposed in 2002. This method soon becomes and still is the most widely used metric. However, the more well-known it is, the less proper citations it has. In our research, we find that a large amount of recent published papers mentioning this method fail to attribute it to its source. It seems that the missing citation does not hinder readers' understanding, for BLEU is basic knowledge that researchers in a related domain should be familiar with. But citing the source paper of BLEU helps to increase the credibility of papers and it is a behavior to credit and honor the authors who proposed it at the very beginning. It also helps new researchers to develop a good grasp of the root of this mature concept. In comparison, the newly introduced entity SciBERT stands in striking contrast to BLEU. Proposed in 2019, SciBERT misses no citation among the papers selected for the experiments in Sect.\u00a0. As can be seen from here, time-honored research with important results is more likely to be used without proper citations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 5, "text": "In summary, citing properly of published scientific entities is no easy task and can create an extra burden to academic writing. To address this issue, we propose a method to map published scientific entities to their source papers. Our main contributions lie in:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 6, "text": "\r\nWe provide a detailed approach to construct a large-scale published scientific entity-papers mapping dataset;\r\n\r\nWe propose a local citation recommendation method Citation Recommendation for Published Scientific Entity (CRPSE) based on the dataset above. The method is proved to produce great performance in the task of citation recommendation for published scientific entities;\r\n\r\nWe conduct an extensive statistical analysis on published scientific entities with missing citations among papers published in prestigious computer science conferences in 2020 with the employment of the method above.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 7, "text": "Our research is closely related to the following two fields.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 8, "text": "First raised by [53], citation recommendation is \u201cthe problem of academic literature search by considering an unpublished manuscript as a query to a search system\u201d. It is later divided into global citation recommendation and local citation recommendation by [23]. [24] first introduce a method to localize in-text citations automatically and [29] later incorporate machine translation with this method for improvement. [30] propose a neural probability model for learning feature representation of distributed word embeddings, and predict the citation probability for papers on the grounds of semantic distance through a multi-layer neural network. [9] describe a citation recommendation algorithm CIRec based on citation tendency, which incorporates a weighted heterogeneous network, a biased random walk procedure and the skip-gram model\u00a0[40]. In recent years, with the development of deep learning, complex neural networks are used in local citation recommendation. [16] present Neural Citation Network (NCN) constructed on the encoder-decoder architecture. Convolutional neural networks (CNN)\u00a0[34], [66], long short-term memory (LSTM)\u00a0[26], [64] and graph convolutional networks (GCN)\u00a0[33], [31] are more examples of successful deep learning-based local citation recommendation. In these methods, deep learning is used to learn the text feature, which greatly enhances the understanding and feature representation of text.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 9, "text": "Many research efforts have been devoted to citation data analysis. Some focus on analyzing the characteristics of the papers cited and the influence of different citation behaviors on the cited papers. [28] study the repetitive mentioning of a reference in the citing paper. Self-citation is a popular topic in citation data analysis. It is found to have a positive impact on the cited paper\u00a0[17]. This is also proved in [1]'s work. They find in the analysis of five self-citation trends that papers without self-citation tend to have dampened research impact. [63] discover a moderate increase in citations of papers after the authors of which are awarded Nobel Prize. Attention is also paid to the research of dataset citation. [67] conduct a content analysis to study the use of datasets in different disciplines. They find that researchers tend to build their own datasets instead of reusing the existing datasets. Some studies of missing citations target for patents and articles. [42] propose a missing citation recommendation method for new patents utilizing the patent citation network. [11] design a bibliography-based recommendation method, which quantifies the similarity between articles and further to uncover missing references. This bibliography-based recommendation method is also found to facilitate the dissemination of scientific results.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 11, "text": "\r\nPaper\r\nA paper is defined as an academic publication in general. It is not restricted to journal or conference papers, but it can also refer to books, reports, patents and standards.\r\n\r\nPublished scientific entity\r\nA published scientific entity is defined as a scientific object that has a published paper deriving from it and the author of the paper is also the researcher that proposes this scientific object. The published scientific entities can be categorized into two types depending on how they are named. The first type is named by the original author, such as the neighbor-joining method\u00a0[50], ATRP\u00a0[61], ImageNet\u00a0[14] and AlphaGo\u00a0[52]. The second type is named later by other researchers, usually named after the original author. Examples include Schr\u00f6dinger equation\u00a0[51], Bradford's law\u00a0[4], Turing test\u00a0[55] and Witten-Bell smoothing\u00a0[62]. Published scientific entity is also referred to as published entity in this paper.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 12, "text": "\\(p\\)  is a paper requiring citation recommendation. \\(s_i\\)  denotes a sentence in \\(p\\) , \\(s_i \\in S=\\lbrace s_1,s_2,\\ldots ,s_I\\rbrace \\) , where \\(I\\)  is the total number of sentences in \\(p\\) . \\(e_j\\)  denotes a published entity in \\(p\\) , \\(e_j \\in E=\\lbrace e_1,e_2,\\ldots ,e_J\\rbrace  (J > 0)\\) , where \\(J\\)  is the total number of published entities in \\(p\\) . When \\(J = 0\\) , \\(E\\)  is \\(\\varnothing \\) . \\(d_j\\)  denotes the source paper that proposes \\(e_j\\) . The problem of CRPSE is defined as processing the sentence set \\(S\\)  in \\(p\\)  to build the entity set \\(E\\)  and finding an ordered set \\(C_j\\)  consisted of \\(K\\)  relevant papers for each entity \\(e_j\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 15, "text": "\\(K\\)  is a parameter that can be set as required. \\(c_{j_k}\\)  is one candidate source paper of \\(e_j\\)  and \\(r_{j_k}\\)  denotes the score of \\(c_{j_k}\\)  used in ordering, \\(k \\in \\lbrace 1,2,\\ldots ,K\\rbrace \\) . The objective of CRPSE is to let \\(d_j\\)  belong to \\(C_j\\)  and its corresponding ordering score be the highest. That is:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 18, "text": "The construction of our published scientific entity-papers mapping dataset is elucidated as follows and is illustrated in Fig.\u00a0REF .The examples in the figure are created for better illustration, not real examples from S2ORC. First, we take each paper with both full text and annotated resolved bibliographic references provided by S2ORC and segment its main text into sentences using scispaCy\u00a0[41]. ScispaCy is a powerful Python library developed from spaCyhttps://github.com/explosion/spaCy. for scientific text processing. It is at an industrial-strength level for tagging, parsing, entity extraction and other tasks in natural language processing. We use scispaCy for sentence segmentation and entity extraction in this paper. Second, for each general entity \\(e_{gen_j}\\)  (words in red color) detected by scispaCy in every segmented sentence, we take all papers with their in-text citations (words in blue color) cooccurring in the same sentence with \\(e_{gen_j}\\)  as the candidate source papers for \\(e_{gen_j}\\) . One candidate source paper is denoted by \\(c_{j_l}\\) , \\(c_{j_l} \\in C_j=\\lbrace c_{j_1},c_{j_2},\\ldots ,c_{j_L}\\rbrace  (L > 0)\\) , where \\(L\\)  is the total number of candidate source papers of \\(e_{gen_j}\\) . When \\(L = 0\\) , \\(C_j\\)  is \\(\\varnothing \\) . The cooccurrence count of \\(e_{gen_j}\\)  and \\(c_{j_l}\\)  is denoted by \\(n_{j_l}\\) . With each cooccurrence, the value of \\(n_{j_l}\\)  will be added 1.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 19, "text": "In the left part of Fig.\u00a0REF , there are two sample sentences with five entities, three in-text citations and two corresponding papers. After being processed following the steps described above, a dataset can be generated as shown in the right part of Fig.\u00a0REF .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 20, "text": "All papers with both full text and annotated resolved bibliographic references in S2ORC are processed following the steps above to construct a raw published scientific entity-papers mapping dataset. Some entities and their candidate source papers only share a small number of cooccurrences, which implies that they do not have a strong correlation. So an entity \\(e_{gen_j}\\)  without a candidate source paper with a cooccurrence count equal to or greater than 20 will be removed from the dataset. That is \\(n_{j_l} < 20, \\forall l \\in \\lbrace 1,2,\\ldots ,L\\rbrace \\) .\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 21, "text": "This raw dataset is full of outliers that need to be removed. ScispaCy detects all entities, including some non-published entities, like NAACL-HLT and NLP in Fig.\u00a0REF . The non-published entities are outliers should be cleared out. We train a binary classification model to separate the outliers from published entities in the raw dataset.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 22, "text": "The positive and negative samples for our classification model are created as follows with both positive and negative samples belonging to the published scientific entity-papers mapping dataset. The positive samples are outliers that should be filtered out. They are composed of high-frequency surnames in S2ORC and high-frequency words. We compile a list of all author surnames in S2ORC and select the top 10,000 high-frequency surnames as positive samples. For high-frequency words, we pick out the top 14,000 words from English Word Frequency datasethttps://www.kaggle.com/rtatman/english-word-frequency. with high-frequency surnames screened out. In total we have 24,000 positive samples. The negative samples are published entities that should be retained in the dataset. To locate such entities, we study a large quantity of academic papers' titles. Through a close study, we find a structural pattern shared by these titles. In most titles, if there is a colon used in the title with only one single uncommon word before the colon, then this word is likely to be a published entity that we are looking for to construct the negative set. Example titles include \u201cAllenNLP: A Deep Semantic Natural Language Processing Platform\u201d\u00a0[18] and \u201cVoxCeleb2: Deep Speaker Recognition\u201d \u00a0[10]. In these two examples, the words \u201cAllenNLP\u201d and \u201cVoxCeleb2\u201d before the colon are the published entities for the negative set. We process all the titles of S2ORC collected papers that follow this pattern and gather the word before the colon to construct the negative set. After those words were collected, we perform filtering to ensure they are not the author surname in S2ORC and are not in the English Word Frequency dataset. Additionally, we manually collect other published entity samples from Computing Research Repository (CoRR)\u00a0[22] manuscripts on arXiv submitted between May 2020 and July 2020. In the end, we have 12,000 negative samples.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 23, "text": "After constructing the two sample sets, we progress to selecting the feature for classification. We sort and convert all the cooccurrence counts of candidate source papers of an entity into a line graph. By looking at the line graphs, we find two different patterns in the distribution of the cooccurrence count. For outliers, the curves in the line graph decrease very slowly. For published entities, the curves present dramatic falls. See Fig.\u00a0REF .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 24, "text": "It is not hard to interpret the difference between the cooccurrence count of outliers and that of published entities. For outliers, the candidate source papers share a similar cooccurrence count, because these outliers, like high-frequency surnames and high-frequency words, have no specific papers that propose them. In contrast, for published entities, there will be one candidate source paper that has a significantly higher cooccurrence count. This is the source paper that introduces the published entity and most researchers cite this paper when mentioning the entity, hence the distinct contrast in cooccurrences between the source paper and the rest. On the basis of the analysis above, the cooccurrence counts are used as the classification feature.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 25, "text": "We divide the overall sample set randomly into a training set, a validation set and a test set at the ratio of 8:1:1. We test the performance of different classifiers and random forests\u00a0[5] is chosen accordingly for this task. It is an ensemble classifier that combines a number of decision trees and the result of final classification is determined by voting of each sub-tree. This classifier has good performance while controlling over-fitting. The F1-score of the trained model on the test set is 0.833. We use the trained model to filter out the outliers. After removing the outliers, we have hundreds of thousands pieces of published entities with candidate source papers and their cooccurrence counts in the published scientific entity-papers mapping dataset.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 26, "text": "For each published entity, there are numerous candidate source papers. Those papers need to be sorted by certain criteria to produce the final result of recommendation. For this purpose, two sorting criteria are designed.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 27, "text": "\r\nCooccurrence count-based sorting criterion\r\nThis is a simple and straightforward method. Each candidate source paper of an entity is given a cooccurrence count (see Fig.\u00a0REF ). This cooccurrence count indicates how many times a paper is presented as an in-text citation in the same sentence with this entity in S2ORC. It is a strong quantitative index of the correlation between an entity and a paper. The higher the cooccurrence count, the stronger the correlation between the entity and the candidate source paper, i.e. higher possibility for this paper to be the entity's source paper. On the basis of this positive correlation, a cooccurrence count-based score is computed to be the first sorting criterion. This scoring makes full use of the citation behavior of researchers, looking into what is most-cited when the researchers mention this entity. The formulation of the cooccurrence count score of the \\(l\\) -th paper \\(r_{count_{l}}\\)  in the mapped candidate source paper set of a published entity \\(e\\)  is as follows:\r\n\\(r_{count_{l}} = w_{count_{l}} = \\frac{n_l}{SC}\\) \r\n\\(SC = \\sum _{l=1}^{L} n_l\\) \r\nwhere \\(n_l\\)  is the cooccurrence count of a candidate source paper \\(c_l\\)  for \\(e\\)  and \\(L\\)  is the total number of candidate source papers of \\(e\\) . \\(SC\\)  is the sum of cooccurrence counts that \\(e\\)  has with all the candidate source papers.\r\n\r\nWeighted context embedding-based sorting criterion\r\nThe first method makes use of the positive correlation of cooccurrence between an entity and its candidate source papers, but it does not take into consideration what the entity is and in which context it is mentioned. It can sort out the candidate source papers in order correctly for the majority of published entities, but not for polysemous entities, i.e. entities that have a name with more than one meaning. SAFD is one example. It can mean Stirring As Foam Disruption\u00a0[27], Statistical Adaptive Fourier Decomposition\u00a0[54] or Single Shot Anchor Free Face Detector\u00a0[60]. It relies largely on the context in which the entity is used to decide its contextual meaning and recommend its source paper. To address the problem of polysemy, we use context embedding to find out the contextual correlation between an entity and its candidate source papers. The computation is shown as follows:\r\n\\(w_{context_{l}} = CosSim(SciBERT(s),SciBERT(t_l))\\) \r\nwhere \\(SciBERT\\)  denotes the method of generating embedding using SciBERT\u00a0[3]. \\(s\\)  is the sentence in which the published entity \\(e\\)  is located. \\(t_l\\)  is the text that concatenates the title and abstract of the \\(l\\) -th candidate source paper of \\(e\\)  in its candidate source paper set. \\(CosSim\\)  denotes the calculation of cosine similarity. Then the weighted context embedding score of the \\(l\\) -th candidate source paper \\(r_{mix_{l}}\\)  can be calculated as the formulation below:\r\n\\(r_{mix_{l}} = \\lambda w_{count_{l}}+(1-\\lambda )w_{context_{l}}\\) \r\nTo improve the performance of this criterion, the hyperparameter \\(\\lambda \\)  is applied.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 28, "text": "The pipeline of CRPSE is as follows. We use scispaCy to detect entities in a paper. For each entity detected, we determine whether this entity is a published entity based off checking whether it is collected in the published scientific entity-papers mapping dataset. If yes, we sort its candidate source papers in order by one of the two sorting criteria above and select the top \\(K\\)  papers as the final recommended source papers. \\(K\\)  is usually set as 1, 5 or 10.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 30, "text": "In this section, we conduct an evaluation on the performance of CRPSE.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 31, "text": "To the best of our knowledge, there is no dataset specialized in evaluating the performance of local citation recommendation for published entities. So we manually construct a dataset for this task. We first download 101 manuscripts covering all categories under CoRR on arXiv submitted in October 2020. Then we screen out all general entities and the sentences they are in from the main text of those manuscripts. Published entities are labeled and their source papers are collected to build the final evaluation dataset.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 36, "text": "The experiments above prove the great performance of CRPSE in mapping published entities with their source papers. We further conduct a statistical analysis on missing citations of published entities among computer science conference papers with this method applied to detect published entities without proper citations. Open access papers published in 14 prestigious computer science conferences in 2020 are collected for the analysis. Even though it is generally considered that conference papers tend to be less rigorous than journal papers, we still use these papers for analysis because of the following reasons: 1) conferences, especially top conferences in computer science attract more attention than journals\u00a0[59]; 2) the conference papers we selected are published under open access license with more friendly accessibility.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 37, "text": "These conferences (sorted by alphabetical order of their abbreviations) are the AAAI Conference on Artificial Intelligence (AAAI), the Annual Meeting of the Association for Computational Linguistics (ACL), the International Conference on Computational Linguistics (COLING), the Annual Conference on Learning Theory (COLT), the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), the European Conference on Computer Vision (ECCV), the Conference on Empirical Methods in Natural Language Processing (EMNLP), the International Conference on Learning Representations (ICLR), the International Conference on Machine Learning (ICML), the International Joint Conference on Artificial Intelligence (IJCAI), the Conference of the International Speech Communication Association (INTERSPEECH), the Annual Conference on Neural Information Processing Systems (NeurIPS), the Robotics: Science and Systems (RSS) conference and the USENIX Security Symposium (USS).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 38, "text": "Missing citation of published scientific entity is defined as inappropriate citing practice of mentioning a published entity in a paper without giving its source paper in the references. Citing a website or other source instead of the source paper that proposes this entity is still deemed as missing citation.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 39, "text": "For a given paper, the collection of all its reference papers is \\(REF\\) . A published entity in the given paper is denoted by \\(e_{j}\\) . The published scientific entity-papers mapping dataset constructed in Sect.\u00a0 is denoted by \\(DS\\) . \\(rec_{j_1}\\)  denotes the top 1 candidate source paper of \\(e_{j}\\)  with the highest weighted context embedding score recommended through CRPSE. The function of detecting missing citations (DMC) of \\(e_j\\)  is defined as follows:\r\n\\(DMC(e_j)=\\left\\lbrace \\begin{array}{rcl}& \\lbrace rec_{j_1}\\rbrace  & \\qquad {e_j \\in DS \\hspace{5.0pt}and \\hspace{5.0pt}rec_{j_1} \\notin REF} \\\\& \\varnothing & \\qquad {e_j \\notin DS} \\\\& \\varnothing & \\qquad {e_j \\in DS \\hspace{5.0pt}and \\hspace{5.0pt}rec_{j_1} \\in REF} \\\\\\end{array} \\right.\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 40, "text": "First, we download all PDF files of regular papers published in the conferences mentioned above and convert them into XML files using GROBID\u00a0[37]. Second, we use scispaCy to segment their main text and detect entities. Third, for those entities, we get the results of CRPSE. Fourth, we extract references from the XML files. Fifth, the recommended papers are checked whether they are included in the references. If the recommended paper of a published entity is not in the references, the entity is considered as a potential entity with missing citation. These potential entities are double checked and confirmed if their recommended papers are not given in the references provided by Semantic Scholar APIhttps://api.semanticscholar.org/. to prevent the possible parsing error of GROBID.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 41, "text": "The authenticity of the recommended papers needs to be confirmed to ensure the statistical analysis is accurate and valid. Taking into consideration our academic backgrounds, we only check and conduct the analysis on recommended papers from computer science and one of its most related domains, mathematics in our research.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 42, "text": "Due to various writing requirements from different researchers, the same entity might have different forms of expression. For example, Adam algorithm, Adam method, Adam optimiser, Adam optimizer and Adam update rule all refer to the same published entity Adam\u00a0[32]. To have a more accurate analysis result, different expressions of the same entity need to be merged into one. The entities with the same recommended paper are merged into a unified one by their longest common subsequences. Few cases are merged manually as they are hard to be merged with this method.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 43, "text": "From these 14 computer science conferences, 12,278Papers with parsing errors are excluded. conference papers are collected. In these papers, 475 published entities in computer science and mathematics are found to have missing citations. This figure reveals that missing citations is not uncommon even in papers published in top computer science conferences.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 44, "text": "In our analysis of the data, inaccurate citations of some published entities are found. For example, for the published entity DeepLabv3+ in one paper, \u201cRethinking Atrous Convolution for Semantic Image Segmentation\u201d\u00a0[7] is given as the reference, whereas the authentic source paper is \u201cEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\u201d\u00a0[8]. The erroneously cited paper is actually the source paper of DeepLabv3, the previous version of DeepLabv3+. Another similar case in point is the published entity VQAv2. In one paper, its reference is given as \u201cVQA: Visual Question Answering\u201d\u00a0[2], but in fact, this entity is proposed in \u201cMaking the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\u201d\u00a0[20]. In addition to wrong references of completely different papers, some references are correct but the titles of the referred papers are mistakenly written. For instance, the source paper of the published entity EPIC-KITCHENS, \u201cScaling Egocentric Vision: The EPIC-KITCHENS Dataset\u201d\u00a0[12], is wrongly given as \u201cScaling egocentric vision: the dataset\u201d. Against such inaccurate citations, we make a strong call for enhanced awareness and stricter prevention to maintain the academic and scientific rigor.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 45, "text": "Apart from inaccurate citations, we also find that many published entities are without citations. A large portion of them are established entities commonly accepted by computer science researchers, or at least within specific subfield communities. To further understand the situation of missing citations, we conduct a statistical analysis to figure out what types of these published entities are and how long it has been since they are proposed in their source papers.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 46, "text": "First, we manually classify these published entities with missing citations into different types. The results are presented in Fig.\u00a0REF .\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 48, "text": "Then, we study how time makes a difference in the practice of missing citations for published entities. The source papers of the entities with missing citations are sorted in the light of how many years have passed since they are published. The results are presented in Table\u00a0REF  with a histogram shown in Fig.\u00a0REF .\r\n{TABLE}{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 49, "text": "Among the papers collected, the earliest published one is \u201cOn Colouring the Nodes of a Network\u201d\u00a0[6], in which the published entity Brooks' theorem is proposed. Contrary to how we estimated before, some newly-proposed entities like SpecAugment also suffer from missing citations. The source paper of SpecAugment, \u201cSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\u201d\u00a0[46], was published in 2019, just a year before the baseline year 2020 in our study. As can be seen in Fig.\u00a0REF , a skewed distribution is registered with the distribution's peak off the center to the left. It means that not a few recent published entities suffer from missing citations. One explanation for this is that recent published entities are used more by researchers, whereas those entities published way before gradually lose their relevance. For the resistance of outliers, we take the median as a measure. The missing citations are among papers that have been published for 8 years. This time frame can be seen as the time needed for research results in computer science to grow into an existence of accepted knowledge. In these 8 years, these entities are first proposed, tested, then promoted while keeping on inspiring later researchers. They are used again and again, or they might even become textbook examples, to the point that some researchers are way too familiar with these entities to use them with citations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 51, "text": "We would like to acknowledge the support of Yingmin Wang for improving the mathematical expressions. We are grateful to Li Lei, Xun Zhou, Lei Lin and Meizhen Zheng for their help in the data processing. We also appreciate two anonymous reviewers for their valuable comments. Special and heartfelt gratitude goes to the first author's wife Fenmei Zhou, for her understanding and love. Her unwavering support and continuous encouragement enable this research to be possible.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 52, "text": "This work is partly funded by the 13th Five-Year Plan project Artificial Intelligence and Language of State Language Commission of China (Grant No. WT135-38).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 53, "text": "Conflict of interest The authors declare that there is no conflict of interest regarding the publication of this paper.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 0, "text": "Named entities are phrases that contain the names of persons, organizations, locations, others. For example, the sentence, T.C.S. CEO Rajesh Gopinathan heads a meeting in their Banglore office, has the following named entities: [ORG T.C.S.], [PER Rajesh Gopinathan], and [LOC Banglore]. ORG, PER, and LOC represent the organization, person, and location, respectively. In this paper, we focus on these three named entities.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 1, "text": "NER is an important task in information extraction systems and very useful in many applications. It has many progress\u00a0[0], [1], [2] and applications such as in optimizing search engine algorithms\u00a0[3], classifying content for news providers\u00a0[4] and recommending content\u00a0[5]. However, despite NER from speech has many applications such as the privacy concerns in medical recordings (e.g., to mute or hide specific words such as patient names)\u00a0[6], it has very limited literature.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 2, "text": "NER from English speech is done by a classical two-step approach\u00a0[6]. It consists of first processing the given audio on an ASR system and then feeding the transcribed ASR output to the NER tagger\u00a0[7], [8] (see Figure\u00a0REF ). Such approaches have several disadvantages, such as existing NER systems are not robust to the noisy output of the ASR, since they are usually designed to process written language. Furthermore, usually, no information corresponding to named entities are used in the ASR system. However, such information could be used to choose better partial hypotheses which are dropped away during the decoding step. As a consequence, even when the decoding goes beyond the 1-best ASR hypothesis for better robustness to ASR errors\u00a0[9], the search space is pruned without taking into account knowledge of the partial named entities. In all these cases the NER component is trained independently. Thus, the error does not propagate from one step to another in an E2E fashion.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 4, "text": "The paper is structured as follows. Section\u00a0 discusses the related work. Section\u00a0 introduces the dataset and Section\u00a0 describes our methodology. We present our experiments in Section\u00a0. Section\u00a0 discusses how to deal with the OOV words, and Section\u00a0 presents the conclusion and future work.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 5, "text": "In the literature many studies\u00a0[11], [2], [12] focused on NER from textual documents. State-of-the-art (SOTA) NER systems leverage advances in deep learning and recent approaches that take advantage from both word and/or character-level embedding\u00a0[13], [14], [15]. However, NER from speech is a less studied problem in the research community. Until very recently, the majority of work in recognizing named entities from speech is done using the two-step approach, including the audio de-identification task\u00a0[6], and the work leveraging OOV information to increase the robustness of NER tagger\u00a0[16].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 6, "text": "Named entity recognition from speech using an E2E approach has a very limited literature, and moreover there is no work on English speech. Recently, Ghannay et al.\u00a0[10] presented an E2E NER on the French datasets. In their work, they used special symbols to achieve NER tagging capabilities in their E2E approach. The special symbols used by them are: \u201c[\u201d, \u201c(\u201d, \u201c{\u201d,\u201c$\u201d, \u201c&\u201d, \u201c%\u201d, \u201c#\u201d, \u201c)\u201d and \u201c]\u201d (the first eight symbols to denote the start of 8 different named entities and the last common symbol to denote an end to all of them). Similar to their work, we use three special symbols ('{','\\(|\\) ', '$') to recognize three most frequent named entities (names of organization, person, and location) from our English speech dataset. In this paper, we compare the E2E and the two-step approach for the English speech. Furthermore, we study the effect of a LM on the E2E NER task.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 8, "text": "Thus, the dataset is prepared into two steps: (i) applying a NER tagger on DATA1 (600,000 files) and (ii) manually annotating the 70,769 files having valid NERs using Doccanohttps://github.com/doccano\u00a0[21].\r\nIn step 1, we re-train the Flair tagger on the capitalized NER benchmark CoNLL-2003\u00a0[22] dataset from scratch. The transcripts at the test time are capitalized, and so is the training data. Furthermore, after an empirical analysis, it was found that a threshold probability of 0.95 rejects the majority of noisy/erroneous named entities from the tagger output. After the Flair NER tagger operation on DATA1, the total number of files is reduced to 70,769 (approx. 150 hrs) from 600,000 (approx. 1,000 hrs). We refer to this reduced data as DATA2. Since DATA1 has audios of different speakers recording the same sentence, DATA2 also has repetitions. To be precise, DATA2 has a total of 31,000 unique sentences, and the rest 39,769 are a repetition of 1238 sentences from 31,000 unique sentences.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 3, "text": "To the best of our knowledge, the only work related to E2E NER from speech is done on French datasets\u00a0[10]. This paper is highly motivated by this work. Additionally, we study the effect of a LM on the E2E NER task. Our major contributions are as follows: (i) we introduce a first publicly available NER annotated datasetwe will release the dataset for research purpose soon. for English speech, (ii) we present a state-of-the-art approach for an E2E named entities recognition on the curated dataset, and (iii) we discuss how an E2E system can be used to handle out of vocabulary words in an ASR system.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 7, "text": "The annotated English dataset we prepare for the NER from speech task is a subset of a combination of Librispeech\u00a0[17], CommonVoice\u00a0[18], Tedlium\u00a0[19] and Voxforge\u00a0[20]. The recordings are comprised mainly of two domains: Reading English and Ted talks. We refer to this combined data as DATA1, which has around 600,000 files (approximately 1,000 hrs). After an empirical analysis, we found that the majority of files did not have any named entities. To remove these files from the manual annotation step, we use Flair\u00a0[8], [7] as a NER tagger with 0.9 F1 score. In this way, we have reduced the number of files having NER to 70,769, which are used for manual annotation.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 9, "text": "In step 2, We manually annotate all the remaining 70,769 files in DATA2 following CoNLL-2003\u00a0[22] guidelines. An example of the manually annotated sentence is shown in Figure\u00a0REF , i.e., character sequence with and without the named entities. To increase the robustness of the model, similar to\u00a0[23], we asked the annotator to randomly mislabel some tokens as named entities (e.g., annotating the CEO token as [PER] or Banglore token as [ORG]).\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 10, "text": "The named entities distribution in DATA2 are shown in Table\u00a0REF , and DATA2 has a total of 38891 unique named entity tokens, as shown in the last row of the Table\u00a0REF . Furthermore, DATA2 is comprised of 34% Librispeech, 36% CommonVoice, 7% Tedlium, and 23% Voxforge of DATA1.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 12, "text": "A language model (LM) is a probability distribution over an arbitrary symbol sequences P(w1, ..., wn) such that more likely sequences are assigned higher probabilities and vice versa. LMs are frequently used at the decoding step to condition beam search. During the decoding, the top-n candidates are evaluated by conditioning the output of acoustic model with the language model. In this study, 4-gram LM is used and is trained on the full dataset i.e., the combined train, dev, and test using the KENLM library\u00a0[24].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 13, "text": "The problem of NER from speech is to assign a special symbol before and after a named entity to identify it. We approach this problem as a sequence labeling task and use an RNN based Baidu's DeepSpeech2 (DS2)\u00a0[25] neural architecture to study NER from speech with modifications in the last layer. DS2 is a combination of Convolution Neural Network (CNN) and Recurrent Neural Network (RNN) layers, with a fully connected followed by a softmax layer. The softmax layer outputs the probabilities of the sequence of characters.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 14, "text": "Let \\(X = \\lbrace x_1, x_2, ..., x_n\\rbrace \\)  be the input utterances and \\(Y = \\lbrace y_1, y_2, ..., y_n\\rbrace \\)  be the corresponding transcripts. For a given input audio \\(x_1\\) , it is transformed into a sequence of \"log-spectrograms of power normalized audio clips, calculated on a 20ms window\", and is then fed to the model, which captures the sequential nature of speech. The output \\(l\\)  is a sequence of defined set of characters ('A-Z' + ' '). Similar to the work\u00a0[10], we add special symbols ('\\(|\\) ', '$', '{' denote the start of named entities person, location and organization, respectively, and '\\(]\\) ' denotes the end of any three named entities) to the pre-defined set of characters to introduce the named entity tagging capabilities in the architecture. An example of a character sequence with and without the special symbols is shown in the Figure\u00a0REF . To recognize named entities, we modify the DS2 architecture by increasing the shape of the fully connected layer by four to accommodate the extra symbols in the output layer (see Figure\u00a0REF ).\r\n{FIGURE}{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 15, "text": "We train the system using the Connectionist temporal classification\u00a0[26] (CTC) loss (see Figure\u00a0REF ) because CTC loss takes into account all the possible character sequences given the output and the true transcript. Thus, CTC loss maximizes the total probability of all the paths which lead to the true transcript. The model predicts \\(p(l\\textsubscript {t}/x_1)\\)  at each time step.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 17, "text": "Where \\(wc(y)\\)  is the word count in the predicted transcript. \\(\\alpha \\)  and \\(\\beta \\)  control the contribution of LM and the number of words in the predicted transcript. In this study, we use values 1.96 & 6.0 for \\(\\alpha \\)  and \\(\\beta \\) , respectively.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 16, "text": "At the test time, the output is conditioned on an N-gram LM using prefix beam search decoding\u00a0[27], is shown as follows.\r\n\\(Q(y) = log(p(l\\textsubscript {t}/x)) + \\alpha *log(pLM(y)) + \\beta *wc(y) \\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 19, "text": "In this section we discuss the experimental setup of E2E and two-step approaches for NER from speech, the method used for evaluating the two systems and the corresponding results.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 20, "text": "All the experiments were carried out on DATA2 prepared in Section\u00a0. Both the dev and test set are created from the 31,000 unique files with a 10% distribution each and the remaining files in DATA2 are used for training.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 11, "text": "This section provides information on the E2E approach used in this study, the training method used to train it and the different components used in the decoding step at the test time.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 18, "text": "To compensate for the limited data for the NER from speech task, DS2 weights are used as a starting point to train the E2E model. We train a standard DS2 architecture on the DATA1 as a baseline. This system achieves a word error rate (WER) of 2.72% on the test set with a 4-gram LM and beam-width equal to 1024. In a similar work on the French dataset\u00a0[10], they achieved a WER of 19.96%. Better WER is the major reason that our E2E NER system achieves better results compared to\u00a0[10].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 21, "text": "We experimented with two different approaches for the NER from speech task: E2E and a classical two-step approach. For the E2E approach, experiments were carried out on the model explained in Section\u00a0REF . At the test time, Prefix beam search decoding with a beam-width of 1024 and a 4-gram LM (trained on DATA2) is used. In the case of classical two-step approach, we use Baidu's DS2 as the ASR component and Flair as a NER tagger component. In the two-step approach, audio is first transcribed using the ASR component, and then the output is passed to the Flair tagger as shown in the Figure\u00a0REF . The Flair tagger used in the two-step approach is explained in the Section\u00a0REF .\r\n", "annotation": {"entities": {"c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#a856e203-ad84-43f6-aca6-8ce20b6e8217", "surface_form": "At the test time", "start": 206, "end": 222}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#d3efabe8-c258-45c1-a01c-2e6fd4fb6772", "surface_form": "Prefix beam search decoding", "start": 224, "end": 251}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#9c820a74-1f17-4bca-80f7-ddd496ea6b40", "surface_form": "beam-width", "start": 259, "end": 269}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#dca65b6e-442e-41a1-9605-dda7416b31e6", "surface_form": "1024", "start": 273, "end": 277}]}}, "relations": {"r2": {"id": "r2", "source": "c1", "target": "v2", "evidences": [{"id": "#86b3da78-d804-45cd-a590-566afa351148", "source_surface_form": "#a856e203-ad84-43f6-aca6-8ce20b6e8217", "target_surface_form": "#dca65b6e-442e-41a1-9605-dda7416b31e6", "evidence_sentence": "At the test time, Prefix beam search decoding with a beam-width of 1024 and a 4-gram LM (trained on DATA2) is used.", "start": 206, "end": 321}]}, "r0": {"id": "r0", "source": "v2", "target": "p3", "evidences": [{"id": "#ad2f7f76-9218-479e-8ec6-1180570838aa", "source_surface_form": "#dca65b6e-442e-41a1-9605-dda7416b31e6", "target_surface_form": "#9c820a74-1f17-4bca-80f7-ddd496ea6b40", "evidence_sentence": "At the test time, Prefix beam search decoding with a beam-width of 1024 and a 4-gram LM (trained on DATA2) is used.", "start": 206, "end": 321}]}, "r1": {"id": "r1", "source": "p3", "target": "a14", "evidences": [{"id": "#da3152e3-36db-4224-9656-6fd5ec9db74d", "source_surface_form": "#9c820a74-1f17-4bca-80f7-ddd496ea6b40", "target_surface_form": "#d3efabe8-c258-45c1-a01c-2e6fd4fb6772", "evidence_sentence": "At the test time, Prefix beam search decoding with a beam-width of 1024 and a 4-gram LM (trained on DATA2) is used.", "start": 206, "end": 321}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 22, "text": "NER, also known as entity identification, and entity extraction, seeks to locate and classify named entities in text into some pre-defined labels. For this study, we use Flair as the NER tagger in the two-step approach. The Flair tagger is trained on a combined dataset of DATA2 and CoNLL-2003\u00a0[22] and the test set is same as in the case of the E2E approach. The results for the Flair NER tagger on the test set are shown in Table\u00a0REF .\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 23, "text": "Similar to the work\u00a0[10], we use F1 score\u00a0[28] for evaluation, which is defined as follows.\r\n\\(F_{1}=\\frac{2\\times \\text{Precision}\\times \\text{Recall}}{ \\text{Precision} + \\text{Recall}}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 24, "text": "Precision is the percentage of named entities that are correct in the predictions compared to true labels and recall is the percentage of ground truth named entities found by the system. A named entity is correct only if it is an exact match to the ground truth entity. We emphasize on using micro average since there is a class imbalance as shown in Table\u00a0REF .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 25, "text": "To consider the case of repeated tags and half-labeled predictions, we slightly modify the precision and recall calculation. For example, if a sentence has two identical named entities, then we collapse them and treat them as one. Secondly, we discard any tags which are half-labeled (e.g., in {T.C.S.] CEO \\(|\\) Rajesh Gopinathan heads a meeting in their $Banglore] office. There is no end label to the PER tag.). Apart from these two special cases, we followed all the standard guidelines for calculating precision and recall.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 26, "text": "In this section, we report results on the DATA2 dataset. In all the experiments, we use batch normalization and weight decay as regularization. The evaluation metrics we report are precision, recall, and F1 score on the NER task. The reader should have in mind that the half-labeled tags are discarded for all the calculations, as mentioned in the Section\u00a0REF .\r\nPre-trained models and training configurations are available on GitHublink to the GitHub repository..\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 27, "text": "Table\u00a0REF  shows our experimental results on the E2E and two-step approach. The scores are in order of person, location, and organization from better to worse because of the class imbalance present in the DATA2, as shown in Table\u00a0REF . Furthermore, our results prove that the E2E approach outperforms the classical two-step approach in all the cases and by a significant margin. Moreover, the recall for both the approaches is lower compared to the precision. we think that it is because of the smaller size of dataset for the task. The similar trend of precision and recall is observed in the work on French datasets\u00a0[10].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 28, "text": "We also studied the effect of language model on the F1 scores. LM improves the NER scores with a significant margin, and the same can be inferred from Table\u00a0REF , which shows an improvement of almost by a factor of 300% with LM. Therefore, based on the quantitative analysis, we conclude that the NER results are closely dependent on the language model, and if an LM is trained on a bigger corpus, then the recall could be increased further.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 29, "text": "Lastly, if we look at the Table\u00a0REF , the F1 scores for detecting a named entity is less compared to the E2E approach as shown in Table\u00a0REF . Therefore, even if we can get the perfect transcriptions from the ASR component, the best we could do is still less than the E2E approach. The reason could be, in the E2E approach the output and the LM (trained with the named entity tags) are conditioned together in the decoding step. Thus, two sources of information are taken into consideration in the E2E approach. Further analysis is required to make any concrete comments.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 30, "text": "In this section, we discuss how the information on named entities can be used to handle OOV words in an ASR system.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 31, "text": "Let us start with some statistics concerning OOV words. We trained the LM on the training data only and found out that the percentage of OOV words in development and test set is around 20% each. Out of those 20% OOV words, around 40% are named entities. If an LM is trained on a significantly bigger corpus, than the share of named entities in the OOV words will increase. Thus most of the OOV words will be the named entities. From this point on, the discussion will be based on just one named entity i.e., person.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 32, "text": "So far, from the E2E model, we have the probability of a token being a named entity, and only if we could have the same information from the LM, we can condition it using the modified prefix beam search decoding. This can be achieved, if we replace all the individual person tokens with a \\(<\\) person\\(>\\)  token as shown in Figure\u00a0REF  and train an LM (we call it semantic LM or S-LM) on the updated corpus again. The S-LM would learn the probability of the next token being a \\(<\\) PER\\(>\\)  instead of the exact names. Therefore, at the test time, we now have the named entity tag information from the E2E model and the S-LM. At the test time, now we can condition the E2E model output with the S-LM to rank the top n-paths using the modified prefix beam search. The modified \"prefix beam search\" decoding is such that, it assigns a higher probability to the output of E2E model if the same can be inferred from the S-LM or otherwise. Furthermore, while scoring the top beams, we can condition the named entities on actual person names from a pre-defined dictionary of names.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 33, "text": "Since we do not have a concept of an individual word now but a \\(<\\) PER\\(>\\)  token. Therefore, the problem of named entities being an OOV word is not anymore. Thus we can now condition the E2E model output heavily on the LM, worrying less about penalizing the OOV words.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 34, "text": "In this paper, we made available a first public English speech dataset with named entities. We also presented a detailed comparison between the E2E and the two-step approaches for NER from speech. Experimental results show that the E2E approach provides better results (F1=0.906) compared to the two-step approach (F1=0.803). Additionally, a LM plays an important role to achieve these numbers. It is the first study to recognize named entities in English speech using an E2E approach. To conclude, this study presents promising results in a first attempt to experiment with the E2E approach to recognize named entities and constitutes an interesting start point for future work. In the future, we can study the effect of other loss metrics including: NE-WER (Named Entity Word Error Rate)\u00a0[29] and ATENE (Automatic Transcription Evaluation for Named Entity)\u00a0[30], instead of just using WER. Additionally, we can further work on our discussions on handling OOV words in an ASR system.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 0, "text": "As scientific communities grow and evolve, new tasks, methods, and datasets are introduced and different methods are compared with each other.\r\nDespite advances in search engines, it is still hard to identify new technologies and their relationships with what existed before.\r\nTo help researchers more quickly identify opportunities for new combinations of tasks, methods and data,\r\nit is important to design intelligent algorithms that can extract and organize scientific information from a large collection of documents.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 3, "text": "To explore this problem, we create a dataset SciERC for scientific information extraction, which includes annotations of scientific terms, relation categories and co-reference links.\r\nOur experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction [28], [7].\r\nIn addition, we\r\nbuild a scientific knowledge graph integrating terms and relations extracted from each article.\r\nHuman evaluation shows that propagating coreference can significantly improve the quality of the automatic constructed knowledge graph.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 5, "text": "There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations\u00a0[6], [5], [20], [13], [37], [11], [19], [0], analyzing research community\u00a0[42], [4], and unsupervised methods for extracting scientific entities and relations\u00a0[16], [41], [14].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 10, "text": "We define six types for annotating scientific entities (Task, Method, Metric, Material, Other-ScientificTerm and Generic) and seven relation types (Compare, Part-of, Conjunction, Evaluate-for, Feature-of, Used-for, Hyponym-Of). Directionality is taken into account except for the two symmetric relation types (Conjunction and Compare).\r\nCoreference links are annotated between identical scientific entities.\r\nA Generic entity is annotated only when the entity is involved in a relation or is coreferred with another entity.\r\nAnnotation guidelines can be found in Appendix\u00a0.\r\nFigure\u00a0REF  shows an annotated example.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 11, "text": "Following annotation guidelines from\u00a0qasemizadeh2016acl and using the BRAT interface [39], our annotators perform a greedy annotation for spans and always prefer the longer span whenever ambiguity occurs. Nested spans are allowed when a subspan has a relation/coreference link with another term outside the span.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 12, "text": "One domain expert annotated all the documents in the dataset; 12% of the data is dually annotated by 4 other domain experts to evaluate the user agreements. The kappa score for annotating entities is 76.9%, relation extraction is 67.8% and coreference is 63.8%.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 13, "text": "SciERC is focused on annotating cross-sentence relations and has more relation coverage than SemEval 17 and SemEval 18, as shown in Table REF .\r\nSemEval 17 is mostly designed for entity recognition and only covers two relation types. The task in SemEval 18 is to classify a relation between a pair of entities given entity boundaries,\r\nbut only intra-sentence relations are annotated and each entity only appears in one relation, resulting in sparser relation coverage than our dataset (3.2 vs. 9.4 relations per abstract).\r\nSciERC extends these datasets by adding more relation types and coreference clusters, which allows representing cross-sentence relations, and removing annotation constraints. Table\u00a0REF  gives a comparison of statistics among the three datasets.\r\nIn addition, SciERC aims at including broader coverage of general AI communities.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 15, "text": "The input is a document represented as a sequence of words \\(D=\\lbrace w_1, \\ldots , w_n\\rbrace \\) , from which we derive\r\n\\(S=\\lbrace s_1,\\ldots , s_N\\rbrace \\) , the set of all possible within-sentence word sequence spans (up to a reasonable length) in the document.\r\nThe output contains three structures:\r\nthe entity types \\(E\\)  for all spans \\(S\\) ,\r\nthe relations \\(R\\)  for all pair of spans \\(S\\times S\\) , and the coreference links \\(C\\)  for all spans in \\(S\\) .\r\nThe output structures are represented with a set of discrete random variables indexed by spans or pairs of spans.\r\nSpecifically, the output structures are defined as follows.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 19, "text": "We formulate the multi-task learning setup as learning the conditional probability distribution \\(P(E,R,C|D)\\) . For efficient training and inference, we decompose \\(P(E,R,C|D)\\)  assuming spans are conditionally independent given \\(D\\) :\r\n\\(\\hspace{-85.35826pt}&P(E, R, C \\mid D) = P(E, R, C, S \\mid D) \\\\&=\\prod _{i=1}^{N}P(e_i \\mid D) P(c_i \\mid D) \\nonumber \\prod _{j=1}^{N} P(r_{ij} \\mid D), \\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 21, "text": "\r\nwhere \\(\\Phi _{\\text{E}}\\)  denotes the unnormalized model score for an entity type \\(e\\)  and a span \\(s_i\\) , \\(\\Phi _{\\text{R}}\\)  denotes the score for a relation type \\(r\\)  and span pairs \\(s_i, s_j\\) , and \\(\\Phi _{\\text{C}}\\)  denotes the score for a binary coreference link between \\(s_i\\)  and \\(s_j\\) .\r\nThese \\(\\Phi \\)  scores are further decomposed into span and pairwise span scores computed from feed-forward networks, as will be explained in Section REF .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 22, "text": "For simplicity, we omit \\(D\\)  from the \\(\\Phi \\)  functions and \\(S\\)  from the observation.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 23, "text": "Given a set of all documents \\(\\mathcal {D}\\) , the model loss function is defined as a weighted sum of the negative log-likelihood loss of all three tasks:\r\n\\(& -\\sum _{(D, R^*, E^*, C^*) \\in \\mathcal {D}} \\Big \\lbrace \\lambda _{\\text{E}}\\log P (E^* \\mid D) \\\\& + \\lambda _{\\text{R}}\\log P (R^* \\mid D) + \\lambda _{\\text{C}}\\log P (C^* \\mid D) \\Big \\rbrace \\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 24, "text": "\r\nwhere \\(E^*\\) , \\(R^*\\) , and \\(C^*\\)  are gold structures of the entity types, relations, and coreference, respectively. The task weights \\(\\lambda _{\\text{E}}\\) , \\(\\lambda _{\\text{R}}\\) , and \\(\\lambda _{\\text{C}}\\)  are introduced as hyper-parameters to control the importance of each task.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 25, "text": "For entity recognition and relation extraction, \\(P(E^*\\mid D)\\)  and \\(P(R^*\\mid D)\\)  are computed with the definition in Equation (REF ).\r\nFor coreference resolution, we use the marginalized loss following Lee2017EndtoendNC since each mention can have multiple correct antecedents. Let \\(C^*_i\\)  be the set of all correct antecedents for span \\(i\\) , we have:\r\n\\(\\log P (C^* \\mid D) = \\sum _{i=1..N} \\log \\sum _{c\\in C^*_i} P(c \\mid D)\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 26, "text": "We use feedforward neural networks (FFNNs) over shared span representations \\(\\mathbf {g}\\)  to compute a set of span and pairwise span scores.\r\nFor the span scores, \\(\\phi _e(s_i)\\)  measures how likely a span \\(s_i\\)  has an entity type \\(e\\) , and \\(\\phi _{\\text{mr}}(s_i)\\)  and \\(\\phi _{\\text{mc}}(s_i)\\)  measure how likely a span \\(s_i\\)  is a mention in a relation or a coreference link, respectively. The pairwise scores \\(\\phi _r(s_i, s_j)\\)  and \\(\\phi _{\\text{c}}(s_i, s_j)\\)  measure how likely two spans are associated in a relation \\(r\\)  or a coreference link, respectively.\r\nLet \\(\\mathbf {g}_i\\)  be the fixed-length vector representation for span \\(s_i\\) .\r\nFor different tasks, the span scores \\(\\phi _{\\text{x}}(s_i)\\)  for \\(\\text{x} \\in \\lbrace e, \\text{mc}, \\text{mr}\\rbrace \\)  and pairwise span scores \\(\\phi _{\\text{y}}(s_i,s_j)\\)  for \\(\\text{y} \\in \\lbrace  r, \\text{c}\\rbrace \\)  are computed as follows:\r\n\\(\\phi _{\\text{x}} (s_i) =& \\mathbf {w}_{\\text{x}} \\cdot \\text{FFNN}_{\\text{x}} (\\mathbf {g}_i) \\\\\\phi _{\\text{y}} (s_i, s_j) =& \\mathbf {w}_{\\text{y}}\\cdot \\text{FFNN}_{\\text{y}} ([\\mathbf {g}_i, \\mathbf {g}_j, \\mathbf {g}_i \\odot \\mathbf {g}_j]),\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 29, "text": "The scores in Equation (REF ) are defined for entity types, relations, and antecedents that are not the null-type \\(\\epsilon \\) .\r\nScores involving the null label are set to a constant 0:\r\n\\( \\Phi _{\\text{E}}(\\epsilon , s_i)=\\Phi _{\\text{R}}(\\epsilon , s_i, s_j)=\\Phi _{\\text{C}}(s_i,\\epsilon )=0\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 28, "text": "We use these scores to compute the different \\(\\Phi \\) :\r\n\\(\\Phi _{\\text{E}}(e, s_i) &= &\\phi _{e}(s_i)  \\\\\\Phi _{\\text{R}}(r, s_i, s_j) &= & \\phi _{\\text{mr}}(s_i) + \\phi _{\\text{mr}}(s_j) + \\phi _{r}(s_i, s_j) \\\\\\Phi _{\\text{C}}(s_i, s_j) &=& \\phi _{\\text{mc}}(s_i) + \\phi _{\\text{mc}}(s_j) + \\phi _{\\text{c}}(s_i, s_j) \\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 30, "text": "We use the same span representations \\(\\mathbf {g}\\)  from [24]\r\nand share them across the three tasks.\r\nWe start by building bi-directional LSTMs [18] from word, character and ELMo [33] embeddings.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 31, "text": "For a span \\(s_i\\) , its vector representation \\(\\mathbf {g}_i\\)  is constructed by concatenating \\(s_i\\) 's left and right end points from the BiLSTM outputs, an attention-based soft \u201cheadword,\u201d and embedded span width features.\r\nHyperparameters and other implementation details will be described in Section .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 32, "text": "Following previous work, we use beam pruning to reduce the number of pairwise span factors from \\(O(n^4)\\)  to \\(O(n^2)\\)  at both training and test time, where \\(n\\)  is the number of words in the document.\r\nWe define two separate beams: \\(B_{\\text{C}}\\)  to prune spans for the coreference resolution task, and \\(B_{\\text{R}}\\)  for relation extraction. The spans in the beams are sorted by their span scores \\(\\phi _{\\text{mc}}\\)  and \\(\\phi _{\\text{mr}}\\)  respectively, and the sizes of the beams are limited by \\(\\lambda _{\\text{C}} n\\)  and \\(\\lambda _{\\text{R}} n\\) .\r\nWe also limit the maximum width of spans to a fixed number \\(W\\) , which further reduces the number of span factors to \\(O(n)\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 34, "text": "The SciIE model extracts entities, their relations, and coreference clusters within one document.\r\nPhrases are heuristically normalized (described in Section ) using entities and coreference links.\r\nIn particular, we link all entities that belong to the same coreference cluster to replace generic terms with any other non-generic term in the cluster. Moreover, we replace all the entities in the cluster with the entity that has the longest string. Our qualitative analysis shows that there are fewer ambiguous phrases using coreference links (Figure\u00a0REF ).\r\nWe calculate the frequency counts of all entities that appear in the whole corpus. We assign nodes in the knowledge graph by selecting the most frequent entities (with counts \\(>k\\) ) in the corpus, and merge in any remaining entities for which a frequent entity is a substring.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 35, "text": "A pair of entities may appear in different contexts, resulting in different relation types between those entities (Figure\u00a0REF ).\r\nFor every pair of entities in the graph, we calculate the frequency of different relation types across the whole corpus.We assign edges between entities by selecting the most frequent relation type.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 58, "text": "In the SemEval task, we compare our model SciIE with the best reported system in the SemEval leaderboard\u00a0[32], which extends E2E Rel with several in-domain features such as\r\ngazetteers extracted from existing knowledge bases and model ensembles.\r\nWe also compare with the state of the art on keyphrase extraction\u00a0[28], which applies semi-supervised methods to a neural tagging model.We compare with the inductive setting results.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 60, "text": "We evaluate SciIE on SciERC and SemEval 17 datasets. We provide qualitative results and human evaluation of the constructed knowledge graph.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 133, "text": "Figure\u00a0REF  shows the historical trend analysis (from 1996 to 2016) of the most popular applications of the phrase neural network,\r\nselected according to the statistics of the extracted relation triples with the `Used-for' relation type from speech, computer vision, and NLP conference papers. We observe that, before 2000, neural network has been applied to a greater percentage of speech applications compared to the NLP and computer vision papers. In NLP, neural networks first gain popularity in language modeling and then extend to other tasks such as POS Tagging and Machine Translation. In computer vision, the application of neural networks gains popularity in object recognition earlier (around 2010) than the other two more complex tasks of object detection and image segmentation (hardest and also the latest).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 134, "text": "Figure\u00a0REF  shows the human evaluation of the constructed knowledge graph, comparing the quality of automatically generated knowledge graphs with and without the coreference links.\r\nWe randomly select 10 frequent scientific entities and extract all the relation triples that include one of the selected entities leading to 1.5k relation triples from both systems. We ask four domain experts to annotate each of these extracted relations to define ground truth labels.\r\nEach domain expert is assigned 2 or 3 entities and all of the corresponding relations.\r\nFigure\u00a0REF  shows precision/recall curves for both systems. Since it is not feasible to compute the actual recall of the systems, we compute the pseudo-recall [45] based on the output of both systems.\r\nWe observe that the knowledge graph curve with coreference linking is mostly above the curve without coreference linking. The precision of both systems is high (above 84% for both systems), but the system with coreference links has significantly higher recall.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 137, "text": "This research was supported by the Office of Naval Research under the MURI grant N00014-18-1-2670, NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We are grateful to Waleed Ammar and AI2 for sharing the Semantic Scholar Corpus. We also thank the anonymous reviewers, UW-NLP group and Shoou-I Yu for their helpful comments.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 2, "text": "In this paper, we introduce SciREX, a new comprehensive dataset for information extraction from scientific articles. Our dataset focuses on the task of identifying the main results of a scientific article as a tuple\r\n(Dataset, Metric, Task, Method) from raw text.\r\nIt consists of three major subtasks, identifying individual entities,\r\ntheir document level relationships,\r\nand predicting their saliency in the document (i.e., entities that take part in the results of the article and are not merely, for example, mentioned in Related Work).\r\nOur dataset is fully annotated with entities, their mentions, their coreferences, and their document level relations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 4, "text": "In addition, we introduce a neural model as a strong baseline\r\nto perform this task end-to-end.\r\nOur model identifies mentions, their saliency, and their coreference links. It then clusters salient mentions into entities and identifies document level relations.\r\nWe did not find other models that can perform the full task,\r\nso we evaluated existing state-of-the-art models on\r\nsubtasks, and found our baseline model to outperform them. Experiments also show that\r\nour end-to-end document level IE task is challenging, with\r\nthe most challenging subtasks being identifying salient entities, and\r\nto a lesser extent, discovering document level relations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 11, "text": "Previous IE work on the TAC KBP competitions\u00a0[6], [8] comprise\r\nmultiple knowledge base population tasks. Our task can be considered a variant of the TAC KBP \u201ccold start\u201d task that discovers new entities and entity attributes (slot filling) from scratch. Two aspects of our task make it more interesting, 1) our model needs to be able to extract facts that are mentioned once or twice rather than rely on the redundancy of information in their documents (e.g\u00a0[18]), 2) TAC KBP relations are usually sentence-level binary relations between a query entity and an attribute (e.g\u00a0[1]), while our relations are 4-ary, span the whole document, and can't be split into multiple binary relations as discussed in Section\u00a0REF .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 12, "text": "With neural networks, a few end-to-end models have been proposed that perform multiple IE tasks jointly\u00a0[16], [13], [23].\r\nThe closest to our work is DyGIE++ \u00a0[23], which does named entity recognition, binary relation extraction, and event extraction in one model. DyGIE++ is a span-enumeration based model which works well for short paragraphs but does not scale well to long documents. Instead, we use a CRF sequence tagger, which scales well. Our model also extracts 4-ary relations between salient entity clusters, which requires a more global view of the document than that needed to extract binary relations between all pairs of entity mentions.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 13, "text": "Our goal is to extend sentence-level IE to documents and construct a dataset for document-level information extraction from scientific articles.\r\nThis section defines the IE tasks we address, and describe the details of building our SciREX dataset.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 18, "text": "Document-level information extraction requires a global understanding of the full document to annotate entities, their relations, and their saliency. However, annotating a scientific article is time-consuming and requires expert annotators.\r\nThis section explains our method for building our SciREX dataset with little annotation effort. It combines distant supervision from an existing KB and noisy automatic labeling, to provide a much simpler annotation task.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 21, "text": "PwC provides arXiv IDs for their papers. To extract raw text and section information, we use LaTeXML (https://dlmf.nist.gov/LaTeXML/) for papers with latex source (all 438 annotated papers), or use Grobid\u00a0[0] for papers in PDF format (only 10% of remaining papers did not have latex source). LaTeXML allowed us to extract clean document text with no figures / tables / equations. We leave it as future work to augment our dataset with these structured fields. To extract tokens and sentences, we use the SpaCy (https://spacy.io/) library.\r\n{TABLE}{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 24, "text": "To get more dense span-level information,\r\nwe want to label salient (corresponding to PwC entities) and also non-salient spans. We train a standard\r\nBERT+CRF sequence labeling model on the SciERC dataset (described in Section\u00a0).\r\nWe run this model on each of the documents in the PwC corpus, and it provides us with automatic (but noisy) predictions for mention span identification.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 25, "text": "The next step is to find mention spans that correspond to PwC entities.\r\nFor each mention predicted by our SciERC-trained model, we compute a Jaccard similarity with each of the PwC entities. Each mention is linked to the entity if the threshold exceeds a certain \\(\\epsilon \\) . To determine \\(\\epsilon \\) , two expert annotators manually went through 10 documents to mark identified mentions with entity names, and \\(\\epsilon \\)  was chosen such that the probability of this assignment is maximized. We use this threshold to determine a mapping for the remaining 1,170 documents. Given that Jaccard-similarity is a coarse measure of similarity, this step favors high recall over precision.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 27, "text": "Using the annotation procedure mentioned above, we build a dataset\r\nof 438 fully annotated documents.\r\nTable\u00a0REF  provides\r\ndataset statistics and shows the proportion of relations in our dataset that requires reasoning across sentence/section. It shows that the majority of the relations, especially 4-ary relations\r\nspan multiple sentences or even multiple sections. An example of such cross-section reasoning can be found in Figure\u00a0REF .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 32, "text": "An input document \\(D\\)  is represented as a list of sections \\([s_1, ..., s_{|S|}]\\) . We encode the document in two steps, section-level, then document-level. We use pretrained contextualized token encodings using SciBERT\u00a0[4] over each section separately to get embeddings for tokens in that section. If the section is bigger than 512 tokens (SciBERT limit), it is broken into 512 token subsections, and each subsection is encoded separately.\r\nTo allow document-level information flow, we concatenate the section-level token embeddings and add a BiLSTM on top of them. This allows the model to take into account cross-section dependencies. Thus for each token \\(w_i\\)  in the document, this step outputs an embedding \\(e_i\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 33, "text": "Given token embeddings, our model applies a sequence tagger that identifies mentions and classifies their types. We train a BIOUL based CRF tagger on top of the BERT-BiLSTM embeddings of words to predict mention spans \\(m_j\\)  and their corresponding types.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 34, "text": "Given the words \\(\\lbrace w_{j_1}, ..., w_{j_N}\\rbrace \\)  of a mention \\(m_j\\) ,\r\nour model learns a mention embedding \\(me_j\\)  of the mention, which will be used in later saliency identification and relation classification steps. The mention embedding is the concatenation of first token embedding \\(e_{j_1}\\) , last token embedding \\(e_{j_N}\\)  and attention weighted average of all embeddings in the mention span \\(\\sum _{k=1}^{N} \\alpha _{j_k} e_{j_k}\\) , where \\(e_{j_k}\\)  is the embedding of word \\(w_{j_k}\\)  and \\(\\alpha _{j_k}\\)  are scalars computed by passing the token embedding through an additive attention layer\u00a0[3]. We concatenate these embeddings with additional features \u2014 span's relative position in the document, an indicator showing if the sentence containing the mention also contains some marker words like `experiment' or `dataset' and the mention type.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 40, "text": "Consider such a candidate relation (4-tuple of clusters) \\(R=(C_1, C_2, C_3, C_4)\\)  where each \\(C_i\\)  is a set of mentions \\(\\lbrace m_{i_1}, ..., m_{i_j}\\rbrace \\)  in the document representing the same entity. We encode this relation into a single vector by following a two-step procedure \u2013 constructing a section embedding and aggregating them to generate a document level embedding. For each section \\(s\\)  of the document, we create a section embedding \\(E_R^s\\)  for this relation as follows -\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 43, "text": "We compare our model with other recently introduced models. Since we cannot apply previous models directly to our task, we evaluate on subtasks of our\r\ndataset\r\nand also evaluate on SciERC (Section\u00a0REF ).\r\nThe other goal of the evaluation is to establish a baseline performance on our dataset and to provide insights into the difficulty of each subtask. To that end, we evaluate the performance of each component separately (Section\u00a0REF ), and in the overall end-to-end system (Section\u00a0REF ). In addition, we perform diagnostic experiments to identify the bottlenecks in the model performance.\r\nWe report experimental setup and hyperparameters in appendix\u00a0.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 49, "text": "We compare our model with DyGIE++\u00a0[23] and DocTAET\u00a0[9] on subtasks of our SciREX dataset and on the SciERC dataset wherever they apply. Our results show that only our model can perform all the subtasks in an end-to-end fashion and performs better than or on par with these baselines on respective subtasks.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 55, "text": "DocTAET\r\n\u00a0[9] is a document-level\r\nrelation classification model that is given a document and a relation tuple to classify if it is expressed in the document. It is formulated as an entailment task with the information encoded as [CLS] document [SEP] relation in a BERT style model. This is equivalent to the last step of our model but with gold salient entity clusters as input.\r\nTable\u00a0REF  shows the result on this subtask,\r\nand it shows that our relation model gives comparable performance (in terms of positive class F1 score) to that of DocTAET.\r\n{TABLE}\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 56, "text": "Table\u00a0REF  summarizes the results of evaluating our model and\r\nDyGIE++ on the SciERC dataset.\r\nFor mention identification, our model performance is a bit worse mostly\r\nbecause SciERC has overlapping entities that a CRF-based model like ours can not\r\nhandle.\r\nFor the task of identifying coreference clusters, we perform significantly worse than DyGIE++'s end-to-end model. This provides future avenues towards improving coreference resolution for SciREX by incorporating it in an end-to-end fashion.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 57, "text": "The main contribution of our model is to connect multiple components to perform our end-to-end task. This section evaluates each step of our model separately from all other components. To do so, we feed each component with gold inputs and evaluate the output. This gives us a good picture of the performance of each component without the accumulation of errors.\r\nThe first block of Table\u00a0REF  summarizes the results of this evaluation setting. We know from Tables\u00a0REF ,\u00a0REF  that\r\nour mention identification and relation identification components are working well.\r\nFor pairwise coreference resolution, we know from Table\u00a0REF \r\nthat it needs to be improved, but it is performing well on our dataset likely because the majority of coreferences in our dataset can be performed using only the surface form of the mentions (for example, abbreviation reference).\r\nThe worst performing component is identifying salient mentions, which requires information to be aggregated from across the document, something the current neural models lack.Performance of Salient Entity Clusters is close to 1.0 because it is a deterministic algorithm (clustering followed by filtering) that gives perfect output given gold input. The reason the recall is not 1.0 as well is because of small inconsistencies in the gold annotations (two distinct entities merged into one).\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 58, "text": "Evaluation with Predicted Input.\r\nThe second block in Table\u00a0REF  gives results for the end-to-end performance of our model in predicting salient entity clusters, binary relations, and 4-ary relations.\r\nWe noticed that there is quite a drop in the end-to-end performance compared to the component-wise performance. This is particularly clear with relations; even though the relation extraction component performance is reasonably good in isolation, its end-to-end performance is quite low because of the accumulation of errors in previous steps.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 59, "text": "Through manual error analysis, we found that the identification of salient clusters\r\nis the most problematic step in our model. The third block in Table\u00a0REF \r\nquantifies this. In this setting, we run our end-to-end model but with\r\n\u201cgold cluster saliency\u201d information.\r\nIn particular, we predict clusters of mentions using our model (mention identification, pairwise coreference, and mention clustering). Then instead of filtering clusters using our mention saliency score, we keep only those clusters that have any overlap with at least one gold cluster. Predicted clusters that match the same gold cluster are then combined. Finally, we feed those to the relation extraction step of our model.\r\nUnder this setting, we found that the performance of 4-ary relations improves considerably\r\nby more than 10x. This confirms our hypothesis that identifying salient clusters is the key bottleneck in the end-to-end system performance. This is also consistent with the component-wise results that show low performance for salient mentions identification.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 60, "text": "Our error analysis shows that the average number of mentions in a salient cluster classified correctly is 15 mentions, whereas for the misclassified ones is six mentions. This indicates that our model judges the saliency of an entity strongly based on how frequently it is mentioned in the document. While this is a perfectly reasonable signal to rely on, the model seems to trust it more than\r\nthe context of the entity mention. For example, in the following snippet,\r\n\u201c... For each model, we report the test perplexity, the computational budget, the parameter counts, the value of DropProb, and the computational efficiency ....\u201d, the entity \u201cthe parameter counts\u201d is misclassified as non-salient, as it only appears twice in the document. One possible way to address this issue with salient entity identification is to replace its simple filtering step with a trained model that can do a better job at aggregating evidence from multiple mentions.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 62, "text": "We introduce SciREX, a comprehensive and challenging dataset for information extraction on full documents. We also develop a baseline model for our dataset, which, to the best of our knowledge, is the first attempt toward a neural document level IE that can perform all the necessary subtasks in an end-to-end manner. We show that using a document level model gave a significant improvement in terms of recall, compared to existing paragraph-level approaches.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 63, "text": "This task poses multiple technical and modeling challenges, including\r\nthe use of transformer-based models on long documents and related device memory issues,\r\n\r\naggregating coreference information from across documents in an end-to-end manner,\r\n\r\nidentifying salient entities in a document and\r\n\r\nperforming N-ary relation extraction of these entities.\r\n\r\nEach of these tasks challenges existing methodologies in the information extraction domain, which, by and large, focus on short text sequences. An analysis of the performance of our model emphasizes the need for better document-level models that can overcome the new challenges posed by our dataset. As our research community moves towards document level IE and discourse modeling, we position this dataset as a testing ground to focus on this important and challenging task.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 67, "text": "Our Annotation guidelines can be found at https://github.com/allenai/SciREX/blob/master/Annotation%20Guidelines.pdf Note, for Method type entities, we specifically ask our annotator to break down complex entities into simpler ones before looking for mentions in the text. For example, a method entity DLDL+VGG-Face is composite and broken into two parts DLDL and VGG-Face. Currently, our model considers all mentions of subentities as mentions of the corresponding Method entity. We leave the task of extracting relation between subentities explicitly as future work.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 65, "text": "We divide our 438 annotated documents into training (70%), validation (30%) and test set (30%).\r\nThe base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state. We use a dropout of 0.2 after BiLSTM embeddings. All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers. For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax. We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights. We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score. All our models were trained using 48Gb Quadro RTX 8000 GPUs. The multitask model takes approximately 3 hrs to train.\r\n", "annotation": {"entities": {"a6": {"id": "a6", "type": "a", "subtype": null, "surface_forms": [{"id": "#fb9fec7c-fff7-44a7-85cc-41f77f52c364", "surface_form": "model", "start": 136, "end": 141}, {"id": "#5628e381-9e76-4263-9125-4acaf18f0cb3", "surface_form": "model", "start": 603, "end": 608}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#1c77af68-2d6e-4db4-9501-57ac38f5287a", "surface_form": "BiLSTM", "start": 176, "end": 182}, {"id": "#3f44cc99-e718-4886-8302-a8b011bfdb26", "surface_form": "BiLSTM", "start": 238, "end": 244}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#32269e63-2aa5-4be7-8673-56fda6e6458e", "surface_form": "128", "start": 188, "end": 191}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#d14e7fbd-e8c5-4b7e-ba1d-f38b8ddd895e", "surface_form": "hidden state", "start": 194, "end": 206}, {"id": "#4c0ef44a-0235-4bf3-b500-2ebeb87ac97e", "surface_form": "hidden state", "start": 551, "end": 563}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#3677f201-e8b6-47c6-98bf-d940c71285d7", "surface_form": "dropout", "start": 217, "end": 224}, {"id": "#77e61c37-b503-4085-9fc5-f28b8380ab12", "surface_form": "dropout", "start": 371, "end": 378}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#48ffc6bb-b356-4f4f-b03c-bd408cccc046", "surface_form": "0.2", "start": 228, "end": 231}]}, "a18": {"id": "a18", "type": "a", "subtype": null, "surface_forms": [{"id": "#82322f99-efc5-4991-b895-e06d06a5442e", "surface_form": "feedforward networks", "start": 261, "end": 281}, {"id": "#a02000ac-a1d0-40d9-8d84-b554328da4a1", "surface_form": "feedforward layer", "start": 522, "end": 539}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#1dc004fa-d05b-4486-a88b-8e37515cddb4", "surface_form": "two", "start": 298, "end": 301}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#2e27d38a-f82e-42cc-a15d-9d1294a95f5f", "surface_form": "hidden layers", "start": 302, "end": 315}, {"id": "#5b6e9c44-2d22-4447-9c6b-f317afcc8cc9", "surface_form": "dimension", "start": 325, "end": 334}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#71e9cfe6-128b-4af0-96cc-85aae4d176e7", "surface_form": "128", "start": 335, "end": 338}]}, "v9": {"id": "v9", "type": "v", "subtype": "o", "surface_forms": [{"id": "#a95b47a5-80ac-4588-9379-288486c3c0c6", "surface_form": "gelu", "start": 344, "end": 348}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#5edd9f78-5718-4998-b0ea-b97f6402a818", "surface_form": "activation", "start": 349, "end": 359}]}, "v11": {"id": "v11", "type": "v", "subtype": "n", "surface_forms": [{"id": "#4f351626-b59d-4e71-a061-b65f4e637c03", "surface_form": "128", "start": 545, "end": 548}]}, "v12": {"id": "v12", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ccf1a670-2b62-44aa-8560-1ef916c44ecc", "surface_form": "30", "start": 613, "end": 615}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#8b428ec9-5ccb-4b0f-9307-2fb6bca904ac", "surface_form": "epochs", "start": 616, "end": 622}]}, "a22": {"id": "a22", "type": "a", "subtype": null, "surface_forms": [{"id": "#09100685-2022-4f8f-b5e4-bd9a3a669e77", "surface_form": "Adam", "start": 629, "end": 633}, {"id": "#ad6dea75-a336-4564-957b-fb5f22e38834", "surface_form": "early stopping", "start": 730, "end": 744}]}, "v13": {"id": "v13", "type": "v", "subtype": "n", "surface_forms": [{"id": "#299b15ad-953a-4664-b9c2-10a330e0a51d", "surface_form": "1e-3", "start": 649, "end": 653}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "surface_form": "learning rate", "start": 657, "end": 670}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#186b0fcb-4905-4782-b328-f0a9d139e50b", "surface_form": "for all non BERT weights", "start": 671, "end": 695}]}, "v14": {"id": "v14", "type": "v", "subtype": "n", "surface_forms": [{"id": "#27ade662-e3eb-4a71-bfa7-e5161909dbde", "surface_form": "2e-5", "start": 700, "end": 704}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#95b12990-5e20-4df2-aba1-ca98d916bbe9", "surface_form": "for BERT weights", "start": 705, "end": 721}]}, "p8": {"id": "p8", "type": "p", "subtype": null, "surface_forms": [{"id": "#5581acf1-60aa-420c-88df-2e2abcf2aaef", "surface_form": "patience value", "start": 752, "end": 766}]}, "v15": {"id": "v15", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3faea6b8-f987-43bf-92f0-b81c15973ee7", "surface_form": "7", "start": 770, "end": 771}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#d78c347f-4a30-484e-bd35-b77c1f156360", "surface_form": "on the validation", "start": 772, "end": 789}]}}, "relations": {"r23": {"id": "r23", "source": "v5", "target": "p1", "evidences": [{"id": "#06db44b0-ddb1-469d-a7b8-e6690c06811e", "source_surface_form": "#32269e63-2aa5-4be7-8673-56fda6e6458e", "target_surface_form": "#d14e7fbd-e8c5-4b7e-ba1d-f38b8ddd895e", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state.", "start": 97, "end": 208}]}, "r0": {"id": "r0", "source": "p1", "target": "a14", "evidences": [{"id": "#708d50d7-1c98-4543-abea-59c5536be8e8", "source_surface_form": "#d14e7fbd-e8c5-4b7e-ba1d-f38b8ddd895e", "target_surface_form": "#1c77af68-2d6e-4db4-9501-57ac38f5287a", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state.", "start": 97, "end": 208}]}, "r1": {"id": "r1", "source": "v6", "target": "p2", "evidences": [{"id": "#4acd1b39-650e-44e9-b226-47994fe4462a", "source_surface_form": "#48ffc6bb-b356-4f4f-b03c-bd408cccc046", "target_surface_form": "#3677f201-e8b6-47c6-98bf-d940c71285d7", "evidence_sentence": "We use a dropout of 0.2 after BiLSTM embeddings.", "start": 209, "end": 257}]}, "r2": {"id": "r2", "source": "v7", "target": "p3", "evidences": [{"id": "#264d74a8-d8b9-40eb-a71b-915886b79739", "source_surface_form": "#1dc004fa-d05b-4486-a88b-8e37515cddb4", "target_surface_form": "#2e27d38a-f82e-42cc-a15d-9d1294a95f5f", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r3": {"id": "r3", "source": "p3", "target": "a18", "evidences": [{"id": "#56c4fd32-f245-4f62-a1d2-3b543ace0833", "source_surface_form": "#2e27d38a-f82e-42cc-a15d-9d1294a95f5f", "target_surface_form": "#82322f99-efc5-4991-b895-e06d06a5442e", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r6": {"id": "r6", "source": "v8", "target": "p3", "evidences": [{"id": "#0cf7b217-68d0-475a-a459-66358e62ca37", "source_surface_form": "#71e9cfe6-128b-4af0-96cc-85aae4d176e7", "target_surface_form": "#5b6e9c44-2d22-4447-9c6b-f317afcc8cc9", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}, {"id": "#81fda297-20ec-407b-a7cd-62fe00c717fc", "source_surface_form": "#c8974ec6-0e80-4c4b-a038-50c7416d7ad6", "target_surface_form": "#77e61c37-b503-4085-9fc5-f28b8380ab12", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r5": {"id": "r5", "source": "p3", "target": "a18", "evidences": [{"id": "#9a5bf55b-dee1-4e0f-8c32-6ee9b21a7852", "source_surface_form": "#5b6e9c44-2d22-4447-9c6b-f317afcc8cc9", "target_surface_form": "#82322f99-efc5-4991-b895-e06d06a5442e", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r7": {"id": "r7", "source": "p2", "target": "a18", "evidences": [{"id": "#07c8d48e-739c-4b77-9868-74d2f43e2ca3", "source_surface_form": "#77e61c37-b503-4085-9fc5-f28b8380ab12", "target_surface_form": "#82322f99-efc5-4991-b895-e06d06a5442e", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r8": {"id": "r8", "source": "v9", "target": "p4", "evidences": [{"id": "#a968b22b-1c6c-4f08-8aca-bb9cc37500db", "source_surface_form": "#a95b47a5-80ac-4588-9379-288486c3c0c6", "target_surface_form": "#5edd9f78-5718-4998-b0ea-b97f6402a818", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r9": {"id": "r9", "source": "p4", "target": "a18", "evidences": [{"id": "#6aced56a-ed26-4354-a8de-14dd1bcc039f", "source_surface_form": "#5edd9f78-5718-4998-b0ea-b97f6402a818", "target_surface_form": "#82322f99-efc5-4991-b895-e06d06a5442e", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r10": {"id": "r10", "source": "v11", "target": "p1", "evidences": [{"id": "#7255e268-ed4c-4c85-9b1b-7b318dee916e", "source_surface_form": "#4f351626-b59d-4e71-a061-b65f4e637c03", "target_surface_form": "#4c0ef44a-0235-4bf3-b500-2ebeb87ac97e", "evidence_sentence": "For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax.", "start": 403, "end": 590}]}, "r11": {"id": "r11", "source": "p1", "target": "a18", "evidences": [{"id": "#be435edd-7ae6-463d-b16c-d15f6fdc7fba", "source_surface_form": "#4c0ef44a-0235-4bf3-b500-2ebeb87ac97e", "target_surface_form": "#a02000ac-a1d0-40d9-8d84-b554328da4a1", "evidence_sentence": "For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax.", "start": 403, "end": 590}]}, "r12": {"id": "r12", "source": "v12", "target": "p5", "evidences": [{"id": "#99f684c4-7805-455f-91b3-affc7c903cdf", "source_surface_form": "#ccf1a670-2b62-44aa-8560-1ef916c44ecc", "target_surface_form": "#8b428ec9-5ccb-4b0f-9307-2fb6bca904ac", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r13": {"id": "r13", "source": "c1", "target": "v13", "evidences": [{"id": "#1f307d9b-bc98-41ee-ac4d-26ace98e8b81", "source_surface_form": "#186b0fcb-4905-4782-b328-f0a9d139e50b", "target_surface_form": "#299b15ad-953a-4664-b9c2-10a330e0a51d", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r14": {"id": "r14", "source": "v13", "target": "p6", "evidences": [{"id": "#fb6a13c7-168a-45db-af49-fbd72c586f22", "source_surface_form": "#299b15ad-953a-4664-b9c2-10a330e0a51d", "target_surface_form": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r15": {"id": "r15", "source": "p6", "target": "a22", "evidences": [{"id": "#c0c79ad0-5e19-4386-b37d-70941edddf4f", "source_surface_form": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "target_surface_form": "#09100685-2022-4f8f-b5e4-bd9a3a669e77", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r16": {"id": "r16", "source": "c2", "target": "v14", "evidences": [{"id": "#81530c42-1957-4256-b4e5-8df6be3ff126", "source_surface_form": "#95b12990-5e20-4df2-aba1-ca98d916bbe9", "target_surface_form": "#27ade662-e3eb-4a71-bfa7-e5161909dbde", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r17": {"id": "r17", "source": "v14", "target": "p6", "evidences": [{"id": "#38308cae-d23f-4819-aadc-de7a09b2ca05", "source_surface_form": "#27ade662-e3eb-4a71-bfa7-e5161909dbde", "target_surface_form": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r18": {"id": "r18", "source": "v15", "target": "p8", "evidences": [{"id": "#0806a9b4-de19-46af-9db6-8ff46e31a11a", "source_surface_form": "#3faea6b8-f987-43bf-92f0-b81c15973ee7", "target_surface_form": "#5581acf1-60aa-420c-88df-2e2abcf2aaef", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}]}, "r19": {"id": "r19", "source": "p8", "target": "a22", "evidences": [{"id": "#ae84153d-0564-4945-9ce9-d2d6083a82f5", "source_surface_form": "#5581acf1-60aa-420c-88df-2e2abcf2aaef", "target_surface_form": "#ad6dea75-a336-4564-957b-fb5f22e38834", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}]}, "r20": {"id": "r20", "source": "c3", "target": "v15", "evidences": [{"id": "#d07071d6-7c2f-4449-9a09-15d70457056b", "source_surface_form": "#d78c347f-4a30-484e-bd35-b77c1f156360", "target_surface_form": "#3faea6b8-f987-43bf-92f0-b81c15973ee7", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}]}, "r21": {"id": "r21", "source": "p2", "target": "a6", "evidences": [{"id": "#c6c9b671-6633-443d-8bed-9e0a79e58c30", "source_surface_form": "#3677f201-e8b6-47c6-98bf-d940c71285d7", "target_surface_form": "#fb9fec7c-fff7-44a7-85cc-41f77f52c364", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state. We use a dropout of 0.2 after BiLSTM embeddings.", "start": 97, "end": 257}]}, "r22": {"id": "r22", "source": "p5", "target": "a6", "evidences": [{"id": "#422aa41a-b031-4a26-86ca-6378d77fc8e2", "source_surface_form": "#8b428ec9-5ccb-4b0f-9307-2fb6bca904ac", "target_surface_form": "#5628e381-9e76-4263-9125-4acaf18f0cb3", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 4, "text": "Our proposed model, GREAD, consists of three parts: an encoder, a reaction-diffusion layer, and an output layer (cf. Eqs.\u00a0(REF ) to\u00a0()). The reaction-diffusion layer has four different types in its core part: i) Fisher (F), ii) Allen-Cahn (AC), iii) Zeldovich (Z), and iv) Blurring-sharpening (BS). The first three reaction-diffusion equations are widely used in many natural science domains, e.g., biology, combustion, etc. In particular, the blurring-sharpening (BS) equation was designed by us for GNNs, which marks the best accuracy in many cases of our experiments.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 6, "text": "\r\nWe design a reaction-diffusion layer that incorporates i) three types of reaction equations popular in natural sciences and ii) one more type proposed by us.\r\n\r\nWe carefully integrate the four reaction equation types into our GNN method and customize its overall architecture for better accuracy. For instance, we use a soft adjacency matrix generating method which shows a synergistic effect with the reaction-diffusion layer.\r\n\r\nWe consider a comprehensive set of 9 datasets and 17 baselines. Our method marks the best accuracy in almost all cases. The ranking and accuracy averaged over all the datasets are summarized in Table\u00a0REF .\r\n\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 18, "text": "After describing an overview of our method, we describe its detailed designs, followed by its training algorithm.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 23, "text": "Given a graph \\(\\mathcal {G}\\) , one can use its original symmetric normalized adjacency matrix \\(\\mathbf {A} \\in [0,1]^{|\\mathcal {V}| \\times |\\mathcal {V}|}\\)  for our method. However, we also provide the method to generate a soft adjacency matrix, in which case \\(\\mathbf {A} \\in [0,1]^{|\\mathcal {V}| \\times |\\mathcal {V}|}\\) . For our experiments, we test both of them.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 26, "text": "Eq.\u00a0() is the main processing layer, called the reaction-diffusion layer, in our method. Given the definition of \\(\\mathbf {f}\\) , \\(-\\mathbf {L}\\mathbf {H}(t)\\)  is a diffusion term, which corresponds to the heat equation describing the spread of heat over \\(\\mathcal {G}\\)  and has been used widely by various GNNs \u00a0[50], [5], [6]. It is known that the diffusion terms cause the problem of oversmoothing, which means that the last hidden states of nodes become too similar when applying only the diffusion processing too much (without the reaction processing). To this end, many models prefer shallow architectures that do not cause the oversmoothing problem\u00a0[51], [31] or use heuristic methods to prevent it\u00a0[57], [7], [9], [33], [35], [29], [8].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 27, "text": "In our case, we prevent the oversmoothing problem by adding the reaction term \\(\\mathbf {r}\\)  and solve Eq.\u00a0() with ODE solvers\u00a0[18] \u2014 in other words, our reaction-diffusion layer is continuous, which is yet another distinguishing point in our method since many other models are based on discrete layers\u00a0[31], [4], [11], [60]. We consider the following options for \\(\\mathbf {r}\\) :\r\n\\(\\begin{small}\\mathbf {r}(\\mathbf {H}(t), \\mathbf {A}) := {\\left\\lbrace \\begin{array}{ll} \\mathbf {H}(t)\\odot (1-\\mathbf {H}(t)),\\textrm { if Fisher (F)}\\\\\\mathbf {H}(t)\\odot (1-\\mathbf {H}(t)^{\\circ 2}),\\textrm { if Allen-Cahn (AC)}\\\\\\mathbf {H}(t)\\odot (\\mathbf {H}(t)-\\mathbf {H}(t)^{\\circ 2}),\\textrm { if Zeldovich (Z)}\\\\(\\mathbf {A}-\\mathbf {A}^2)\\mathbf {H}(t),\\textrm { if Blurring-Sharpening (BS)}\\end{array}\\right.}\\end{small}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 30, "text": "Our proposed blurring-sharpening process is to alternately perform the blurring and the sharpening operations in a layer.\r\nWe show that our proposed blurring-sharpening process reduces to a certain form of the reaction-diffusion process. Many GNNs can be generalized to the following blurring (or diffusion) process, i.e., the low-pass graph convolutional filtering for blurring. We also use the same blurring operation at first:\r\n\\(\\begin{split}\\mathbf {B}(t+h) &= \\mathbf {H}(t) -\\mathbf {L}\\mathbf {H}(t),\\\\&\\Rightarrow \\mathbf {H}(t) + (\\mathbf {A}-\\mathbf {I})\\mathbf {H}(t),\\\\&\\Rightarrow \\mathbf {A}\\mathbf {H}(t).\\end{split}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 41, "text": "We use Alg.\u00a0(REF ) to train our proposed model. The full training process minimizes the cross-entropy loss:\r\n\\(\\mathcal {L} = \\sum _{i}^{n}{\\mathbf {y}^T_i\\log {\\hat{\\mathbf {y}_i}}},\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 44, "text": "We first compare our method with other baselines for node classification tasks. We then discuss the ability of mitigating oversmoothing on a synthetic graph and show the experiment with different heterophily levels on other synthetic graphs.\r\nThe following software and hardware environments were used for all experiments: Ubuntu 18.04 LTS, Python 3.9.12, PyTorch 1.11.0, PyTorch Geometric 2.0.4, torchdiffeq 0.2.3, CUDA 11.3, NVIDIA Driver 465.19, i9 CPU, and NVIDIA RTX 3090.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 48, "text": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2. The weight decay is 0 to 0.01. The detailed search space and other hyperparameters are in Appendix. We also list the best hyperparameter configuration for each data in Appendix. If a baseline's accuracy is known and its experimental environments are the same as ours, we use the officially announced accuracy. If not, we execute a baseline using its official codes and the hyperparameter search procedures based on their suggested hyperparameter ranges.\r\n{FIGURE}", "annotation": {"entities": {"a4": {"id": "a4", "type": "a", "subtype": null, "surface_forms": [{"id": "#1888bc79-c627-4006-9344-61019a409969", "surface_form": "method", "start": 8, "end": 14}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#3b4698e3-5b98-44a4-87c1-fb9bc66564cb", "surface_form": "we test with the following hyperparameter configurations", "start": 16, "end": 72}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ef50a9d7-eff4-4aa8-ac76-e985e329bfb9", "surface_form": "200", "start": 87, "end": 90}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#8656a316-20b9-4c03-a82a-3edeeca48326", "surface_form": "epochs", "start": 91, "end": 97}]}, "a57": {"id": "a57", "type": "a", "subtype": null, "surface_forms": [{"id": "#1a8fc2e1-3008-4a44-9fbe-fed9f9216ca3", "surface_form": "Adam", "start": 108, "end": 112}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#447ded9b-045c-41c7-b2f7-e2f99a75f65b", "surface_form": "dimensionality of \\(\\mathbf {H}\\)", "start": 128, "end": 161}, {"id": "#6f4c2ee1-4431-4141-a75e-a65b68cae64f", "surface_form": "\\dim (\\mathbf {H})", "start": 174, "end": 192}]}, "v2": {"id": "v2", "type": "v", "subtype": "s", "surface_forms": [{"id": "#fd3be1ef-8a08-4ab0-9507-382992cf009f", "surface_form": "{32, 64, 128, 256}", "start": 203, "end": 221}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#c777b8b1-81be-4ce3-8dc4-3cded78f328a", "surface_form": "T", "start": 225, "end": 226}]}, "v3": {"id": "v3", "type": "v", "subtype": "r", "surface_forms": [{"id": "#b4d2eb9b-5031-4df2-9874-11998f4cf62e", "surface_form": "1.0 to 4.0", "start": 248, "end": 258}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#999884cb-6e0b-435e-acea-05dfd823ec9e", "surface_form": "learning rate", "start": 264, "end": 277}]}, "v4": {"id": "v4", "type": "v", "subtype": "r", "surface_forms": [{"id": "#e61d1bcb-04b6-4268-8eb7-06aad8e568c0", "surface_form": "1e-4 to 2e-2", "start": 293, "end": 305}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#e179576d-6e02-4e44-b326-4a47a45e38ac", "surface_form": "weight decay", "start": 311, "end": 323}]}, "v5": {"id": "v5", "type": "v", "subtype": "r", "surface_forms": [{"id": "#3a9360c8-286b-4f3c-b90e-6d50a04cbab7", "surface_form": "0 to 0.01", "start": 327, "end": 336}]}}, "relations": {"r14": {"id": "r14", "source": "c1", "target": "v1", "evidences": [{"id": "#c7c7f5f0-a968-4ac7-b992-0f259fd75a80", "source_surface_form": "#3b4698e3-5b98-44a4-87c1-fb9bc66564cb", "target_surface_form": "#ef50a9d7-eff4-4aa8-ac76-e985e329bfb9", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer.", "start": 0, "end": 123}]}, "r0": {"id": "r0", "source": "v1", "target": "p1", "evidences": [{"id": "#4d6aec5a-833b-4314-9836-f28db6c5460f", "source_surface_form": "#ef50a9d7-eff4-4aa8-ac76-e985e329bfb9", "target_surface_form": "#8656a316-20b9-4c03-a82a-3edeeca48326", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer.", "start": 0, "end": 123}]}, "r1": {"id": "r1", "source": "p1", "target": "a57", "evidences": [{"id": "#a328b567-e049-47a3-8537-9a28bada5dfd", "source_surface_form": "#8656a316-20b9-4c03-a82a-3edeeca48326", "target_surface_form": "#1a8fc2e1-3008-4a44-9fbe-fed9f9216ca3", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer.", "start": 0, "end": 123}]}, "r2": {"id": "r2", "source": "v2", "target": "p2", "evidences": [{"id": "#5177554e-89b5-4baf-9b8a-67c4034b17b6", "source_surface_form": "#fd3be1ef-8a08-4ab0-9507-382992cf009f", "target_surface_form": "#447ded9b-045c-41c7-b2f7-e2f99a75f65b", "evidence_sentence": "The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}.", "start": 124, "end": 222}]}, "r3": {"id": "r3", "source": "c1", "target": "v2", "evidences": [{"id": "#bb01e8c7-b784-4e27-8356-6e4039f85f72", "source_surface_form": "#3b4698e3-5b98-44a4-87c1-fb9bc66564cb", "target_surface_form": "#fd3be1ef-8a08-4ab0-9507-382992cf009f", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}.", "start": 0, "end": 222}]}, "r4": {"id": "r4", "source": "c1", "target": "v3", "evidences": [{"id": "#74ffbbcd-9793-4470-b749-dabf0897db39", "source_surface_form": "#3b4698e3-5b98-44a4-87c1-fb9bc66564cb", "target_surface_form": "#b4d2eb9b-5031-4df2-9874-11998f4cf62e", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0.", "start": 0, "end": 259}]}, "r5": {"id": "r5", "source": "v3", "target": "p3", "evidences": [{"id": "#66334c55-5580-4076-a6be-7b6d22b0688c", "source_surface_form": "#b4d2eb9b-5031-4df2-9874-11998f4cf62e", "target_surface_form": "#c777b8b1-81be-4ce3-8dc4-3cded78f328a", "evidence_sentence": "\\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0.", "start": 223, "end": 259}]}, "r6": {"id": "r6", "source": "c1", "target": "v4", "evidences": [{"id": "#9b19a16d-b4a5-4fb5-8d1e-247c5dc95aee", "source_surface_form": "#3b4698e3-5b98-44a4-87c1-fb9bc66564cb", "target_surface_form": "#e61d1bcb-04b6-4268-8eb7-06aad8e568c0", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2.", "start": 0, "end": 306}]}, "r7": {"id": "r7", "source": "v4", "target": "p4", "evidences": [{"id": "#b1e93316-763f-4cdd-8d9a-693ff5cfcdca", "source_surface_form": "#e61d1bcb-04b6-4268-8eb7-06aad8e568c0", "target_surface_form": "#999884cb-6e0b-435e-acea-05dfd823ec9e", "evidence_sentence": "The learning rate is set to from 1e-4 to 2e-2.", "start": 260, "end": 306}]}, "r8": {"id": "r8", "source": "p4", "target": "a57", "evidences": [{"id": "#2a34a212-8953-49de-8f38-d2a88149fbe5", "source_surface_form": "#999884cb-6e0b-435e-acea-05dfd823ec9e", "target_surface_form": "#1a8fc2e1-3008-4a44-9fbe-fed9f9216ca3", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2.", "start": 0, "end": 306}]}, "r9": {"id": "r9", "source": "c1", "target": "v5", "evidences": [{"id": "#3d418130-cc02-4154-8273-5e7a949d4829", "source_surface_form": "#3b4698e3-5b98-44a4-87c1-fb9bc66564cb", "target_surface_form": "#3a9360c8-286b-4f3c-b90e-6d50a04cbab7", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2. The weight decay is 0 to 0.01.", "start": 0, "end": 337}]}, "r10": {"id": "r10", "source": "v5", "target": "p5", "evidences": [{"id": "#c942c457-91ea-4cf2-ad12-e129e9cbafe4", "source_surface_form": "#3a9360c8-286b-4f3c-b90e-6d50a04cbab7", "target_surface_form": "#e179576d-6e02-4e44-b326-4a47a45e38ac", "evidence_sentence": "The weight decay is 0 to 0.01.", "start": 307, "end": 337}]}, "r11": {"id": "r11", "source": "p5", "target": "a57", "evidences": [{"id": "#8103d0e4-5a21-4a3c-9a5f-db69e2f2130a", "source_surface_form": "#e179576d-6e02-4e44-b326-4a47a45e38ac", "target_surface_form": "#1a8fc2e1-3008-4a44-9fbe-fed9f9216ca3", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2. The weight decay is 0 to 0.01.", "start": 0, "end": 337}]}, "r12": {"id": "r12", "source": "p2", "target": "a4", "evidences": [{"id": "#b5b1a767-5b49-4017-8fe8-01c3e408bc77", "source_surface_form": "#447ded9b-045c-41c7-b2f7-e2f99a75f65b", "target_surface_form": "#1888bc79-c627-4006-9344-61019a409969", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}.", "start": 0, "end": 222}]}, "r13": {"id": "r13", "source": "p3", "target": "a4", "evidences": [{"id": "#e0c91203-80d8-4457-9608-b70c9dce2f26", "source_surface_form": "#c777b8b1-81be-4ce3-8dc4-3cded78f328a", "target_surface_form": "#1888bc79-c627-4006-9344-61019a409969", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.", "start": 0, "end": 236}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 50, "text": "Tables\u00a0REF  presents the detailed classification performance. As reported, our method marks the best accuracy in all cases except for Squirrel and Citeseer. GCNII and H2GCN show comparable accuracy values from time to time. However, there are no existing methods that are as stable as GREAD-BS. For example, GCNII shows reasonably high accuracy in homophilic datasets, but not heterophilic ones. BLEND fails to mark the best or the second-best place in all cases. While GREAD-BS is the best method overall, GREAD-F is the best method for Texas and is the second-best for Chameleon. GREAD-AC marks the best accuracy on Cornell and third place in Film and Chameleon.\r\n{TABLE}{TABLE}{FIGURE}{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 57, "text": "The oversmoothing phenomenon means that as the depth increase, all node features converge to a constant. Thus, \\(E(\\mathbf {H}, \\mathbf {A})\\)  decays to zero asymptotically in time. We will show that our proposed method allows mitigating oversmoothing via the evolution of the Dirichlet energy.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 62, "text": "We presented the concept of graph neural reaction-diffusion equation, called GREAD. Our proposed GREAD is one of the most generalized architectures considering both the diffusion and reaction processes. We design a reaction-diffusion layer that has three types of reaction equations widely used in natural sciences. We also add one special reaction term, called Blurring-sharpening (BS), designed by us for GNNs. Therefore, our reaction-diffusion layer has four types. We consider a comprehensive set of 9 real-world datasets with various homophily difficulties and 17 baselines. GREAD marks the best accuracy in almost all cases. In our experiments with the two kinds of synthetic datasets, GREAD shows that it alleviates the oversmoothing problem and performs well on various homophily rates. This shows that our proposed model is a novel framework for constructing GNNs using the concept of the reaction-diffusion equation.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 63, "text": "We show the average ranking and accuracy of all methods in Table\u00a0REF . Our methods occupy all the top-4 positions.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 65, "text": "We also analyze the time complexity of the reaction-diffusion layer in Eq.\u00a0(). Our proposed model has different complexity depending on the reaction term \\(r\\)  in Eq.\u00a0(REF ).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 79, "text": "In order to show the effectiveness of our proposed model more intuitively, we further conduct visualization tasks for all datasets. We extract the output vector in the final layer of GREAD and visualize those vectors using t-SNE. Fig.\u00a0REF  shows the visualization results on each dataset. Different colors mean different ground-truth classes.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 80, "text": "We perform the ablation study on \\(\\beta \\)  from the perspective of the Dirichlet energy. \\(\\beta \\)  can be either a scalar parameter (SC) or a learnable vector parameter (VC). In Fig.\u00a0REF , we show the evolution of the Dirichlet energy on the synthetic random graph created from cSBM\u00a0[16], and compare SC and VC for our proposed method. In the case of GREAD-F, GREAD-BS, and GREAD-AC, VC conserves more energy than SC, so the reaction term multiplied with \\(\\beta \\)  successfully mitigates the oversmoothing problem.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 76, "text": "We present the training time of GREAD and some selected baselines in Fig.\u00a0REF . In general, our method's training time is little larger than those of the existing baselines because GREAD has an additional operation in its reaction term. The ODE solvers significantly affect training time. For example, using RK4 increases the time by 76.4% more than Euler solver.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 10, "text": "In this section, we describe our method of Citation Recommendation for Published Scientific Entity (CRPSE) in detail. We first give the definition of terms and the problem. Then we provide the step-by-step construction process of the published scientific entity-papers mapping dataset which is used as the basis for our citation recommendation method. The process mainly contains two steps: collecting and filtering. After that, we present two sorting criteria to put the candidate source papers in order with the support of the constructed dataset. Last but not least, we delineate the recommendation pipeline with an example.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 17, "text": "To achieve the objective of CRPSE, we track the cooccurrences between published entities and in-text citations in the citing behaviors of researchers. Based on the cooccurrences, we analyze a large quantity of citations and construct a published scientific entity-papers mapping dataset as the base to guide our own citation recommendation. Semantic Scholar Open Research Corpus (S2ORC)\u00a0[36] is used to construct this dataset. S2ORC is by far the largest public corpus of English academic papers. The corpus covers 81.1 million papers from various domains with rich metadata, including titles, abstracts, authors, publication year, references and full text of 8.1 million open access papers. In S2ORC, in-text citations of these open access papers are annotated and linked to their resolved references, which creates 380.5 million pieces of citation data. We take great advantage of this corpus, mining citation information from its data to construct our dataset. It is important to note that the construction method proposed here is not specially designed for S2ORC. It is a universal method and can be transferred to any other corpus with in-text citation markers.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 32, "text": "We use recall@1, recall@5, recall@10, mean average precision (MAP)\u00a0[58] and mean reciprocal rank (MRR)\u00a0[57] as metrics for the evaluation of CRPSE. Results are shown in Table\u00a0REF . As we can see from Table\u00a0REF , our method produces good performance as evaluated by all the metrics. The sorting criterion of weighted context embedding outperforms the criterion of cooccurrence count under the evaluation of most metrics. We have tried to compare our method with the state-of-the-art (SOTA) local citation recommendation methods, but the comparison is hard to conduct because of the low comparability between our method and other local citation recommendation methods. This low comparability is explained as follows:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 33, "text": "\r\nThe comparison will be biased if we test our method and other general-purpose citation recommendation methods on the dataset constructed in Sect.\u00a0REF . This is because our method is designed for recommending citations for published entities, but as far as our literature search goes, none of the existing citation recommendation methods are specially aimed at published entity citation recommendation, hence the biased comparison.\r\n\r\nThe existing datasets for citation recommendation evaluation are not compatible with our citation recommendation method. These datasets are not designed for published entity citation recommendation and contain non-entity citation data other than entity citations. The performance of our method will be inappropriately compromised when tested on non-entity citation recommendation, for it is not the field for our method. In addition, some published entities are used without citations in these datasets. If using our method to recommend citations for such entities with poor citation practices, the recommendation will be mistakenly determined as wrong.\r\n\r\nThe published scientific entity-papers mapping dataset is not suitable for the training of some SOTA methods. The dataset is particularly designed for recommending citations for published entities. It does not contain paper metadata (e.g., published years, published venues and author information) for the reason that the use of CRPSE does not need such metadata. But this metadata is indispensable to some SOTA methods.\r\n\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 34, "text": "There are two typical error types found in our method.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 35, "text": "\r\nErroneous extraction\r\nWe use scispaCy for entity extraction because it is a well-developed tool at an industrial-strength level. Nonetheless, extraction errors can still happen, especially in the processing of sentences with special symbols like mathematical expressions. We are looking for improvement in scispaCy to reduce these extraction errors.\r\n\r\nOver-screening\r\nThe threshold for an entity in S2ORC to be included in our dataset is to have a cooccurrence count of 20 at least with any of its candidate source papers. Some published entities that are newly-proposed or less-mentioned are thus screened out by this threshold as these entities have rather low citations. We are working on a solution to address this problem of over-screening.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 47, "text": "As shown in Fig.\u00a0REF , most of the entities with missing citations are models and algorithms, accounting for more than half of the total. Models and algorithms are at the core of computer science. The great proportion of models and algorithms with missing citations naturally speaks to their extensiveness in this field, i.e. most research in computer science is proposing models and algorithms as results. K-means is a fine example. This famous algorithm is presented by [38] in 1967. Later, k-means become one of the basic knowledge that every researcher in the field of machine learning knows it. They use this algorithm to process data or develop new methods. But as it becomes overly popular, some researchers use it without citation. Datasets are also in the same situation. Over one-fourth of the published entities without citations are found to be datasets. Datasets are applied to test the performance of models and algorithms. Some datasets are used extensively and gradually become gold standards for certain tasks. In this process, some researchers take these gold standard datasets for granted and use them without citations. Two prominent examples are the Penn Treebank\u00a0[39] and LibriSpeech\u00a0[44].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 50, "text": "In this paper, we propose a new method Citation Recommendation for Published Scientific Entity (CRPSE). Experimental results demonstrate that this method is effective in recommending source papers for published scientific entities. We further conduct a statistical analysis on missing citations of published scientific entities using CRPSE in the detection of missing citations. With analysis on open access papers from prestigious computer science conferences in 2020, we find that missing citations of published scientific entities are quite common, even for papers published in top computer science conferences. Most of the published scientific entities with missing citations are models and algorithms. As measured by the median, papers that propose published scientific entities with missing citations have been published for 8 years. This finding reveals the amount of time needed for a published scientific entity to develop into widely accepted knowledge to the point that researchers find it needless to give it citation in the domain of computer science. In view of the large amount of entities suffering from missing citations detected in our research, we call for proper citation practices for all published scientific entities to satisfy academic criteria.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 29, "text": "We use the sentence in Fig.\u00a0REF , \u201cBERT, formally published at NAACL-HLT 2019, leads to a significant change of NLP\u201d as an example of using CRPSE for citation recommendation. First, using scispaCy, entities BERT, NAACL-HLT and NLP are detected. Through the checking in the constructed published scientific entity-papers mapping dataset, only BERT is included in this dataset. Therefore, only the entity BERT will be passed to the next step for recommendation. When \\(K\\)  is set to be 5 which means 5 papers are required to be recommended and the cooccurrence count-based sorting criterion is used, the recommended papers for the entity BERT in order are \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u201d\u00a0[15], \u201cDeep Contextualized Word Representations\u201d\u00a0[47], \u201cAttention Is All You Need\u201d\u00a0[56], \u201cImproving Language Understanding by Generative Pre-Training\u201d\u00a0[48] and \u201cXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u00a0[65]. Among these papers, the top one is the source paper proposing the entity BERT and the other four papers are also related to the entity BERT.\r\n", "annotation": {"entities": {"c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#030c4afb-81c8-4f15-9db9-383018094ea6", "surface_form": "an example", "start": 120, "end": 130}]}, "a6": {"id": "a6", "type": "a", "subtype": null, "surface_forms": [{"id": "#df81c776-4cb3-4774-993f-2575fe626db8", "surface_form": "CRPSE", "start": 140, "end": 145}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#c1fb074f-a939-492c-8831-606351fad9f1", "surface_form": "K", "start": 467, "end": 468}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#c9401f66-d139-41d5-ab04-3ac6baa15322", "surface_form": "5", "start": 485, "end": 486}]}}, "relations": {"r2": {"id": "r2", "source": "v1", "target": "p1", "evidences": [{"id": "#68d59be5-48fa-45dd-84f7-a165e0da38dd", "source_surface_form": "#c9401f66-d139-41d5-ab04-3ac6baa15322", "target_surface_form": "#c1fb074f-a939-492c-8831-606351fad9f1", "evidence_sentence": "When \\(K\\)  is set to be 5 which means 5 papers are required to be recommended and the cooccurrence count-based sorting criterion is used, the recommended papers for the entity BERT in order are \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u201d\u00a0[15], \u201cDeep Contextualized Word Representations\u201d\u00a0[47], \u201cAttention Is All You Need\u201d\u00a0[56], \u201cImproving Language Understanding by Generative Pre-Training\u201d\u00a0[48] and \u201cXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u00a0[65].", "start": 460, "end": 978}]}, "r0": {"id": "r0", "source": "p1", "target": "a6", "evidences": [{"id": "#088e9f09-e424-4dd8-8eeb-2680da950c8f", "source_surface_form": "#c1fb074f-a939-492c-8831-606351fad9f1", "target_surface_form": "#df81c776-4cb3-4774-993f-2575fe626db8", "evidence_sentence": "REF , \u201cBERT, formally published at NAACL-HLT 2019, leads to a significant change of NLP\u201d as an example of using CRPSE for citation recommendation. First, using scispaCy, entities BERT, NAACL-HLT and NLP are detected. Through the checking in the constructed published scientific entity-papers mapping dataset, only BERT is included in this dataset. Therefore, only the entity BERT will be passed to the next step for recommendation. When \\(K\\)  is set to be 5 which means 5 papers are required to be recommended and the cooccurrence count-based sorting criterion is used, the recommended papers for the entity BERT in order are \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u201d\u00a0[15], \u201cDeep Contextualized Word Representations\u201d\u00a0[47], \u201cAttention Is All You Need\u201d\u00a0[56], \u201cImproving Language Understanding by Generative Pre-Training\u201d\u00a0[48] and \u201cXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u00a0[65].", "start": 28, "end": 978}]}, "r1": {"id": "r1", "source": "c1", "target": "v1", "evidences": [{"id": "#ead5a6d1-149b-4ec2-8d0f-b38c5d30f233", "source_surface_form": "#030c4afb-81c8-4f15-9db9-383018094ea6", "target_surface_form": "#c9401f66-d139-41d5-ab04-3ac6baa15322", "evidence_sentence": "REF , \u201cBERT, formally published at NAACL-HLT 2019, leads to a significant change of NLP\u201d as an example of using CRPSE for citation recommendation. First, using scispaCy, entities BERT, NAACL-HLT and NLP are detected. Through the checking in the constructed published scientific entity-papers mapping dataset, only BERT is included in this dataset. Therefore, only the entity BERT will be passed to the next step for recommendation. When \\(K\\)  is set to be 5 which means 5 papers are required to be recommended and the cooccurrence count-based sorting criterion is used, the recommended papers for the entity BERT in order are \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u201d\u00a0[15], \u201cDeep Contextualized Word Representations\u201d\u00a0[47], \u201cAttention Is All You Need\u201d\u00a0[56], \u201cImproving Language Understanding by Generative Pre-Training\u201d\u00a0[48] and \u201cXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u00a0[65].", "start": 28, "end": 978}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 1, "text": "Organizing scientific information into structured knowledge bases requires information extraction (IE) about scientific entities and their relationships.\r\nHowever, the challenges associated with scientific IE are greater than for a general domain. First, annotation of scientific text requires domain expertise which makes annotation costly and limits resources. In addition, most relation extraction systems are designed for within-sentence relations. However, extracting information from scientific articles requires extracting relations across sentences.\r\nFigure\u00a0REF  illustrates this problem. The cross-sentence relations between some entities can only be connected by entities that refer to the same scientific concept, including generic terms (such as the pronoun it, or phrases like our method) that are not informative by themselves. With co-reference, context-free grammar can be connected to MORPA through the intermediate co-referred pronoun it.\r\nApplying existing IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base).\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 2, "text": "In this paper, we\r\ndevelop a unified learning model for extracting scientific entities, relations, and coreference resolution.\r\nThis is different from previous work\u00a0[28], [16], [41], [12] which often addresses these tasks as independent components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links.\r\nSpecifically, we extend prior work for learning span representations and coreference resolution [24], [17].\r\nDifferent from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans.\r\nIt\r\navoids cascading errors between tasks by jointly modeling all spans and span-span relations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 4, "text": "In summary we make the following contributions. We create a dataset for scientific information extraction by jointly annotating scientific entities, relations, and coreference links. Extending a previous end-to-end coreference resolution system, we develop a multi-task learning framework that can detect scientific entities, relations, and coreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 6, "text": "More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17\u00a0[7] includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synonym-of. SemEval 18\u00a0[12] is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models\u00a0[3], [2], [28], [8] are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints.\r\nDifferent from most previous IE systems for scientific literature and general domains\u00a0[30], [44], [31], [35], [29], [1], which use preprocessed syntactic, discourse or coreference features as input,\r\nour unified framework does not rely on any pipeline processing and is able to model overlapping spans.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 7, "text": "While Singh2013JointIO show improvements by jointly modeling entities, relations, and coreference links,\r\nmost recent neural models for these tasks focus on single tasks [9], [43], [24], [23], [31] or joint entity and relation extraction [21], [46], [1], [47]. Among those studies, many papers assume the entity boundaries are given, such as [9], adel2017global and peng2017cross.\r\nOur work relaxes this constraint and predicts entity boundaries by\r\noptimizing over all possible spans.\r\nOur model draws from recent end-to-end span-based models for coreference resolution [24], [25] and semantic role labeling [17] and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 8, "text": "Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations [10], [22], [27], [26], [36], while peng2017cross uses high-order cross-task factors. Our model instead propagates cross-task information via span representations, which is related to Swayamdipta2017FrameSemanticPW.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 9, "text": "Our dataset (called SciERC) includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts. These abstracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Semantic Scholar CorpusThese conferences include general AI\u00a0(AAAI, IJCAI), NLP\u00a0(ACL, EMNLP, IJCNLP), speech\u00a0(ICASSP, Interspeech), machine learning\u00a0(NIPS, ICML), and computer vision\u00a0(CVPR, ICCV, ECCV) at http://labs.semanticscholar.org/corpus/.\r\nSciERC extends previous datasets in scientific articles SemEval 2017 Task 10 (SemEval 17)\u00a0[7] and SemEval 2018 Task 7 (SemEval 18)\u00a0[12] by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links. Our dataset is publicly available at: http://nlp.cs.washington.edu/sciIE/.\r\nTable\u00a0REF  shows the statistics of SciERC.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 14, "text": "We develop a unified framework (called SciIE) to identify and classify scientific entities, relations, and coreference resolution across sentences. SciIE is a multi-task learning setup that extends previous span-based models for coreference resolution [24] and semantic role labeling [17]. All three tasks of entity recognition, relation extraction, and coreference resolution\r\nare treated as multinomial classification problems with shared span representations.\r\nSciIE benefits from expressive contextualized span representations as classifier features. By sharing span representations, sentence-level tasks can benefit from information propagated from coreference resolution across sentences, without increasing the complexity of inference.\r\nFigure REF  shows a high-level overview of the SciIE multi-task framework.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 16, "text": "Entity recognition\r\nis to predict the best entity type for every candidate span.\r\nLet \\(L_{\\text{E}}\\)  represent the set of all possible entity types including the null-type \\(\\epsilon \\) . The output structure \\(E\\)  is a set of random variables indexed by spans:\r\n\\(e_i \\in L_{\\text{E}}\\)  for \\(i = 1 ,\\ldots , N\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 17, "text": "Relation extraction\r\nis to predict the best relation type given an ordered pair of spans \\((s_i,s_j)\\) .\r\nLet \\(L_{\\text{R}}\\)  be the set of all possible relation types including the null-type \\(\\epsilon \\) . The output structure \\(R\\)  is a set of random variables indexed over pairs of spans \\((i, j)\\)  that belong to the same sentence:\r\n\\(r_{ij}\\in L_{\\text{R}}\\)  for \\( i,j = 1, \\ldots , N\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 18, "text": "Coreference resolution is to predict the best antecedent (including a special null antecedent) given a span, which is the same mention-ranking model used in Lee2017EndtoendNC.\r\nThe output structure \\(C\\)  is a set of random variables defined as:\r\n\\(c_i \\in \\lbrace 1,\\ldots , i-1, \\epsilon \\rbrace \\)  for \\(i= 1,\\ldots , N\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 33, "text": "We construct a scientific knowledge graph from a large corpus of scientific articles. The corpus includes all abstracts (110k in total) from 12 AI conference proceedings from the Semantic Scholar Corpus. Nodes in the knowledge graph correspond to scientific entities. Edges correspond to scientific relations between pairs of entities. The edges are typed according to the relation types defined in Section\u00a0. Figure\u00a0REF  shows a part of a knowledge graph created by our method. For example, Statistical Machine Translation (SMT) and grammatical error correction are nodes in the graph, and they are connected through a Used-for relation type.\r\nIn order to construct the knowledge graph for the whole corpus, we first apply the SciIE model over single documents and then integrate the entities and relations across multiple documents (Figure\u00a0REF ).\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 55, "text": "We evaluate our unified framework SciIE on SciERC and SemEval 17. The knowledge graph for scientific community analysis is built using the Semantic Scholar Corpus (110k abstracts in total).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 56, "text": "We compare our model with the following baselines on SciERCdataset:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 57, "text": "\r\nLSTM+CRF The state-of-the-art NER system\u00a0[23], which applies CRF on top of LSTM for named entity tagging, the approach has also been used in scientific term extraction\u00a0[28].\r\n\r\nLSTM+CRF+ELMo LSTM+CRF with ELMo as an additional input feature.\r\n\r\nE2E Rel State-of-the-art joint entity and relation extraction system\u00a0[30] that has also been used in scientific literature\u00a0[32], [7]. This system\r\nuses syntactic features such as part-of-speech tagging and dependency parsing.\r\n\r\nE2E Rel(Pipeline) Pipeline setting of E2E Rel. Extract entities first and use entity results as input to relation extraction task.\r\n\r\nE2E Rel+ELMo E2E Rel with ELMo as an additional input feature.\r\n\r\nE2E Coref State-of-the-art coreference system Lee2017EndtoendNC combined with ELMo. Our system SciIE extends E2E Coref with multi-task learning.\r\n\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 61, "text": "Table\u00a0REF  compares the result of our model with baselines on the three tasks: entity recognition (Table\u00a0REF ), relation extraction (Table\u00a0REF ), and coreference resolution (Table\u00a0REF ). As evidenced by the table, our unified multi-task setup SciIE outperforms all the baselines.\r\nFor entity recognition, our model achieves 1.3% and 2.4% relative improvement over LSTM+CRF with and without ELMo, respectively.\r\nMoreover, it achieves 1.8% and 2.7% relative improvement over E2E Rel with and without ELMo, respectively. For relation extraction, we observe more significant improvement with 13.1% relative improvement over E2E Rel and 7.4% improvement over E2E Rel with ELMo. For coreference resolution, SciIE outperforms E2E Coref with 4.5% relative improvement. We still observe a large gap between human-level performance and a machine learning system. We invite the community to address this challenging task.\r\n{TABLE}{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 62, "text": "We evaluate the effect of multi-task learning in each of the three tasks defined in our dataset.\r\nTable\u00a0REF  reports the results for individual tasks when additional tasks are included in the learning objective function.\r\nWe observe that performance improves with each added task in the objective.\r\nFor example, Entity recognition (65.7) benefits from both coreference resolution (67.5) and relation extraction (66.8). Relation extraction (37.9) significantly benefits when multi-tasked with coreference resolution (7.1% relative improvement). Coreference resolution benefits when multi-tasked with relation extraction, with 4.9% relative improvement.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 63, "text": "Table\u00a0REF  compares the results of our model with the state of the art on the SemEval 17 dataset for tasks of span identification, keyphrase extraction and relation extraction as well as the overall score. Span identification aims at identifying spans of entities. Keyphrase classification and relation extraction has the same setting with the entity and relation extraction in SciERC. Our model outperforms all the previous models that use hand-designed features.\r\nWe observe more significant improvement in span identification than keyphrase classification. This confirms the benefit of our model in enumerating spans (rather than BIO tagging in state-of-the-art systems).\r\nMoreover, we have competitive results compared to the previous state of the art in relation extraction. We observe less gain compared to the SciERC dataset mainly because there are no coference links, and the relation types are not comprehensive.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 135, "text": "In this paper, we create a new dataset and develop a multi-task model for identifying entities, relations, and coreference clusters in scientific articles.\r\nBy sharing span representations and leveraging cross-sentence information, our multi-task setup effectively improves performance across all tasks.\r\nMoreover, we show that our multi-task model is better at predicting span boundaries and outperforms previous state-of-the-art scientific IE systems on entity and relation extraction, without using any hand-engineered features or pipeline processing.\r\nUsing our model, we are able to automatically organize the extracted information from a large collection of scientific articles into a knowledge graph.\r\nOur analysis shows the importance of coreference links in making a dense, useful graph.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 136, "text": "We still observe a large gap between the performance of our model and human performance, confirming the challenges of scientific IE.\r\nFuture work includes improving the performance using semi-supervised techniques and providing in-domain features. We also plan to extend our multi-task framework to information extraction tasks in other domains.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 0, "text": "Information extraction systems are a key component in making scientific literature more consumable.\r\nWith the large amount of scientific works which are constantly being published (e.g., more than 60,000 machine learning papers per year [4]), indexing techniques that go beyond keyword searches are becoming more important.\r\nWhile many efforts have focused on the processing of abstracts as a way of building representations of publications [5], [7], methods processing full text documents will be needed to accurately capture their contents for use cases such as academic search and recommender systems and scientific impact quantification.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 1, "text": "The task tackled in this paper, consisting of linking mathematical symbols to their descriptions in LaTeX documents, is a joint entity and relation extraction task.\r\nWhile earlier work tackled both subtasks sequentially via separate models, more recent approaches tend to use a single joint model [7], [0], [8], [3].\r\nIn contrast to early approaches, which are based on Bi-LSTMs [7], [0], [8], more recent approaches [11], [3] make use of transformer-based language models, such as BERT [2].\r\nA key challenge in joint models is the computational complexity stemming from pairwise comparisons between entity spans required for relation extraction.\r\nPrevious works tackle this using a span scoring mechanism based on a feed forward neural network, which produces a score indicating the likelihood that a span is in a relation [7], [11].\r\nRelation extraction is then performed on only those spans with the highest scores.\r\nFor data sets which include span annotations even for entities which are not in any relation, such as DocRED [14], as examined by [3], such a scoring mechanism is not necessary, because the entity extraction component of the model can be trained on these annotations.\r\nFor the task tackled in this paper, complete annotations for entity spans are not provided, making the use of a span scoring mechanism necessary.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 2, "text": "In this paper, we propose an end-to-end approach for joint entity and relation extraction. The approach is based on a transformer-based language model, following previous work [3], but is peculiar in the sense that it incorporates a span scoring mechanism based on dot product similarity to reduce the computational complexity considerably, making joint entity and relation extraction feasible on standard hardware.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 3, "text": "The task tackled in this paper is one of joint entity and relation extraction. This means, given an unannotated text as input, a system needs to (1)\u00a0return annotations of relevant entity mention spans, (2)\u00a0perform coreference resolution, (3)\u00a0entity type classification, and finally (4)\u00a0relation extraction on the identified spans.\r\nThe specific task at hand has a number of key features that separate it from similar\r\nsettings.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 4, "text": "First, regarding entity extraction, the annotations and, thus, the final scoring are restricted to those entities which participate in relations.\r\nThis means that a system which correctly identifies all symbols and descriptions in the input will score poorly even on the entity extraction portion of the final benchmark if the relation extraction is incorrect.\r\nMore importantly from an engineering perspective, the resulting span annotations are incomplete in that they only include a partial set of valid spans for each document.\r\nIn the entity extraction step we can, therefore, only reliably identify true positives and false negatives, not, however, false positives and true negatives.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 5, "text": "Second, while coreference resolution (i.e., the linking of multiple mentions to a single entity) is part of the task, relation extraction is to be performed on a mention-level rather than the entity-level.\r\nThis means that although a system may correctly identify a text span as being the description of a certain symbol, this classification will only be deemed correct in the evaluation if linked to the correct mention of said symbol.\r\nAs a result, coreference links are interpreted as relations between mentions and thereby as part of the relation extraction subtask, rather than as part of the entity extraction subtask.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 6, "text": "Third, entity types can be reliably inferred from the relations between them, meaning that instances of relations are only found between certain entity types.\r\nThis feature can be used to inform the design of a system in two ways: Either, the task of relation extraction can be simplified by reducing the choices given to a classifier based on the entity types of two spans (i.e., a symbol cannot be the description to another symbol, therefore any such prediction can be disregarded), or the entity type classification can be informed by the relation extraction (i.e., if we identify a span \\(A\\)  as the description of another span \\(B\\) , span \\(A\\)  must be of type \"PRIMARY\" or \"ORDERED\", while span \\(B\\)  must have the type \"SYMBOL\").\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 7, "text": "We propose an end-to-end entity and relation extraction system using a transformer-based language model, as illustrated in figure REF .\r\nThe system consists of 4 modules: (1) The input encoding module tokenizes the input text and produces contextualized embeddings for each token, (2) the soft mention detection module ranks possible token spans by the likelihood with which they contain an entity mention, (3) the relation extraction module extracts relations on a subset of the highest ranked spans from the previous step, and finally (4) the entity type classification module assigns entity types to spans based on the relations detected between them.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 8, "text": "We examine two separate options of encoding the input:\r\nFor the first option, we pass the input text to the language\r\nmodel without prior modification, whereas for the second option, we\r\nperform preprocessing on the input to remove LaTeX code from the text portions of the input. Any\r\ninput in LaTeX math mode is passed to the model\r\nunchanged.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 9, "text": "Since our approach uses a transformer-based language model, the input needs to be tokenized.As a result of the tokenization, there are instances of relations which cannot be matched correctly by our model, due to the annotated span boundaries being contained within a token.\r\nFor the training and development sets, this occurs in 1.99% and 2.84% of relation instances, respectively, and in these cases we adjust the labels accordingly.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 10, "text": "Given that we cannot reliably identify false positives and true negatives from our labeled data, a mention detection strategy based on cross-entropy loss cannot be used for this task.\r\nInstead of following previous approaches in using feed-forward neural networks [7], [11], we propose a linear similarity based approach which ranks possible spans based on their similarity to multiple prototype embeddings (one prototype per entity type).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 11, "text": "We begin by computing the set of all possible continuous spans up to a maximum length \\(n\\)  and produce a fixed-size embedding \\(e_{s}\\)  for each span by pooling the contextualized embeddings of all tokens within it.\r\nAs pooling strategies we use either mean or max pooling.\r\nFor each span embedding \\(e_{s}\\)  we compute a span score \\(X_{s}\\) :\r\n\\(X_{s} = \\max _{a_{i} \\in A} (sim(e_{s}, a_{i}))\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 12, "text": "where \\(A\\)  is the set of prototype embeddings which contains an embedding for each entity type and \\(sim(a,b)\\)  is the dot product similarity of two vectors.\r\nWe select the \\(k\\)  spans with the highest values for \\(X_{s}\\)  as our candidate mentions \\(M\\)  for relation extraction.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 14, "text": "where \\(p_{label}\\)  is the prototype embedding corresponding to the entity type of \\(e_{true,i}\\) .\r\nWe define the mention loss \\(L_{m}\\)  as the mean value of all \\(l_{m,ij}\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 13, "text": "We use the following loss function to learn the soft mention detection:\r\nFor each \\(e_{true,i} \\in M_{true}\\)  and each \\(e_{false,j} \\in M_{true}^{c}\\) , where \\(M_{true}\\)  contains all spans in \\(M\\)  for which we know that they are correct, we compute\r\n\\(l_{m,ij} = triplet\\_loss(p_{label}, e_{true,i}, e_{false,j})\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 15, "text": "For relation extraction, we use the concatenation of two span representations as a representation for the relation between them [12].\r\nThe resulting relation representations are compared to a single relation prototype embedding per relation type, as well as \\(m\\)  additional prototypes representing the none-of-the-above class (this follows the MNAV model [9]).\r\nThe relation type corresponding to the prototype resulting in the highest dot product similarity for a relation representation is used as the predicted type.\r\nAs loss function for the relation classification we use adaptive thresholding loss [15] as it is capable of handling the large imbalance between positive and negative training examples present in document-level relation extraction tasks.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 16, "text": "Due to quadratic scaling of the pairwise comparisons it is not feasible to perform relation extraction on all possible continuous spans.\r\nWe, therefore, perform relation classification on the top \\(k\\)  spansDuring training we add annotated spans which are not among the top \\(k\\)  spans. with the highest span scores, meaning that we have to classify a maximum of \\(k(k-1)\\)  relation representations for a given input text.\r\nThe computational complexity of the system can, therefore, be adjusted dynamically at inference time by changing \\(k\\) , for example to be run on GPUs with smaller memory capacity or on GPUs with higher memory capacity to improve the quality of predictions.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 17, "text": "As a result of the soft mention detection, it is possible that some of the \\(k\\)  spans are overlapping and correspond to the same target.\r\nThis means that the relation classifier may output multiple predictions for the same relation instance with slightly different mention spans.\r\nFor predictions in which both the head and tail entity overlap, we therefore output only the prediction with the highest classification score.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 18, "text": "Finally, we use a simple mapping to determine the entity type of the spans which participate in the relations predicted by the relation classifier.\r\nThe mapping used can be found in appendix REF . For spans classified as \"PRIMARY\" we additionally change the predicted type to \"ORDERED\", if they are the head entity of more than one \"Direct\" relation.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 19, "text": "For our language model we use SciBERT [1], which is trained on scientific text, via Huggingface's Transformers library [13].\r\nFor LaTeX preprocessing (see section REF ) we use Pylatexenchttps://github.com/phfaist/pylatexenc.\r\nAs our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.\r\nDuring training, we randomly downsample the amount of candidate spans for soft mention detection to 1000, while ensuring that all labeled spans are included. During training and development set evaluation, we set \\(k\\) , the number of spans to perform relation classification on, to 50, as preliminary experiments showed this value to yield a good compromise between model performance and training time. For test set evaluation we increase \\(k\\)  to 400.\r\nTraining takes approximately 10 hours on a single NVIDIA V100 GPU using mixed precision.\r\nWe perform early stopping based on the micro \\(F_1\\)  score for relation extraction on the development set.\r\nWe train each hyperparameter configuration 3 times using different random seeds and report the median and standard deviation for each metric.\r\nAs a result of the different combinations of preprocessing and mean-/max-pooling, we examine the performance of 4 configurations on the test set.\r\nFor our evaluation, we report the micro \\(F_1\\)  scores for NER metrics as used in SemEval-2013 Task 9.1 [10]We use the following implementation: https://github.com/davidsbatista/NER-Evaluation.\r\nFor relation extraction we report micro precision, recall and \\(F_1\\)  scores, unless otherwise indicated.\r\n", "annotation": {"entities": {"a16": {"id": "a16", "type": "a", "subtype": null, "surface_forms": [{"id": "#d0d6e2b8-6d57-44a7-9ec0-3b90cd6af918", "surface_form": "AdamW", "start": 249, "end": 254}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#955bb94b-5764-4c59-8df3-0f906a357e16", "surface_form": "learning rates", "start": 264, "end": 278}]}, "v1": {"id": "v1", "type": "v", "subtype": "s", "surface_forms": [{"id": "#d783e7cb-c4d2-47b6-a360-5a3f20d55830", "surface_form": "[3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]", "start": 285, "end": 339}]}, "a17": {"id": "a17", "type": "a", "subtype": null, "surface_forms": [{"id": "#88aab9ee-f7ca-4d1f-8cd7-4102473a9f58", "surface_form": "linear warmup", "start": 346, "end": 359}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ee80ef66-41be-4b59-8cb0-08a7f2737e8b", "surface_form": "1", "start": 363, "end": 364}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#992db906-f9f6-40eb-8a50-e9063d38e606", "surface_form": "epoch", "start": 365, "end": 370}, {"id": "#533956e4-dffd-4cb4-a495-7b5cbb6afa8b", "surface_form": "epochs", "start": 425, "end": 431}]}, "a18": {"id": "a18", "type": "a", "subtype": null, "surface_forms": [{"id": "#441a074e-3e57-4a52-8e08-9c256fbc3049", "surface_form": "linear decay", "start": 385, "end": 397}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#fa39eacf-977e-4f37-846f-d0fd9fa8a2f0", "surface_form": "60", "start": 422, "end": 424}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#fbcd82a7-df05-4a3e-972b-66e41a20b48b", "surface_form": "batch size", "start": 521, "end": 531}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3f0da8bb-f5bd-4a6a-93d5-de113eadbf17", "surface_form": "4", "start": 535, "end": 536}]}, "a19": {"id": "a19", "type": "a", "subtype": null, "surface_forms": [{"id": "#cca9d1b5-ad37-42a4-ae1b-2e07875dba98", "surface_form": "gradient clipping", "start": 548, "end": 565}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#2c83b01d-2970-47e1-928f-2258fd5b2e08", "surface_form": "max norm", "start": 573, "end": 581}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#092d7da1-0de2-4b0e-8afb-4bec277f0ed5", "surface_form": "1", "start": 585, "end": 586}]}}, "relations": {"r13": {"id": "r13", "source": "v1", "target": "p2", "evidences": [{"id": "#904b431c-868c-411b-ba43-dbd4fe4b364b", "source_surface_form": "#d783e7cb-c4d2-47b6-a360-5a3f20d55830", "target_surface_form": "#955bb94b-5764-4c59-8df3-0f906a357e16", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r0": {"id": "r0", "source": "p2", "target": "a16", "evidences": [{"id": "#d56fc5ba-b5b8-4464-a9fa-93edfba259b3", "source_surface_form": "#955bb94b-5764-4c59-8df3-0f906a357e16", "target_surface_form": "#d0d6e2b8-6d57-44a7-9ec0-3b90cd6af918", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r1": {"id": "r1", "source": "v2", "target": "p3", "evidences": [{"id": "#4298cbf3-7e3f-4d8d-acae-3fb1569f5f4e", "source_surface_form": "#ee80ef66-41be-4b59-8cb0-08a7f2737e8b", "target_surface_form": "#992db906-f9f6-40eb-8a50-e9063d38e606", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r2": {"id": "r2", "source": "p3", "target": "a17", "evidences": [{"id": "#2748db06-ef52-4a8f-922d-d78438cbad6f", "source_surface_form": "#992db906-f9f6-40eb-8a50-e9063d38e606", "target_surface_form": "#88aab9ee-f7ca-4d1f-8cd7-4102473a9f58", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r3": {"id": "r3", "source": "v4", "target": "p3", "evidences": [{"id": "#8e25fd99-23a7-4df4-89dc-554740f9e29e", "source_surface_form": "#fa39eacf-977e-4f37-846f-d0fd9fa8a2f0", "target_surface_form": "#533956e4-dffd-4cb4-a495-7b5cbb6afa8b", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r4": {"id": "r4", "source": "p3", "target": "a18", "evidences": [{"id": "#fc7356c2-ab3a-4e9e-baee-a8f04c39352b", "source_surface_form": "#533956e4-dffd-4cb4-a495-7b5cbb6afa8b", "target_surface_form": "#441a074e-3e57-4a52-8e08-9c256fbc3049", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r5": {"id": "r5", "source": "v7", "target": "p4", "evidences": [{"id": "#f1ba8d59-78d2-403b-93f0-0e9f75389e8c", "source_surface_form": "#3f0da8bb-f5bd-4a6a-93d5-de113eadbf17", "target_surface_form": "#fbcd82a7-df05-4a3e-972b-66e41a20b48b", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r6": {"id": "r6", "source": "v8", "target": "p5", "evidences": [{"id": "#7f8fbdd3-3c87-4efb-aace-62e58e465a0f", "source_surface_form": "#092d7da1-0de2-4b0e-8afb-4bec277f0ed5", "target_surface_form": "#2c83b01d-2970-47e1-928f-2258fd5b2e08", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r7": {"id": "r7", "source": "p5", "target": "a19", "evidences": [{"id": "#00b84d45-12b3-4e8f-8215-d3e215a7fb1f", "source_surface_form": "#2c83b01d-2970-47e1-928f-2258fd5b2e08", "target_surface_form": "#cca9d1b5-ad37-42a4-ae1b-2e07875dba98", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}, "r8": {"id": "r8", "source": "p4", "target": "a16", "evidences": [{"id": "#325342fc-1257-4cbe-a45d-971f59b0131b", "source_surface_form": "#fbcd82a7-df05-4a3e-972b-66e41a20b48b", "target_surface_form": "#d0d6e2b8-6d57-44a7-9ec0-3b90cd6af918", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 20, "text": "The results of the 4 model configurations on the test set are reported in table REF .\r\nIn comparison to the other approaches taking part in SemEval-2022 Task 12, our system ranks in place 3/9 in terms of relation extraction \\(F_1\\)  score.Scores for other metrics are not publicly visible on the leaderboard at the time of writing.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 21, "text": "In general, we find that our model produces predictions with significantly higher precision than recall.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 22, "text": "With respect to the preprocessing procedure, we observe no clear performance impact.\r\nWe conclude that SciBERT appears to cope well with LaTeX code and preprocessing, as described in this paper, is not required.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 23, "text": "Regarding the pooling procedures we find that mean pooling tends to cause higher variability in the classification performance of the models.\r\nFor the models trained using mean pooling and preprocessing, 1 of 3 models performed significantly worse than the others, causing the large standard deviation in the results.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 24, "text": "In table REF , we show the relation extraction \\(F_1\\)  scores for a model across the 4 different domains covered by the development set paired with the distribution of training data across domains.\r\nWe observe large performance differences depending on the domain with math and physics showing very high macro \\(F_1\\)  scores (79.17% / 95.43%) and computer science and economics performing poorly (19.23% / 13.84%).\r\nWhile physics content does represent the majority of training examples, the distribution of domains across training examples does not fully explain the disparity.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 25, "text": "In figure REF , we show the change in relation extraction performance across different values for \\(k\\) .\r\nWe also include in the plot the percentage of entity spans in the top \\(k\\)  ranked spans (entity recall).\r\nWhile the relation extraction performance improves proportional to the entity recall for \\(k \\le 100\\)  the improvement slows down for higher \\(k\\) .\r\nWe hypothesize that this is due to the limiting of \\(k=50\\)  and the candidate span downsampling during training, which prevents the model from seeing some of the more difficult cases.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 26, "text": "In order to measure the impact of tokenization errors produced by adjusting labels during training, we perform a partial matching of relation labels as follows:\r\nFor predicted relation triples which are false positives, we accept them as true positives for an annotated instance if the intersection-over-union (IOU) scores of both head and tail entities are greater than 67% and the predicted relation type matches the label. In table REF  we show the results of both strict and partial matching for our best model on the development set.\r\nWe find that the relaxed requirements for span accuracy result in an increase in the \\(F_1\\)  score of 3.99%.\r\nWe conclude that tokenization errors, while measurable, do not account for the majority of errors of our model.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 27, "text": "In this paper, we present an end-to-end joint entity and relation extraction approach for linking mathematical symbols to their descriptions in LaTeX documents.\r\nOur model appears to be sensitive to the domain of the input documents, achieving high macro \\(F_1\\)  scores of 95.43% and 79.17% for physics and math content, respectively, while achieving macro \\(F_1\\)  scores of only 19.23% and 13.84% for computer science and economics related content.\r\nWe find that the model's predictions are higher in precision than in recall.\r\nWe perform a detailed error analysis and identify cross-domain generalization as the most critical problem to tackle in future work.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 28, "text": "The authors acknowledge support by the state of Baden-W\u00fcrttemberg through bwHPC.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 29, "text": "Table REF  shows the classification map used for determining entity types based on relations for SemEval-2022 Task 12.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 0, "text": "Citing is an important aspect of scientific discourse and important for quantifying the scientific impact quantification of researchers. Widely used importance metrics, such as the citation count and the h-index\u00a0[14], are based on citations.\r\nThey are sometimes used to judge the quality of research presented by an article\u00a0[25].\r\nHowever, several works have observed that publications are cited not only based on the pure scholarly contributions but also based on non-scholarly attributes such as gender, author affiliation, and funding.\r\nFor instance, articles authored by women might be under-cited\u00a0[3], [6], [34].\r\nSuch distortions concerning citations\u2014also called \u201ccitation bias\u201d\u2014can distort the perception of available scholarly contributions among users of publications\u00a0[16].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 1, "text": "While citation bias has been studied in regular journal articles\u00a0[6], [34], [36], [0], citation bias in preprints\u2014completed scientific manuscripts that are uploaded by the authors to a public server without formal review\u00a0[1]\u2014has not been investigated.\r\nHowever, preprints play an increasingly important role in modern scholarly communication.\r\nSeveral preprint servers have emerged within the last decades\u00a0[39], covering various disciplines: arXiv in physics, mathematics, and computer science, bioRxiv in biology, medRxiv in medicine, and SSRN in social science.\r\nVarious works have observed benefits of preprints, such as early disclosure, wider dissemination\u00a0[31] resulting in a higher number of citations\u00a0[5], [11], [9], and creating opportunities for collaborations\u00a0[19], [31], [28].\r\nIn the recent COVID-19 pandemic, preprints have received even greater scientific and public engagement\u00a0[10].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 2, "text": "In this paper, we investigate if preprints are affected by citation bias concerning the author affiliation.\r\nWe focus on the author affiliation, as a survey by Soderberg et al.\u00a0[32] observed that 35% of respondents consider the author's institution as extremely or very important to assess the credibility of preprints.\r\nTherefore, we assume that author affiliation has an influence on the citation counts of preprints.\r\nWe verify the existence of citation bias by computing citation inequality.\r\nTo this end, we measure to which degree the number of citations that preprints and their publisher versions receive is unequally distributed.\r\nSpecifically, we measure citation bias with regard to author affiliation on the institution level and country level.\r\nComparing differences in the citation inequality between preprints and their respective publisher versions allows us to mitigate the effects of confounding factors and see whether or not citation biases related to author affiliation have an increased effect on preprint citations.\r\nConclusions drawn from this type of investigation are based on the assumption that the process of peer-review and formal publication is generally perceived as an assurance of quality [25] and therefore \u201clevels the playing field\u201d among articles in terms of citability.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 3, "text": "We examine citation bias in bioRxiv, a preprint server in the field of biology, because preprints deposited to bioRxiv provide sufficient information regarding author affiliations.\r\nWe analyze citations of more than 36,000 preprints deposited between November 2013 (i.e., the launch of bioRxiv) and June 2019 and their publisher versions.\r\nWe use the COCI (OpenCitations Index of Crossref open DOI-to-DOI references)\u00a0[13] as citation data.\r\nTo measure citation inequality, we calculate the Gini coefficients \\(G\\) , following previous studies\u00a0[24], [8].\r\nIn our analysis, we can confirm a citation bias, especially for preprints at different affiliation levels (i.e., institutions, countries), as we find that preprints have twice the citation inequality as the publisher versions (e.g., \\(G=0.23\\)  for preprints and \\(G=0.12\\)  for publisher versions at the institution level).\r\nFurthermore, we observe larger citation inequalities for preprints than those for publisher versions in different journal types that are mega-journalsMega-journals are journals that solely focus on scientific trustworthiness\u00a0[2] in the process of peer-review, compared to other journals., disciplinary journals, and prestigious journals (e.g., Nature and Science).\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 4, "text": "Preprints begin to be increasingly considered in various contexts, such as funding applications and recruitment. Therefore, citations of preprints gain in importance.\r\nGiven our results, funding agencies or referees are advised to be even more careful when they apply citation-based metrics to preprints for their assessment and judgment than applying to journal articles.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 5, "text": "The reminder of the paper is organized as follows: In the subsequent section, we describe the related works. Thereafter, we show the procedure of the data collection for the analysis in Section\u00a0 and the analysis methods in Section\u00a0. Section\u00a0 presents the results of the analysis. In Section\u00a0, we discuss and outline the analysis, its limitation, and future direction, before concluding the paper in Section\u00a0.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 6, "text": "We provide our dataset and source codes used for the analysis online\u00a0[26].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 7, "text": "In this section, we outline related works. We first describe studies related to preprint characteristics. Thereafter, we mention related analyses that investigate factors affecting citations and citation bias. Finally, we show different studies in terms of biases in academia.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 8, "text": "The advent of preprint servers has brought about a change in citation behavior. For instance, researchers who no longer need the publication of papers from publishers for their career may skip the review process and no longer publish in journals of publishers [17].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 9, "text": "Soderberg et al.\u00a0[32] conducted a survey of\r\nalmost 4,000\r\nresearchers across different disciplines to determine the importance of different cues for assessing the credibility of preprints and preprint services.\r\nThey found that cues related to information about open science content (e.g., links to available material, data, and scripts) as well as independent verification of author claims (e.g., information about independent reproductions) were rated as highly important for judging preprint credibility. In comparison, peer views and author information were rated as less important.\r\n35% of respondents marked the author's institution as extremely or very important, and 28% of respondents answered moderately important.\r\nThis motivated us to consider citation bias with respect to the author affiliations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 10, "text": "Tahamtan et al.\u00a0[33] outlined factors that affect the number of citations. Specifically, they identified three categories with 28 factors to be related to the number of citations: paper related factors (e\u00a0.g., quality of paper, document type), journal related factors (e\u00a0.g., journal impact factor), and author(s) related factors (e\u00a0.g., number of authors). They concluded that some factors, such as the journal impact factor, international cooperation, and number of authors, are more strongly correlated with the number of citations than the other factors.\r\nThese factors include non-scholarly attributes such as gender, author affiliation, and funding.\r\nThis phenomenon of inequality can be referred to as \u201ccitation bias,\u201d and it distorts the perception of available scholarly contributions among users of articles\u00a0[16].\r\nSeveral studies found that papers authored by women might be under-cited\u00a0[3], [6], [34], while Copenheaver et al.\u00a0 [4] did not observe citation bias based on gender.\r\nLou and He\u00a0[22] observed a significant negative correlation between the reputation of author affiliations (i\u00a0.e., rank of an affiliation at the U.S. News Best Global University Subject rankings) and uncitedness of journal articles.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 11, "text": "Citation bias has been analyzed in two main contexts in the literature: to explain the scholars' self-citation behavior [36], [0], and\r\nto show that scholars cite papers but disproportionally criticize papers or specific claims less often.\r\nBesides the citation bias, also other kinds of biases in academia have been studied.\r\nFor instance, Liang et al. [21] discussed the recommender systems' \u201cexposure problem,\u201d which can result in frequent recommendation of popular scientific articles.\r\nSalman et al.\u00a0[30] observed gender and racial biases and location-based biases in academic expert recommendations, used to find reviewers or to assemble a conference program committee.\r\nPolonioli et al.\u00a0[29] claimed\r\nthat recommender systems might put users in information bubbles by isolating them from exposure to different academic viewpoints, creating a self-reinforcing bias damaging to scientific progress. Finally, [12] found that scholarly recommender systems are biased as they underexpose users to equally relevant items.\r\nA paper that tackles the popularity bias of recommending scientific articles [38] won the Test of Time award at the KDD 2021 conference.https://kdd.org/awards/view/2021-sigkdd-test-of-time-award-winners, last accessed on 2021-12-14\r\nAll this shows that bias is an important and timely topic to consider.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 12, "text": "This section describes how we collect preprints, their respective publisher versions, and citation data.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 13, "text": "bioRxiv is a widely used preprint server in biology and provides\u2014in contrast to other preprint servers, such as arXiv\u2014easy access to the author affiliation information of the preprints.In arXiv, we need to extract author affiliations from the body of PDF or LaTex. On the other hand, bioRxiv provides metadata and text in a unified JATS format, which makes easy to retrieve author affiliations.\r\nWe therefore harvest metadata of all bioRxiv preprints submitted between November 2013 (i.e., the launch of bioRxiv) and June 2019 via the bioRxiv API.https://api.biorxiv.org/, last accessed on 2022-04-29\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 14, "text": "We harvest metadata until June 2019 to ensure that all preprints and their respective publisher versions have at least a 24-month period to receive citations after publication of preprints.\r\nTherefore, this paper does not cover preprints and publisher versions that are related to COVID-19.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 15, "text": "In total, we retrieve 73,946 records.\r\nAfter removing duplicate records we obtain 73,920 records.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 16, "text": "Thereafter, using a bioRxiv metadata field \u201cJATS URL\u201d in the records, we download JATS\u00a0[15] XML files for each of the 73,920 records.\r\nOn bioRxiv, authors can update their preprints. Therefore, some submissions are available in several versions.\r\nIn this paper, we only use the metadata and JATS XML files of the first version of a preprint, as we assume that metadata, such as the author information, do not change between versions.\r\nFollowing above steps, we acquire metadata and JATS XML files of 53,240 preprints.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 17, "text": "We identify the publisher version of a preprint using a bioRxiv metadata field that provides a link to the publisher version. These links, DOIs in most cases, are identified and stored as bioRxiv metadata automatically whenever a corresponding author confirms the publication of a preprint via email.https://www.biorxiv.org/about/FAQ, last accessed on 2021-12-14\r\nWe fetch the metadata of publisher versions, such as journal informationWe use the fields journal_name and journal_issn_l. and publication month,We use the field published_date that corresponds to the field created in Crossref. from Unpaywall, using the DOIs.\r\nWe filter out 50 publisher versions whose publication month cannot be identified and 471 publisher versions that have been published before preprint publication.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 18, "text": "Following above procedure, we identify 36,651 pairs of a preprint and its respective publisher version.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 19, "text": "We obtain author information including affiliations from the JATS XML files of preprints.\r\nWe assume that the author information is identical in a preprint and its publisher version.\r\nWe specifically fetch the following information:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 21, "text": "There are 23 pairs where author information is unavailable. We exclude them from the analysis. Thus, we finally get 36,628 pairs of a preprint and its respective publisher version.\r\nFor the 36,628 pairs, there are 260,231 authors.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 22, "text": "Author affiliations appear in different variations (e\u00a0.g., \u201cMIT\u201d and \u201cMassachusetts Institute of Technology\u201d) in JATS XML files.\r\nWe normalize author affiliations using the Research Organization Registry (ROR)\u00a0[20].\r\nThe ROR is a community-led registry of identifiers for research organizations.https://ror.org/about/, last accessed on 2021-12-14\r\nIt provides an API, which allows to retrieve, search, and filter the organizations indexed in the ROR.\r\nWe identify the corresponding ROR entity for each author affiliation string using this API.https://github.com/ror-community/ror-api, last accessed on 2021-12-14\r\nAlthough strings of some author affiliations are marked up with \u201cinstitution\u201d and \u201ccountry,\u201dAn example: <institution>University of Minnesota</institution>, Minneapolis, MN 55455, <country>USA</country> we used full strings of author affiliation as queries for consistency.\r\nThe ROR API returns a list of organizations that are matched to a query sorted by confidence scores.\r\nWe use the returned organization with the highest confidence score and if the field \u201cchosen\u201d (i.e., binary indicator of whether the score is high enough to consider the organization correctly matched) is true.\r\nFor the 260,231 authors, there are 335,188 affiliations. Among them, 273,804 affiliations (81.69%) can be linked to a ROR entity.\r\nIn our analysis, we consider citation bias on the institution and country level. Thus, we use the name and country information of the organizations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 24, "text": "In this section, we describe how we count the number of citations and how we identify citation bias.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 25, "text": "Following Thelwall\u00a0[35] and Fraser et al.\u00a0[11], we log-transform the number of citations of an article (i\u00a0.e., preprint, publisher version) after an addition of 1, to reduce the influence of articles with a high number of citations.\r\nFinally, we take the arithmetic mean of the log-transformed number of citations of all articles with respect to an affiliation (i.e., institution, country) as\r\n\\(c_{m} = \\frac{1}{n}\\sum _{i=1}^{n}log(c_i + 1),\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 27, "text": "We examine citation bias concerning author affiliations at institution level and country level by measuring to which degree the number of citations that preprints and their publisher versions receive is unequally distributed.\r\nWe assume that comparing differences in citation inequality between preprints and their publisher versions allows us to mitigate the effects of confounding factors and see whether or not citation biases related to author affiliation have an increased effect on preprint citations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 28, "text": "Specifically, we plot the Lorenz curve and calculate the Gini coefficient\u00a0[7] to measure inequality in the number of citations, as used by authors of similar studies, such as Nielsen and Andersen\u00a0[24].\r\nIn this paper, the Lorenz curve presents the distribution of citations accumulated across different affiliations, where it shows for the bottom \\(x\\) % of affiliations, what percentage (\\(y\\) %) of the total number of citations they received. If the distribution of the number of citations for different affiliations is perfectly equal, the Lorenz curve is depicted by the straight line \\(y = x\\) .\r\nThe Gini coefficient is calculated as the ratio of the area between the line of perfect equality and the observed Lorenz curve to the area between the line of perfect equality and the line of perfect inequality.\r\nThe Gini coefficient can range from 0 to 1. A higher Gini coefficient indicates a high degree of inequality in the distribution.\r\n{FIGURE}{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 29, "text": "This section presents the results of our analysis.\r\nFirst, we show citations of preprints and their publisher versions.\r\nThereafter, Section\u00a0REF  presents citation inequality using the Lorenz curves and Gini coefficients. Sections\u00a0REF  and REF  verify the influence of duration between publications of preprint and publisher version and different journals on the results, respectively.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 30, "text": "We first show in Figure\u00a0REF  how the number of citations of preprints and publisher versions evolve over time, starting from the publication month of the preprint.\r\nIn the upper graph of the figure, we observe an acceleration of the number of citations of preprints within the first 10 months following publications, and an approximate plateau between the months 10 and 24.\r\nOn the other hand, the number of citations of publisher versions rises continuously over 24 months.\r\nThe lower graph presents the number of preprints as well as publisher versions.\r\nWe see that over half of the preprints have published their publisher versions within 8 months after their preprint publication.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 31, "text": "Figure\u00a0REF  presents Lorenz curves with different affiliation levels (i\u00a0.e., institution or country) and target authors (i\u00a0.e., first author, last author, corresponding author, or all authors). Please note that we adopt fractional counting, where a co-authored article's citations are assigned fractionally to each of the co-authors' affiliations. In addition, if an author belongs to more than one affiliations, citations are apportioned to the affiliations.\r\nTo eliminate the influence of affiliations with a small number of articles, we filter out institutions and countries that publish fewer than 5 and 10 articles, respectively, which are equivalent to approximately 30% of preprints and their publisher versions.\r\nIf we include filtered-out institutions and countries in the analysis, we observe even larger inequalities in both preprints and publisher versions and disparities between Lorenz curves for preprints and publisher versions.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 32, "text": "In Figure\u00a0REF , we consistently observe larger citation inequalities in preprints than in publisher versions. Larger disparities between Lorenz curves for preprints and publisher versions are shown on the institution level than on the country level.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 23, "text": "As citation data, we used the COCI (OpenCitations Index of Crossref open DOI-to-DOI references)\u00a0[13].\r\nThe citation data of the COCI are originally from publishers, thus they are of high quality.\r\nWe used the COCI CSV dataset Version 11 released on 2021-09-03https://doi.org/10.6084/m9.figshare.6741422.v11, last accessed on 2021-12-14 that lists pairs of DOIs denoting citations.\r\nThe 36,628 preprints and their publisher versions receive 331,839 citations in total in the given 24 months.\r\n", "annotation": {"entities": {"a3": {"id": "a3", "type": "a", "subtype": null, "surface_forms": [{"id": "#ec609b6f-2693-4aca-a4fb-29fe4f2d24de", "surface_form": "COCI (OpenCitations Index of Crossref open DOI-to-DOI references)", "start": 30, "end": 95}, {"id": "#24eda2c4-7e11-4493-a3af-ac6337c7076c", "surface_form": "COCI", "start": 127, "end": 131}, {"id": "#5054f1af-a651-41a5-bb7e-42250b315968", "surface_form": "COCI CSV dataset", "start": 207, "end": 223}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#5dfeb3c5-9033-4d50-8af8-9c2a63c782ab", "surface_form": "Version", "start": 224, "end": 231}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#b2c6c971-c05c-472a-bb0d-a6a922fd7796", "surface_form": "11", "start": 232, "end": 234}]}}, "relations": {"r1": {"id": "r1", "source": "v1", "target": "p1", "evidences": [{"id": "#1bcee865-c9ad-42b4-99bc-110804f0bea1", "source_surface_form": "#b2c6c971-c05c-472a-bb0d-a6a922fd7796", "target_surface_form": "#5dfeb3c5-9033-4d50-8af8-9c2a63c782ab", "evidence_sentence": "We used the COCI CSV dataset Version 11 released on 2021-09-03https://doi.org/10.6084/m9.figshare.6741422.v11, last accessed on 2021-12-14 that lists pairs of DOIs denoting citations.", "start": 197, "end": 380}]}, "r0": {"id": "r0", "source": "p1", "target": "a3", "evidences": [{"id": "#0d138638-57d0-48b9-9a10-fc6bc658df7d", "source_surface_form": "#5dfeb3c5-9033-4d50-8af8-9c2a63c782ab", "target_surface_form": "#5054f1af-a651-41a5-bb7e-42250b315968", "evidence_sentence": "We used the COCI CSV dataset Version 11 released on 2021-09-03https://doi.org/10.6084/m9.figshare.6741422.v11, last accessed on 2021-12-14 that lists pairs of DOIs denoting citations.", "start": 197, "end": 380}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 33, "text": "Table\u00a0REF  shows the Gini coefficients calculated based on the Lorenz curves shown in Figure\u00a0REF .\r\nThe Gini coefficients are consistently higher for preprints than publisher versions.\r\nThe coefficients for preprints are almost twice as those for publisher versions.\r\nThus, there are larger citation inequalities in preprints than in publisher versions, and there could exist a larger citation bias in preprints.\r\nIn addition, we consistently observe higher Gini coefficients on the institution level than on the country level.\r\nIn other words, there is a greater imbalance in received citations across institutions than across countries.\r\nThe differences are smaller when we consider all authors,\r\nas the Gini coefficients for preprints get smaller.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 34, "text": "We further investigate biased author affiliations, specifically countries, by examining differences in the ranks of the number of citations in preprints and publisher versions, considering all authors of the articles.\r\nCountries of authors in preprints seem to be more decisive than countries of authors in publisher versions. We rank countries in the order of the number of citations with respect to preprints and publisher versions.\r\nCountries that benefit from author affiliations (i.e., countries ranked higher in preprints) include the United States and the United Kingdom, which have the highest number of articles, but also developing countries such as the Kenya and Tanzania.The full list of countries is as follows: United States, United Kingdom, Germany, Canada, Netherlands, Sweden, India, Israel, Brazil, Denmark, Norway, South Korea, Russia, South Africa, Chile, Greece, Iran, Ethiopia, Kenya, Colombia, Croatia, Uganda, Iceland, Tanzania.\r\nIn contrast, countries ranked higher in publisher versions include Asian countries such as China, Japan, and Taiwan, and Latin American countries such as Mexico and Argentina.The full list of countries is as follows: China, Australia, Switzerland, Japan, Italy, Finland, Singapore, Austria, New Zealand, Portugal, Czechia, Mexico, Argentina, Hungary, Ireland, Taiwan, Saudi Arabia, Turkey, Thailand, Malaysia, Estonia, Bangladesh, Nigeria, Slovenia, Luxembourg, Vietnam.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 35, "text": "There are possible factors that cause a bias in the Lorenz curves and Gini coefficients.\r\nOne of them is the number of months from publication of a preprint to its publisher version.\r\nPeer-review is thought to improve the credibility of articles\u00a0[32], leading to increased citations.\r\nThe distribution of the number of months from publication of a preprint to its publisher version varies greatly among journals and articles.\r\nIf the number of articles by journal and the number of months from publication of a preprint to its publisher version are unbalanced among affiliations, the results shown in Figure\u00a0REF  and Table\u00a0REF  would be biased.\r\nHence, we explore the Gini coefficients for preprints and publisher versions grouped by months from publication of a preprint to its publisher version.\r\nWe consider institutions and countries that have published at least 3 and 5 articles at each month, respectively.\r\n{FIGURE}{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 36, "text": "Figures\u00a0REF  and REF  show the Gini coefficients at institution and country levels, along with the number of articles and affiliations.\r\nIn these figures, we set all authors as target authors.\r\nFor instance, as can be seen in Figure\u00a0REF , the Gini coefficient of citations of preprints that have been published 8 months after publication is 0.30.\r\nWe observe larger Gini coefficients for preprints than those for publisher versions with one exception at the country level (e.g., articles that spend 18 months from publication of the preprint to its publisher version, as can be seen in Figure\u00a0REF ).\r\nThe differences between Gini coefficients get smaller as the duration between the publication of a preprint to its publisher version gets longer. This is because Gini coefficients for preprints get smaller while those for publisher versions become slightly larger.\r\nThese tendencies are caused by the length of the observation period of citations.\r\nFor example, if a publisher version has been published 5 months after preprint publication, the observation period of citations for the preprint and its publisher version is 4 months and 20 months, respectively.\r\nIn a shorter observation period, the variance of the number of citations of the institution is larger.\r\nEven if the observation period is shorter, preprints have higher Gini coefficients than publisher versions. Hence, in our view, the influence of the length from publication of preprints compared to their publisher versions is limited.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 37, "text": "Another possible factor is the journal in which the publisher version appeared.\r\nThe journal can be considered as a kind of indicator of the quality of an article.\r\nThus, it has a considerable influence on the number of citations, thereby affecting the Lorenz curves and the magnitude of the Gini coefficients.\r\nHowever, if an article is cited solely based on its quality, it does not make a difference in the Lorenz curve and Gini coefficient of citations between preprints and the publisher versions with respect to affiliations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 38, "text": "Table\u00a0REF  shows a large fraction of the 36,628 preprints have been published in mega-journals, a type of open access journal.\r\nWhat distinguishes mega-journals from other open access journals is that their peer-review process can solely focus on scientific trustworthiness\u00a0[2], because they have no need to filter articles due to restricted numbers of slots in their publishing schedule\u00a0[2].\r\nPLOS ONE, Scientific Reports, eLife, and Nature Communications in Table\u00a0REF  are considered as mega-journals\u00a0[2], [23], [37].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 39, "text": "We investigate the citation inequality with respect to various journals of different types.\r\nWe randomly select two mega-journals and three disciplinary journals from journals with at least 100 articles.\r\nWe also include the most prestigious multidisciplinary journals\u2014i\u00a0.e., Nature and Science\u2014in the analysis.\r\nTable\u00a0REF  presents the Gini coefficients of citations in each of the selected journals.\r\nAgain, we set all authors as target authors.\r\nAs we do not filter affiliation by the number of articles, the Gini coefficients in Table\u00a0REF  are higher than those including all journals (see Table\u00a0REF ).\r\nWe consistently observe higher Gini coefficients for preprints than those for publisher versions in different journal types.\r\nEspecially, the gap of citation inequality in mega-journals is large.\r\nThe large gaps come from a large fraction of uncited preprints for mega-journals.\r\nFor PLOS ONE, 82.20% of preprints have been not cited. After the publication of the publisher versions, the percentage decreased to 16.71%.\r\nThis result aligns with Lou and He\u00a0[22] and we observe even larger uncitedness in preprints than in publisher versions.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 40, "text": "In this paper, we explore citation bias in preprints associated with the author affiliations. The results of the analysis show larger citation inequalities in preprints than in publisher versions that indicate the author affiliations might influence the readership and the perception of preprints. The main difference between preprints and their respective publisher versions is the presence of peer-review process. Hence, the peer-review process mitigates citation bias.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 41, "text": "However, as Tahamtan et al.\u00a0[33] outlined, there are other factors that could influence on the number of citations. Other factors are obvious to be investigated in the future.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 42, "text": "In addition, we do not consider discrepancies between preprints and publisher versions.\r\nIn other words, we assume that there are revisions at the same degree between all pairs of preprints and publisher versions.\r\nKlein et al.\u00a0[18] investigated textual similarity of preprints and publisher versions using arXiv and bioRxiv, and reported that there are no significant difference between them.\r\nOn the other hand, Oikonomidi et al.\u00a0[27] stated that the evidence components reported across preprints and publisher versions are not stable over time, focusing on COVID-19 research.\r\nIf publisher versions authored by some institutions are revised and improved to a greater extent than those authored by the other institutions, the inequalities in the number of citations between affiliations could be explained by the amount of revisions. Hence, citation bias caused by author affiliations can be considered less than that shown in the previous section.\r\nWe plan to include the influence from discrepancies between preprints and publisher versions in our analysis in the future.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 43, "text": "Specific preprint servers, such as bioRxiv, provide a comment function to users. The comment function enables quick feedback for the authors. Soderberg et al.\u00a0[32] stated that 37% of user study participants considered user comments as being extremely or moderately important. Furthermore, the comments might influence the users' judgments regarding whether they would read and cite the preprint. Therefore, we plan to investigate how the number of comments and the polarity of comments (i.e., positive or negative) influence the number of citations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 44, "text": "In this paper, we examined the presence of citation bias in preprints and their publisher versions with respect to the articles' author affiliations. We observed larger citation inequalities in preprints than in publisher versions, indicating that author affiliations might influence the readership and the perception of preprints in general. The peer-review process mitigates this inequality. Ultimately, our study shows that authors need to be careful when citing works and that they should not be blinded by the author affiliation information. In addition, preprints increasingly attract attention in various cases, such as funding applications and recruitment. In these cases, funding agencies or referees would pay attention to citation-based metrics of preprints. As we observed even larger citation inequalities in preprints than in publisher versions, such institutions might want to be even more careful when using citations of preprints.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 45, "text": "This work was supported by JSPS KAKENHI Grant Number 20K20132.\r\nWe also thank anonymous reviewers for their constructive feedback.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 0, "text": "Named Entity Recognition (NER) aims to detect entity mentions in the text and classify them into predefined types, such as person, location, and organization. It is a fundamental task in information extraction and benefits many downstream NLP applications (e.g., relation extraction [3], co-reference resolution [4], entity linking [8] and event extraction [34]). In recent years, deep supervised models [14], [9], [15] have achieved superior success in the NER field. However, these supervised NER methods demand a large amount of high-quality annotation, which is extremely labor-intensive and time-consuming as NER demands token-level annotation.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 1, "text": "To solve this problem, Distantly-Supervised Named Entity Recognition (DS-NER) has attracted increasing attention. It automatically annotates training data based on external knowledge such as easily-obtained dictionaries and knowledge bases, which effectively relieves the annotation difficulty.\r\nUnfortunately, such a distant labeling procedure naturally introduces incomplete and inaccurate labels. As depicted in Figure REF  (a), \u201cWhite House\" is unlabeled because the distant supervision source has limited coverage of the entity mentions. Meanwhile, \u201cWashington\" is inaccurately labeled as this entity belongs to location types in the distant supervision source.\r\nDue to the existence of such noise in the distantly labeled data, straightforward application of supervised learning will yield deteriorated performance as deep neural models have a strong capacity of fitting the given noisy data. Thus, the robustness and generalization of learned DS-NER models are restricted.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 3, "text": "However, the above self-training methods have the following shortcomings: (1) inadequate student learning. As shown in Figure REF  (b), previous methods only focus on the consistent prediction from two teachers [32] or simply consider the high-confidence part from a single teacher [17]. In this way, these models tend to learn uncomplicated mentions, and the entity recall rate will decrease. (2) coarse-grained teacher updating. In Figure REF  (c), previous works absorb a whole student by exponential moving average (EMA) [32] or directly copy the student as a new teacher [17] when updating the teacher. Such coarse-grained ensemble methods treat each model fragment equally while the noise sensitivity is diverse among different model fragments.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 4, "text": "In this paper, we try to reconcile the above shortcomings with our newly proposed Adaptive Teacher Learning and Fine-grained Student ENsemble (ATSEN) for DS-NER.\r\nSpecifically, we first apply two teacher networks to provide multi-view predictions on training samples. Then we propose an adaptive teacher learning which supervises agreement predictions by cross-entropy loss and accommodates disagreement parts with adaptive distillation. In this way, the student can be trained with more comprehensive knowledge.\r\nSubsequently, we update the new teacher with a fine-grained student ensemble, which updates a fragment of the teacher model with a temporal moving average of the corresponding fragment of the student. Therefore, the teacher model achieves more robustness for noise.\r\nWith both adaptive learning and fine-grained ensemble, ATSEN is more effective than previous methods.\r\nWe evaluate ATSEN on four DS-NER datasets. Experimental results demonstrate that our method significantly outperforms previous approaches.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 6, "text": "\r\nTo our best knowledge, this paper presents the first attempt to explore both agreement and conflicts among multiple teachers for the DS-NER by adaptive teacher learning, promoting comprehensive student learning.\r\n\r\nTo further enhance the consistent prediction of model fragments, we devise a novel fine-grained student ensemble that stitches different fragments of previous student models into a unity.\r\nIn this way, the updated teacher achieves a more robust generalization ability.\r\n\r\nOn four benchmark DS-NER datasets (Conll03, OntoNotes 5.0, WebPage, and Twitter), our ATSEN outperforms existing approaches by significant margins.\r\n\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 7, "text": "Traditionally, many works have been proposed for supervised named entity recognition. For instance, [12] utilized the BiLSTM as an encoder to learn the contextual representation and then exploited Conditional Random Field (CRF) as a decoder to label the tokens. More recently, deep learning methods [30], [23] are introduced to different NLP fields, and strong pre-trained language models such as ELMo [22] and BERT [5] are incorporated to further enhance the performance of NER. However, most of these works rely on high-quality labels, which are expensive. Meanwhile, the reliance on labeled data also limits their applications in open situations.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 2, "text": "To address the above challenges, several DS-NER models have been proposed. [27] obtained high-quality phrases and designed TieOrBreak architecture to model those phrases that may be potential entities. [21] adopt PU learning to perform classification using only limited labeled positive data and unlabeled data. However, these works mainly focus on designing network architectures that can cope with the incomplete annotations to partially alleviate the impact of the noisy annotations.\r\nRecently, the self-training teacher-student framework is applied to DS-NER tasks [17], [32] to reduce the negative effect of both incomplete and inaccurate labels.\r\nThis self-looping framework first selects high-confidence annotations from noisy labels to train the student network, and then updates a new teacher by the trained student. In this way, the training labels are gradually refined and model generalization can be improved.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 8, "text": "DS-NER\r\nTo address the labeled data scarcity problem, distantly-supervised named entity recognition methods are proposed.\r\nAutoNER [27] proposed a sequence labeling framework TieOrBreak and modify the standard CRF for adapting to the scenario of label noise. [1] promoted the quality of data by exploiting\r\nlabels in Wikipedia. AdaPU [21] employed Positive-Unlabeled Learning to obtain unbiased estimation of the loss value. Conf-MPU [33] further formulated the DS-NER problem via Multi-class Positive and Unlabeled (MPU) learning.\r\nBOND [17] adopted a teacher-student network to drop distant labels and use pseudo labels to gradually improve the model generalization ability. Similar to BOND, SCDL [32] co-trained two teacher-student networks to form inner and outer loops for coping with label noise. In this paper, we propose a novel self-training framework to adaptively learn from multiple teachers and achieve a fine-grained student ensemble. In this way, our method achieves a more robust ability for noise in the DS-NER task.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 9, "text": "Teacher-Student Framework The teacher-student framework is a popular architecture in many semi-supervised [13] and self-supervised [0] learning tasks, as well as knowledge distillation [11]. Recently, teacher-student framework attracts increasing attention in both computer vision [10], [7] and natural language processing [17], [32]. The teacher selects reliable annotations with devised strategies for student training and then the new teacher is updated based on the trained student. The optimization goal is to ensure the prediction consistency between the student and the teacher. In particular, there are several variants of teacher-student networks proposed for DS-NER. BOND devised a self-training teacher-student strategy that copies the student as a new teacher. With this self-training loop, the training pseudo labels are gradually refined. To improve the quality of pseudo labels and remove noise, SCDL designs two teachers and reaches an agreement between them to generate pseudo labels. Meanwhile, SCDL uses exponential moving average (EMA) to update the teacher based on the re-trained student. Following the self-training framework, we further improve the training process of both the teacher and student network to alleviate the noise problem.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 10, "text": "Here we briefly describe the task definition of DS-NER. Formally, given the training data \\(D\\) , where each sentence is denoted as \\((X^i, Y^i)\\) . \\(X^i\\)  is a token list that represents each word, and \\(Y^i\\)  is the corresponding tag list in the form of BIO schema. For DS-NER, we do not have access to human-annotated true labels, but only distant labels by matching unlabeled sentences with external dictionaries or knowledge bases (KBs). Thus, \\(Y^i\\)  may not be the underlying correct one. To generate distant labels,\r\nin this work, we follow the previous work [17].\r\nThe biggest challenge in DS-NER is how to reduce the label noise in the training samples and train a robust NER model as there is much ambiguity and limited coverage over entity types.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 11, "text": "In this work, considering the memory capacity and model efficiency, we train two sets of teacher-student networks instead of more pairs while our method can easily extend to more pairs.\r\nThe main procedure is shown in Figure 2.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 12, "text": "The training procedure can be divided into three stages:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 13, "text": "(1) Pretraining with initial noisy labels. In this stage, we train two NER models (\\(\\theta _1\\) , \\(\\theta _2\\) ) using the distant labels. These two models have different architectures in this work. Then, we duplicate these two models for the initialization of two sets of teacher networks, namely \\(\\theta _{t1}=\\theta _1\\)  and \\(\\theta _{t2}=\\theta _2\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 14, "text": "The training target of \\(\\theta _1\\)  and \\(\\theta _2\\)  is:\r\n\\({L(\\theta ) = -\\frac{1}{MN}\\sum ^{M}_{i=1}\\sum ^N_{j=1}\\overset{*}{y}_j^i log(p(y_j^i|X^i;\\theta ))}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 16, "text": "(2) Training student with adaptive teacher learning. In this phrase, we select reliable labels by predictions of teachers from the first stage and supervise the students with cross-entropy loss. Meanwhile, considering the potential conflicts or competitions that exist among teachers, we investigate the diversity of teachers in the gradient space and recast the knowledge distillation from two teachers as a multi-objective optimization problem so that we can determine a better optimization direction for the training of student. To this end, an adaptive knowledge distillation loss is also adopted in this stage.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 17, "text": "Reliable Labels Selection. Without any prior knowledge about which tokens are mislabeled or unlabeled, it is challenging to automatically detect them. Here we adopt two strategies to select reliable labels. (i) Consistent Prediction. The first token selection strategy is based on the pseudo labels prediction consistency between two teachers.\r\n\\({(X^i, Y^i)_\\text{CP} = \\lbrace (x_j, {y_j})| {y_j} = (y_{j,t1} == y_{j,t2})\\rbrace }\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 19, "text": "where \\(\\sigma _1\\)  is the confidence threshold, \\({p_{j,t1}}\\)  is the label distribution of the \\(j\\) -th token predicted by the teacher \\(t_1\\) . Thus, the tokens with label confidence lower than \\(\\sigma _1\\)  will also be set to \u201cO\" labels.\r\nAfter these two steps, we can obtain reliable labels \\(\\overline{Y}\\) .\r\nWith these reliable labels, we can supervise the student models with the cross-entropy loss as follows:\r\n\\({L_{ce}(\\theta ) = -\\frac{1}{MN}\\sum ^{M}_{i=1}\\sum ^N_{j=1}\\overline{y}_j^i log(p(y_j^i|X_i;\\theta ))}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 20, "text": "The above selection procedure only considers consistent parts between two teachers while the conflicts among teachers are not squared up.\r\nTo handle the inner conflicts, we formulate ensemble knowledge from teachers as a multi-objective optimization (MOO) problem [26] and use multiple gradient descent algorithms (MGDA) to probe a Pareto optimal solution that accommodates all teachers as much as possible.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 21, "text": "Specifically, we first formally introduce the standard knowledge distillation loss which encourages the logits of the student network to mimic the teacher network:\r\n\\(\\begin{split}L_{kd}^t ({\\theta }) = H(p^s, p^t) = H(\\sigma (a^s; T), \\sigma (a^t; T)) = \\\\-\\sum _{k=1}^{K} p^t \\text{log} p^s[k] = - \\left\\langle p^t, \\text{log} p^s \\right\\rangle \\end{split}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 22, "text": "where \\(\\sigma \\)  is softmax operation, \\(a^s\\)  and \\(a^t\\)  are the logits of student and teacher networks, \\(T\\)  is the temperature to soften the logits. \\(K\\)  is the number of classification types. \\(H(\\cdot , \\cdot )\\)  is the cross-entropy loss to measure the discrepancy of softened probabilistic output between the student and teacher.\r\nIn this work, we have two teachers, thus the naive solution for distilling from two teachers is:\r\n\\(\\begin{split}L_{kd}({\\theta }) = L_{kd}^{t1} ({\\theta }) + L_{kd}^{t2} ({\\theta }) = H(p^s, p^{t1}) + H(p^s, p^{t2})\\end{split}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 23, "text": "However, conservatively accepting the directions from all teachers, i.e., accumulating the separate distillation loss from each teacher, is not a good option, since the diversity of teachers could be significant and there might be some weak or noisy teachers mingled in the ensemble.\r\nWhen distilling knowledge from multiple teachers, we need to incorporate the disagreement into the determination of the descent direction.\r\nRecently, a novel method is proposed to find one single Pareto optimal solution with a good trade-off among conflicting optimization targets.\r\nFollowing [26], [18], we can reformulate the Pareto solution of learning from two teachers as a linear scalarization of tasks with adaptive weight assignment as follows:\r\n\\(L(\\theta ) = \\alpha _1 L_{kd}^{t1} + \\alpha _2 L_{kd}^{t2}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 24, "text": "where we adaptively assign the weights \\(\\alpha _m\\)  by solving the following problem in each iteration:\r\n\\(\\begin{split}\\text{min} \\ \\frac{1}{2} || \\sum _{m=1}^M \\alpha _m \\nabla _\\theta L_{kd}^m (\\theta ^\\tau )|| ^2 , s.t. \\\\\\sum _{m=1}^M \\alpha _m = 1, \\ 0 \\le \\alpha _m \\le C, \\forall m \\in [1:M]\\end{split}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 25, "text": "where \\(C >0\\)  is the regularization parameter, and \\(M\\)  is the number of teachers. \\(L_{kd}^m (\\theta ^\\tau )\\)  is the knowledge distillation loss at Eq. 5 corresponding to the student and \\(m\\) -th teacher. \\(\\theta ^\\tau \\)  is the parameter of the student network at iteration \\(\\tau \\) .\r\nConsidering that calculating the gradient over parameters \\(\\theta ^\\tau \\)  can be fairly time-consuming. Following [26], we turn to its upper bound:\r\n\\({\\text{min} \\ \\frac{1}{2} || \\sum _{m=1}^M \\alpha _m \\nabla _Z L_m (\\theta ^\\tau ) ||^2, s.t.}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 26, "text": "where \\(\\sum _{m=1}^M \\alpha _m = 1, \\ 0 \\le \\alpha _m \\le C, \\forall m \\in [1:M]\\) , \\(Z\\)  is the feature over the corresponding teacher. In this way, Eq. 9 is a typical One-class SVM problem and can be solved by LIBSVM [2]. More intuitively, as shown in Figure 2, we first compute the standard distillation loss according to the student logit and each teacher logit. Through the back-propagation algorithm, we can obtain the gradients corresponding to each teacher for the student model. Subsequently, we solve the loss weights through the gradients with the LIBSVM tool.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 30, "text": "Subsequently, the segment ensemble can further integrate with EMA to incorporate temporal property. Here we first review the traditional EMA strategy:\r\n\\({\\theta ^t(\\tau ) = \\lbrace  m \\theta ^ t (\\tau -1) + (1 - m) \\theta ^s (\\tau )\\rbrace }\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 27, "text": "Finally, the total training loss for the student model in the second stage is:\r\n\\({L(\\theta ) = L_{ce}(\\theta ) + \\alpha L_{kd}^{t1} ({\\theta }) + (1-\\alpha ) L_{kd}^{t2} ({\\theta })}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 32, "text": "when \\(m=0\\) , it becomes segment ensemble. Similarly, it degenerates to EMA when \\( \\sigma _2 = 0\\) . In this manner, the fine-grained ensemble not only possesses the temporal property of traditional EMA, but also enhances the robustness of each segment to noise.\r\nAs a result, the teacher tends to generate more reliable pseudo labels, which can be used as new supervision signals in the next round self-training.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 33, "text": "To sum up, the first stage is executed once for a moderate initialization with distant labels. The second and third phases will be conducted alternately in a loop for better student and teacher models.\r\nFinally, only the best model \\(\\theta \\in \\lbrace \\theta _{t1}, \\theta _{t2}, \\theta _{s1}, \\theta _{s2}\\rbrace \\)  will be used for prediction.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 55, "text": "To verify the effectiveness of our proposed ATSEN, we conduct experiments on four DS-NER datasets. Here we give a short description of them as follows:\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 56, "text": "CoNLL03 [25] consists of 1393 English news articles and is annotated with four entity types: person, location, organization, and miscellaneous.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 57, "text": "OntoNotes 5.0 [29] contains documents from multiple domains, including broadcast conversation, P2.5 data, and Web data. It consists of 18 entity types.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 58, "text": "Webpage [24] comprises of personal, academic, and computer science conference webpages. It consists of\r\n20 webpages that cover 783 entities.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 59, "text": "Twitter [6] is from the WNUT 2016 NER shared task. It consists of 10 entity types.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 60, "text": "The detailed statistics of each dataset are listed in Table 1.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 61, "text": "We compare our ATSEN with a wide range of state-of-the-art DS-NER methods and supervised methods. Fully supervised methods use the ground truth annotation for model training. DS-NER methods use the distantly-labeled training set provided in [17].\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 62, "text": "Fully-supervised Methods. We include two supervised NER methods for comparison. (1) RoBERTa [19] adopts RoBERTa model as backbone and a top linear layer for token-level classification. (2) BiLSTM-CRF [20] uses bi-directional LSTM with character-level CNN to produce token embeddings, which are then fed into a CRF layer to predict token labels.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 64, "text": "The architecture of the teachers is the backbone language model and a top classification layer for token-level classification. Specifically, we adopt RoBERTa and DistilRoBERTa as backbone for teacher 1 and teacher 2.\r\nThe corresponding student has the same architecture as their teacher. The max training epoch is 50 for all datasets. The training batch size is 16 for CoNLL03, Webpage, and Twitter and 32 for OntoNotes 5.0. The learning rate is set to 1e-5 for CoNLL03 and Webpage, and 2e-5 for OntoNotes 5.0 and Twitter.\r\nFor the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets. For adaptive teacher learning, the confidence threshold \\(\\sigma _1\\)  is 0.9 for all datasets.\r\nIn the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 31, "text": "where \\(m\\)  denotes the smoothing coefficient. As shown in this equation, EMA treats the model as a whole. We can integrate these two ensemble methods as fine-grained ensemble:\r\n\\(\\begin{aligned}\\theta ^\\tau (\\tau ) &= \\lbrace |P_i < \\sigma _2 | \\theta ^t(\\tau -1) + (1-|P_i < \\sigma _2 |)m\\theta _i^t(\\tau -1) \\\\&+ (1-|P_i < \\sigma _2 |)(1-m)\\theta _i^s(\\tau ) \\rbrace \\end{aligned}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 29, "text": "where \\(P_i\\)  is random probability distribution in [0,1] for the i-th unit of the teacher which is independent of each other. If \\(P_i < \\sigma _2\\) , then \\(|P_i < \\sigma _2 | = 1\\) , the i-th unit parameter of teacher is to be preserved. In our paper, each unit corresponds to one network layer of the student network.\r\nThe motivation of our segment ensemble is from Dropout [28] while Dropout works when training a network.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 28, "text": "(3) Updating teacher with fine-grained student ensemble.\r\nAfter training the students, we devise a fine-grained student ensemble to update the parameters of the teachers. Before describing the concrete fine-grained ensemble, we first introduce a preliminary version, named segment ensemble (SE).\r\nDuring each iteration, the segment ensemble picks up some units of the student model to replace the corresponding units of the teacher model, leaving the remaining parts of the teacher unchanged. Formally, at iteration \\(\\tau \\) ,\r\n\\({{\\theta }^t({\\tau }) = \\lbrace  |P_i < {\\sigma _2} | \\theta _i^t({\\tau }-1) + (1-|P_i < {\\sigma _2} |)\\theta _i^s({\\tau }) \\rbrace }\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 18, "text": "where \\(y_{j,t1}, y_{j,t2}\\)  are predicted one-hot pseudo labels on training corpus for two teachers. If two teacher models predict the same labels on specific tokens, then the labels of these tokens are set to corresponding labels. Meanwhile, if two teacher models have different predictions, the labels of tokens will be set to the \u201cO\" label.\r\n(ii) Threshold Prediction. We propose a simple threshold-based strategy to further filter reliable labels as\r\nthe tokens with high confidence are more likely to be reliable.\r\nFor teacher \\(t_1\\) ,\r\n\\({(X^i, Y^i)_\\text{TP} = \\lbrace  (x_j, {y_j}) | \\text{max}({p_{j,t1}) > \\sigma _1 \\rbrace  }}\\) \r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 63, "text": "Distantly-supervised Methods. (1) KB-Matching reports the distant supervision quality. (2) Distant BiLSTM-CRF, Distant DistilRoBERTa, and Distant RoBERTa fine-tune the corresponding models on distantly-labeled data as if they are ground truth with the standard supervised learning. (3) AutoNER [27] trains the model by assigning\r\nambiguous tokens with all possible labels and then maximizing the overall likelihood using a fuzzy CRF model. LRNT [1] applies partial-CRFs on high-quality data with non-entity sampling. Co-teaching+ [31] is a classic de-nosing method in computer vision. NegSampling [16] only handles incomplete annotations by negative sampling. BOND and SCDL both adopt self-training strategies that are straightforward competitors to ATSEN.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 65, "text": "Table 2 presents the performance of all methods measured by precision, recall, and F1 scores. The results are summarized as follows: On all four datasets, ATSEN achieves the best performance among all distantly-supervised methods. Specifically, the distant DistilRoBERTa and RoBERTa only slightly improve the distant labeling performance compared to the naive KB-Matching, showing that directly applying supervised learning to distantly-labeled data will lead to poor model generalization.\r\nIn addition, ATSEN performs much better than\r\nprevious studies which consider the noisy labels in\r\nNER, including AutoNER, LRNT, Co-teaching+, and NegSampling.\r\nWhen compared to strong self-training methods BOND and SCDL, our ATSEN achieves new state-of-the-art performance, demonstrating the superiority of our proposed adaptive teacher learning and fine-grained student ensemble when trained on distantly-labeled data.\r\nConcretely, on CoNLL03, ATSEN achieves 1.90 absolute F1 improvements over the strong method SCDL. On the biggest and most difficult dataset OntoNotes V5.0, we obtain a decent improvement compared to the SOTA approach SCDL by 0.34 F1 score. In addition, we get 2.08 and 1.37 F1 scores improvement on Webpage and Twitter respectively.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 66, "text": "To further validate the effectiveness of each component in our ATSEN, we compare ATSEN with the following ablations by removing specific components: (1) remove the consistent prediction (w/o CP) in Eq.2. (2) remove the threshold prediction (w/o TP) in Eq.3.\r\n(3) do not perform cross-entropy loss (w/o CE).\r\n(4) do not perform adaptive distillation (w/o AD), namely, only cross-entropy loss is adopted in Eq.10.\r\n(5) do not perform fine-grained ensemble (w/o FE), namely, directly copy the trained student as a new teacher.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 67, "text": "As shown in Table 3, it can be observed that w/o CP and w/o TP lead to a significant performance drop, indicating these strategies are important for cross-entropy learning. Meanwhile, the result of w/o CE do not cause huge performance as adaptive distillation also considers the agreement part between teachers. The results from w/o CP, w/o TP, and w/o CE also imply that the cross-entropy loss from ambiguous labels may damage the performance.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 68, "text": "In addition, the result of w/o AD decreases the recall largely compared to ATSEN. It shows that considering knowledge from the disagreement part of two teachers can effectively help comprehensive student learning.\r\nFinally, w/o FE significantly reduces performance, showing that our fine-grained ensemble indeed benefits the model's generalization ability.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 69, "text": "In this section, we study the effectiveness of adaptive distillation for the student training process. Here we implement several ablations as shown in Table 4.\r\nThe baseline is the adaptive teacher learning used in Eq.10, where\r\nthe weight \\(\\alpha \\)  is computed by LIBSVM from the gradients of two teachers corresponding to the student.\r\nWe devise four variants. The first variant is averaging the distillation loss to substitute the adaptive distillation. It decreases by about 0.4 F1 scores. We also devise the second variant by manually setting the weights. Here we set 0.7 and 0.3 for distillation loss from teacher 1 (Roberta) and teacher 2 (DistilRoBERTa). This variant still performs worse than adaptive distillation. We have also tried other weight combinations such as 0.8 and 0.2 but achieved even worse results. Furthermore, we directly learn a dynamic weight \\(\\alpha \\)  and achieve similar results with the manual setting.\r\nFinally, we consider a variant that only considers the disagreement part between two teachers during distillation, thus the training tokens may not be continuous and complete. The result presents a performance drop, indicating that the gradient optimization for adaptive distillation should be conducted for the whole sentence.\r\n{FIGURE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 70, "text": "We investigate the effectiveness of different student ensemble methods. For comprehensive evaluation, we experiment on a relatively smaller dataset Twitter instead of CoNLL03. As shown in Table 5: (1) remove all ensemble strategies (w/o all) and directly copy the student as a new teacher. (2) remove the segment ensemble (w/o SE), namely \\(\\sigma _2=0\\)  in Eq.13. (3) remove the EMA (w/o EMA), namely \\(m=0\\)  in Eq.13. As shown in Table 5, w/o all lead to the most significant performance drop. Meanwhile, removing either SE or EMA cause decreased results, demonstrating these two kinds of ensemble method can complement each other. It is worth noting ATSEN achieves significantly better precision than variants, indicating fine-grained ensemble can effectively enhance consistent predictions by performing on model fragments.\r\nFurthermore, we investigate the parameter influence of fine-grained ensemble in Fig. 3. As shown in this figure, we can observe \\(m=0.995\\)  and \\(\\sigma _2=0.75\\)  achieve the best performance. We also notice\r\nan interesting fact is that with the increase of \\(m\\) , the model achieves its best performance at a relatively smaller value of \\(\\sigma _2\\) .\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 71, "text": "We perform case study to understand the advantage of our proposed ATSEN with a concrete example in Table 6. We show the prediction result of BOND, SCDL, and ATSEN on a training sequence with label noise.\r\nBOND can slightly generalize to unseen mentions and relieve partial incomplete annotation. For example, BOND can locate the \u201cJohn McNamara\" and \u201cNew York\" while distant labels only can match partial person names.\r\nSCDL is able to generalize better for more accurate entity detection because it has a co-training step. For instance, SCDL can further locate the entity \u201cColumbia Presby Hospital\". However, it is still impacted by label noise. For comparision, for hard labels \u201cCalifornia Angels\",\r\nour ATSEN is able to detect them with both adaptive teacher learning and fine-grained student ensemble, instead of relying purely on distant labels.\r\n{TABLE}{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 72, "text": "In this paper, we present a novel self-training framework ATSEN for DS-NER. Specifically, ATSEN adopts adaptive teacher learning to train student networks, considering both consistent and inconsistent predictions between them. Furthermore, we devise a fine-grained student ensemble to update the teacher model. With it, each fragment of the teacher benefits from a temporal moving average of the corresponding fragment of the student.\r\nThe experiment results illustrate that ATSEN significantly outperforms SOTA methods.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 73, "text": "Pan Zhou is funded by the National Natural Science Foundation of China (NSFC) with grant numbers 61972448.\r\n", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 75, "text": "We fine-tune our model within the hyperparameter search space in Table\u00a0REF . Our hyperparameter search used the method of W&B Sweeps\u00a0[3] with a standard random search with 500 counts. We introduce the best hyperparameter configuration in Tables\u00a0REF  to\u00a0REF .\r\n{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}", "annotation": {"entities": {"c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#391edf7d-c0b5-4482-9878-28cc8745eaf0", "surface_form": "hyperparameter search", "start": 81, "end": 102}]}, "a64": {"id": "a64", "type": "a", "subtype": null, "surface_forms": [{"id": "#24c28ff7-0c58-48c2-a887-e03286ca5458", "surface_form": "W&B Sweeps", "start": 122, "end": 132}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#a079cf00-cd3d-4a75-97ec-703dd7a6a850", "surface_form": "500", "start": 172, "end": 175}]}, "p7": {"id": "p7", "type": "p", "subtype": null, "surface_forms": [{"id": "#9bc700fa-13bf-47f9-87a4-3b5ee5ada26d", "surface_form": "counts", "start": 176, "end": 182}]}}, "relations": {"r2": {"id": "r2", "source": "v1", "target": "p7", "evidences": [{"id": "#7ff7cafa-dc21-4c3f-8e95-548287f5a026", "source_surface_form": "#a079cf00-cd3d-4a75-97ec-703dd7a6a850", "target_surface_form": "#9bc700fa-13bf-47f9-87a4-3b5ee5ada26d", "evidence_sentence": "Our hyperparameter search used the method of W&B Sweeps\u00a0[3] with a standard random search with 500 counts.", "start": 77, "end": 183}]}, "r0": {"id": "r0", "source": "p7", "target": "a64", "evidences": [{"id": "#0f7f5ce3-81b6-47ea-a64d-2af0f8960a0c", "source_surface_form": "#9bc700fa-13bf-47f9-87a4-3b5ee5ada26d", "target_surface_form": "#24c28ff7-0c58-48c2-a887-e03286ca5458", "evidence_sentence": "Our hyperparameter search used the method of W&B Sweeps\u00a0[3] with a standard random search with 500 counts.", "start": 77, "end": 183}]}, "r1": {"id": "r1", "source": "c1", "target": "v1", "evidences": [{"id": "#256b3414-9a4f-48c2-86f0-8cae0f5a175c", "source_surface_form": "#391edf7d-c0b5-4482-9878-28cc8745eaf0", "target_surface_form": "#a079cf00-cd3d-4a75-97ec-703dd7a6a850", "evidence_sentence": "Our hyperparameter search used the method of W&B Sweeps\u00a0[3] with a standard random search with 500 counts.", "start": 77, "end": 183}]}}}, "annotation_raw": null}, {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 59, "text": "Our system extends the implementation and hyper-parameters from Lee2017EndtoendNC with the following adjustments. We use a 1 layer BiLSTM with 200-dimensional hidden layers. All the FFNNs have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.\r\nWe model spans up to 8 words. For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.\r\nFor constructing the knowledge graph, we use the following heuristics to normalize the entity phrases. We replace all acronyms with their corresponding full name and normalize all the plural terms with their singular counterparts.\r\n{TABLE}", "annotation": {"entities": {"a1": {"id": "a1", "type": "a", "subtype": null, "surface_forms": [{"id": "#b2d4d4a5-5f8d-40e2-bd67-6f3afc608a6c", "surface_form": "system", "start": 4, "end": 10}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ee6b8954-71ff-42df-bc49-4eadfd4adc97", "surface_form": "1", "start": 123, "end": 124}]}, "p10": {"id": "p10", "type": "p", "subtype": null, "surface_forms": [{"id": "#b2aed499-58f1-4903-b2c3-6f2358a34bce", "surface_form": "layer", "start": 125, "end": 130}]}, "a9": {"id": "a9", "type": "a", "subtype": null, "surface_forms": [{"id": "#7aecb6e0-0308-46e1-be32-214fcee1c188", "surface_form": "BiLSTM", "start": 131, "end": 137}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#cbfb0b48-9529-4d84-b9e6-87fc669e0d8a", "surface_form": "200", "start": 143, "end": 146}]}, "p7": {"id": "p7", "type": "p", "subtype": null, "surface_forms": [{"id": "#e6fe9711-dbce-410a-b1f3-4ac3898e3980", "surface_form": "dimensional hidden layers", "start": 147, "end": 172}, {"id": "#c6382a04-3f91-408f-8381-48d120b5ae56", "surface_form": "dimensions", "start": 216, "end": 226}]}, "a7": {"id": "a7", "type": "a", "subtype": null, "surface_forms": [{"id": "#c43b9207-2ee8-4508-a7f0-331042c4404e", "surface_form": "FFNNs", "start": 182, "end": 187}, {"id": "#601bfd1b-6e9c-4f38-af1b-582839e3b664", "surface_form": "FFNNs", "start": 304, "end": 309}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#de830b2e-f991-4840-948f-ce79f567db28", "surface_form": "2", "start": 193, "end": 194}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#b71512c4-b438-4995-aa0c-6b693cf97162", "surface_form": "hidden layers", "start": 195, "end": 208}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#763aef5e-3184-404f-8423-166e96ad6ff5", "surface_form": "150", "start": 212, "end": 215}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#4dc24675-3ccb-4abf-a0a2-7f6e55e84cf9", "surface_form": "0.4", "start": 240, "end": 243}]}, "p8": {"id": "p8", "type": "p", "subtype": null, "surface_forms": [{"id": "#201b4465-4ade-44c4-8674-a95e1131cb29", "surface_form": "variational dropout", "start": 244, "end": 263}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#36128376-8bd1-499d-a097-a49f3e797901", "surface_form": "LSTMs", "start": 277, "end": 282}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#e42d18c1-ff86-4d2f-a6e2-16833f0c7709", "surface_form": "0.4", "start": 284, "end": 287}]}, "p9": {"id": "p9", "type": "p", "subtype": null, "surface_forms": [{"id": "#3542116a-764b-43ba-8fc8-5666dad4a539", "surface_form": "dropout", "start": 288, "end": 295}, {"id": "#4baefd6d-8ef2-4c09-8b7d-5415213f6359", "surface_form": "dropout", "start": 319, "end": 326}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#e6c60c21-609d-4695-b23f-0ed97825d1fc", "surface_form": "0.5", "start": 315, "end": 318}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#3f7f6fe8-1503-4a96-8ec9-d1bfa5859076", "surface_form": "input embeddings", "start": 335, "end": 351}]}, "a11": {"id": "a11", "type": "a", "subtype": null, "surface_forms": [{"id": "#0da6a273-aa5d-4c24-8afa-fd7cd88b58fc", "surface_form": "beam pruning", "start": 387, "end": 399}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#76ae0167-1c39-4398-8ebf-91b52f729463", "surface_form": "\\lambda _{\\text{C}}", "start": 410, "end": 429}]}, "v9": {"id": "v9", "type": "v", "subtype": "n", "surface_forms": [{"id": "#f437394c-9eaa-48fa-b960-b4c5e8dfeeac", "surface_form": "0.3", "start": 430, "end": 433}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#080ad41d-1a87-4acb-b004-00015546142f", "surface_form": "coreference resolution", "start": 441, "end": 463}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#ab9234c2-262d-45d5-9e88-dbb61dedf3a1", "surface_form": "\\lambda _{\\text{R}}", "start": 470, "end": 489}]}, "v10": {"id": "v10", "type": "v", "subtype": "n", "surface_forms": [{"id": "#17d6fd6c-2db9-4031-bc32-e453b22810a3", "surface_form": "0.4", "start": 490, "end": 493}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#20d8a801-e382-49dd-b965-45aceb1d2020", "surface_form": "relation extraction", "start": 501, "end": 520}]}}, "relations": {"r20": {"id": "r20", "source": "v3", "target": "p6", "evidences": [{"id": "#bf25c371-19f3-4446-a603-51f4b4043afb", "source_surface_form": "#de830b2e-f991-4840-948f-ce79f567db28", "target_surface_form": "#b71512c4-b438-4995-aa0c-6b693cf97162", "evidence_sentence": "All the FFNNs have 2 hidden layers of 150 dimensions each.", "start": 174, "end": 232}]}, "r0": {"id": "r0", "source": "p6", "target": "a7", "evidences": [{"id": "#c6b58ab1-16c4-482b-8e81-640138fdaeaa", "source_surface_form": "#b71512c4-b438-4995-aa0c-6b693cf97162", "target_surface_form": "#c43b9207-2ee8-4508-a7f0-331042c4404e", "evidence_sentence": "All the FFNNs have 2 hidden layers of 150 dimensions each.", "start": 174, "end": 232}]}, "r1": {"id": "r1", "source": "v4", "target": "p7", "evidences": [{"id": "#d1753e87-1795-4a3e-b871-a148236c4fb2", "source_surface_form": "#763aef5e-3184-404f-8423-166e96ad6ff5", "target_surface_form": "#c6382a04-3f91-408f-8381-48d120b5ae56", "evidence_sentence": "All the FFNNs have 2 hidden layers of 150 dimensions each.", "start": 174, "end": 232}]}, "r2": {"id": "r2", "source": "p7", "target": "a7", "evidences": [{"id": "#e38b9bc5-0c1f-4b30-b287-b63cdb45bfb6", "source_surface_form": "#c6382a04-3f91-408f-8381-48d120b5ae56", "target_surface_form": "#c43b9207-2ee8-4508-a7f0-331042c4404e", "evidence_sentence": "All the FFNNs have 2 hidden layers of 150 dimensions each.", "start": 174, "end": 232}]}, "r3": {"id": "r3", "source": "v7", "target": "p9", "evidences": [{"id": "#1ef11e94-513e-4a86-9a9d-a8b5a177aa59", "source_surface_form": "#e6c60c21-609d-4695-b23f-0ed97825d1fc", "target_surface_form": "#4baefd6d-8ef2-4c09-8b7d-5415213f6359", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}]}, "r4": {"id": "r4", "source": "v6", "target": "p9", "evidences": [{"id": "#e334b074-71cb-4c51-bc95-fe526e490f86", "source_surface_form": "#e42d18c1-ff86-4d2f-a6e2-16833f0c7709", "target_surface_form": "#3542116a-764b-43ba-8fc8-5666dad4a539", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}]}, "r5": {"id": "r5", "source": "p9", "target": "a7", "evidences": [{"id": "#deede13b-21fe-4df8-9a3f-105d7518e4e7", "source_surface_form": "#3542116a-764b-43ba-8fc8-5666dad4a539", "target_surface_form": "#601bfd1b-6e9c-4f38-af1b-582839e3b664", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}]}, "r6": {"id": "r6", "source": "v5", "target": "p8", "evidences": [{"id": "#092d8fd2-eec7-4954-88ad-3f2d158d56ea", "source_surface_form": "#4dc24675-3ccb-4abf-a0a2-7f6e55e84cf9", "target_surface_form": "#201b4465-4ade-44c4-8674-a95e1131cb29", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}]}, "r7": {"id": "r7", "source": "p8", "target": "a14", "evidences": [{"id": "#b53e0a62-2042-4b14-b800-1c6cb05e6fc2", "source_surface_form": "#201b4465-4ade-44c4-8674-a95e1131cb29", "target_surface_form": "#36128376-8bd1-499d-a097-a49f3e797901", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}]}, "r8": {"id": "r8", "source": "c1", "target": "v7", "evidences": [{"id": "#f84c9ee4-0b17-4c3a-8f77-f48c54f1abd0", "source_surface_form": "#3f7f6fe8-1503-4a96-8ec9-d1bfa5859076", "target_surface_form": "#e6c60c21-609d-4695-b23f-0ed97825d1fc", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}]}, "r9": {"id": "r9", "source": "c3", "target": "v10", "evidences": [{"id": "#c7153f37-6836-4a09-b664-19557953a8b5", "source_surface_form": "#20d8a801-e382-49dd-b965-45aceb1d2020", "target_surface_form": "#17d6fd6c-2db9-4031-bc32-e453b22810a3", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}]}, "r10": {"id": "r10", "source": "v10", "target": "p5", "evidences": [{"id": "#5117d873-b561-44d4-b8e5-598545810a3f", "source_surface_form": "#17d6fd6c-2db9-4031-bc32-e453b22810a3", "target_surface_form": "#ab9234c2-262d-45d5-9e88-dbb61dedf3a1", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}]}, "r11": {"id": "r11", "source": "p5", "target": "a11", "evidences": [{"id": "#95f2e627-a730-43b7-9ed9-9e74c70ec079", "source_surface_form": "#ab9234c2-262d-45d5-9e88-dbb61dedf3a1", "target_surface_form": "#0da6a273-aa5d-4c24-8afa-fd7cd88b58fc", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}]}, "r12": {"id": "r12", "source": "c2", "target": "v9", "evidences": [{"id": "#b61663bf-441f-4720-acf2-4ed670482a68", "source_surface_form": "#080ad41d-1a87-4acb-b004-00015546142f", "target_surface_form": "#f437394c-9eaa-48fa-b960-b4c5e8dfeeac", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}]}, "r13": {"id": "r13", "source": "v9", "target": "p3", "evidences": [{"id": "#5c1a126d-1c96-4bb8-934b-deb08a75d164", "source_surface_form": "#f437394c-9eaa-48fa-b960-b4c5e8dfeeac", "target_surface_form": "#76ae0167-1c39-4398-8ebf-91b52f729463", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}]}, "r14": {"id": "r14", "source": "p3", "target": "a11", "evidences": [{"id": "#f465fee0-e798-440b-a4fb-50bc85f909d5", "source_surface_form": "#76ae0167-1c39-4398-8ebf-91b52f729463", "target_surface_form": "#0da6a273-aa5d-4c24-8afa-fd7cd88b58fc", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}]}, "r15": {"id": "r15", "source": "p9", "target": "a1", "evidences": [{"id": "#28676e42-829f-4f2f-9378-82e4efe385b4", "source_surface_form": "#4baefd6d-8ef2-4c09-8b7d-5415213f6359", "target_surface_form": "#b2d4d4a5-5f8d-40e2-bd67-6f3afc608a6c", "evidence_sentence": "Our system extends the implementation and hyper-parameters from Lee2017EndtoendNC with the following adjustments. We use a 1 layer BiLSTM with 200-dimensional hidden layers. All the FFNNs have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 0, "end": 352}]}, "r16": {"id": "r16", "source": "v2", "target": "p7", "evidences": [{"id": "#bafba58a-4a2b-49e9-8550-000e7a9d34b4", "source_surface_form": "#cbfb0b48-9529-4d84-b9e6-87fc669e0d8a", "target_surface_form": "#e6fe9711-dbce-410a-b1f3-4ac3898e3980", "evidence_sentence": "We use a 1 layer BiLSTM with 200-dimensional hidden layers.", "start": 114, "end": 173}]}, "r17": {"id": "r17", "source": "p7", "target": "a9", "evidences": [{"id": "#b98f40df-96c5-45de-b7a9-a26ca28b4213", "source_surface_form": "#e6fe9711-dbce-410a-b1f3-4ac3898e3980", "target_surface_form": "#7aecb6e0-0308-46e1-be32-214fcee1c188", "evidence_sentence": "We use a 1 layer BiLSTM with 200-dimensional hidden layers.", "start": 114, "end": 173}]}, "r18": {"id": "r18", "source": "v1", "target": "p10", "evidences": [{"id": "#4b19dc6f-5ca7-4856-b732-a4d96bfe48ac", "source_surface_form": "#ee6b8954-71ff-42df-bc49-4eadfd4adc97", "target_surface_form": "#b2aed499-58f1-4903-b2c3-6f2358a34bce", "evidence_sentence": "We use a 1 layer BiLSTM with 200-dimensional hidden layers.", "start": 114, "end": 173}]}, "r19": {"id": "r19", "source": "p10", "target": "a9", "evidences": [{"id": "#1c80cc4c-d610-456f-8ed4-c282ef5bd553", "source_surface_form": "#b2aed499-58f1-4903-b2c3-6f2358a34bce", "target_surface_form": "#7aecb6e0-0308-46e1-be32-214fcee1c188", "evidence_sentence": "We use a 1 layer BiLSTM with 200-dimensional hidden layers.", "start": 114, "end": 173}]}}}, "annotation_raw": null}]