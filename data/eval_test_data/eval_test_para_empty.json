[{"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 59, "text": "Our system extends the implementation and hyper-parameters from Lee2017EndtoendNC with the following adjustments. We use a 1 layer BiLSTM with 200-dimensional hidden layers. All the FFNNs have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.\r\nWe model spans up to 8 words. For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.\r\nFor constructing the knowledge graph, we use the following heuristics to normalize the entity phrases. We replace all acronyms with their corresponding full name and normalize all the plural terms with their singular counterparts.\r\n{TABLE}", "annotation": {"entities": {}, "relations": {}}, "annotation_raw": []}]