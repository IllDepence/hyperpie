{"orig": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2. The weight decay is 0 to 0.01. The detailed search space and other hyperparameters are in Appendix. We also list the best hyperparameter configuration for each data in Appendix. If a baseline's accuracy is known and its experimental environments are the same as ours, we use the officially announced accuracy. If not, we execute a baseline using its official codes and the hyperparameter search procedures based on their suggested hyperparameter ranges.\r\n{FIGURE}", "annot": "For our [e1|method], we test with the following hyperparameter configurations: we train for 200 epochs using the [e1|Adam optimizer]. The [e2|dimensionality of H], denoted \\(\\dim (\\mathbf {H})\\) , is in {[v2.1|32], [v2.2|64], [v2.3|128], [v2.4|256]}. [e3|T] in Eq.\u00a0() is from [v3.1|1.0] to [v3.2|4.0]. The [p1.1|learning rate] is set to from [v1.1.1|1e-4] to [v1.1.2|2e-2]. The [p1.2|weight decay] is [v1.2.1|0] to [v1.2.2|0.01]. The detailed search space and other hyperparameters are in Appendix. We also list the best hyperparameter configuration for each data in Appendix. If a baseline's accuracy is known and its experimental environments are the same as ours, we use the officially announced accuracy. If not, we execute a baseline using its official codes and the hyperparameter search procedures based on their suggested hyperparameter ranges.\n{FIGURE}"}