{"2211.15202_30": {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 30, "text": "We test our models in the 40-fold cross-validation settings. It implies that each result is an average F1 score of 40 runs. In line with the conclusions of the paper\u00a0[10], we restricted our study to the few-shot learning settings limiting the datasets to 20, 100 and 1,000 observations. For each dataset, we generated 40 folds with the same seed for different test models. Each of the 40 folds consisted of training and test sets, from which we sampled the training set with the same seed so that it was limited to 20, 100, or 1,000 observations in different experiments. It ensured that each test model was trained and tested on the same data. The best hyperparameters were chosen based on the model with the best average F1 score.\r\nAlthough the 40-fold cross-validation is very time-consuming, we decided to apply it to tackle the problem of high variance, which is common when the amount of training data is limited\u00a0[23].\r\n", "entities": {"v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#498f4d2e-17db-4f25-ab8c-0bc8fa363e83", "surface_form": "40", "start": 26, "end": 28}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#7e8514ec-e6d0-4f99-9ba6-26733bfb36c8", "surface_form": "fold", "start": 29, "end": 33}, {"id": "#03360f42-afb0-4e8a-9ccf-713ad201463c", "surface_form": "fold", "start": 749, "end": 753}]}, "a17": {"id": "a17", "type": "a", "subtype": null, "surface_forms": [{"id": "#087f080b-5408-4d5f-a42b-9bf0b2c51a30", "surface_form": "cross-validation", "start": 34, "end": 50}, {"id": "#c21c940b-1e61-44ec-bb0e-7cef7bac271c", "surface_form": "cross-validation", "start": 754, "end": 770}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#18e9118b-faad-4c21-91c7-dc098f0bb48b", "surface_form": "40", "start": 115, "end": 117}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#62bae5de-d7a0-4c67-a967-112e5faea414", "surface_form": "20", "start": 255, "end": 257}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9fab2a47-8a9c-436d-9224-6aadade11ef2", "surface_form": "100", "start": 259, "end": 262}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#e684a89d-b5b5-40d1-8259-83ed99dc0919", "surface_form": "1,000", "start": 267, "end": 272}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#1e0d8eb7-1133-4ffe-b548-79b0bfcd1d63", "surface_form": "40", "start": 318, "end": 320}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#7c81aa51-f3d3-494d-948d-b9f9434d9ce2", "surface_form": "40", "start": 385, "end": 387}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#59dcc7c7-b7ad-468d-94a8-3a3f2cb183fc", "surface_form": "20", "start": 515, "end": 517}]}, "v9": {"id": "v9", "type": "v", "subtype": "n", "surface_forms": [{"id": "#5529a6c3-07c1-446e-bd93-0202533f382b", "surface_form": "100", "start": 519, "end": 522}]}, "v10": {"id": "v10", "type": "v", "subtype": "n", "surface_forms": [{"id": "#d11ac012-5ea6-4979-9d1e-b61f83b21679", "surface_form": "1,000", "start": 527, "end": 532}]}, "v11": {"id": "v11", "type": "v", "subtype": "n", "surface_forms": [{"id": "#116fc7a7-c50b-4b24-b7ab-8b5665825d55", "surface_form": "40", "start": 746, "end": 748}]}}, "relations": {"r3": {"id": "r3", "source": "v1", "target": "p1", "evidences": [{"id": "#7d3dd206-1a3d-435a-8d07-f6396a7e3b7c", "evidence_sentence": "We test our models in the 40-fold cross-validation settings.", "start": 0, "end": 60}, {"id": "#eb603716-b3b7-49eb-a9b0-45d739d4c40e", "evidence_sentence": "Although the 40-fold cross-validation is very time-consuming, we decided to apply it to tackle the problem of high variance, which is common when the amount of training data is limited\u00a0[23].", "start": 734, "end": 924}], "label": "vp"}, "r0": {"id": "r0", "source": "p1", "target": "a17", "evidences": [{"id": "#f04d2e3c-16db-4044-ae2c-99e3af900179", "evidence_sentence": "We test our models in the 40-fold cross-validation settings.", "start": 0, "end": 60}], "label": "pa"}, "r2": {"id": "r2", "source": "p1", "target": "a17", "evidences": [{"id": "#e240e90b-4664-47da-8d8e-ff3122689bfe", "evidence_sentence": "Although the 40-fold cross-validation is very time-consuming, we decided to apply it to tackle the problem of high variance, which is common when the amount of training data is limited\u00a0[23].", "start": 734, "end": 924}], "label": "pa"}}}, "2211.15202_31": {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 31, "text": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .\r\nThe final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.\r\n", "entities": {"c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#dc6597af-8f33-42c7-b926-70f1c101484b", "surface_form": "hyperparameter search", "start": 111, "end": 132}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#d24f48b9-c9fe-48e4-b8fd-2c1866fcde04", "surface_form": "batch size", "start": 147, "end": 157}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ff3fd496-b807-430d-982d-5058981fa756", "surface_form": "64", "start": 161, "end": 163}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#d5c59168-ce1f-4953-a8e6-80660dfa485f", "surface_form": "learning rates", "start": 168, "end": 182}, {"id": "#37074879-9891-49e6-93aa-159977ef9a06", "surface_form": "learning rate", "start": 431, "end": 444}]}, "v2": {"id": "v2", "type": "v", "subtype": "s", "surface_forms": [{"id": "#46e605e6-487c-407d-98ad-70df31ac314d", "surface_form": "\\lbrace 1e-5, 2e-5, 3e-5\\rbrace", "start": 189, "end": 220}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#de2d2274-2549-47c8-a1bc-f6b845be3ff0", "surface_form": "epochs number", "start": 226, "end": 239}]}, "v3": {"id": "v3", "type": "v", "subtype": "s", "surface_forms": [{"id": "#dc23bd03-5b07-4068-aae0-590c5977fbbf", "surface_form": "\\lbrace 8,16,64,128\\rbrace", "start": 246, "end": 272}]}, "a18": {"id": "a18", "type": "a", "subtype": null, "surface_forms": [{"id": "#9bf99390-8cd9-494e-847f-7f17172896c9", "surface_form": "linear warmup", "start": 278, "end": 291}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ec590506-a241-41b2-8b85-db94b6f42386", "surface_form": "6%", "start": 306, "end": 308}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#5de35b8f-3dd4-4c3e-ba34-cdde419c1f7d", "surface_form": "steps", "start": 312, "end": 317}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#9c5f0939-8540-411e-87b3-99579a6d64d0", "surface_form": "weight decay coefficient", "start": 322, "end": 346}, {"id": "#b46d884d-e05d-4994-8c96-93575556d98e", "surface_form": "epochs", "start": 482, "end": 488}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#42b429ae-3492-4b51-b8b5-758e1413e4f8", "surface_form": "0.01", "start": 350, "end": 354}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#6376eb29-7cee-412d-a0dc-f01ca5abaaed", "surface_form": "1e-5", "start": 450, "end": 454}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#83a9975a-8b64-47b7-b82a-830a327a917c", "surface_form": "each dataset", "start": 462, "end": 474}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9f1fa2eb-0168-4be9-b14b-25a3e590c988", "surface_form": "8", "start": 480, "end": 481}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#d3407fa2-04d4-4602-9df2-ed7de3c3a5d8", "surface_form": "1,000 elements datasets", "start": 493, "end": 516}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#30ba0f0e-8221-4f22-8b6e-95f8436ce3cd", "surface_form": "64", "start": 518, "end": 520}]}, "c4": {"id": "c4", "type": "c", "subtype": null, "surface_forms": [{"id": "#2bd0c31b-3ebb-4797-961a-9e7449abb781", "surface_form": "100-element datasets", "start": 525, "end": 545}]}, "v9": {"id": "v9", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9a02f561-5d32-426d-98d8-424601f845a3", "surface_form": "128", "start": 550, "end": 553}]}, "c5": {"id": "c5", "type": "c", "subtype": null, "surface_forms": [{"id": "#85406784-f98a-4937-aa7c-cd60896a468e", "surface_form": "20-element datasets", "start": 558, "end": 577}]}}, "relations": {"r18": {"id": "r18", "source": "c1", "target": "v1", "evidences": [{"id": "#9fff637e-476d-4197-b771-d04f7bc7826b", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "cv"}, "r0": {"id": "r0", "source": "v1", "target": "p2", "evidences": [{"id": "#1f4bab67-3623-40b5-b150-abb175d76a10", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "vp"}, "r1": {"id": "r1", "source": "c1", "target": "v2", "evidences": [{"id": "#aad38e4a-02db-4d04-9371-69de1a1cab3d", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "cv"}, "r2": {"id": "r2", "source": "v2", "target": "p3", "evidences": [{"id": "#057e3405-ce28-461e-aa67-afd4c756ccf9", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "vp"}, "r3": {"id": "r3", "source": "c1", "target": "v3", "evidences": [{"id": "#032086aa-d7c8-4d40-b751-c09200a50ce4", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "cv"}, "r4": {"id": "r4", "source": "v3", "target": "p4", "evidences": [{"id": "#64539824-40cb-4b81-a4a4-774a2279503f", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "vp"}, "r5": {"id": "r5", "source": "c1", "target": "v5", "evidences": [{"id": "#016d5cb9-58ef-427c-bc5d-3b038b1ae95c", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "cv"}, "r6": {"id": "r6", "source": "v5", "target": "p6", "evidences": [{"id": "#c61c53a7-0b39-419b-b51a-831359a7c59a", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "vp"}, "r7": {"id": "r7", "source": "c1", "target": "v4", "evidences": [{"id": "#47e19318-a524-499d-a72d-adbc3760cedd", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "cv"}, "r8": {"id": "r8", "source": "v4", "target": "p5", "evidences": [{"id": "#cadd14a8-f448-4115-bdcb-3949d5e8a229", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "vp"}, "r9": {"id": "r9", "source": "p5", "target": "a18", "evidences": [{"id": "#867febf1-a996-421d-aad9-dd13da5d95e4", "evidence_sentence": "For each dataset, we compare our results with the baseline which results were separately obtained based on the hyperparameter search with the same batch size \\(=64\\) , learning rates \\(\\in \\lbrace 1e-5, 2e-5, 3e-5\\rbrace \\) , epochs number \\(\\in \\lbrace 8,16,64,128\\rbrace \\) , linear warmup for the first 6% of steps and weight decay coefficient \\(=0.01\\) .", "start": 0, "end": 358}], "label": "pa"}, "r10": {"id": "r10", "source": "c2", "target": "v6", "evidences": [{"id": "#a76db1dd-aad8-4c88-ab6b-43bc702ebe9e", "evidence_sentence": "The final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.", "start": 360, "end": 579}], "label": "cv"}, "r11": {"id": "r11", "source": "v6", "target": "p3", "evidences": [{"id": "#68b17d58-d862-41a5-8d69-0aea602f6d42", "evidence_sentence": "The final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.", "start": 360, "end": 579}], "label": "vp"}, "r12": {"id": "r12", "source": "c3", "target": "v7", "evidences": [{"id": "#5a61c133-90cb-4d60-91dd-e0203373e335", "evidence_sentence": "The final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.", "start": 360, "end": 579}], "label": "cv"}, "r13": {"id": "r13", "source": "v7", "target": "p6", "evidences": [{"id": "#5318f646-f1f1-4d6d-9f26-e47d4b384e34", "evidence_sentence": "The final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.", "start": 360, "end": 579}], "label": "vp"}, "r14": {"id": "r14", "source": "c4", "target": "v8", "evidences": [{"id": "#a2d8d1a2-a377-4c2e-989b-b9e58b9ba1e4", "evidence_sentence": "The final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.", "start": 360, "end": 579}], "label": "cv"}, "r15": {"id": "r15", "source": "v8", "target": "p6", "evidences": [{"id": "#fb8fff3e-bf8c-4d41-b2d6-71e46a8a9ef8", "evidence_sentence": "The final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.", "start": 360, "end": 579}], "label": "vp"}, "r16": {"id": "r16", "source": "c5", "target": "v9", "evidences": [{"id": "#e0be0940-c33e-4216-aaa7-22d6e6ba7745", "evidence_sentence": "The final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.", "start": 360, "end": 579}], "label": "cv"}, "r17": {"id": "r17", "source": "v9", "target": "p6", "evidences": [{"id": "#442dcff8-2ead-4b75-8d2f-b843dcfbdaee", "evidence_sentence": "The final best hyperparameters for the baselines are the following: the learning rate of \\(1e-5\\)  for each dataset, and 8 epochs for 1,000 elements datasets, 64 for 100-element datasets and 128 for 20-element datasets.", "start": 360, "end": 579}], "label": "vp"}}}, "2211.15202_32": {"annotator_id": "tsa", "document_id": "2211.15202", "paragraph_index": 32, "text": "Each DML loss has its own hyperparameter search. The search space of hyperparameter \\(\\beta \\)  was the same for all methods: \\(\\beta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . Apart from that, there are parameters depending on the method.\r\nFor the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) . The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .\r\nThe ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .\r\n", "entities": {"p7": {"id": "p7", "type": "p", "subtype": null, "surface_forms": [{"id": "#e0b12a31-5381-4b44-a253-509cc8185be1", "surface_form": "\\beta", "start": 86, "end": 91}, {"id": "#aa4464d7-c1c2-45f3-9f7c-ae5ea15c5685", "surface_form": "\\beta", "start": 128, "end": 133}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#b05a59bc-9e69-4c56-8648-4bdfce9194b5", "surface_form": "for all methods", "start": 109, "end": 124}]}, "v1": {"id": "v1", "type": "v", "subtype": "s", "surface_forms": [{"id": "#1d7a03b1-3403-4daf-bf7e-add2dfec9007", "surface_form": "\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace", "start": 138, "end": 176}]}, "a8": {"id": "a8", "type": "a", "subtype": null, "surface_forms": [{"id": "#e7fea104-b107-4596-8fcf-3076e77cd822", "surface_form": "Triplet Loss", "start": 253, "end": 265}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#b2b203f6-fa81-4314-bb5a-1b2fb8c2b26b", "surface_form": "grid search", "start": 270, "end": 281}]}, "p8": {"id": "p8", "type": "p", "subtype": null, "surface_forms": [{"id": "#de22dbd1-a690-4c0e-8cc1-4ca89347e42c", "surface_form": "m", "start": 319, "end": 320}]}, "v2": {"id": "v2", "type": "v", "subtype": "s", "surface_forms": [{"id": "#804d17da-bdd3-4b57-8872-11e88d0748f1", "surface_form": "\\lbrace 1, 3 ,5, 7, 9\\rbrace", "start": 325, "end": 353}]}, "a12": {"id": "a12", "type": "a", "subtype": null, "surface_forms": [{"id": "#8902d870-d816-4740-9bca-7ec1fe6d91a6", "surface_form": "SupCon Loss", "start": 378, "end": 389}]}, "p9": {"id": "p9", "type": "p", "subtype": null, "surface_forms": [{"id": "#96a366c6-20cd-4d71-85e8-5eae6b77fe89", "surface_form": "\\tau", "start": 415, "end": 419}, {"id": "#04cea9a8-288c-48f7-b72c-df2c202f261a", "surface_form": "k", "start": 540, "end": 541}]}, "v3": {"id": "v3", "type": "v", "subtype": "s", "surface_forms": [{"id": "#a2378ab8-b008-4c00-a764-6c00703b76a4", "surface_form": "\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace", "start": 424, "end": 462}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#2c91357f-b459-4e02-bf3b-9f1f63aa2047", "surface_form": "SoftTriple loss,", "start": 483, "end": 499}]}, "v4": {"id": "v4", "type": "v", "subtype": "s", "surface_forms": [{"id": "#5442c00c-4855-4c9b-bf0e-65f937ead631", "surface_form": "\\lbrace 5, 25, 1,000, 2000\\rbrace", "start": 546, "end": 579}]}, "p10": {"id": "p10", "type": "p", "subtype": null, "surface_forms": [{"id": "#ebb7c44a-a329-413f-b8bd-fac1acc9e211", "surface_form": "\\gamma", "start": 587, "end": 593}]}, "v5": {"id": "v5", "type": "v", "subtype": "s", "surface_forms": [{"id": "#036f92ac-a48f-4cae-b5b2-c214be31cd7a", "surface_form": "\\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace", "start": 598, "end": 640}]}, "p11": {"id": "p11", "type": "p", "subtype": null, "surface_forms": [{"id": "#9b0f8b69-1ad7-49ad-85f7-92946941b46b", "surface_form": "\\lambda", "start": 648, "end": 655}]}, "v6": {"id": "v6", "type": "v", "subtype": "s", "surface_forms": [{"id": "#8f67ba66-34aa-4366-a137-73607b115fe5", "surface_form": "\\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace", "start": 660, "end": 693}]}, "p12": {"id": "p12", "type": "p", "subtype": null, "surface_forms": [{"id": "#3645faaf-040d-49d8-9e9a-e2d60bfa8566", "surface_form": "\\delta", "start": 705, "end": 711}, {"id": "#5c30eb7a-fbe5-4969-a824-293eff7a42a3", "surface_form": "softmax scale", "start": 840, "end": 853}]}, "v7": {"id": "v7", "type": "v", "subtype": "s", "surface_forms": [{"id": "#385a8b3d-6dcd-4935-a418-eff3050a964b", "surface_form": "\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace", "start": 716, "end": 757}]}, "a13": {"id": "a13", "type": "a", "subtype": null, "surface_forms": [{"id": "#fa4c8cba-1fd0-4181-99f7-bb78031f1a2f", "surface_form": "ProxyNCA Loss", "start": 767, "end": 780}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#fdfc7f26-23b1-4832-b02b-9623efe3b6fa", "surface_form": "grid search", "start": 812, "end": 823}]}, "v8": {"id": "v8", "type": "v", "subtype": "s", "surface_forms": [{"id": "#1179b259-b968-4a68-87a7-1a4917b9d9a9", "surface_form": "\\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace", "start": 858, "end": 918}]}, "a15": {"id": "a15", "type": "a", "subtype": null, "surface_forms": [{"id": "#9da89b93-4e7e-487b-bcf9-6c8d4e60570d", "surface_form": "ProxyAnchor Loss", "start": 928, "end": 944}]}, "p13": {"id": "p13", "type": "p", "subtype": null, "surface_forms": [{"id": "#f256b126-e8cd-4c78-9d13-bad846997db2", "surface_form": "\\alpha", "start": 1012, "end": 1018}]}, "v9": {"id": "v9", "type": "v", "subtype": "s", "surface_forms": [{"id": "#b58f8e42-9981-4d83-b1f1-fc8e024ab9dd", "surface_form": "\\lbrace 16, 32, 64, 128\\rbrace", "start": 1023, "end": 1053}]}, "p14": {"id": "p14", "type": "p", "subtype": null, "surface_forms": [{"id": "#455ac1dc-e358-4ae9-891f-68e228413904", "surface_form": "\\delta", "start": 1064, "end": 1070}]}, "v10": {"id": "v10", "type": "v", "subtype": "s", "surface_forms": [{"id": "#da9ba332-c002-426d-9c4c-5d1dc436b789", "surface_form": "\\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace", "start": 1075, "end": 1116}]}}, "relations": {"r28": {"id": "r28", "source": "c1", "target": "v1", "evidences": [{"id": "#252285a1-9a81-4fe5-953b-ecfddc41c5e2", "evidence_sentence": "The search space of hyperparameter \\(\\beta \\)  was the same for all methods: \\(\\beta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 49, "end": 181}], "label": "cv"}, "r0": {"id": "r0", "source": "v1", "target": "p7", "evidences": [{"id": "#9adcba40-6d23-487e-b90a-08bacccace7f", "evidence_sentence": "The search space of hyperparameter \\(\\beta \\)  was the same for all methods: \\(\\beta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 49, "end": 181}], "label": "vp"}, "r1": {"id": "r1", "source": "v2", "target": "p8", "evidences": [{"id": "#1191866b-a978-4aad-94d9-978fdaad8c5b", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) .", "start": 246, "end": 359}], "label": "vp"}, "r2": {"id": "r2", "source": "p8", "target": "a8", "evidences": [{"id": "#65301f38-6c2d-4b16-a3a9-625a1a320fa1", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) .", "start": 246, "end": 359}], "label": "pa"}, "r3": {"id": "r3", "source": "v3", "target": "p9", "evidences": [{"id": "#745927ff-d162-456b-83a6-5a8f68924978", "evidence_sentence": "The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 360, "end": 468}], "label": "vp"}, "r4": {"id": "r4", "source": "p9", "target": "a12", "evidences": [{"id": "#6927f811-0e1f-4814-81f5-089996fd6118", "evidence_sentence": "The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 360, "end": 468}], "label": "pa"}, "r5": {"id": "r5", "source": "v4", "target": "p9", "evidences": [{"id": "#3931a6ab-115c-4576-a030-06ba40c9af12", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}], "label": "vp"}, "r6": {"id": "r6", "source": "p9", "target": "a14", "evidences": [{"id": "#cfaace1b-5cff-4a84-9fd3-ab677cb25fbd", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}], "label": "pa"}, "r7": {"id": "r7", "source": "v5", "target": "p10", "evidences": [{"id": "#9fac43fd-3341-4ca3-9987-5db2df07e803", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}], "label": "vp"}, "r8": {"id": "r8", "source": "p10", "target": "a14", "evidences": [{"id": "#d21b5871-9606-4a43-900e-f12317166423", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}], "label": "pa"}, "r9": {"id": "r9", "source": "v6", "target": "p11", "evidences": [{"id": "#6d0bf432-8041-4372-b43e-772ca0aeb5c6", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}], "label": "vp"}, "r10": {"id": "r10", "source": "p11", "target": "a14", "evidences": [{"id": "#fba0b4eb-f0a5-4de9-88a1-49ea38b87d10", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}], "label": "pa"}, "r11": {"id": "r11", "source": "v7", "target": "p12", "evidences": [{"id": "#631300cb-575b-497c-b228-14dfa10d754a", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}], "label": "vp"}, "r12": {"id": "r12", "source": "p12", "target": "a14", "evidences": [{"id": "#f8ec9aaf-c962-4261-822f-3e37b365c93e", "evidence_sentence": "In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 469, "end": 763}], "label": "pa"}, "r13": {"id": "r13", "source": "c2", "target": "v2", "evidences": [{"id": "#984c7168-36fb-42fd-ba0a-1dfae10e0310", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) .", "start": 246, "end": 359}], "label": "cv"}, "r14": {"id": "r14", "source": "c2", "target": "v3", "evidences": [{"id": "#e96e9617-984f-45d4-8ce2-96fdbaa7b768", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 246, "end": 468}], "label": "cv"}, "r15": {"id": "r15", "source": "c2", "target": "v4", "evidences": [{"id": "#f6ebe47b-ddce-4e0e-8935-9e98a4e74b2c", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 246, "end": 763}], "label": "cv"}, "r16": {"id": "r16", "source": "c2", "target": "v5", "evidences": [{"id": "#fe88ff63-eced-4900-8075-796562a41b50", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 246, "end": 763}], "label": "cv"}, "r17": {"id": "r17", "source": "c2", "target": "v6", "evidences": [{"id": "#024d8e97-22e1-4276-ada8-e7ac329e0ffb", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 246, "end": 763}], "label": "cv"}, "r18": {"id": "r18", "source": "c2", "target": "v7", "evidences": [{"id": "#a8fc1c24-f14e-40de-81ac-014da2805dca", "evidence_sentence": "For the Triplet Loss the grid search included additional hyperparameter \\(m \\in \\lbrace 1, 3 ,5, 7, 9\\rbrace \\) . The model with the SupCon Loss was optimised based on \\(\\tau \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) . In the case of SoftTriple loss, we searched the following parameters: \\(k \\in \\lbrace 5, 25, 1,000, 2000\\rbrace \\) , \\(\\gamma \\in \\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\rbrace \\) , \\(\\lambda \\in \\lbrace 1,3,3.3,4,6,\\\\8,10\\rbrace \\) , and \\(\\delta \\in \\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\rbrace \\) .", "start": 246, "end": 763}], "label": "cv"}, "r19": {"id": "r19", "source": "c3", "target": "v8", "evidences": [{"id": "#add34fe3-e3f9-4fa3-aea9-c0265d9fc8fe", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .", "start": 764, "end": 924}], "label": "cv"}, "r20": {"id": "r20", "source": "v8", "target": "p12", "evidences": [{"id": "#8c634374-1f5e-4a7e-9ff4-a69e7f16590c", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .", "start": 764, "end": 924}], "label": "vp"}, "r21": {"id": "r21", "source": "p12", "target": "a13", "evidences": [{"id": "#135a96f0-48b2-488e-b791-eb28960fac01", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .", "start": 764, "end": 924}], "label": "pa"}, "r22": {"id": "r22", "source": "c3", "target": "v9", "evidences": [{"id": "#60b9d144-cb7d-4757-983c-650b391f4456", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .\r\nThe ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 764, "end": 1123}], "label": "cv"}, "r23": {"id": "r23", "source": "v9", "target": "p13", "evidences": [{"id": "#6c98b8c8-1df2-4da0-99ce-2b1bbec46a28", "evidence_sentence": "The ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 926, "end": 1123}], "label": "vp"}, "r24": {"id": "r24", "source": "p13", "target": "a15", "evidences": [{"id": "#082865c0-cb07-4b9f-947d-cf0a833bd478", "evidence_sentence": "The ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 926, "end": 1123}], "label": "pa"}, "r25": {"id": "r25", "source": "c3", "target": "v10", "evidences": [{"id": "#8a332a13-3ba5-42fb-b07f-7699652de925", "evidence_sentence": "The ProxyNCA Loss was optimised according to the grid search that included \\(softmax scale \\in \\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\rbrace \\) .\r\nThe ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 764, "end": 1123}], "label": "cv"}, "r26": {"id": "r26", "source": "v10", "target": "p14", "evidences": [{"id": "#a2f4f0b0-7fdb-461f-8b3d-af98bb9c4e5e", "evidence_sentence": "The ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 926, "end": 1123}], "label": "vp"}, "r27": {"id": "r27", "source": "p14", "target": "a15", "evidences": [{"id": "#e2bb2a0e-7749-44ab-9aa4-1a89d0469736", "evidence_sentence": "The ProxyAnchor Loss was optimised based on the following additional hyperparameters: \\(\\alpha \\in \\lbrace 16, 32, 64, 128\\rbrace \\)  and \\(\\delta \\in \\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\rbrace \\) .", "start": 926, "end": 1123}], "label": "pa"}}}, "2211.14208_53": {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 53, "text": "By varying \\(T\\) , we investigate how the model accuracy changes. The detailed results are in Fig.\u00a0REF . In Chameleon, GREAD-BS achieves the highest mean test accuracy at \\(T=1.7\\) .\r\n", "entities": {"p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#d447165d-adb8-4dc7-88c2-58919f450ef1", "surface_form": "T", "start": 13, "end": 14}, {"id": "#85f17879-2375-4fcd-ba46-7e6abb97f2be", "surface_form": "T", "start": 173, "end": 174}]}, "a40": {"id": "a40", "type": "a", "subtype": null, "surface_forms": [{"id": "#ef054e96-a44f-4683-90e0-034064921f5d", "surface_form": "Chameleon", "start": 108, "end": 117}]}, "a4": {"id": "a4", "type": "a", "subtype": null, "surface_forms": [{"id": "#d804c00e-f16b-4d8d-86ea-c2fd9af8a9ad", "surface_form": "GREAD", "start": 119, "end": 124}]}, "a8": {"id": "a8", "type": "a", "subtype": null, "surface_forms": [{"id": "#eedd9438-2d7f-4f41-9068-c0e36e41213a", "surface_form": "BS", "start": 125, "end": 127}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#779ad168-f4a9-4ea2-b9ca-6b5510c15c5f", "surface_form": "achieves the highest mean test accuracy", "start": 128, "end": 167}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9040dbfd-140c-4d5c-a203-1ce7764068c5", "surface_form": "1.7", "start": 175, "end": 178}]}}, "relations": {"r1": {"id": "r1", "source": "v1", "target": "p3", "evidences": [{"id": "#2e08c922-9274-44eb-aed7-67fb08e6f540", "evidence_sentence": "In Chameleon, GREAD-BS achieves the highest mean test accuracy at \\(T=1.7\\) .", "start": 105, "end": 182}], "label": "vp"}, "r0": {"id": "r0", "source": "c1", "target": "v1", "evidences": [{"id": "#6edb526d-246b-487d-86d9-fa0cfbe3825b", "evidence_sentence": "In Chameleon, GREAD-BS achieves the highest mean test accuracy at \\(T=1.7\\) .", "start": 105, "end": 182}], "label": "cv"}}}, "2211.14208_58": {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 58, "text": "We use the synthetic dataset, called cSBMs\u00a0[16], to demonstrate the mitigation of oversmoothing. This synthetic data is an undirected graph representing 100 nodes in a two-dimensional space with two classes randomly connected with a probability of \\(p=0.9\\) . We report the layer-wise Dirichlet energy given a GNN of 40 layers.\r\n", "entities": {"a58": {"id": "a58", "type": "a", "subtype": null, "surface_forms": [{"id": "#a4cf4268-95d0-4154-8de2-664ff0535caa", "surface_form": "cSBMs", "start": 37, "end": 42}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#bdeb7658-07f5-4622-9466-0ef019bec09b", "surface_form": "demonstrate the mitigation of oversmoothing", "start": 52, "end": 95}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#bacf5fb3-175b-42c8-ad33-c195b8d9abc5", "surface_form": "100", "start": 153, "end": 156}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#b70c2872-21c4-4e85-bc78-779a04697cf3", "surface_form": "two", "start": 168, "end": 171}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#424f680a-6eca-47aa-b335-3ceb7cba3c1f", "surface_form": "two", "start": 195, "end": 198}]}, "a1": {"id": "a1", "type": "a", "subtype": null, "surface_forms": [{"id": "#5bc6ba3e-5d23-4f07-a6d8-cfac43c00f83", "surface_form": "GNN", "start": 310, "end": 313}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#f2d09225-e3b5-485c-99a7-258b282d99c8", "surface_form": "40", "start": 317, "end": 319}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#d65c1c12-b83b-4e54-8e5c-d09318022871", "surface_form": "layers", "start": 320, "end": 326}]}}, "relations": {"r2": {"id": "r2", "source": "c1", "target": "v4", "evidences": [{"id": "#7ba29bde-0be1-4512-9461-0a8d6414c8e1", "evidence_sentence": "We use the synthetic dataset, called cSBMs\u00a0[16], to demonstrate the mitigation of oversmoothing. This synthetic data is an undirected graph representing 100 nodes in a two-dimensional space with two classes randomly connected with a probability of \\(p=0.9\\) . We report the layer-wise Dirichlet energy given a GNN of 40 layers.", "start": 0, "end": 327}], "label": "cv"}, "r0": {"id": "r0", "source": "v4", "target": "p6", "evidences": [{"id": "#82189298-e992-4047-9aa6-d49ce96ee9a4", "evidence_sentence": "We report the layer-wise Dirichlet energy given a GNN of 40 layers.", "start": 260, "end": 327}], "label": "vp"}, "r1": {"id": "r1", "source": "p6", "target": "a1", "evidences": [{"id": "#8d2df259-dc93-482f-9acf-1a2376fd8b9a", "evidence_sentence": "We report the layer-wise Dirichlet energy given a GNN of 40 layers.", "start": 260, "end": 327}], "label": "pa"}}}, "2210.10073_28": {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 28, "text": "The pipeline of CRPSE is as follows. We use scispaCy to detect entities in a paper. For each entity detected, we determine whether this entity is a published entity based off checking whether it is collected in the published scientific entity-papers mapping dataset. If yes, we sort its candidate source papers in order by one of the two sorting criteria above and select the top \\(K\\)  papers as the final recommended source papers. \\(K\\)  is usually set as 1, 5 or 10.\r\n", "entities": {"a6": {"id": "a6", "type": "a", "subtype": null, "surface_forms": [{"id": "#35bee849-3ae6-4a45-a2b9-b281266bfa28", "surface_form": "CRPSE", "start": 16, "end": 21}]}, "a7": {"id": "a7", "type": "a", "subtype": null, "surface_forms": [{"id": "#d5fec7b5-ae28-4bb5-8984-3c32f6f1e90c", "surface_form": "published scientific entity-papers mapping dataset", "start": 215, "end": 265}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#5c28b5c7-2002-422d-b24b-14574fb082bd", "surface_form": "one", "start": 323, "end": 326}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#253cad3a-04ed-4d9c-94b7-a1908a0022b5", "surface_form": "two", "start": 334, "end": 337}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#16718786-e466-475e-996d-3cdac931e3b9", "surface_form": "K", "start": 382, "end": 383}, {"id": "#1afd769e-0f04-4068-9823-84845964f876", "surface_form": "K", "start": 436, "end": 437}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#50d65153-f356-4c44-81b3-fd6853f233be", "surface_form": "usually set as", "start": 444, "end": 458}]}, "v3": {"id": "v3", "type": "v", "subtype": "s", "surface_forms": [{"id": "#a1b91977-ab56-4fda-9122-d7bfb886d180", "surface_form": "1, 5 or 10", "start": 459, "end": 469}]}}, "relations": {"r3": {"id": "r3", "source": "p1", "target": "a6", "evidences": [{"id": "#7c6e85b8-4580-42e0-b4d3-5f284f4bcc47", "evidence_sentence": "The pipeline of CRPSE is as follows. We use scispaCy to detect entities in a paper. For each entity detected, we determine whether this entity is a published entity based off checking whether it is collected in the published scientific entity-papers mapping dataset. If yes, we sort its candidate source papers in order by one of the two sorting criteria above and select the top \\(K\\)  papers as the final recommended source papers.", "start": 0, "end": 433}], "label": "pa"}, "r1": {"id": "r1", "source": "p1", "target": "a6", "evidences": [{"id": "#6eeef99c-95cb-4d13-98ab-3aac83d345f9", "evidence_sentence": "The pipeline of CRPSE is as follows. We use scispaCy to detect entities in a paper. For each entity detected, we determine whether this entity is a published entity based off checking whether it is collected in the published scientific entity-papers mapping dataset. If yes, we sort its candidate source papers in order by one of the two sorting criteria above and select the top \\(K\\)  papers as the final recommended source papers. \\(K\\)  is usually set as 1, 5 or 10.", "start": 0, "end": 470}, {"id": "#6420b2da-24be-4450-a401-6bb97b132962", "evidence_sentence": "\\(K\\)  is usually set as 1, 5 or 10.", "start": 434, "end": 470}], "label": "pa"}, "r2": {"id": "r2", "source": "c1", "target": "v3", "evidences": [{"id": "#c5cfc29f-88d3-4c7c-ab3e-bc27dcd787f9", "evidence_sentence": "\\(K\\)  is usually set as 1, 5 or 10.", "start": 434, "end": 470}], "label": "cv"}}}, "2005.11184_17": {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 17, "text": "Where \\(wc(y)\\)  is the word count in the predicted transcript. \\(\\alpha \\)  and \\(\\beta \\)  control the contribution of LM and the number of words in the predicted transcript. In this study, we use values 1.96 & 6.0 for \\(\\alpha \\)  and \\(\\beta \\) , respectively.\r\n", "entities": {"p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#a6976b86-ac16-47fe-83bb-6bed39ea2c40", "surface_form": "\\alpha", "start": 66, "end": 72}, {"id": "#037bf717-2658-41e1-befc-676e7e4ce133", "surface_form": "\\alpha", "start": 223, "end": 229}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#98d4c39b-e63e-4d41-96c6-c312a8dee4cf", "surface_form": "\\beta", "start": 83, "end": 88}, {"id": "#d7949b19-6ee6-4ffd-9850-9ebc94e3a768", "surface_form": "\\beta", "start": 240, "end": 245}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#c970245d-3a51-42df-9453-6ae93687bc7b", "surface_form": "In this study", "start": 177, "end": 190}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#0f6cbc3e-7936-493d-a998-0381d1ba2b1a", "surface_form": "1.96", "start": 206, "end": 210}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#fa5b688c-73be-4493-8819-070c6b91b53a", "surface_form": "6.0", "start": 213, "end": 216}]}}, "relations": {"r3": {"id": "r3", "source": "c1", "target": "v1", "evidences": [{"id": "#a0751c9a-ea65-41dc-8f08-33d5215e793c", "evidence_sentence": "In this study, we use values 1.96 & 6.0 for \\(\\alpha \\)  and \\(\\beta \\) , respectively.", "start": 177, "end": 264}], "label": "cv"}, "r0": {"id": "r0", "source": "v1", "target": "p1", "evidences": [{"id": "#c3da6ef9-3785-4527-87bf-8bb3407ee8da", "evidence_sentence": "In this study, we use values 1.96 & 6.0 for \\(\\alpha \\)  and \\(\\beta \\) , respectively.", "start": 177, "end": 264}], "label": "vp"}, "r1": {"id": "r1", "source": "c1", "target": "v2", "evidences": [{"id": "#f7df2127-f569-4152-8b40-3ab802b620fd", "evidence_sentence": "In this study, we use values 1.96 & 6.0 for \\(\\alpha \\)  and \\(\\beta \\) , respectively.", "start": 177, "end": 264}], "label": "cv"}, "r2": {"id": "r2", "source": "v2", "target": "p2", "evidences": [{"id": "#8a61b616-e10f-48c8-a4b5-822d38f6a19e", "evidence_sentence": "In this study, we use values 1.96 & 6.0 for \\(\\alpha \\)  and \\(\\beta \\) , respectively.", "start": 177, "end": 264}], "label": "vp"}}}, "2005.11184_16": {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 16, "text": "At the test time, the output is conditioned on an N-gram LM using prefix beam search decoding\u00a0[27], is shown as follows.\r\n\\(Q(y) = log(p(l\\textsubscript {t}/x)) + \\alpha *log(pLM(y)) + \\beta *wc(y) \\) \r\n", "entities": {"a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#a80666a1-ef63-4b65-af0d-c0109bef25c2", "surface_form": "prefix beam search decoding", "start": 66, "end": 93}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#d7f33376-a944-4552-a32e-928703324ebc", "surface_form": "\\alpha", "start": 162, "end": 168}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#7632f8f3-7b12-4ce6-aa9a-6c0ea1e6518c", "surface_form": "\\beta", "start": 184, "end": 189}]}}, "relations": {"r1": {"id": "r1", "source": "p1", "target": "a14", "evidences": [{"id": "#cd567754-a64c-4650-a8ac-baf9688b1da5", "evidence_sentence": "At the test time, the output is conditioned on an N-gram LM using prefix beam search decoding\u00a0[27], is shown as follows.\r\n\\(Q(y) = log(p(l\\textsubscript {t}/x)) + \\alpha *log(pLM(y)) + \\beta *wc(y) \\)", "start": 0, "end": 200}], "label": "pa"}, "r0": {"id": "r0", "source": "p2", "target": "a14", "evidences": [{"id": "#c0313bc0-2019-4b00-9f93-a2f344461ea9", "evidence_sentence": "At the test time, the output is conditioned on an N-gram LM using prefix beam search decoding\u00a0[27], is shown as follows.\r\n\\(Q(y) = log(p(l\\textsubscript {t}/x)) + \\alpha *log(pLM(y)) + \\beta *wc(y) \\)", "start": 0, "end": 200}], "label": "pa"}}}, "2005.11184_21": {"annotator_id": "tsa", "document_id": "2005.11184", "paragraph_index": 21, "text": "We experimented with two different approaches for the NER from speech task: E2E and a classical two-step approach. For the E2E approach, experiments were carried out on the model explained in Section\u00a0REF . At the test time, Prefix beam search decoding with a beam-width of 1024 and a 4-gram LM (trained on DATA2) is used. In the case of classical two-step approach, we use Baidu's DS2 as the ASR component and Flair as a NER tagger component. In the two-step approach, audio is first transcribed using the ASR component, and then the output is passed to the Flair tagger as shown in the Figure\u00a0REF . The Flair tagger used in the two-step approach is explained in the Section\u00a0REF .\r\n", "entities": {"v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#aec2a4ec-8a9a-4e3b-8dda-cefdca9b4b1e", "surface_form": "two", "start": 21, "end": 24}]}, "a2": {"id": "a2", "type": "a", "subtype": null, "surface_forms": [{"id": "#78513a26-4be5-4e1b-9b7c-c122acd74961", "surface_form": "E2E", "start": 76, "end": 79}, {"id": "#2caee9ab-0ad4-4d6d-b409-e7f5e5576ae6", "surface_form": "E2E approach", "start": 123, "end": 135}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#a856e203-ad84-43f6-aca6-8ce20b6e8217", "surface_form": "At the test time", "start": 206, "end": 222}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#d3efabe8-c258-45c1-a01c-2e6fd4fb6772", "surface_form": "Prefix beam search decoding", "start": 224, "end": 251}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#9c820a74-1f17-4bca-80f7-ddd496ea6b40", "surface_form": "beam-width", "start": 259, "end": 269}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#dca65b6e-442e-41a1-9605-dda7416b31e6", "surface_form": "1024", "start": 273, "end": 277}]}, "a8": {"id": "a8", "type": "a", "subtype": null, "surface_forms": [{"id": "#b7b10682-c200-401b-8cd2-bf0cf54a8ca3", "surface_form": "DATA2", "start": 306, "end": 311}]}, "a10": {"id": "a10", "type": "a", "subtype": null, "surface_forms": [{"id": "#cc653e98-4336-405b-935f-c68d90f15584", "surface_form": "Baidu's DS2", "start": 373, "end": 384}]}}, "relations": {"r2": {"id": "r2", "source": "c1", "target": "v2", "evidences": [{"id": "#86b3da78-d804-45cd-a590-566afa351148", "evidence_sentence": "At the test time, Prefix beam search decoding with a beam-width of 1024 and a 4-gram LM (trained on DATA2) is used.", "start": 206, "end": 321}], "label": "cv"}, "r0": {"id": "r0", "source": "v2", "target": "p3", "evidences": [{"id": "#ad2f7f76-9218-479e-8ec6-1180570838aa", "evidence_sentence": "At the test time, Prefix beam search decoding with a beam-width of 1024 and a 4-gram LM (trained on DATA2) is used.", "start": 206, "end": 321}], "label": "vp"}, "r1": {"id": "r1", "source": "p3", "target": "a14", "evidences": [{"id": "#da3152e3-36db-4224-9656-6fd5ec9db74d", "evidence_sentence": "At the test time, Prefix beam search decoding with a beam-width of 1024 and a 4-gram LM (trained on DATA2) is used.", "start": 206, "end": 321}], "label": "pa"}}}, "1808.09602_23": {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 23, "text": "Given a set of all documents \\(\\mathcal {D}\\) , the model loss function is defined as a weighted sum of the negative log-likelihood loss of all three tasks:\r\n\\(& -\\sum _{(D, R^*, E^*, C^*) \\in \\mathcal {D}} \\Big \\lbrace \\lambda _{\\text{E}}\\log P (E^* \\mid D) \\\\& + \\lambda _{\\text{R}}\\log P (R^* \\mid D) + \\lambda _{\\text{C}}\\log P (C^* \\mid D) \\Big \\rbrace \\) \r\n", "entities": {"a1": {"id": "a1", "type": "a", "subtype": null, "surface_forms": [{"id": "#6e0d19e0-c338-4264-ab7f-e98b4c2d0862", "surface_form": "model", "start": 52, "end": 57}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#974e36bd-14c3-43c9-81fd-eec40f7e9ccc", "surface_form": "three", "start": 144, "end": 149}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#1f17a11e-6542-42c9-9566-6a5c9ac025a7", "surface_form": "\\lambda _{\\text{E}}", "start": 219, "end": 238}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#b87f52b9-3191-4dd0-a832-83b8813bb831", "surface_form": "\\lambda _{\\text{R}}", "start": 264, "end": 283}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#61da1670-00ad-4970-a1c7-0a08e2fada0c", "surface_form": "\\lambda _{\\text{C}}", "start": 305, "end": 324}]}}, "relations": {"r2": {"id": "r2", "source": "p1", "target": "a1", "evidences": [{"id": "#ef38d032-9cc2-4890-a3df-ea2054cd0dac", "evidence_sentence": "Given a set of all documents \\(\\mathcal {D}\\) , the model loss function is defined as a weighted sum of the negative log-likelihood loss of all three tasks:\r\n\\(& -\\sum _{(D, R^*, E^*, C^*) \\in \\mathcal {D}} \\Big \\lbrace \\lambda _{\\text{E}}\\log P (E^* \\mid D) \\\\& + \\lambda _{\\text{R}}\\log P (R^* \\mid D) + \\lambda _{\\text{C}}\\log P (C^* \\mid D) \\Big \\rbrace \\)", "start": 0, "end": 360}], "label": "pa"}, "r0": {"id": "r0", "source": "p2", "target": "a1", "evidences": [{"id": "#7938fa03-0e29-44f7-80d0-2a625bf145c1", "evidence_sentence": "Given a set of all documents \\(\\mathcal {D}\\) , the model loss function is defined as a weighted sum of the negative log-likelihood loss of all three tasks:\r\n\\(& -\\sum _{(D, R^*, E^*, C^*) \\in \\mathcal {D}} \\Big \\lbrace \\lambda _{\\text{E}}\\log P (E^* \\mid D) \\\\& + \\lambda _{\\text{R}}\\log P (R^* \\mid D) + \\lambda _{\\text{C}}\\log P (C^* \\mid D) \\Big \\rbrace \\)", "start": 0, "end": 360}], "label": "pa"}, "r1": {"id": "r1", "source": "p3", "target": "a1", "evidences": [{"id": "#a100f5cd-6225-4361-9caa-a2749f1af3de", "evidence_sentence": "Given a set of all documents \\(\\mathcal {D}\\) , the model loss function is defined as a weighted sum of the negative log-likelihood loss of all three tasks:\r\n\\(& -\\sum _{(D, R^*, E^*, C^*) \\in \\mathcal {D}} \\Big \\lbrace \\lambda _{\\text{E}}\\log P (E^* \\mid D) \\\\& + \\lambda _{\\text{R}}\\log P (R^* \\mid D) + \\lambda _{\\text{C}}\\log P (C^* \\mid D) \\Big \\rbrace \\)", "start": 0, "end": 360}], "label": "pa"}}}, "1808.09602_32": {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 32, "text": "Following previous work, we use beam pruning to reduce the number of pairwise span factors from \\(O(n^4)\\)  to \\(O(n^2)\\)  at both training and test time, where \\(n\\)  is the number of words in the document.\r\nWe define two separate beams: \\(B_{\\text{C}}\\)  to prune spans for the coreference resolution task, and \\(B_{\\text{R}}\\)  for relation extraction. The spans in the beams are sorted by their span scores \\(\\phi _{\\text{mc}}\\)  and \\(\\phi _{\\text{mr}}\\)  respectively, and the sizes of the beams are limited by \\(\\lambda _{\\text{C}} n\\)  and \\(\\lambda _{\\text{R}} n\\) .\r\nWe also limit the maximum width of spans to a fixed number \\(W\\) , which further reduces the number of span factors to \\(O(n)\\) .\r\n", "entities": {"a11": {"id": "a11", "type": "a", "subtype": null, "surface_forms": [{"id": "#ccfb7477-f46e-48ab-9a9e-c77c9adc8e11", "surface_form": "beam pruning", "start": 32, "end": 44}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#c428bcb1-ff99-4dfa-8976-043efee5e442", "surface_form": "two", "start": 218, "end": 221}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#08f6bccf-eaee-4fa3-87f7-34aabb0d324c", "surface_form": "\\lambda _{\\text{C}}", "start": 518, "end": 537}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#0be1b3e2-544e-42d1-b7ed-988c4591e88e", "surface_form": "\\lambda _{\\text{R}}", "start": 549, "end": 568}]}}, "relations": {"r1": {"id": "r1", "source": "p4", "target": "a11", "evidences": [{"id": "#920f59e1-6e36-425e-8293-f4033ccf0b63", "evidence_sentence": "Following previous work, we use beam pruning to reduce the number of pairwise span factors from \\(O(n^4)\\)  to \\(O(n^2)\\)  at both training and test time, where \\(n\\)  is the number of words in the document.\r\nWe define two separate beams: \\(B_{\\text{C}}\\)  to prune spans for the coreference resolution task, and \\(B_{\\text{R}}\\)  for relation extraction. The spans in the beams are sorted by their span scores \\(\\phi _{\\text{mc}}\\)  and \\(\\phi _{\\text{mr}}\\)  respectively, and the sizes of the beams are limited by \\(\\lambda _{\\text{C}} n\\)  and \\(\\lambda _{\\text{R}} n\\) .", "start": 0, "end": 575}], "label": "pa"}, "r0": {"id": "r0", "source": "p5", "target": "a11", "evidences": [{"id": "#13ddb662-0820-4842-937f-0530508ff2fa", "evidence_sentence": "Following previous work, we use beam pruning to reduce the number of pairwise span factors from \\(O(n^4)\\)  to \\(O(n^2)\\)  at both training and test time, where \\(n\\)  is the number of words in the document.\r\nWe define two separate beams: \\(B_{\\text{C}}\\)  to prune spans for the coreference resolution task, and \\(B_{\\text{R}}\\)  for relation extraction. The spans in the beams are sorted by their span scores \\(\\phi _{\\text{mc}}\\)  and \\(\\phi _{\\text{mr}}\\)  respectively, and the sizes of the beams are limited by \\(\\lambda _{\\text{C}} n\\)  and \\(\\lambda _{\\text{R}} n\\) .", "start": 0, "end": 575}], "label": "pa"}}}, "2005.00512_65": {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 65, "text": "We divide our 438 annotated documents into training (70%), validation (30%) and test set (30%).\r\nThe base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state. We use a dropout of 0.2 after BiLSTM embeddings. All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers. For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax. We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights. We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score. All our models were trained using 48Gb Quadro RTX 8000 GPUs. The multitask model takes approximately 3 hrs to train.\r\n", "entities": {"v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#7a488cfe-37bd-4cd4-9d52-fcd1c13ec360", "surface_form": "438", "start": 14, "end": 17}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#2697a762-d293-419c-82da-11889a6ca84f", "surface_form": "70%", "start": 53, "end": 56}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#28b88957-6457-4f46-8eef-82ef0e2a3d72", "surface_form": "30%", "start": 71, "end": 74}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#d86a6b4e-a3d8-4f55-9b25-be36f91ca242", "surface_form": "30%", "start": 90, "end": 93}]}, "a6": {"id": "a6", "type": "a", "subtype": null, "surface_forms": [{"id": "#fb9fec7c-fff7-44a7-85cc-41f77f52c364", "surface_form": "model", "start": 136, "end": 141}, {"id": "#5628e381-9e76-4263-9125-4acaf18f0cb3", "surface_form": "model", "start": 603, "end": 608}]}, "a13": {"id": "a13", "type": "a", "subtype": null, "surface_forms": [{"id": "#89035c95-5a40-4865-8e1b-28e69f5a0190", "surface_form": "SciBERT-base", "start": 155, "end": 167}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#1c77af68-2d6e-4db4-9501-57ac38f5287a", "surface_form": "BiLSTM", "start": 176, "end": 182}, {"id": "#3f44cc99-e718-4886-8302-a8b011bfdb26", "surface_form": "BiLSTM", "start": 238, "end": 244}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#32269e63-2aa5-4be7-8673-56fda6e6458e", "surface_form": "128", "start": 188, "end": 191}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#d14e7fbd-e8c5-4b7e-ba1d-f38b8ddd895e", "surface_form": "hidden state", "start": 194, "end": 206}, {"id": "#4c0ef44a-0235-4bf3-b500-2ebeb87ac97e", "surface_form": "hidden state", "start": 551, "end": 563}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#3677f201-e8b6-47c6-98bf-d940c71285d7", "surface_form": "dropout", "start": 217, "end": 224}, {"id": "#77e61c37-b503-4085-9fc5-f28b8380ab12", "surface_form": "dropout", "start": 371, "end": 378}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#48ffc6bb-b356-4f4f-b03c-bd408cccc046", "surface_form": "0.2", "start": 228, "end": 231}]}, "a18": {"id": "a18", "type": "a", "subtype": null, "surface_forms": [{"id": "#82322f99-efc5-4991-b895-e06d06a5442e", "surface_form": "feedforward networks", "start": 261, "end": 281}, {"id": "#a02000ac-a1d0-40d9-8d84-b554328da4a1", "surface_form": "feedforward layer", "start": 522, "end": 539}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#1dc004fa-d05b-4486-a88b-8e37515cddb4", "surface_form": "two", "start": 298, "end": 301}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#2e27d38a-f82e-42cc-a15d-9d1294a95f5f", "surface_form": "hidden layers", "start": 302, "end": 315}, {"id": "#5b6e9c44-2d22-4447-9c6b-f317afcc8cc9", "surface_form": "dimension", "start": 325, "end": 334}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#71e9cfe6-128b-4af0-96cc-85aae4d176e7", "surface_form": "128", "start": 335, "end": 338}]}, "v9": {"id": "v9", "type": "v", "subtype": "o", "surface_forms": [{"id": "#a95b47a5-80ac-4588-9379-288486c3c0c6", "surface_form": "gelu", "start": 344, "end": 348}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#5edd9f78-5718-4998-b0ea-b97f6402a818", "surface_form": "activation", "start": 349, "end": 359}]}, "v10": {"id": "v10", "type": "v", "subtype": "n", "surface_forms": [{"id": "#c8974ec6-0e80-4c4b-a038-50c7416d7ad6", "surface_form": "0.2", "start": 382, "end": 385}]}, "v11": {"id": "v11", "type": "v", "subtype": "n", "surface_forms": [{"id": "#4f351626-b59d-4e71-a061-b65f4e637c03", "surface_form": "128", "start": 545, "end": 548}]}, "a21": {"id": "a21", "type": "a", "subtype": null, "surface_forms": [{"id": "#3fcb480d-edff-4f32-b82c-673f99603ad8", "surface_form": "softmax", "start": 581, "end": 588}]}, "v12": {"id": "v12", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ccf1a670-2b62-44aa-8560-1ef916c44ecc", "surface_form": "30", "start": 613, "end": 615}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#8b428ec9-5ccb-4b0f-9307-2fb6bca904ac", "surface_form": "epochs", "start": 616, "end": 622}]}, "a22": {"id": "a22", "type": "a", "subtype": null, "surface_forms": [{"id": "#09100685-2022-4f8f-b5e4-bd9a3a669e77", "surface_form": "Adam", "start": 629, "end": 633}, {"id": "#ad6dea75-a336-4564-957b-fb5f22e38834", "surface_form": "early stopping", "start": 730, "end": 744}]}, "v13": {"id": "v13", "type": "v", "subtype": "n", "surface_forms": [{"id": "#299b15ad-953a-4664-b9c2-10a330e0a51d", "surface_form": "1e-3", "start": 649, "end": 653}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "surface_form": "learning rate", "start": 657, "end": 670}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#186b0fcb-4905-4782-b328-f0a9d139e50b", "surface_form": "for all non BERT weights", "start": 671, "end": 695}]}, "v14": {"id": "v14", "type": "v", "subtype": "n", "surface_forms": [{"id": "#27ade662-e3eb-4a71-bfa7-e5161909dbde", "surface_form": "2e-5", "start": 700, "end": 704}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#95b12990-5e20-4df2-aba1-ca98d916bbe9", "surface_form": "for BERT weights", "start": 705, "end": 721}]}, "p8": {"id": "p8", "type": "p", "subtype": null, "surface_forms": [{"id": "#5581acf1-60aa-420c-88df-2e2abcf2aaef", "surface_form": "patience value", "start": 752, "end": 766}]}, "v15": {"id": "v15", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3faea6b8-f987-43bf-92f0-b81c15973ee7", "surface_form": "7", "start": 770, "end": 771}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#d78c347f-4a30-484e-bd35-b77c1f156360", "surface_form": "on the validation", "start": 772, "end": 789}]}, "v16": {"id": "v16", "type": "v", "subtype": "n", "surface_forms": [{"id": "#4875b32b-6be8-449d-93a3-329990103742", "surface_form": "48", "start": 864, "end": 866}]}, "v17": {"id": "v17", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9830730d-7d13-4dc7-9301-cc5454534a5f", "surface_form": "3", "start": 931, "end": 932}]}}, "relations": {"r23": {"id": "r23", "source": "v5", "target": "p1", "evidences": [{"id": "#06db44b0-ddb1-469d-a7b8-e6690c06811e", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state.", "start": 97, "end": 208}], "label": "vp"}, "r0": {"id": "r0", "source": "p1", "target": "a14", "evidences": [{"id": "#708d50d7-1c98-4543-abea-59c5536be8e8", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state.", "start": 97, "end": 208}], "label": "pa"}, "r1": {"id": "r1", "source": "v6", "target": "p2", "evidences": [{"id": "#4acd1b39-650e-44e9-b226-47994fe4462a", "evidence_sentence": "We use a dropout of 0.2 after BiLSTM embeddings.", "start": 209, "end": 257}], "label": "vp"}, "r2": {"id": "r2", "source": "v7", "target": "p3", "evidences": [{"id": "#264d74a8-d8b9-40eb-a71b-915886b79739", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}], "label": "vp"}, "r3": {"id": "r3", "source": "p3", "target": "a18", "evidences": [{"id": "#56c4fd32-f245-4f62-a1d2-3b543ace0833", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}], "label": "pa"}, "r6": {"id": "r6", "source": "v8", "target": "p3", "evidences": [{"id": "#0cf7b217-68d0-475a-a459-66358e62ca37", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}, {"id": "#81fda297-20ec-407b-a7cd-62fe00c717fc", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}], "label": "vp"}, "r5": {"id": "r5", "source": "p3", "target": "a18", "evidences": [{"id": "#9a5bf55b-dee1-4e0f-8c32-6ee9b21a7852", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}], "label": "pa"}, "r7": {"id": "r7", "source": "p2", "target": "a18", "evidences": [{"id": "#07c8d48e-739c-4b77-9868-74d2f43e2ca3", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}], "label": "pa"}, "r8": {"id": "r8", "source": "v9", "target": "p4", "evidences": [{"id": "#a968b22b-1c6c-4f08-8aca-bb9cc37500db", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}], "label": "vp"}, "r9": {"id": "r9", "source": "p4", "target": "a18", "evidences": [{"id": "#6aced56a-ed26-4354-a8de-14dd1bcc039f", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}], "label": "pa"}, "r10": {"id": "r10", "source": "v11", "target": "p1", "evidences": [{"id": "#7255e268-ed4c-4c85-9b1b-7b318dee916e", "evidence_sentence": "For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax.", "start": 403, "end": 590}], "label": "vp"}, "r11": {"id": "r11", "source": "p1", "target": "a18", "evidences": [{"id": "#be435edd-7ae6-463d-b16c-d15f6fdc7fba", "evidence_sentence": "For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax.", "start": 403, "end": 590}], "label": "pa"}, "r12": {"id": "r12", "source": "v12", "target": "p5", "evidences": [{"id": "#99f684c4-7805-455f-91b3-affc7c903cdf", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}], "label": "vp"}, "r13": {"id": "r13", "source": "c1", "target": "v13", "evidences": [{"id": "#1f307d9b-bc98-41ee-ac4d-26ace98e8b81", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}], "label": "cv"}, "r14": {"id": "r14", "source": "v13", "target": "p6", "evidences": [{"id": "#fb6a13c7-168a-45db-af49-fbd72c586f22", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}], "label": "vp"}, "r15": {"id": "r15", "source": "p6", "target": "a22", "evidences": [{"id": "#c0c79ad0-5e19-4386-b37d-70941edddf4f", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}], "label": "pa"}, "r16": {"id": "r16", "source": "c2", "target": "v14", "evidences": [{"id": "#81530c42-1957-4256-b4e5-8df6be3ff126", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}], "label": "cv"}, "r17": {"id": "r17", "source": "v14", "target": "p6", "evidences": [{"id": "#38308cae-d23f-4819-aadc-de7a09b2ca05", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}], "label": "vp"}, "r18": {"id": "r18", "source": "v15", "target": "p8", "evidences": [{"id": "#0806a9b4-de19-46af-9db6-8ff46e31a11a", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}], "label": "vp"}, "r19": {"id": "r19", "source": "p8", "target": "a22", "evidences": [{"id": "#ae84153d-0564-4945-9ce9-d2d6083a82f5", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}], "label": "pa"}, "r20": {"id": "r20", "source": "c3", "target": "v15", "evidences": [{"id": "#d07071d6-7c2f-4449-9a09-15d70457056b", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}], "label": "cv"}, "r21": {"id": "r21", "source": "p2", "target": "a6", "evidences": [{"id": "#c6c9b671-6633-443d-8bed-9e0a79e58c30", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state. We use a dropout of 0.2 after BiLSTM embeddings.", "start": 97, "end": 257}], "label": "pa"}, "r22": {"id": "r22", "source": "p5", "target": "a6", "evidences": [{"id": "#422aa41a-b031-4a26-86ca-6378d77fc8e2", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}], "label": "pa"}}}, "2211.14208_48": {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 48, "text": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2. The weight decay is 0 to 0.01. The detailed search space and other hyperparameters are in Appendix. We also list the best hyperparameter configuration for each data in Appendix. If a baseline's accuracy is known and its experimental environments are the same as ours, we use the officially announced accuracy. If not, we execute a baseline using its official codes and the hyperparameter search procedures based on their suggested hyperparameter ranges.\r\n{FIGURE}", "entities": {"a4": {"id": "a4", "type": "a", "subtype": null, "surface_forms": [{"id": "#1888bc79-c627-4006-9344-61019a409969", "surface_form": "method", "start": 8, "end": 14}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#3b4698e3-5b98-44a4-87c1-fb9bc66564cb", "surface_form": "we test with the following hyperparameter configurations", "start": 16, "end": 72}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ef50a9d7-eff4-4aa8-ac76-e985e329bfb9", "surface_form": "200", "start": 87, "end": 90}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#8656a316-20b9-4c03-a82a-3edeeca48326", "surface_form": "epochs", "start": 91, "end": 97}]}, "a57": {"id": "a57", "type": "a", "subtype": null, "surface_forms": [{"id": "#1a8fc2e1-3008-4a44-9fbe-fed9f9216ca3", "surface_form": "Adam", "start": 108, "end": 112}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#447ded9b-045c-41c7-b2f7-e2f99a75f65b", "surface_form": "dimensionality of \\(\\mathbf {H}\\)", "start": 128, "end": 161}, {"id": "#6f4c2ee1-4431-4141-a75e-a65b68cae64f", "surface_form": "\\dim (\\mathbf {H})", "start": 174, "end": 192}]}, "v2": {"id": "v2", "type": "v", "subtype": "s", "surface_forms": [{"id": "#fd3be1ef-8a08-4ab0-9507-382992cf009f", "surface_form": "{32, 64, 128, 256}", "start": 203, "end": 221}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#c777b8b1-81be-4ce3-8dc4-3cded78f328a", "surface_form": "T", "start": 225, "end": 226}]}, "v3": {"id": "v3", "type": "v", "subtype": "r", "surface_forms": [{"id": "#b4d2eb9b-5031-4df2-9874-11998f4cf62e", "surface_form": "1.0 to 4.0", "start": 248, "end": 258}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#999884cb-6e0b-435e-acea-05dfd823ec9e", "surface_form": "learning rate", "start": 264, "end": 277}]}, "v4": {"id": "v4", "type": "v", "subtype": "r", "surface_forms": [{"id": "#e61d1bcb-04b6-4268-8eb7-06aad8e568c0", "surface_form": "1e-4 to 2e-2", "start": 293, "end": 305}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#e179576d-6e02-4e44-b326-4a47a45e38ac", "surface_form": "weight decay", "start": 311, "end": 323}]}, "v5": {"id": "v5", "type": "v", "subtype": "r", "surface_forms": [{"id": "#3a9360c8-286b-4f3c-b90e-6d50a04cbab7", "surface_form": "0 to 0.01", "start": 327, "end": 336}]}}, "relations": {"r14": {"id": "r14", "source": "c1", "target": "v1", "evidences": [{"id": "#c7c7f5f0-a968-4ac7-b992-0f259fd75a80", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer.", "start": 0, "end": 123}], "label": "cv"}, "r0": {"id": "r0", "source": "v1", "target": "p1", "evidences": [{"id": "#4d6aec5a-833b-4314-9836-f28db6c5460f", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer.", "start": 0, "end": 123}], "label": "vp"}, "r1": {"id": "r1", "source": "p1", "target": "a57", "evidences": [{"id": "#a328b567-e049-47a3-8537-9a28bada5dfd", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer.", "start": 0, "end": 123}], "label": "pa"}, "r2": {"id": "r2", "source": "v2", "target": "p2", "evidences": [{"id": "#5177554e-89b5-4baf-9b8a-67c4034b17b6", "evidence_sentence": "The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}.", "start": 124, "end": 222}], "label": "vp"}, "r3": {"id": "r3", "source": "c1", "target": "v2", "evidences": [{"id": "#bb01e8c7-b784-4e27-8356-6e4039f85f72", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}.", "start": 0, "end": 222}], "label": "cv"}, "r4": {"id": "r4", "source": "c1", "target": "v3", "evidences": [{"id": "#74ffbbcd-9793-4470-b749-dabf0897db39", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0.", "start": 0, "end": 259}], "label": "cv"}, "r5": {"id": "r5", "source": "v3", "target": "p3", "evidences": [{"id": "#66334c55-5580-4076-a6be-7b6d22b0688c", "evidence_sentence": "\\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0.", "start": 223, "end": 259}], "label": "vp"}, "r6": {"id": "r6", "source": "c1", "target": "v4", "evidences": [{"id": "#9b19a16d-b4a5-4fb5-8d1e-247c5dc95aee", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2.", "start": 0, "end": 306}], "label": "cv"}, "r7": {"id": "r7", "source": "v4", "target": "p4", "evidences": [{"id": "#b1e93316-763f-4cdd-8d9a-693ff5cfcdca", "evidence_sentence": "The learning rate is set to from 1e-4 to 2e-2.", "start": 260, "end": 306}], "label": "vp"}, "r8": {"id": "r8", "source": "p4", "target": "a57", "evidences": [{"id": "#2a34a212-8953-49de-8f38-d2a88149fbe5", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2.", "start": 0, "end": 306}], "label": "pa"}, "r9": {"id": "r9", "source": "c1", "target": "v5", "evidences": [{"id": "#3d418130-cc02-4154-8273-5e7a949d4829", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2. The weight decay is 0 to 0.01.", "start": 0, "end": 337}], "label": "cv"}, "r10": {"id": "r10", "source": "v5", "target": "p5", "evidences": [{"id": "#c942c457-91ea-4cf2-ad12-e129e9cbafe4", "evidence_sentence": "The weight decay is 0 to 0.01.", "start": 307, "end": 337}], "label": "vp"}, "r11": {"id": "r11", "source": "p5", "target": "a57", "evidences": [{"id": "#8103d0e4-5a21-4a3c-9a5f-db69e2f2130a", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.\u00a0() is from 1.0 to 4.0. The learning rate is set to from 1e-4 to 2e-2. The weight decay is 0 to 0.01.", "start": 0, "end": 337}], "label": "pa"}, "r12": {"id": "r12", "source": "p2", "target": "a4", "evidences": [{"id": "#b5b1a767-5b49-4017-8fe8-01c3e408bc77", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}.", "start": 0, "end": 222}], "label": "pa"}, "r13": {"id": "r13", "source": "p3", "target": "a4", "evidences": [{"id": "#e0c91203-80d8-4457-9608-b70c9dce2f26", "evidence_sentence": "For our method, we test with the following hyperparameter configurations: we train for 200 epochs using the Adam optimizer. The dimensionality of \\(\\mathbf {H}\\) , denoted \\(\\dim (\\mathbf {H})\\) , is in {32, 64, 128, 256}. \\(T\\)  in Eq.", "start": 0, "end": 236}], "label": "pa"}}}, "2210.10073_29": {"annotator_id": "tsa", "document_id": "2210.10073", "paragraph_index": 29, "text": "We use the sentence in Fig.\u00a0REF , \u201cBERT, formally published at NAACL-HLT 2019, leads to a significant change of NLP\u201d as an example of using CRPSE for citation recommendation. First, using scispaCy, entities BERT, NAACL-HLT and NLP are detected. Through the checking in the constructed published scientific entity-papers mapping dataset, only BERT is included in this dataset. Therefore, only the entity BERT will be passed to the next step for recommendation. When \\(K\\)  is set to be 5 which means 5 papers are required to be recommended and the cooccurrence count-based sorting criterion is used, the recommended papers for the entity BERT in order are \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u201d\u00a0[15], \u201cDeep Contextualized Word Representations\u201d\u00a0[47], \u201cAttention Is All You Need\u201d\u00a0[56], \u201cImproving Language Understanding by Generative Pre-Training\u201d\u00a0[48] and \u201cXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u00a0[65]. Among these papers, the top one is the source paper proposing the entity BERT and the other four papers are also related to the entity BERT.\r\n", "entities": {"a25": {"id": "a25", "type": "a", "subtype": null, "surface_forms": [{"id": "#c47fc460-37b8-47df-a6b7-4d9c6962a76d", "surface_form": "BERT", "start": 35, "end": 39}, {"id": "#04f74068-8165-4b64-9ae4-a900e9113c8a", "surface_form": "BERT", "start": 207, "end": 211}, {"id": "#b0389a32-1454-45b0-9bee-e6f6843f3186", "surface_form": "BERT", "start": 342, "end": 346}, {"id": "#32d613bf-e10f-4944-8223-9968985530b7", "surface_form": "BERT", "start": 403, "end": 407}, {"id": "#0540ce0d-0519-431c-b736-ebacaee3d38c", "surface_form": "BERT", "start": 637, "end": 641}, {"id": "#12eaa4f3-7e7a-412a-8f44-f4296eed5e25", "surface_form": "BERT", "start": 1052, "end": 1056}, {"id": "#bad1f6c8-f97c-41d6-ba9a-a31f4f36e08a", "surface_form": "BERT", "start": 1114, "end": 1118}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#030c4afb-81c8-4f15-9db9-383018094ea6", "surface_form": "an example", "start": 120, "end": 130}]}, "a6": {"id": "a6", "type": "a", "subtype": null, "surface_forms": [{"id": "#df81c776-4cb3-4774-993f-2575fe626db8", "surface_form": "CRPSE", "start": 140, "end": 145}]}, "a7": {"id": "a7", "type": "a", "subtype": null, "surface_forms": [{"id": "#8052105b-cfad-497b-a140-64bd505af7a4", "surface_form": "published scientific entity-papers mapping dataset", "start": 285, "end": 335}, {"id": "#bee56a57-8893-49cc-8114-874b594067cc", "surface_form": "dataset", "start": 367, "end": 374}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#c1fb074f-a939-492c-8831-606351fad9f1", "surface_form": "K", "start": 467, "end": 468}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#c9401f66-d139-41d5-ab04-3ac6baa15322", "surface_form": "5", "start": 485, "end": 486}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#7f02ba44-9a9d-4811-85d5-8a38e742f01c", "surface_form": "5", "start": 499, "end": 500}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ef57f3bb-1278-46a6-9e66-3334fed8c721", "surface_form": "one", "start": 1007, "end": 1010}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#e9a3189f-e300-478d-b362-11ad42cf4583", "surface_form": "four", "start": 1071, "end": 1075}]}}, "relations": {"r2": {"id": "r2", "source": "v1", "target": "p1", "evidences": [{"id": "#68d59be5-48fa-45dd-84f7-a165e0da38dd", "evidence_sentence": "When \\(K\\)  is set to be 5 which means 5 papers are required to be recommended and the cooccurrence count-based sorting criterion is used, the recommended papers for the entity BERT in order are \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u201d\u00a0[15], \u201cDeep Contextualized Word Representations\u201d\u00a0[47], \u201cAttention Is All You Need\u201d\u00a0[56], \u201cImproving Language Understanding by Generative Pre-Training\u201d\u00a0[48] and \u201cXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u00a0[65].", "start": 460, "end": 978}], "label": "vp"}, "r0": {"id": "r0", "source": "p1", "target": "a6", "evidences": [{"id": "#088e9f09-e424-4dd8-8eeb-2680da950c8f", "evidence_sentence": "REF , \u201cBERT, formally published at NAACL-HLT 2019, leads to a significant change of NLP\u201d as an example of using CRPSE for citation recommendation. First, using scispaCy, entities BERT, NAACL-HLT and NLP are detected. Through the checking in the constructed published scientific entity-papers mapping dataset, only BERT is included in this dataset. Therefore, only the entity BERT will be passed to the next step for recommendation. When \\(K\\)  is set to be 5 which means 5 papers are required to be recommended and the cooccurrence count-based sorting criterion is used, the recommended papers for the entity BERT in order are \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u201d\u00a0[15], \u201cDeep Contextualized Word Representations\u201d\u00a0[47], \u201cAttention Is All You Need\u201d\u00a0[56], \u201cImproving Language Understanding by Generative Pre-Training\u201d\u00a0[48] and \u201cXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u00a0[65].", "start": 28, "end": 978}], "label": "pa"}, "r1": {"id": "r1", "source": "c1", "target": "v1", "evidences": [{"id": "#ead5a6d1-149b-4ec2-8d0f-b38c5d30f233", "evidence_sentence": "REF , \u201cBERT, formally published at NAACL-HLT 2019, leads to a significant change of NLP\u201d as an example of using CRPSE for citation recommendation. First, using scispaCy, entities BERT, NAACL-HLT and NLP are detected. Through the checking in the constructed published scientific entity-papers mapping dataset, only BERT is included in this dataset. Therefore, only the entity BERT will be passed to the next step for recommendation. When \\(K\\)  is set to be 5 which means 5 papers are required to be recommended and the cooccurrence count-based sorting criterion is used, the recommended papers for the entity BERT in order are \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u201d\u00a0[15], \u201cDeep Contextualized Word Representations\u201d\u00a0[47], \u201cAttention Is All You Need\u201d\u00a0[56], \u201cImproving Language Understanding by Generative Pre-Training\u201d\u00a0[48] and \u201cXLNet: Generalized Autoregressive Pretraining for Language Understanding\u201d\u00a0[65].", "start": 28, "end": 978}], "label": "cv"}}}, "1808.09602_59": {"annotator_id": "tsa", "document_id": "1808.09602", "paragraph_index": 59, "text": "Our system extends the implementation and hyper-parameters from Lee2017EndtoendNC with the following adjustments. We use a 1 layer BiLSTM with 200-dimensional hidden layers. All the FFNNs have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.\r\nWe model spans up to 8 words. For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.\r\nFor constructing the knowledge graph, we use the following heuristics to normalize the entity phrases. We replace all acronyms with their corresponding full name and normalize all the plural terms with their singular counterparts.\r\n{TABLE}", "entities": {"a1": {"id": "a1", "type": "a", "subtype": null, "surface_forms": [{"id": "#b2d4d4a5-5f8d-40e2-bd67-6f3afc608a6c", "surface_form": "system", "start": 4, "end": 10}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ee6b8954-71ff-42df-bc49-4eadfd4adc97", "surface_form": "1", "start": 123, "end": 124}]}, "a9": {"id": "a9", "type": "a", "subtype": null, "surface_forms": [{"id": "#7aecb6e0-0308-46e1-be32-214fcee1c188", "surface_form": "BiLSTM", "start": 131, "end": 137}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#cbfb0b48-9529-4d84-b9e6-87fc669e0d8a", "surface_form": "200", "start": 143, "end": 146}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#7fe614e6-eff5-4a69-8c17-05fed29e4812", "surface_form": "hidden layers", "start": 159, "end": 172}, {"id": "#b71512c4-b438-4995-aa0c-6b693cf97162", "surface_form": "hidden layers", "start": 195, "end": 208}]}, "a7": {"id": "a7", "type": "a", "subtype": null, "surface_forms": [{"id": "#c43b9207-2ee8-4508-a7f0-331042c4404e", "surface_form": "FFNNs", "start": 182, "end": 187}, {"id": "#601bfd1b-6e9c-4f38-af1b-582839e3b664", "surface_form": "FFNNs", "start": 304, "end": 309}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#de830b2e-f991-4840-948f-ce79f567db28", "surface_form": "2", "start": 193, "end": 194}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#763aef5e-3184-404f-8423-166e96ad6ff5", "surface_form": "150", "start": 212, "end": 215}]}, "p7": {"id": "p7", "type": "p", "subtype": null, "surface_forms": [{"id": "#c6382a04-3f91-408f-8381-48d120b5ae56", "surface_form": "dimensions", "start": 216, "end": 226}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#4dc24675-3ccb-4abf-a0a2-7f6e55e84cf9", "surface_form": "0.4", "start": 240, "end": 243}]}, "p8": {"id": "p8", "type": "p", "subtype": null, "surface_forms": [{"id": "#201b4465-4ade-44c4-8674-a95e1131cb29", "surface_form": "variational dropout", "start": 244, "end": 263}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#36128376-8bd1-499d-a097-a49f3e797901", "surface_form": "LSTMs", "start": 277, "end": 282}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#e42d18c1-ff86-4d2f-a6e2-16833f0c7709", "surface_form": "0.4", "start": 284, "end": 287}]}, "p9": {"id": "p9", "type": "p", "subtype": null, "surface_forms": [{"id": "#3542116a-764b-43ba-8fc8-5666dad4a539", "surface_form": "dropout", "start": 288, "end": 295}, {"id": "#4baefd6d-8ef2-4c09-8b7d-5415213f6359", "surface_form": "dropout", "start": 319, "end": 326}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#e6c60c21-609d-4695-b23f-0ed97825d1fc", "surface_form": "0.5", "start": 315, "end": 318}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#3f7f6fe8-1503-4a96-8ec9-d1bfa5859076", "surface_form": "input embeddings", "start": 335, "end": 351}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3ea180c6-54fc-4014-8bd3-1c1fbf66d55f", "surface_form": "8", "start": 374, "end": 375}]}, "a11": {"id": "a11", "type": "a", "subtype": null, "surface_forms": [{"id": "#0da6a273-aa5d-4c24-8afa-fd7cd88b58fc", "surface_form": "beam pruning", "start": 387, "end": 399}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#76ae0167-1c39-4398-8ebf-91b52f729463", "surface_form": "\\lambda _{\\text{C}}", "start": 410, "end": 429}]}, "v9": {"id": "v9", "type": "v", "subtype": "n", "surface_forms": [{"id": "#f437394c-9eaa-48fa-b960-b4c5e8dfeeac", "surface_form": "0.3", "start": 430, "end": 433}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#080ad41d-1a87-4acb-b004-00015546142f", "surface_form": "coreference resolution", "start": 441, "end": 463}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#ab9234c2-262d-45d5-9e88-dbb61dedf3a1", "surface_form": "\\lambda _{\\text{R}}", "start": 470, "end": 489}]}, "v10": {"id": "v10", "type": "v", "subtype": "n", "surface_forms": [{"id": "#17d6fd6c-2db9-4031-bc32-e453b22810a3", "surface_form": "0.4", "start": 490, "end": 493}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#20d8a801-e382-49dd-b965-45aceb1d2020", "surface_form": "relation extraction", "start": 501, "end": 520}]}, "a3": {"id": "a3", "type": "a", "subtype": null, "surface_forms": [{"id": "#c2269994-4d32-4745-96df-b57873bfa784", "surface_form": "knowledge graph", "start": 543, "end": 558}]}}, "relations": {"r18": {"id": "r18", "source": "v2", "target": "p6", "evidences": [{"id": "#9fc94ff5-cf98-416d-91d6-ee3fc85ea578", "evidence_sentence": "We use a 1 layer BiLSTM with 200-dimensional hidden layers.", "start": 114, "end": 173}], "label": "vp"}, "r0": {"id": "r0", "source": "p6", "target": "a9", "evidences": [{"id": "#e51cc342-b36e-47e8-a1af-f253c95cac48", "evidence_sentence": "We use a 1 layer BiLSTM with 200-dimensional hidden layers.", "start": 114, "end": 173}], "label": "pa"}, "r1": {"id": "r1", "source": "v3", "target": "p6", "evidences": [{"id": "#bf25c371-19f3-4446-a603-51f4b4043afb", "evidence_sentence": "All the FFNNs have 2 hidden layers of 150 dimensions each.", "start": 174, "end": 232}], "label": "vp"}, "r2": {"id": "r2", "source": "p6", "target": "a7", "evidences": [{"id": "#c6b58ab1-16c4-482b-8e81-640138fdaeaa", "evidence_sentence": "All the FFNNs have 2 hidden layers of 150 dimensions each.", "start": 174, "end": 232}], "label": "pa"}, "r3": {"id": "r3", "source": "v4", "target": "p7", "evidences": [{"id": "#d1753e87-1795-4a3e-b871-a148236c4fb2", "evidence_sentence": "All the FFNNs have 2 hidden layers of 150 dimensions each.", "start": 174, "end": 232}], "label": "vp"}, "r4": {"id": "r4", "source": "p7", "target": "a7", "evidences": [{"id": "#e38b9bc5-0c1f-4b30-b287-b63cdb45bfb6", "evidence_sentence": "All the FFNNs have 2 hidden layers of 150 dimensions each.", "start": 174, "end": 232}], "label": "pa"}, "r5": {"id": "r5", "source": "v7", "target": "p9", "evidences": [{"id": "#1ef11e94-513e-4a86-9a9d-a8b5a177aa59", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}], "label": "vp"}, "r6": {"id": "r6", "source": "v6", "target": "p9", "evidences": [{"id": "#e334b074-71cb-4c51-bc95-fe526e490f86", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}], "label": "vp"}, "r7": {"id": "r7", "source": "p9", "target": "a7", "evidences": [{"id": "#deede13b-21fe-4df8-9a3f-105d7518e4e7", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}], "label": "pa"}, "r8": {"id": "r8", "source": "v5", "target": "p8", "evidences": [{"id": "#092d8fd2-eec7-4954-88ad-3f2d158d56ea", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}], "label": "vp"}, "r9": {"id": "r9", "source": "p8", "target": "a14", "evidences": [{"id": "#b53e0a62-2042-4b14-b800-1c6cb05e6fc2", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}], "label": "pa"}, "r10": {"id": "r10", "source": "c1", "target": "v7", "evidences": [{"id": "#f84c9ee4-0b17-4c3a-8f77-f48c54f1abd0", "evidence_sentence": "We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 233, "end": 352}], "label": "cv"}, "r11": {"id": "r11", "source": "c3", "target": "v10", "evidences": [{"id": "#c7153f37-6836-4a09-b664-19557953a8b5", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}], "label": "cv"}, "r12": {"id": "r12", "source": "v10", "target": "p5", "evidences": [{"id": "#5117d873-b561-44d4-b8e5-598545810a3f", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}], "label": "vp"}, "r13": {"id": "r13", "source": "p5", "target": "a11", "evidences": [{"id": "#95f2e627-a730-43b7-9ed9-9e74c70ec079", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}], "label": "pa"}, "r14": {"id": "r14", "source": "c2", "target": "v9", "evidences": [{"id": "#b61663bf-441f-4720-acf2-4ed670482a68", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}], "label": "cv"}, "r15": {"id": "r15", "source": "v9", "target": "p3", "evidences": [{"id": "#5c1a126d-1c96-4bb8-934b-deb08a75d164", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}], "label": "vp"}, "r16": {"id": "r16", "source": "p3", "target": "a11", "evidences": [{"id": "#f465fee0-e798-440b-a4fb-50bc85f909d5", "evidence_sentence": "For beam pruning, we use \\(\\lambda _{\\text{C}}=0.3\\)  for coreference resolution and \\(\\lambda _{\\text{R}}=0.4\\)  for relation extraction.", "start": 384, "end": 522}], "label": "pa"}, "r17": {"id": "r17", "source": "p9", "target": "a1", "evidences": [{"id": "#28676e42-829f-4f2f-9378-82e4efe385b4", "evidence_sentence": "Our system extends the implementation and hyper-parameters from Lee2017EndtoendNC with the following adjustments. We use a 1 layer BiLSTM with 200-dimensional hidden layers. All the FFNNs have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout [15] for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.", "start": 0, "end": 352}], "label": "pa"}}}, "2203.05325_16": {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 16, "text": "Due to quadratic scaling of the pairwise comparisons it is not feasible to perform relation extraction on all possible continuous spans.\r\nWe, therefore, perform relation classification on the top \\(k\\)  spansDuring training we add annotated spans which are not among the top \\(k\\)  spans. with the highest span scores, meaning that we have to classify a maximum of \\(k(k-1)\\)  relation representations for a given input text.\r\nThe computational complexity of the system can, therefore, be adjusted dynamically at inference time by changing \\(k\\) , for example to be run on GPUs with smaller memory capacity or on GPUs with higher memory capacity to improve the quality of predictions.\r\n", "entities": {"p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#bdca2c65-87dd-4555-9f97-c27bb4339710", "surface_form": "k", "start": 197, "end": 198}, {"id": "#d9ef6434-04df-4b7e-a9d6-439e1aba0de4", "surface_form": "k", "start": 276, "end": 277}, {"id": "#2237327d-731c-42b5-bc9d-6d8900eaacd0", "surface_form": "k", "start": 366, "end": 367}, {"id": "#6c94a8c5-883e-442a-8bdb-6b3037a23992", "surface_form": "k", "start": 368, "end": 369}, {"id": "#ebffef25-9a53-4b88-91e8-c163a7a1fc32", "surface_form": "k", "start": 540, "end": 541}]}, "a1": {"id": "a1", "type": "a", "subtype": null, "surface_forms": [{"id": "#cccc5e78-61c9-4857-b0cf-09ad992cc64a", "surface_form": "system", "start": 461, "end": 467}]}}, "relations": {"r0": {"id": "r0", "source": "p1", "target": "a1", "evidences": [{"id": "#430c7ba9-b5dd-48ac-ab73-fe9ac40d2cef", "evidence_sentence": "The computational complexity of the system can, therefore, be adjusted dynamically at inference time by changing \\(k\\) , for example to be run on GPUs with smaller memory capacity or on GPUs with higher memory capacity to improve the quality of predictions.", "start": 427, "end": 684}], "label": "pa"}}}, "2203.05325_19": {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 19, "text": "For our language model we use SciBERT [1], which is trained on scientific text, via Huggingface's Transformers library [13].\r\nFor LaTeX preprocessing (see section REF ) we use Pylatexenchttps://github.com/phfaist/pylatexenc.\r\nAs our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.\r\nDuring training, we randomly downsample the amount of candidate spans for soft mention detection to 1000, while ensuring that all labeled spans are included. During training and development set evaluation, we set \\(k\\) , the number of spans to perform relation classification on, to 50, as preliminary experiments showed this value to yield a good compromise between model performance and training time. For test set evaluation we increase \\(k\\)  to 400.\r\nTraining takes approximately 10 hours on a single NVIDIA V100 GPU using mixed precision.\r\nWe perform early stopping based on the micro \\(F_1\\)  score for relation extraction on the development set.\r\nWe train each hyperparameter configuration 3 times using different random seeds and report the median and standard deviation for each metric.\r\nAs a result of the different combinations of preprocessing and mean-/max-pooling, we examine the performance of 4 configurations on the test set.\r\nFor our evaluation, we report the micro \\(F_1\\)  scores for NER metrics as used in SemEval-2013 Task 9.1 [10]We use the following implementation: https://github.com/davidsbatista/NER-Evaluation.\r\nFor relation extraction we report micro precision, recall and \\(F_1\\)  scores, unless otherwise indicated.\r\n", "entities": {"a15": {"id": "a15", "type": "a", "subtype": null, "surface_forms": [{"id": "#62d5c42c-5741-4f64-a424-12521341fcdd", "surface_form": "SciBERT", "start": 30, "end": 37}]}, "a16": {"id": "a16", "type": "a", "subtype": null, "surface_forms": [{"id": "#d0d6e2b8-6d57-44a7-9ec0-3b90cd6af918", "surface_form": "AdamW", "start": 249, "end": 254}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#955bb94b-5764-4c59-8df3-0f906a357e16", "surface_form": "learning rates", "start": 264, "end": 278}]}, "v1": {"id": "v1", "type": "v", "subtype": "s", "surface_forms": [{"id": "#d783e7cb-c4d2-47b6-a360-5a3f20d55830", "surface_form": "[3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]", "start": 285, "end": 339}]}, "a17": {"id": "a17", "type": "a", "subtype": null, "surface_forms": [{"id": "#88aab9ee-f7ca-4d1f-8cd7-4102473a9f58", "surface_form": "linear warmup", "start": 346, "end": 359}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ee80ef66-41be-4b59-8cb0-08a7f2737e8b", "surface_form": "1", "start": 363, "end": 364}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#992db906-f9f6-40eb-8a50-e9063d38e606", "surface_form": "epoch", "start": 365, "end": 370}, {"id": "#533956e4-dffd-4cb4-a495-7b5cbb6afa8b", "surface_form": "epochs", "start": 425, "end": 431}]}, "a18": {"id": "a18", "type": "a", "subtype": null, "surface_forms": [{"id": "#441a074e-3e57-4a52-8e08-9c256fbc3049", "surface_form": "linear decay", "start": 385, "end": 397}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ad40e648-fafa-4263-88df-0b06b0543a6e", "surface_form": "zero", "start": 401, "end": 405}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#fa39eacf-977e-4f37-846f-d0fd9fa8a2f0", "surface_form": "60", "start": 422, "end": 424}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#21935b2c-890f-427f-9d7f-179ed8643fbb", "surface_form": "one", "start": 445, "end": 448}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#fa3de059-36ab-4624-8f23-27840b5a70af", "surface_form": "3119", "start": 512, "end": 516}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#fbcd82a7-df05-4a3e-972b-66e41a20b48b", "surface_form": "batch size", "start": 521, "end": 531}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3f0da8bb-f5bd-4a6a-93d5-de113eadbf17", "surface_form": "4", "start": 535, "end": 536}]}, "a19": {"id": "a19", "type": "a", "subtype": null, "surface_forms": [{"id": "#cca9d1b5-ad37-42a4-ae1b-2e07875dba98", "surface_form": "gradient clipping", "start": 548, "end": 565}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#2c83b01d-2970-47e1-928f-2258fd5b2e08", "surface_form": "max norm", "start": 573, "end": 581}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#092d7da1-0de2-4b0e-8afb-4bec277f0ed5", "surface_form": "1", "start": 585, "end": 586}]}, "v9": {"id": "v9", "type": "v", "subtype": "n", "surface_forms": [{"id": "#4bb898b8-865e-4d47-83de-854d781f78d0", "surface_form": "1000", "start": 688, "end": 692}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#24806b71-1239-4d26-b640-b621150305d4", "surface_form": "During training and development set evaluation", "start": 746, "end": 792}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#4f7d4475-3c48-4467-a5e4-9148824f420b", "surface_form": "k", "start": 803, "end": 804}, {"id": "#45ce0a1d-779e-4715-8904-b244f1be1430", "surface_form": "number of spans to perform relation classification on", "start": 813, "end": 866}, {"id": "#921ed219-0f33-4511-9f9c-52a1842556d7", "surface_form": "k", "start": 1030, "end": 1031}]}, "v10": {"id": "v10", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9d267ef1-6205-4cbc-9a86-d6d64c5ed247", "surface_form": "50", "start": 871, "end": 873}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#d3f11b3b-12f7-49ac-b9e9-526b9684e6d1", "surface_form": "For test set evaluation", "start": 992, "end": 1015}]}, "v11": {"id": "v11", "type": "v", "subtype": "n", "surface_forms": [{"id": "#49267439-e631-44ff-b39f-4aff8a040545", "surface_form": "400", "start": 1038, "end": 1041}]}, "v12": {"id": "v12", "type": "v", "subtype": "n", "surface_forms": [{"id": "#4333cfc7-660c-4bdd-bff0-f7554964d39f", "surface_form": "10", "start": 1072, "end": 1074}]}, "a20": {"id": "a20", "type": "a", "subtype": null, "surface_forms": [{"id": "#fdc6437b-0c59-4a3e-87e9-749dcd349cd1", "surface_form": "early stopping", "start": 1143, "end": 1157}]}, "v13": {"id": "v13", "type": "v", "subtype": "n", "surface_forms": [{"id": "#1a6794fc-0dae-4399-ba58-4f0733b5b068", "surface_form": "3", "start": 1283, "end": 1284}]}, "a9": {"id": "a9", "type": "a", "subtype": null, "surface_forms": [{"id": "#1a90bd59-0ff7-47ad-836b-31ed2d05e37a", "surface_form": "mean-", "start": 1445, "end": 1450}]}, "a10": {"id": "a10", "type": "a", "subtype": null, "surface_forms": [{"id": "#ee8a4e29-8cfc-44a7-991d-c44974de5ec7", "surface_form": "max-pooling", "start": 1451, "end": 1462}]}, "v14": {"id": "v14", "type": "v", "subtype": "n", "surface_forms": [{"id": "#6678e57e-b24b-4137-a449-f06ad23aef74", "surface_form": "4", "start": 1494, "end": 1495}]}}, "relations": {"r13": {"id": "r13", "source": "v1", "target": "p2", "evidences": [{"id": "#904b431c-868c-411b-ba43-dbd4fe4b364b", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "vp"}, "r0": {"id": "r0", "source": "p2", "target": "a16", "evidences": [{"id": "#d56fc5ba-b5b8-4464-a9fa-93edfba259b3", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "pa"}, "r1": {"id": "r1", "source": "v2", "target": "p3", "evidences": [{"id": "#4298cbf3-7e3f-4d8d-acae-3fb1569f5f4e", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "vp"}, "r2": {"id": "r2", "source": "p3", "target": "a17", "evidences": [{"id": "#2748db06-ef52-4a8f-922d-d78438cbad6f", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "pa"}, "r3": {"id": "r3", "source": "v4", "target": "p3", "evidences": [{"id": "#8e25fd99-23a7-4df4-89dc-554740f9e29e", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "vp"}, "r4": {"id": "r4", "source": "p3", "target": "a18", "evidences": [{"id": "#fc7356c2-ab3a-4e9e-baee-a8f04c39352b", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "pa"}, "r5": {"id": "r5", "source": "v7", "target": "p4", "evidences": [{"id": "#f1ba8d59-78d2-403b-93f0-0e9f75389e8c", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "vp"}, "r6": {"id": "r6", "source": "v8", "target": "p5", "evidences": [{"id": "#7f8fbdd3-3c87-4efb-aace-62e58e465a0f", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "vp"}, "r7": {"id": "r7", "source": "p5", "target": "a19", "evidences": [{"id": "#00b84d45-12b3-4e8f-8215-d3e215a7fb1f", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "pa"}, "r8": {"id": "r8", "source": "p4", "target": "a16", "evidences": [{"id": "#325342fc-1257-4cbe-a45d-971f59b0131b", "evidence_sentence": "As our optimizer, we use AdamW [6] with learning rates \\(\\in [3\\mathrm {e}{-5}, 5\\mathrm {e}{-5}, 7\\mathrm {e}{-5}]\\) , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochsThe length of one epoch is dictated by the number of training examples, which is 3119., a batch size of 4, and apply gradient clipping with a max norm of 1.", "start": 226, "end": 589}], "label": "pa"}, "r9": {"id": "r9", "source": "c1", "target": "v10", "evidences": [{"id": "#deb151fa-46e5-491a-bede-3bb43289ceab", "evidence_sentence": "During training, we randomly downsample the amount of candidate spans for soft mention detection to 1000, while ensuring that all labeled spans are included. During training and development set evaluation, we set \\(k\\) , the number of spans to perform relation classification on, to 50, as preliminary experiments showed this value to yield a good compromise between model performance and training time.", "start": 591, "end": 994}], "label": "cv"}, "r10": {"id": "r10", "source": "v10", "target": "p1", "evidences": [{"id": "#b734ed59-17d1-4673-a9dd-8d8946ac8d5e", "evidence_sentence": "During training and development set evaluation, we set \\(k\\) , the number of spans to perform relation classification on, to 50, as preliminary experiments showed this value to yield a good compromise between model performance and training time.", "start": 749, "end": 994}], "label": "vp"}, "r11": {"id": "r11", "source": "c2", "target": "v11", "evidences": [{"id": "#d974eea2-eae5-49c8-9770-858a04394295", "evidence_sentence": "During training and development set evaluation, we set \\(k\\) , the number of spans to perform relation classification on, to 50, as preliminary experiments showed this value to yield a good compromise between model performance and training time. For test set evaluation we increase \\(k\\)  to 400.", "start": 749, "end": 1045}], "label": "cv"}, "r12": {"id": "r12", "source": "v11", "target": "p1", "evidences": [{"id": "#c00b32a2-6d10-409a-b611-8409c7f978b2", "evidence_sentence": "For test set evaluation we increase \\(k\\)  to 400.", "start": 995, "end": 1045}], "label": "vp"}}}, "2203.05325_25": {"annotator_id": "tsa", "document_id": "2203.05325", "paragraph_index": 25, "text": "In figure REF , we show the change in relation extraction performance across different values for \\(k\\) .\r\nWe also include in the plot the percentage of entity spans in the top \\(k\\)  ranked spans (entity recall).\r\nWhile the relation extraction performance improves proportional to the entity recall for \\(k \\le 100\\)  the improvement slows down for higher \\(k\\) .\r\nWe hypothesize that this is due to the limiting of \\(k=50\\)  and the candidate span downsampling during training, which prevents the model from seeing some of the more difficult cases.\r\n", "entities": {"p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#75ad6e47-984c-4cfa-8398-e4abfca02cea", "surface_form": "k", "start": 100, "end": 101}, {"id": "#2de2cefc-b24b-479e-a8a8-e41d4e017b18", "surface_form": "k", "start": 178, "end": 179}, {"id": "#cac77a2c-93fd-4228-bf0c-a4c55fc4e543", "surface_form": "k", "start": 304, "end": 305}, {"id": "#82454c95-2e73-4891-88ea-7fae85c40b1f", "surface_form": "k", "start": 357, "end": 358}, {"id": "#6e6c93f9-384f-4f6f-b610-981e0bd8e6fa", "surface_form": "k", "start": 416, "end": 417}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#ce605704-a9ea-41f8-96ec-978090743965", "surface_form": "relation extraction performance improves proportional to the entity recall", "start": 223, "end": 297}]}, "v1": {"id": "v1", "type": "v", "subtype": "o", "surface_forms": [{"id": "#3e715e80-ed78-4d66-b8ac-589814c5906f", "surface_form": "\\le 100", "start": 306, "end": 313}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#19e83a73-5a79-4113-8a50-bd2c23f159d8", "surface_form": "the improvement slows down", "start": 317, "end": 343}]}, "v2": {"id": "v2", "type": "v", "subtype": "o", "surface_forms": [{"id": "#115f15ba-262b-467a-9a21-e6a2bc6a69d1", "surface_form": "higher", "start": 348, "end": 354}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#a7fe916d-3e13-4661-98a3-95acb2f8a291", "surface_form": "50", "start": 418, "end": 420}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#9906bdb0-370b-4a63-9516-dd732570c1bc", "surface_form": "during training", "start": 460, "end": 475}]}}, "relations": {"r5": {"id": "r5", "source": "c1", "target": "v1", "evidences": [{"id": "#ffd50554-ce6d-4eea-9c6c-996067721ca3", "evidence_sentence": "While the relation extraction performance improves proportional to the entity recall for \\(k \\le 100\\)  the improvement slows down for higher \\(k\\) .", "start": 215, "end": 364}], "label": "cv"}, "r0": {"id": "r0", "source": "v1", "target": "p1", "evidences": [{"id": "#234449b4-06cc-4f4f-bf17-f98ea8411ef8", "evidence_sentence": "While the relation extraction performance improves proportional to the entity recall for \\(k \\le 100\\)  the improvement slows down for higher \\(k\\) .", "start": 215, "end": 364}], "label": "vp"}, "r1": {"id": "r1", "source": "v2", "target": "p1", "evidences": [{"id": "#b3b83dd5-d8fa-44ef-b07d-332109e95d85", "evidence_sentence": "While the relation extraction performance improves proportional to the entity recall for \\(k \\le 100\\)  the improvement slows down for higher \\(k\\) .", "start": 215, "end": 364}], "label": "vp"}, "r2": {"id": "r2", "source": "c2", "target": "v2", "evidences": [{"id": "#602af87e-1bd7-4aff-a1e7-efafcd79715a", "evidence_sentence": "While the relation extraction performance improves proportional to the entity recall for \\(k \\le 100\\)  the improvement slows down for higher \\(k\\) .", "start": 215, "end": 364}], "label": "cv"}, "r3": {"id": "r3", "source": "v3", "target": "p1", "evidences": [{"id": "#b1fdc0ff-9774-4792-822a-3043d10c396f", "evidence_sentence": "We hypothesize that this is due to the limiting of \\(k=50\\)  and the candidate span downsampling during training, which prevents the model from seeing some of the more difficult cases.", "start": 366, "end": 550}], "label": "vp"}, "r4": {"id": "r4", "source": "c3", "target": "v3", "evidences": [{"id": "#e6610dcd-bd2c-433b-b28e-cc9c70cc31a5", "evidence_sentence": "We hypothesize that this is due to the limiting of \\(k=50\\)  and the candidate span downsampling during training, which prevents the model from seeing some of the more difficult cases.", "start": 366, "end": 550}], "label": "cv"}}}, "2205.02033_23": {"annotator_id": "tsa", "document_id": "2205.02033", "paragraph_index": 23, "text": "As citation data, we used the COCI (OpenCitations Index of Crossref open DOI-to-DOI references)\u00a0[13].\r\nThe citation data of the COCI are originally from publishers, thus they are of high quality.\r\nWe used the COCI CSV dataset Version 11 released on 2021-09-03https://doi.org/10.6084/m9.figshare.6741422.v11, last accessed on 2021-12-14 that lists pairs of DOIs denoting citations.\r\nThe 36,628 preprints and their publisher versions receive 331,839 citations in total in the given 24 months.\r\n", "entities": {"a3": {"id": "a3", "type": "a", "subtype": null, "surface_forms": [{"id": "#ec609b6f-2693-4aca-a4fb-29fe4f2d24de", "surface_form": "COCI (OpenCitations Index of Crossref open DOI-to-DOI references)", "start": 30, "end": 95}, {"id": "#24eda2c4-7e11-4493-a3af-ac6337c7076c", "surface_form": "COCI", "start": 127, "end": 131}, {"id": "#5054f1af-a651-41a5-bb7e-42250b315968", "surface_form": "COCI CSV dataset", "start": 207, "end": 223}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#5dfeb3c5-9033-4d50-8af8-9c2a63c782ab", "surface_form": "Version", "start": 224, "end": 231}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#b2c6c971-c05c-472a-bb0d-a6a922fd7796", "surface_form": "11", "start": 232, "end": 234}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3ab7afcd-86fb-4c7b-b193-c88539e07477", "surface_form": "36,628", "start": 383, "end": 389}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#8b9ec3bf-8dfd-4a94-9570-8fafa50f0b9a", "surface_form": "331,839", "start": 437, "end": 444}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#745ec64f-8288-4e5e-9598-ee03e79ab2ca", "surface_form": "24", "start": 477, "end": 479}]}}, "relations": {"r1": {"id": "r1", "source": "v1", "target": "p1", "evidences": [{"id": "#1bcee865-c9ad-42b4-99bc-110804f0bea1", "evidence_sentence": "We used the COCI CSV dataset Version 11 released on 2021-09-03https://doi.org/10.6084/m9.figshare.6741422.v11, last accessed on 2021-12-14 that lists pairs of DOIs denoting citations.", "start": 197, "end": 380}], "label": "vp"}, "r0": {"id": "r0", "source": "p1", "target": "a3", "evidences": [{"id": "#0d138638-57d0-48b9-9a10-fc6bc658df7d", "evidence_sentence": "We used the COCI CSV dataset Version 11 released on 2021-09-03https://doi.org/10.6084/m9.figshare.6741422.v11, last accessed on 2021-12-14 that lists pairs of DOIs denoting citations.", "start": 197, "end": 380}], "label": "pa"}}}, "2212.06522_22": {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 22, "text": "where \\(\\sigma \\)  is softmax operation, \\(a^s\\)  and \\(a^t\\)  are the logits of student and teacher networks, \\(T\\)  is the temperature to soften the logits. \\(K\\)  is the number of classification types. \\(H(\\cdot , \\cdot )\\)  is the cross-entropy loss to measure the discrepancy of softened probabilistic output between the student and teacher.\r\nIn this work, we have two teachers, thus the naive solution for distilling from two teachers is:\r\n\\(\\begin{split}L_{kd}({\\theta }) = L_{kd}^{t1} ({\\theta }) + L_{kd}^{t2} ({\\theta }) = H(p^s, p^{t1}) + H(p^s, p^{t2})\\end{split}\\) \r\n", "entities": {"a25": {"id": "a25", "type": "a", "subtype": null, "surface_forms": [{"id": "#c9c601fa-5eea-4fc9-bb6d-1dc7df186ece", "surface_form": "\\sigma", "start": 8, "end": 14}, {"id": "#3b5ab4ac-0c16-4542-a051-561d95141e4f", "surface_form": "softmax operation", "start": 22, "end": 39}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#55c7a53d-53ff-4028-8b34-0d615f56bb0e", "surface_form": "T", "start": 113, "end": 114}, {"id": "#6fe9b7b0-079c-470c-bae1-9af5e9874aa5", "surface_form": "temperature", "start": 125, "end": 136}]}, "a5": {"id": "a5", "type": "a", "subtype": null, "surface_forms": [{"id": "#50767b58-db75-4931-904c-e6f22da24b99", "surface_form": "H", "start": 207, "end": 208}, {"id": "#d5853313-2165-459e-b460-4a5a30ed0991", "surface_form": "cross-entropy loss", "start": 235, "end": 253}, {"id": "#dbc9d933-5892-4944-b683-580f6ff23d17", "surface_form": "H", "start": 531, "end": 532}, {"id": "#80f83fb1-43e2-4dc8-92dd-5406c86533b5", "surface_form": "H", "start": 548, "end": 549}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#efd91880-822b-4591-ae94-c1b4bf878976", "surface_form": "two", "start": 369, "end": 372}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#c2ed0dee-446f-48e9-bc0d-5270aaa4134b", "surface_form": "two", "start": 427, "end": 430}]}, "a22": {"id": "a22", "type": "a", "subtype": null, "surface_forms": [{"id": "#fdf00a8d-77cb-44fc-ad95-1d949a0eeb43", "surface_form": "L_{kd}", "start": 459, "end": 465}, {"id": "#670bd8d9-723e-41da-99e8-41a0f524ba1a", "surface_form": "L_{kd}^{t1}", "start": 479, "end": 490}, {"id": "#5a584a5f-face-4382-8eed-42c0b3476f96", "surface_form": "L_{kd}^{t2}", "start": 505, "end": 516}]}}, "relations": {"r0": {"id": "r0", "source": "p1", "target": "a25", "evidences": [{"id": "#67059f99-3373-4e64-8249-445393b75bbe", "evidence_sentence": "where \\(\\sigma \\)  is softmax operation, \\(a^s\\)  and \\(a^t\\)  are the logits of student and teacher networks, \\(T\\)  is the temperature to soften the logits.", "start": 0, "end": 158}], "label": "pa"}}}, "2212.06522_23": {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 23, "text": "However, conservatively accepting the directions from all teachers, i.e., accumulating the separate distillation loss from each teacher, is not a good option, since the diversity of teachers could be significant and there might be some weak or noisy teachers mingled in the ensemble.\r\nWhen distilling knowledge from multiple teachers, we need to incorporate the disagreement into the determination of the descent direction.\r\nRecently, a novel method is proposed to find one single Pareto optimal solution with a good trade-off among conflicting optimization targets.\r\nFollowing [26], [18], we can reformulate the Pareto solution of learning from two teachers as a linear scalarization of tasks with adaptive weight assignment as follows:\r\n\\(L(\\theta ) = \\alpha _1 L_{kd}^{t1} + \\alpha _2 L_{kd}^{t2}\\) \r\n", "entities": {"a22": {"id": "a22", "type": "a", "subtype": null, "surface_forms": [{"id": "#0ce725ff-f3cb-44e5-96f5-40beda5ab573", "surface_form": "distillation loss", "start": 100, "end": 117}, {"id": "#64b8839a-0585-494f-961b-5c30af5ae150", "surface_form": "L_{kd}^{t1}", "start": 760, "end": 771}, {"id": "#2d507691-84a2-4519-85f3-648daff32818", "surface_form": "L_{kd}^{t2}", "start": 784, "end": 795}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9d1df3a3-f5cc-4747-a64e-158f0de0238d", "surface_form": "one", "start": 468, "end": 471}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9fc14e60-cccb-488c-a831-1fd5c8535043", "surface_form": "two", "start": 643, "end": 646}]}, "a26": {"id": "a26", "type": "a", "subtype": null, "surface_forms": [{"id": "#f8e82004-ca7b-4b65-81f7-d086d74a2f55", "surface_form": "L", "start": 737, "end": 738}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#4e36acf3-567d-47e7-a55b-b4775668b24e", "surface_form": "\\alpha _1", "start": 750, "end": 759}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#fe9c2e87-0e4a-4b69-adad-82a14c95c680", "surface_form": "\\alpha _2", "start": 774, "end": 783}]}}, "relations": {"r1": {"id": "r1", "source": "p2", "target": "a26", "evidences": [{"id": "#1614ff8f-c85e-4040-8fec-c04e1047ef43", "evidence_sentence": "Following [26], [18], we can reformulate the Pareto solution of learning from two teachers as a linear scalarization of tasks with adaptive weight assignment as follows:\r\n\\(L(\\theta ) = \\alpha _1 L_{kd}^{t1} + \\alpha _2 L_{kd}^{t2}\\)", "start": 568, "end": 801}], "label": "pa"}, "r0": {"id": "r0", "source": "p3", "target": "a26", "evidences": [{"id": "#265d6063-b4df-4c5e-9d0a-06cd483f647e", "evidence_sentence": "Following [26], [18], we can reformulate the Pareto solution of learning from two teachers as a linear scalarization of tasks with adaptive weight assignment as follows:\r\n\\(L(\\theta ) = \\alpha _1 L_{kd}^{t1} + \\alpha _2 L_{kd}^{t2}\\)", "start": 568, "end": 801}], "label": "pa"}}}, "2212.06522_30": {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 30, "text": "Subsequently, the segment ensemble can further integrate with EMA to incorporate temporal property. Here we first review the traditional EMA strategy:\r\n\\({\\theta ^t(\\tau ) = \\lbrace  m \\theta ^ t (\\tau -1) + (1 - m) \\theta ^s (\\tau )\\rbrace }\\) \r\n", "entities": {"a23": {"id": "a23", "type": "a", "subtype": null, "surface_forms": [{"id": "#61becb1d-a578-42b9-a004-36ea476d38d7", "surface_form": "EMA", "start": 62, "end": 65}, {"id": "#6f02e0c4-3034-4a2c-aef5-a616889c7158", "surface_form": "EMA", "start": 137, "end": 140}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#88510f32-2577-4d04-a7cb-197e78b31c68", "surface_form": "m", "start": 182, "end": 183}, {"id": "#af61b9bd-ad4b-47db-8a5b-e2de98a82bdc", "surface_form": "m", "start": 212, "end": 213}]}}, "relations": {"r0": {"id": "r0", "source": "p6", "target": "a23", "evidences": [{"id": "#19883a91-7529-43a0-aa24-0df24a12a13e", "evidence_sentence": "Here we first review the traditional EMA strategy:\r\n\\({\\theta ^t(\\tau ) = \\lbrace  m \\theta ^ t (\\tau -1) + (1 - m) \\theta ^s (\\tau )\\rbrace }\\)", "start": 100, "end": 244}], "label": "pa"}}}, "2212.06522_27": {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 27, "text": "Finally, the total training loss for the student model in the second stage is:\r\n\\({L(\\theta ) = L_{ce}(\\theta ) + \\alpha L_{kd}^{t1} ({\\theta }) + (1-\\alpha ) L_{kd}^{t2} ({\\theta })}\\) \r\n", "entities": {"a29": {"id": "a29", "type": "a", "subtype": null, "surface_forms": [{"id": "#ea928cdf-5091-49aa-b7c9-fbf20f1b0706", "surface_form": "L", "start": 82, "end": 83}]}, "a5": {"id": "a5", "type": "a", "subtype": null, "surface_forms": [{"id": "#b35e484b-9e96-41f7-a4d9-3c61e9a8f9c2", "surface_form": "L_{ce}", "start": 95, "end": 101}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#141b3708-fb26-450e-a7bd-e632eb98dd01", "surface_form": "\\alpha", "start": 113, "end": 119}, {"id": "#38108283-4dea-454c-b120-5ddfc5d8bf62", "surface_form": "alpha", "start": 150, "end": 155}]}, "a22": {"id": "a22", "type": "a", "subtype": null, "surface_forms": [{"id": "#92442653-cafa-4db4-872e-9f48c8358b9d", "surface_form": "L_{kd}^{t1}", "start": 120, "end": 131}, {"id": "#0c22d1f2-5deb-4f4c-a178-cfa9dc4b6375", "surface_form": "L_{kd}^{t2}", "start": 158, "end": 169}]}}, "relations": {"r0": {"id": "r0", "source": "p5", "target": "a29", "evidences": [{"id": "#5b0eea82-691f-4961-9b9c-0fe2f2ee4c93", "evidence_sentence": "Finally, the total training loss for the student model in the second stage is:\r\n\\({L(\\theta ) = L_{ce}(\\theta ) + \\alpha L_{kd}^{t1} ({\\theta }) + (1-\\alpha ) L_{kd}^{t2} ({\\theta })}\\)", "start": 0, "end": 185}], "label": "pa"}}}, "2212.06522_32": {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 32, "text": "when \\(m=0\\) , it becomes segment ensemble. Similarly, it degenerates to EMA when \\( \\sigma _2 = 0\\) . In this manner, the fine-grained ensemble not only possesses the temporal property of traditional EMA, but also enhances the robustness of each segment to noise.\r\nAs a result, the teacher tends to generate more reliable pseudo labels, which can be used as new supervision signals in the next round self-training.\r\n", "entities": {"p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#7d106d4f-6984-4a94-b178-a7914f9f3b32", "surface_form": "m", "start": 7, "end": 8}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#6938f268-460a-4abe-9e32-8497fd2b99d6", "surface_form": "0", "start": 9, "end": 10}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#1e6e7fa1-defb-4ebc-a8f5-208e26a6add1", "surface_form": "becomes segment ensemble", "start": 18, "end": 42}]}, "a23": {"id": "a23", "type": "a", "subtype": null, "surface_forms": [{"id": "#79c555ac-2318-40e7-b508-7780e0cc1d9c", "surface_form": "EMA", "start": 73, "end": 76}, {"id": "#029260a7-6f6b-4edc-a2f7-4f09bc99b747", "surface_form": "EMA", "start": 201, "end": 204}]}}, "relations": {"r1": {"id": "r1", "source": "c1", "target": "v1", "evidences": [{"id": "#9380c753-912a-47cb-9eb3-adf30e4209e5", "evidence_sentence": "when \\(m=0\\) , it becomes segment ensemble.", "start": 0, "end": 43}], "label": "cv"}, "r0": {"id": "r0", "source": "v1", "target": "p6", "evidences": [{"id": "#813990b1-d63e-4d06-860d-b560589a7b80", "evidence_sentence": "when \\(m=0\\) , it becomes segment ensemble.", "start": 0, "end": 43}], "label": "vp"}}}, "2212.06522_64": {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 64, "text": "The architecture of the teachers is the backbone language model and a top classification layer for token-level classification. Specifically, we adopt RoBERTa and DistilRoBERTa as backbone for teacher 1 and teacher 2.\r\nThe corresponding student has the same architecture as their teacher. The max training epoch is 50 for all datasets. The training batch size is 16 for CoNLL03, Webpage, and Twitter and 32 for OntoNotes 5.0. The learning rate is set to 1e-5 for CoNLL03 and Webpage, and 2e-5 for OntoNotes 5.0 and Twitter.\r\nFor the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets. For adaptive teacher learning, the confidence threshold \\(\\sigma _1\\)  is 0.9 for all datasets.\r\nIn the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.\r\n", "entities": {"a21": {"id": "a21", "type": "a", "subtype": null, "surface_forms": [{"id": "#4d82151b-281e-47b8-af55-2b508340f9fb", "surface_form": "RoBERTa", "start": 150, "end": 157}]}, "a33": {"id": "a33", "type": "a", "subtype": null, "surface_forms": [{"id": "#6a873387-3428-4c97-b6b9-bf366b84d1a0", "surface_form": "DistilRoBERTa", "start": 162, "end": 175}]}, "p7": {"id": "p7", "type": "p", "subtype": null, "surface_forms": [{"id": "#39980aba-4d1b-4d66-a62c-3b2ef5a104a8", "surface_form": "max training epoch", "start": 291, "end": 309}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#90dd8e03-b2e8-4f3a-ba63-8705ee730406", "surface_form": "50", "start": 313, "end": 315}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#5ea3c1a9-2089-4bfd-bf4c-2af7497402e2", "surface_form": "for all datasets", "start": 316, "end": 332}]}, "p8": {"id": "p8", "type": "p", "subtype": null, "surface_forms": [{"id": "#d99e78bf-53c0-493c-b0d4-734e2898d1c7", "surface_form": "training batch size", "start": 338, "end": 357}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ca9b08e3-be50-4cc2-ade1-b837c868a104", "surface_form": "16", "start": 361, "end": 363}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#6e68ca8d-ecd2-4291-bdf3-eb56e901654b", "surface_form": "for CoNLL03, Webpage, and Twitter", "start": 364, "end": 397}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#a334ffab-8033-4e90-b962-b0cabcb881e7", "surface_form": "32", "start": 402, "end": 404}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#07fe7512-ca40-4816-8483-b4a4acf8e6de", "surface_form": "for OntoNotes 5.0", "start": 405, "end": 422}]}, "p9": {"id": "p9", "type": "p", "subtype": null, "surface_forms": [{"id": "#51e345dd-5eae-4d6a-849f-ae9c5e1ad45d", "surface_form": "learning rate", "start": 428, "end": 441}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#595dd4dd-ffb0-4771-95db-84a504b3a1b5", "surface_form": "1e-5", "start": 452, "end": 456}]}, "c4": {"id": "c4", "type": "c", "subtype": null, "surface_forms": [{"id": "#6966836e-a6f4-41bf-9e02-06bcd5cc7d54", "surface_form": "for CoNLL03 and Webpage", "start": 457, "end": 480}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#59d01ddc-eaba-41ab-b1df-416ebabb7100", "surface_form": "2e-5", "start": 486, "end": 490}]}, "c5": {"id": "c5", "type": "c", "subtype": null, "surface_forms": [{"id": "#8bdf8dab-8036-428c-b948-9b019dae494f", "surface_form": "for OntoNotes 5.0 and Twitter", "start": 491, "end": 520}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#bd35aa2f-1236-4f12-8847-aa95bc32910a", "surface_form": "1", "start": 587, "end": 588}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#9ddd8307-9079-439c-94eb-8e03b7b46f85", "surface_form": "2", "start": 590, "end": 591}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#49025fa5-68f5-4a29-96dc-fa4e7ac3b8e7", "surface_form": "12", "start": 593, "end": 595}]}, "v9": {"id": "v9", "type": "v", "subtype": "n", "surface_forms": [{"id": "#f830186e-d2a0-4c50-a2fb-91aad8fac94c", "surface_form": "6", "start": 601, "end": 602}]}, "p10": {"id": "p10", "type": "p", "subtype": null, "surface_forms": [{"id": "#8c2f8f23-c2d5-48f7-b44e-e0136170bec5", "surface_form": "epochs", "start": 603, "end": 609}]}, "c6": {"id": "c6", "type": "c", "subtype": null, "surface_forms": [{"id": "#62a8118f-9e25-47ea-a804-2c2f0fdf4453", "surface_form": "for CoNLL03", "start": 610, "end": 621}]}, "c7": {"id": "c7", "type": "c", "subtype": null, "surface_forms": [{"id": "#c2687f6f-7ad7-41cb-a56a-10be0a08db7a", "surface_form": "OntoNotes 5.0", "start": 623, "end": 636}]}, "c8": {"id": "c8", "type": "c", "subtype": null, "surface_forms": [{"id": "#eb7e0193-36c5-4403-8c90-d5d60af642a9", "surface_form": "Webpage", "start": 638, "end": 645}]}, "c9": {"id": "c9", "type": "c", "subtype": null, "surface_forms": [{"id": "#3e554faa-5c60-49f6-ab40-8639e75a24bc", "surface_form": "Twitter", "start": 651, "end": 658}]}, "p11": {"id": "p11", "type": "p", "subtype": null, "surface_forms": [{"id": "#d2721d72-4658-40a5-bfd2-7df8b8713557", "surface_form": "confidence threshold", "start": 704, "end": 724}, {"id": "#fa054e4f-8e6b-4caf-8927-cb41609f1000", "surface_form": "sigma _1", "start": 728, "end": 736}]}, "v10": {"id": "v10", "type": "v", "subtype": "n", "surface_forms": [{"id": "#c222da00-6c3d-4058-8c8f-4a66dddd3e5a", "surface_form": "0.9", "start": 743, "end": 746}]}, "c10": {"id": "c10", "type": "c", "subtype": null, "surface_forms": [{"id": "#9252282a-d027-4541-904b-926762c67da8", "surface_form": "for all datasets", "start": 747, "end": 763}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#904c2210-2a13-46a7-a98b-79459f19496b", "surface_form": "m", "start": 805, "end": 806}]}, "v11": {"id": "v11", "type": "v", "subtype": "n", "surface_forms": [{"id": "#1d414fa9-551c-4a97-b49a-7ab29ce90c11", "surface_form": "0.995", "start": 814, "end": 819}]}, "v12": {"id": "v12", "type": "v", "subtype": "n", "surface_forms": [{"id": "#724a33c3-a59c-46db-ae3b-78a5e577f4bf", "surface_form": "0.995", "start": 821, "end": 826}]}, "v13": {"id": "v13", "type": "v", "subtype": "n", "surface_forms": [{"id": "#7076235c-22a9-482b-bc24-7dfa29b4d7da", "surface_form": "0.99", "start": 828, "end": 832}]}, "v14": {"id": "v14", "type": "v", "subtype": "n", "surface_forms": [{"id": "#7fc88f86-1750-4416-a945-d30f49815f93", "surface_form": "0.995", "start": 834, "end": 839}]}, "p12": {"id": "p12", "type": "p", "subtype": null, "surface_forms": [{"id": "#3d82a3d3-7086-44e5-a2df-9340c701070a", "surface_form": "\\sigma _2", "start": 846, "end": 855}]}, "v15": {"id": "v15", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3552d2cb-60c3-4897-9bdb-46afd400c263", "surface_form": "0.8", "start": 869, "end": 872}]}, "v16": {"id": "v16", "type": "v", "subtype": "n", "surface_forms": [{"id": "#b16668b7-6a81-45ed-a812-2bbeb3bb1257", "surface_form": "0.995", "start": 874, "end": 879}]}, "v17": {"id": "v17", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3c285a78-7c54-4250-bd3e-9ea5b47593f5", "surface_form": "0.8", "start": 881, "end": 884}]}, "v18": {"id": "v18", "type": "v", "subtype": "n", "surface_forms": [{"id": "#0285cbb3-93ff-49b3-81a5-77838611160e", "surface_form": "0.75", "start": 890, "end": 894}]}, "c11": {"id": "c11", "type": "c", "subtype": null, "surface_forms": [{"id": "#94297e7a-eb22-4898-9a87-e16b5267896c", "surface_form": "for dataset CoNLL03", "start": 895, "end": 914}]}, "c12": {"id": "c12", "type": "c", "subtype": null, "surface_forms": [{"id": "#15d5e206-cc2f-4c06-bff7-3c7814a0298e", "surface_form": "OntoNotes 5.0", "start": 916, "end": 929}]}, "c13": {"id": "c13", "type": "c", "subtype": null, "surface_forms": [{"id": "#1e305897-7039-456a-8b2c-2275d1bc07b3", "surface_form": "Webpage", "start": 931, "end": 938}]}, "c14": {"id": "c14", "type": "c", "subtype": null, "surface_forms": [{"id": "#8bf42e7d-6b2d-42c6-a203-c77ab98e1dc4", "surface_form": "Twitter", "start": 944, "end": 951}]}}, "relations": {"r35": {"id": "r35", "source": "c1", "target": "v1", "evidences": [{"id": "#f92755d9-e850-4d24-ba61-252fb9df11bb", "evidence_sentence": "The max training epoch is 50 for all datasets.", "start": 288, "end": 334}], "label": "cv"}, "r0": {"id": "r0", "source": "v1", "target": "p7", "evidences": [{"id": "#bdbb6a67-44ed-49e6-bc71-f33c675f0173", "evidence_sentence": "The max training epoch is 50 for all datasets.", "start": 288, "end": 334}], "label": "vp"}, "r1": {"id": "r1", "source": "c3", "target": "v3", "evidences": [{"id": "#ef68b9ec-3693-40b2-92ac-044b953efe51", "evidence_sentence": "The training batch size is 16 for CoNLL03, Webpage, and Twitter and 32 for OntoNotes 5.0.", "start": 335, "end": 424}], "label": "cv"}, "r2": {"id": "r2", "source": "c2", "target": "v2", "evidences": [{"id": "#bcfc3645-90cc-4f78-b463-e43706a2cdb7", "evidence_sentence": "The training batch size is 16 for CoNLL03, Webpage, and Twitter and 32 for OntoNotes 5.0.", "start": 335, "end": 424}], "label": "cv"}, "r3": {"id": "r3", "source": "v2", "target": "p8", "evidences": [{"id": "#77f957b9-d760-4124-b788-7b26ec627bef", "evidence_sentence": "The training batch size is 16 for CoNLL03, Webpage, and Twitter and 32 for OntoNotes 5.0.", "start": 335, "end": 424}], "label": "vp"}, "r4": {"id": "r4", "source": "v3", "target": "p8", "evidences": [{"id": "#049e868b-5a19-4ea8-b65b-d897268cc159", "evidence_sentence": "The training batch size is 16 for CoNLL03, Webpage, and Twitter and 32 for OntoNotes 5.0.", "start": 335, "end": 424}], "label": "vp"}, "r5": {"id": "r5", "source": "c4", "target": "v4", "evidences": [{"id": "#fcb2891b-686e-44c6-b05f-23f707530b37", "evidence_sentence": "The learning rate is set to 1e-5 for CoNLL03 and Webpage, and 2e-5 for OntoNotes 5.0 and Twitter.", "start": 425, "end": 522}], "label": "cv"}, "r6": {"id": "r6", "source": "v4", "target": "p9", "evidences": [{"id": "#626b7085-a1ac-4e99-9751-d924662deee0", "evidence_sentence": "The learning rate is set to 1e-5 for CoNLL03 and Webpage, and 2e-5 for OntoNotes 5.0 and Twitter.", "start": 425, "end": 522}], "label": "vp"}, "r7": {"id": "r7", "source": "c5", "target": "v5", "evidences": [{"id": "#b293cc22-9402-450e-9f2e-dde1e3ca518f", "evidence_sentence": "The learning rate is set to 1e-5 for CoNLL03 and Webpage, and 2e-5 for OntoNotes 5.0 and Twitter.", "start": 425, "end": 522}], "label": "cv"}, "r8": {"id": "r8", "source": "v5", "target": "p9", "evidences": [{"id": "#7c18b216-6dfa-4167-b79f-3d4998927a29", "evidence_sentence": "The learning rate is set to 1e-5 for CoNLL03 and Webpage, and 2e-5 for OntoNotes 5.0 and Twitter.", "start": 425, "end": 522}], "label": "vp"}, "r9": {"id": "r9", "source": "c6", "target": "v6", "evidences": [{"id": "#355d2ab4-315e-4d67-9c5a-b097d23aeb1e", "evidence_sentence": "For the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets.", "start": 524, "end": 670}], "label": "cv"}, "r10": {"id": "r10", "source": "v6", "target": "p10", "evidences": [{"id": "#0f7a3877-ea6c-4477-8e7a-fa9c3641b232", "evidence_sentence": "For the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets.", "start": 524, "end": 670}], "label": "vp"}, "r11": {"id": "r11", "source": "c7", "target": "v7", "evidences": [{"id": "#3f4baf61-9bf1-4bcf-a218-92b136e90eca", "evidence_sentence": "For the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets.", "start": 524, "end": 670}], "label": "cv"}, "r12": {"id": "r12", "source": "v7", "target": "p10", "evidences": [{"id": "#e6046ec9-3571-4714-81bf-a7352852c1d1", "evidence_sentence": "For the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets.", "start": 524, "end": 670}], "label": "vp"}, "r13": {"id": "r13", "source": "c8", "target": "v8", "evidences": [{"id": "#fa4958fe-d005-4469-8ec1-33a8c0e1dffd", "evidence_sentence": "For the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets.", "start": 524, "end": 670}], "label": "cv"}, "r14": {"id": "r14", "source": "v8", "target": "p10", "evidences": [{"id": "#cfcbb33e-30c2-413d-a86a-2be8e63dc573", "evidence_sentence": "For the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets.", "start": 524, "end": 670}], "label": "vp"}, "r15": {"id": "r15", "source": "c9", "target": "v9", "evidences": [{"id": "#ab6a6c34-94dd-4718-a313-d9013d0b4c6b", "evidence_sentence": "For the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets.", "start": 524, "end": 670}], "label": "cv"}, "r16": {"id": "r16", "source": "v9", "target": "p10", "evidences": [{"id": "#303dcd4c-096a-4ffa-9d8c-24143d18845f", "evidence_sentence": "For the pretraining stage with noisy labels, we separately train 1, 2, 12, and 6 epochs for CoNLL03, OntoNotes 5.0, Webpage, and Twitter datasets.", "start": 524, "end": 670}], "label": "vp"}, "r17": {"id": "r17", "source": "c10", "target": "v10", "evidences": [{"id": "#ab373df2-f624-4070-ac58-57ea6391e675", "evidence_sentence": "For adaptive teacher learning, the confidence threshold \\(\\sigma _1\\)  is 0.9 for all datasets.", "start": 671, "end": 766}], "label": "cv"}, "r18": {"id": "r18", "source": "v10", "target": "p11", "evidences": [{"id": "#3d02a2bd-b704-4716-9fe4-8c96959e7eee", "evidence_sentence": "For adaptive teacher learning, the confidence threshold \\(\\sigma _1\\)  is 0.9 for all datasets.", "start": 671, "end": 766}], "label": "vp"}, "r19": {"id": "r19", "source": "c11", "target": "v11", "evidences": [{"id": "#8263173a-92cb-4d50-b7e4-72565eb287f5", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "cv"}, "r20": {"id": "r20", "source": "v11", "target": "p6", "evidences": [{"id": "#89711796-f41f-421c-94fd-a98ef2b44baa", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "vp"}, "r21": {"id": "r21", "source": "c12", "target": "v12", "evidences": [{"id": "#bd26107a-59a3-45f8-9924-60c2e406b667", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "cv"}, "r22": {"id": "r22", "source": "v12", "target": "p6", "evidences": [{"id": "#668ac464-8a11-4940-a3d8-40ad3351f0ca", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "vp"}, "r23": {"id": "r23", "source": "c13", "target": "v13", "evidences": [{"id": "#ca8fbd35-7c56-487e-b2d3-2499056c61bf", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "cv"}, "r24": {"id": "r24", "source": "v13", "target": "p6", "evidences": [{"id": "#52f9a2ae-47d0-46a5-bace-144b38370561", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "vp"}, "r25": {"id": "r25", "source": "c14", "target": "v14", "evidences": [{"id": "#1159871e-9e48-407d-ab5e-2bd36625e8b8", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "cv"}, "r26": {"id": "r26", "source": "v14", "target": "p6", "evidences": [{"id": "#894151b7-e619-4652-9347-d7c1bc137c71", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "vp"}, "r27": {"id": "r27", "source": "c11", "target": "v15", "evidences": [{"id": "#7ad38a1f-15e1-4e37-8411-5fc80fc04566", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "cv"}, "r28": {"id": "r28", "source": "v15", "target": "p12", "evidences": [{"id": "#d4a6f333-124c-45f3-9e32-bd2e9825b4a5", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "vp"}, "r29": {"id": "r29", "source": "c12", "target": "v16", "evidences": [{"id": "#769f1c65-f117-4485-bfde-c5bcdd6e810d", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "cv"}, "r30": {"id": "r30", "source": "v16", "target": "p12", "evidences": [{"id": "#5760e871-445a-43ea-b67d-8e5ec9012980", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "vp"}, "r31": {"id": "r31", "source": "c13", "target": "v17", "evidences": [{"id": "#172b3f18-0deb-4db8-91e5-0af87aaf6791", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "cv"}, "r32": {"id": "r32", "source": "v17", "target": "p12", "evidences": [{"id": "#bdda6397-1ca9-4121-b5ab-b8e8a8d12efd", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "vp"}, "r33": {"id": "r33", "source": "c14", "target": "v18", "evidences": [{"id": "#2e6d2fc7-55a2-4e55-a306-c9379c105d24", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "cv"}, "r34": {"id": "r34", "source": "v18", "target": "p12", "evidences": [{"id": "#5af2f9dc-2d44-453e-8c6a-d6ad97793e64", "evidence_sentence": "In the fine-grained student ensemble, \\(m\\)  are 0.995, 0.995, 0.99, 0.995 and \\(\\sigma _2\\)  is set to 0.8, 0.995, 0.8, and 0.75 for dataset CoNLL03, OntoNotes 5.0, Webpage, and Twitter, respectively.", "start": 768, "end": 969}], "label": "vp"}}}, "2212.06522_70": {"annotator_id": "tsa", "document_id": "2212.06522", "paragraph_index": 70, "text": "We investigate the effectiveness of different student ensemble methods. For comprehensive evaluation, we experiment on a relatively smaller dataset Twitter instead of CoNLL03. As shown in Table 5: (1) remove all ensemble strategies (w/o all) and directly copy the student as a new teacher. (2) remove the segment ensemble (w/o SE), namely \\(\\sigma _2=0\\)  in Eq.13. (3) remove the EMA (w/o EMA), namely \\(m=0\\)  in Eq.13. As shown in Table 5, w/o all lead to the most significant performance drop. Meanwhile, removing either SE or EMA cause decreased results, demonstrating these two kinds of ensemble method can complement each other. It is worth noting ATSEN achieves significantly better precision than variants, indicating fine-grained ensemble can effectively enhance consistent predictions by performing on model fragments.\r\nFurthermore, we investigate the parameter influence of fine-grained ensemble in Fig. 3. As shown in this figure, we can observe \\(m=0.995\\)  and \\(\\sigma _2=0.75\\)  achieve the best performance. We also notice\r\nan interesting fact is that with the increase of \\(m\\) , the model achieves its best performance at a relatively smaller value of \\(\\sigma _2\\) .\r\n", "entities": {"c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#4a0e9879-fd2e-4344-9368-f475a5ce7e72", "surface_form": "investigate the effectiveness of different student ensemble methods", "start": 3, "end": 70}]}, "a10": {"id": "a10", "type": "a", "subtype": null, "surface_forms": [{"id": "#5d2e35b6-32a2-40f4-af98-2bcd67d7e56a", "surface_form": "Twitter", "start": 148, "end": 155}]}, "a7": {"id": "a7", "type": "a", "subtype": null, "surface_forms": [{"id": "#32314766-2f82-40da-b1a8-64de85727c64", "surface_form": "CoNLL03", "start": 167, "end": 174}]}, "p12": {"id": "p12", "type": "p", "subtype": null, "surface_forms": [{"id": "#0f068c62-33b5-417e-a372-8302c483fb1a", "surface_form": "sigma _2", "start": 342, "end": 350}, {"id": "#3d4016b5-2b89-4c1d-acfc-a8a863515df1", "surface_form": "sigma _2", "start": 978, "end": 986}, {"id": "#ca164fc8-0493-46f8-9ec4-3917a3d2bfd6", "surface_form": "\\sigma _2", "start": 1172, "end": 1181}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#44c5d760-d6dd-474a-9c84-985b9bc1f087", "surface_form": "0", "start": 351, "end": 352}]}, "a23": {"id": "a23", "type": "a", "subtype": null, "surface_forms": [{"id": "#86080d07-6746-4e8c-83f0-6a5b1381ef03", "surface_form": "EMA", "start": 381, "end": 384}, {"id": "#55b2fcdd-e194-4935-84da-9fda59c5501e", "surface_form": "EMA", "start": 390, "end": 393}, {"id": "#02c6d2dd-866a-4115-a9db-f1f404f96070", "surface_form": "EMA", "start": 531, "end": 534}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#4dfc8248-bdff-4399-acf2-9d8172b29e03", "surface_form": "m", "start": 405, "end": 406}, {"id": "#5757964b-7e2d-486d-a157-4085dd79d3b2", "surface_form": "m", "start": 960, "end": 961}, {"id": "#4c1b452c-43b6-4c22-a18e-79b650bc07aa", "surface_form": "m", "start": 1091, "end": 1092}]}, "v2": {"id": "v2", "type": "v", "subtype": "n", "surface_forms": [{"id": "#deee8fe5-12b0-4b63-815b-62c1dd6f0c2e", "surface_form": "0", "start": 407, "end": 408}]}, "v3": {"id": "v3", "type": "v", "subtype": "n", "surface_forms": [{"id": "#1fa27998-68ed-4a84-8754-6f811e58cb12", "surface_form": "two", "start": 580, "end": 583}]}, "a1": {"id": "a1", "type": "a", "subtype": null, "surface_forms": [{"id": "#8ca2e203-1bbc-408f-81f3-524c35ec0daf", "surface_form": "ATSEN", "start": 655, "end": 660}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#6b4407d1-6acf-46fe-8bff-5ac8d1fd1111", "surface_form": "investigate the parameter influence of fine-grained ensemble", "start": 846, "end": 906}]}, "v4": {"id": "v4", "type": "v", "subtype": "n", "surface_forms": [{"id": "#c1840f52-7e45-4dea-9fbf-47a5607675e3", "surface_form": "3", "start": 915, "end": 916}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#f989e0f0-64d1-4a95-a58c-19a6a31c1808", "surface_form": "0.995", "start": 962, "end": 967}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#b7180398-4223-4550-82e5-6eda123b0257", "surface_form": "0.75", "start": 987, "end": 991}]}}, "relations": {"r7": {"id": "r7", "source": "v1", "target": "p12", "evidences": [{"id": "#4ff1f7d0-c3d6-49ee-9f33-8c73c6987a67", "evidence_sentence": "(2) remove the segment ensemble (w/o SE), namely \\(\\sigma _2=0\\)  in Eq.13.", "start": 290, "end": 365}], "label": "vp"}, "r0": {"id": "r0", "source": "v2", "target": "p6", "evidences": [{"id": "#93b7928e-5d95-4c33-af90-c850e39e865a", "evidence_sentence": "(3) remove the EMA (w/o EMA), namely \\(m=0\\)  in Eq.13.", "start": 366, "end": 421}], "label": "vp"}, "r1": {"id": "r1", "source": "c1", "target": "v1", "evidences": [{"id": "#3b336583-2c8f-455d-a2b8-4893ad994362", "evidence_sentence": "We investigate the effectiveness of different student ensemble methods. For comprehensive evaluation, we experiment on a relatively smaller dataset Twitter instead of CoNLL03. As shown in Table 5: (1) remove all ensemble strategies (w/o all) and directly copy the student as a new teacher. (2) remove the segment ensemble (w/o SE), namely \\(\\sigma _2=0\\)  in Eq.13.", "start": 0, "end": 365}], "label": "cv"}, "r2": {"id": "r2", "source": "c1", "target": "v2", "evidences": [{"id": "#0994fbba-754e-4357-88d1-354d295b5bae", "evidence_sentence": "We investigate the effectiveness of different student ensemble methods. For comprehensive evaluation, we experiment on a relatively smaller dataset Twitter instead of CoNLL03. As shown in Table 5: (1) remove all ensemble strategies (w/o all) and directly copy the student as a new teacher. (2) remove the segment ensemble (w/o SE), namely \\(\\sigma _2=0\\)  in Eq.13. (3) remove the EMA (w/o EMA), namely \\(m=0\\)  in Eq.13.", "start": 0, "end": 421}], "label": "cv"}, "r3": {"id": "r3", "source": "c2", "target": "v5", "evidences": [{"id": "#bf619e31-8580-4c0b-8fb2-78ad19ecaa12", "evidence_sentence": "Furthermore, we investigate the parameter influence of fine-grained ensemble in Fig. 3. As shown in this figure, we can observe \\(m=0.995\\)  and \\(\\sigma _2=0.75\\)  achieve the best performance.", "start": 831, "end": 1025}], "label": "cv"}, "r4": {"id": "r4", "source": "v5", "target": "p6", "evidences": [{"id": "#dec066f0-56e8-4eb5-bf31-be5c71d404b7", "evidence_sentence": "As shown in this figure, we can observe \\(m=0.995\\)  and \\(\\sigma _2=0.75\\)  achieve the best performance.", "start": 919, "end": 1025}], "label": "vp"}, "r5": {"id": "r5", "source": "c2", "target": "v6", "evidences": [{"id": "#d882300f-535a-4743-b791-8cdb0115a325", "evidence_sentence": "Furthermore, we investigate the parameter influence of fine-grained ensemble in Fig. 3. As shown in this figure, we can observe \\(m=0.995\\)  and \\(\\sigma _2=0.75\\)  achieve the best performance.", "start": 831, "end": 1025}], "label": "cv"}, "r6": {"id": "r6", "source": "v6", "target": "p12", "evidences": [{"id": "#e2e3712a-d66a-4e3d-bd4b-690ccc1b2244", "evidence_sentence": "As shown in this figure, we can observe \\(m=0.995\\)  and \\(\\sigma _2=0.75\\)  achieve the best performance.", "start": 919, "end": 1025}], "label": "vp"}}}, "2211.14208_75": {"annotator_id": "tsa", "document_id": "2211.14208", "paragraph_index": 75, "text": "We fine-tune our model within the hyperparameter search space in Table\u00a0REF . Our hyperparameter search used the method of W&B Sweeps\u00a0[3] with a standard random search with 500 counts. We introduce the best hyperparameter configuration in Tables\u00a0REF  to\u00a0REF .\r\n{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}{TABLE}", "entities": {"a4": {"id": "a4", "type": "a", "subtype": null, "surface_forms": [{"id": "#312d693a-f8f8-40e4-8310-f56bf9db7781", "surface_form": "model", "start": 17, "end": 22}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#391edf7d-c0b5-4482-9878-28cc8745eaf0", "surface_form": "hyperparameter search", "start": 81, "end": 102}]}, "a64": {"id": "a64", "type": "a", "subtype": null, "surface_forms": [{"id": "#24c28ff7-0c58-48c2-a887-e03286ca5458", "surface_form": "W&B Sweeps", "start": 122, "end": 132}]}, "v1": {"id": "v1", "type": "v", "subtype": "n", "surface_forms": [{"id": "#a079cf00-cd3d-4a75-97ec-703dd7a6a850", "surface_form": "500", "start": 172, "end": 175}]}, "p7": {"id": "p7", "type": "p", "subtype": null, "surface_forms": [{"id": "#9bc700fa-13bf-47f9-87a4-3b5ee5ada26d", "surface_form": "counts", "start": 176, "end": 182}]}}, "relations": {"r2": {"id": "r2", "source": "v1", "target": "p7", "evidences": [{"id": "#7ff7cafa-dc21-4c3f-8e95-548287f5a026", "evidence_sentence": "Our hyperparameter search used the method of W&B Sweeps\u00a0[3] with a standard random search with 500 counts.", "start": 77, "end": 183}], "label": "vp"}, "r0": {"id": "r0", "source": "p7", "target": "a64", "evidences": [{"id": "#0f7f5ce3-81b6-47ea-a64d-2af0f8960a0c", "evidence_sentence": "Our hyperparameter search used the method of W&B Sweeps\u00a0[3] with a standard random search with 500 counts.", "start": 77, "end": 183}], "label": "pa"}, "r1": {"id": "r1", "source": "c1", "target": "v1", "evidences": [{"id": "#256b3414-9a4f-48c2-86f0-8cae0f5a175c", "evidence_sentence": "Our hyperparameter search used the method of W&B Sweeps\u00a0[3] with a standard random search with 500 counts.", "start": 77, "end": 183}], "label": "cv"}}}}