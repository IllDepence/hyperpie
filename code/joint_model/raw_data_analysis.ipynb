{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e8bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a23955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('v number', '1e-5'), 60, 64)\n",
      "(('v number', '123'), 70, 73)\n",
      "(('v number', '3.14'), 75, 79)\n",
      "(('v range', '[0, 1]'), 99, 105)\n",
      "(('v set', ['10', '20', '30']), 137, 149)\n"
     ]
    }
   ],
   "source": [
    "def find_numbers_with_offsets(text):\n",
    "    result = []    \n",
    "    # number\n",
    "    #numbers = re.finditer(r'\\b\\d+(?:\\.\\d+)?\\b', text)\n",
    "    numbers = re.finditer(r'\\b(?:\\d+(?:\\.\\d+)?(?:[eE][-+^]?\\d+)?|\\d+(?:[eE][-+]?\\d+))\\b', text)\n",
    "    for match in numbers:\n",
    "        number = match.group()\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        # next to [ ] or ( ) \n",
    "        if '[' in text[start-1:end+1] or '(' in text[start-1:end+1] or ']' in text[start-1:end+1] or ')' in text[start-1:end+1]:\n",
    "            continue\n",
    "        # inside [ ] or ( ) \n",
    "        if '[' in text[start-10:start] and ']' in text[end:end+10] or '(' in text[start-10:start] and ')' in text[end:end+10]:\n",
    "            continue\n",
    "        result.append((('v number', number), start, end))\n",
    "    \n",
    "    # range\n",
    "    number_ranges = re.finditer(r'\\[([\\d.]+),\\s*([\\d.]+)\\]', text)\n",
    "    for match in number_ranges:\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        result.append((('v range', match.group()), start, end))\n",
    "\n",
    "    # set\n",
    "    number_sets = re.finditer(r'\\(([\\d\\s,]+)\\)', text)\n",
    "    for match in number_sets:\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        numbers = re.findall(r'\\d+', match.group(1))\n",
    "        result.append((('v set', numbers), start, end))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# text axample\n",
    "text = 'This is a sample [hhh] text deep learning (dl) with numbers 1e-5 like 123, 3.14, and number ranges [0, 1]. Also, it has number sets like (10, 20, 30).'\n",
    "numbers_with_offsets = find_numbers_with_offsets(text)\n",
    "\n",
    "for item in numbers_with_offsets:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b996850",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ap_rel.json', 'r') as f:\n",
    "    # pair: (artifact parameter)\n",
    "    ap_rel = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1b215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = KeywordProcessor()\n",
    "for (a,p) in ap_rel:\n",
    "    kp.add_keyword(a, ('artifact', a))\n",
    "    kp.add_keyword(p, ('parameter', p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6001c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_fpath = './transformed_pprs_filtered.jsonl'\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "jsonl_data = read_jsonl_file(rawdata_fpath)\n",
    "# sample\n",
    "jsonl_data = jsonl_data[:10]\n",
    "# jsonl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d9b1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匹配并转换 LaTeX 文本中的科学计数法表示\n",
    "def convert_scientific_notation(match):\n",
    "    power = int(match.group(1))\n",
    "    number = 10 ** (power)\n",
    "    return str(number)\n",
    "\n",
    "\n",
    "def latex_to_text(latex_string):\n",
    "    \n",
    "    # LaTeX command\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\*?', '', latex_string)\n",
    "    \n",
    "    text = re.sub(r'10\\^\\{(-\\d+)\\}', convert_scientific_notation, text)\n",
    "    \n",
    "    # 去除格式标记\n",
    "    text = re.sub(r'\\{[^}]+\\}', '', text)\n",
    "    \n",
    "    # 去除注释\n",
    "    #text = re.sub(r'%.*', '', text)\n",
    "    \n",
    "    # 去除引用标记\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # 去除\\)\n",
    "    #text = re.sub(r'\\\\[()]', '', text)\n",
    "    text = re.sub(r'[\\\\/\\\\()]', '', text)\n",
    "    \n",
    "    # space and \\n\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ef8bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'1507.01422': [[[(('parameter', 'dropout'), 17, 24),\n",
       "                (('parameter', 'layer'), 25, 30),\n",
       "                (('parameter', 'layer'), 63, 68),\n",
       "                (('parameter', 'dropout'), 77, 84),\n",
       "                (('v number', '0.5'), 94, 97),\n",
       "                (('v number', '50'), 98, 100)],\n",
       "               'We also tested a dropout layer after the first fully connected layer, with a dropout ratio of 0.5 50% of probability to set a neuron’s output value to zero']],\n",
       "             '1506.05929': [[[(('parameter', 'learning rate'), 25, 38),\n",
       "                (('parameter', 'weight decay'), 54, 66),\n",
       "                (('v number', '0.01'), 89, 93),\n",
       "                (('v number', '0.9'), 96, 99),\n",
       "                (('v number', '5'), 106, 107),\n",
       "                (('v number', '0.0001'), 108, 114)],\n",
       "               ', with the values of the learning rate, momentum, and weight decay hyperparameters being 0.01 , 0.9 , and 5 0.0001 respectively'],\n",
       "              [[(('parameter', 'learning rate'), 64, 77),\n",
       "                (('v number', '10'), 104, 106)],\n",
       "               'Whenever the error on the validation set stopped decreasing the learning rate was decreased by a factor 10']],\n",
       "             '1501.05759': [[[(('artifact', 'model'), 4, 9),\n",
       "                (('artifact', 'model'), 97, 102),\n",
       "                (('v number', '60'), 19, 21),\n",
       "                (('v number', '120'), 22, 25),\n",
       "                (('v number', '32'), 108, 110),\n",
       "                (('v number', '512'), 127, 130),\n",
       "                (('v number', '1024'), 132, 136),\n",
       "                (('v number', '2048'), 138, 142),\n",
       "                (('v number', '4096'), 144, 148)],\n",
       "               'The model has size 60 120 , and is built via four rounds of hard negative mining starting from a model with 32 trees, and then 512, 1024, 2048, 4096 trees']],\n",
       "             '1511.04587': [[[(('parameter', 'weight decay'), 13, 25),\n",
       "                (('v number', '0.9'), 48, 51),\n",
       "                (('v number', '0.0001'), 56, 62)],\n",
       "               'Momentum and weight decay parameters are set to 0.9 and 0.0001 , respectively.']],\n",
       "             '1511.04491': [[[(('parameter', 'layers'), 0, 6),\n",
       "                (('v number', '41'), 26, 28),\n",
       "                (('v number', '41'), 32, 34)],\n",
       "               'layers receptive field of 41 by 41'],\n",
       "              [[(('parameter', 'weight decay'), 41, 53),\n",
       "                (('v number', '0.9'), 33, 36),\n",
       "                (('v number', '0.0001'), 57, 63)],\n",
       "               'We set the momentum parameter to 0.9 and weight decay to 0.0001'],\n",
       "              [[(('parameter', 'layers'), 50, 56),\n",
       "                (('v number', '256'), 7, 10),\n",
       "                (('v number', '3'), 31, 32),\n",
       "                (('v number', '3'), 33, 34)],\n",
       "               'We use 256 filters of the size 3 3 for all weight layers']],\n",
       "             '1509.07308': [[[(('parameter', 'steps'), 24, 29),\n",
       "                (('v number', '7'), 60, 61),\n",
       "                (('v number', '000'), 62, 65),\n",
       "                (('v number', '13'), 70, 72),\n",
       "                (('v number', '000'), 73, 76),\n",
       "                (('v number', '1'), 198, 199)],\n",
       "               'After the preprocessing steps vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair, and the training corpora are quite small: ranging from approximately 1.5M tokens for NL-EN to 4M for ES-EN']],\n",
       "             '1512.09272': [[[(('artifact', 'model'), 4, 9),\n",
       "                (('parameter', 'learning rate'), 93, 106),\n",
       "                (('parameter', 'epochs'), 176, 182),\n",
       "                (('parameter', 'epoch'), 221, 226),\n",
       "                (('parameter', 'weight decay'), 281, 293),\n",
       "                (('v number', '1'), 78, 79),\n",
       "                (('v number', '0.01'), 110, 114),\n",
       "                (('v number', '0.0001'), 244, 250),\n",
       "                (('v number', '2'), 253, 254),\n",
       "                (('v number', '0.9'), 273, 276),\n",
       "                (('v number', '3'), 279, 280),\n",
       "                (('v number', '0.0005'), 297, 303),\n",
       "                (('v number', '4'), 310, 311),\n",
       "                (('v number', '90'), 365, 367),\n",
       "                (('v number', '180'), 369, 372),\n",
       "                (('v number', '270'), 378, 381),\n",
       "                (('v number', '5'), 455, 456),\n",
       "                (('v number', '3'), 464, 465),\n",
       "                (('v number', '2'), 480, 481)],\n",
       "               'The model training is based on stochastic gradient descent SGD that involves: 1 the use of a learning rate of 0.01 that gradually automatically computed based on the number of epochs set for training decreases after each epoch until it reaches 0.0001 ; 2 a momentum set at 0.9 , 3 weight decay of 0.0005 , and 4 data augmentation by rotating the pair of patches by 90, 180, and 270 degrees, and flipping the images horizontally and vertically , augmented 5 times: 3 rotations and 2 flippings '],\n",
       "              [[(('artifact', 'model'), 56, 61),\n",
       "                (('v number', '56'), 32, 34),\n",
       "                (('v number', '980'), 114, 117)],\n",
       "               'Our Matlab implementation takes 56 hours for training a model and processes 16K imagessec during testing on a GTX 980 GPU.']],\n",
       "             '1611.02064': [[[(('parameter', 'learning rate'), 46, 59),\n",
       "                (('v number', '0.0001'), 66, 72),\n",
       "                (('v number', '0.7'), 139, 142)],\n",
       "               'Throughout the experiments, we have fixed the learning rate to be 0.0001 and RMSprop optimization algorithm is used with momentum fixed at 0.7 '],\n",
       "              [[(('artifact', 'model'), 4, 9),\n",
       "                (('parameter', 'epochs'), 28, 34),\n",
       "                (('parameter', 'batch size'), 42, 52),\n",
       "                (('v number', '60'), 25, 27),\n",
       "                (('v number', '32'), 56, 58)],\n",
       "               'Our model is trained for 60 epochs with a batch size of 32.']],\n",
       "             '1611.01487': [[[(('artifact', 'model'), 107, 112),\n",
       "                (('parameter', 'steps'), 224, 229)],\n",
       "               'To train our models, we used the train portion of the datasets as-is and evaluated on the test portion the model which performed best on the development portion of the dataset, without conducting any specific pre-processing steps on the data'],\n",
       "              [[(('parameter', 'epochs'), 41, 47),\n",
       "                (('v number', '100'), 37, 40)],\n",
       "               'We train the models for a maximum of 100 epochs over the training set'],\n",
       "              [[(('artifact', 'model'), 44, 49),\n",
       "                (('parameter', 'epochs'), 57, 63),\n",
       "                (('parameter', 'epochs'), 113, 119),\n",
       "                (('v number', '20'), 54, 56),\n",
       "                (('v number', '5'), 111, 112)],\n",
       "               'To avoid long training time, we trained the model for 20 epochs for datasets larger than 50k examples, and for 5 epochs for datasets larger than 200k examples']],\n",
       "             '1606.02003': [[[(('parameter', 'layer'), 67, 72),\n",
       "                (('v number', '512'), 36, 39),\n",
       "                (('v number', '1024'), 76, 80)],\n",
       "               'The dimensions of word embedding is 512 and the size of the hidden layer is 1024']]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_info = defaultdict(list)\n",
    "for item in jsonl_data:\n",
    "    text = item['paragraphs'][0]\n",
    "    text = latex_to_text(text)\n",
    "    #print(text)\n",
    "    sentences = re.split(r'\\.\\s', text)\n",
    "    for sentence in sentences:\n",
    "        entities = kp.extract_keywords(sentence, span_info=True)\n",
    "        values = find_numbers_with_offsets(sentence)\n",
    "#         print(entities, values)\n",
    "        if values and len(entities) > 0:\n",
    "            for v in values:\n",
    "                entities.append(v)\n",
    "        if len(entities) > 1:\n",
    "#             print(entities,'\\n', sentence)\n",
    "            extracted_info[item['id']].append([entities, sentence]) \n",
    "            \n",
    "        # if (a,p,v), then add\n",
    "        \n",
    "            \n",
    "        # creat entity-relation pair:\n",
    "        # select the nearest value\n",
    "        \n",
    "#         print(v)\n",
    "        \n",
    "#         print(x,'\\n', sentence)        \n",
    "#     print(len(sentence))\n",
    "    \n",
    "#     x = kp.extract_keywords(text, span_info=True)\n",
    "#     v = numbers_with_offsets = find_numbers_with_offsets(text)\n",
    "#     x.append(v)\n",
    "#     print(x,'\\n', text)\n",
    "    \n",
    "#     print('\\n')\n",
    "extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b3a93fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343f10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to JSON file\n",
    "with open(\"extracted_info.json\", \"w\") as json_file:\n",
    "    json.dump(extracted_info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3c4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72c0f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[(('parameter', 'dropout'), 17, 24), (('parameter', 'layer'), 25, 30), (('parameter', 'layer'), 63, 68), (('parameter', 'dropout'), 77, 84), (('v number', '0.5'), 94, 97), (('v number', '50'), 98, 100)], 'We also tested a dropout layer after the first fully connected layer, with a dropout ratio of 0.5 50% of probability to set a neuron’s output value to zero']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 25, 38), (('parameter', 'weight decay'), 54, 66), (('v number', '0.01'), 89, 93), (('v number', '0.9'), 96, 99), (('v number', '5'), 106, 107), (('v number', '0.0001'), 108, 114)], ', with the values of the learning rate, momentum, and weight decay hyperparameters being 0.01 , 0.9 , and 5 0.0001 respectively'], [[(('parameter', 'learning rate'), 64, 77), (('v number', '10'), 104, 106)], 'Whenever the error on the validation set stopped decreasing the learning rate was decreased by a factor 10']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('artifact', 'model'), 97, 102), (('v number', '60'), 19, 21), (('v number', '120'), 22, 25), (('v number', '32'), 108, 110), (('v number', '512'), 127, 130), (('v number', '1024'), 132, 136), (('v number', '2048'), 138, 142), (('v number', '4096'), 144, 148)], 'The model has size 60 120 , and is built via four rounds of hard negative mining starting from a model with 32 trees, and then 512, 1024, 2048, 4096 trees']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 13, 25), (('v number', '0.9'), 48, 51), (('v number', '0.0001'), 56, 62)], 'Momentum and weight decay parameters are set to 0.9 and 0.0001 , respectively.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 0, 6), (('v number', '41'), 26, 28), (('v number', '41'), 32, 34)], 'layers receptive field of 41 by 41'], [[(('parameter', 'weight decay'), 41, 53), (('v number', '0.9'), 33, 36), (('v number', '0.0001'), 57, 63)], 'We set the momentum parameter to 0.9 and weight decay to 0.0001'], [[(('parameter', 'layers'), 50, 56), (('v number', '256'), 7, 10), (('v number', '3'), 31, 32), (('v number', '3'), 33, 34)], 'We use 256 filters of the size 3 3 for all weight layers']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 24, 29), (('v number', '7'), 60, 61), (('v number', '000'), 62, 65), (('v number', '13'), 70, 72), (('v number', '000'), 73, 76), (('v number', '1'), 198, 199)], 'After the preprocessing steps vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair, and the training corpora are quite small: ranging from approximately 1.5M tokens for NL-EN to 4M for ES-EN']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('parameter', 'learning rate'), 93, 106), (('parameter', 'epochs'), 176, 182), (('parameter', 'epoch'), 221, 226), (('parameter', 'weight decay'), 281, 293), (('v number', '1'), 78, 79), (('v number', '0.01'), 110, 114), (('v number', '0.0001'), 244, 250), (('v number', '2'), 253, 254), (('v number', '0.9'), 273, 276), (('v number', '3'), 279, 280), (('v number', '0.0005'), 297, 303), (('v number', '4'), 310, 311), (('v number', '90'), 365, 367), (('v number', '180'), 369, 372), (('v number', '270'), 378, 381), (('v number', '5'), 455, 456), (('v number', '3'), 464, 465), (('v number', '2'), 480, 481)], 'The model training is based on stochastic gradient descent SGD that involves: 1 the use of a learning rate of 0.01 that gradually automatically computed based on the number of epochs set for training decreases after each epoch until it reaches 0.0001 ; 2 a momentum set at 0.9 , 3 weight decay of 0.0005 , and 4 data augmentation by rotating the pair of patches by 90, 180, and 270 degrees, and flipping the images horizontally and vertically , augmented 5 times: 3 rotations and 2 flippings '], [[(('artifact', 'model'), 56, 61), (('v number', '56'), 32, 34), (('v number', '980'), 114, 117)], 'Our Matlab implementation takes 56 hours for training a model and processes 16K imagessec during testing on a GTX 980 GPU.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 46, 59), (('v number', '0.0001'), 66, 72), (('v number', '0.7'), 139, 142)], 'Throughout the experiments, we have fixed the learning rate to be 0.0001 and RMSprop optimization algorithm is used with momentum fixed at 0.7 '], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 28, 34), (('parameter', 'batch size'), 42, 52), (('v number', '60'), 25, 27), (('v number', '32'), 56, 58)], 'Our model is trained for 60 epochs with a batch size of 32.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 107, 112), (('parameter', 'steps'), 224, 229)], 'To train our models, we used the train portion of the datasets as-is and evaluated on the test portion the model which performed best on the development portion of the dataset, without conducting any specific pre-processing steps on the data'], [[(('parameter', 'epochs'), 41, 47), (('v number', '100'), 37, 40)], 'We train the models for a maximum of 100 epochs over the training set'], [[(('artifact', 'model'), 44, 49), (('parameter', 'epochs'), 57, 63), (('parameter', 'epochs'), 113, 119), (('v number', '20'), 54, 56), (('v number', '5'), 111, 112)], 'To avoid long training time, we trained the model for 20 epochs for datasets larger than 50k examples, and for 5 epochs for datasets larger than 200k examples']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 67, 72), (('v number', '512'), 36, 39), (('v number', '1024'), 76, 80)], 'The dimensions of word embedding is 512 and the size of the hidden layer is 1024']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 4, 10), (('v number', '600'), 16, 19)], 'The layers have 600 hidden neurons'], [[(('parameter', 'learning rate'), 74, 87), (('parameter', 'weight decay'), 108, 120), (('v number', '0.7'), 145, 148), (('v number', '0.3'), 151, 154), (('v number', '0.5'), 159, 162)], 'Hyperparamenters were are fine-tuned by a genetic algorithm and the final learning rate, learning decay and weight decay are respectively set to 0.7 , 0.3 and 0.5 ']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 12, 18), (('artifact', 'model'), 27, 32)], 'We used two layers of LSTM model'], [[(('artifact', 'method'), 29, 35), (('parameter', 'learning rate'), 56, 69), (('v number', '2'), 73, 74), (('v number', '0.0001'), 75, 81), (('v number', '0.99'), 95, 99)], 'For optimzation, the RMSProp method is used with a base learning rate of 2 0.0001 and momentum 0.99 '], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 35, 41), (('parameter', 'epochs'), 100, 106), (('v number', '256'), 31, 34), (('v number', '5'), 98, 99)], 'The model is trained for up to 256 epochs until the validation error has not improved in the last 5 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 62, 67), (('v number', '1'), 86, 87), (('v number', '000'), 88, 91)], 'For both the reader and the decoder networks, we choose a one-layer LSTM network with 1,000 memory cells'], [[(('parameter', 'learning rate'), 43, 56), (('v number', '5.0'), 9, 12), (('v number', '5.0'), 17, 20), (('v number', '0.01'), 60, 64)], 'We set = 5.0 , = 5.0 , and used a constant learning rate of 0.01 for A .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 11, 21), (('v number', '32'), 98, 100)], 'We set the batch size to be the maximum that we could save in the GPU RAM, having a value of just 32 samples'], [[(('artifact', 'model'), 58, 63), (('parameter', 'layers'), 133, 139)], 'We need to consider that we also need to fit the compiled model its weights in the GPU RAM and this is very expensive as some of our layers, and thus its weights, are huge as we will see now.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 23, 28), (('parameter', 'layer'), 69, 74), (('artifact', 'LSTMs'), 75, 80), (('v number', '128'), 59, 62), (('v number', '2'), 67, 68)], 'For the discriminative model, we used hidden dimensions of 128 and 2-layer LSTMs larger numbers of dimensions reduced validation set performance'], [[(('artifact', 'model'), 19, 24), (('parameter', 'layer'), 55, 60), (('artifact', 'LSTMs'), 61, 66), (('v number', '256'), 34, 37), (('v number', '2'), 53, 54)], 'For the generative model, we used 256 dimensions and 2-layer LSTMs'], [[(('parameter', 'dropout'), 30, 37), (('v number', '0.2'), 109, 112), (('v number', '0.3'), 132, 135)], 'For both models, we tuned the dropout rate to maximize validation set likelihood, obtaining optimal rates of 0.2 discriminative and 0.3 generative'], [[(('artifact', 'model'), 50, 55), (('parameter', 'dropout'), 82, 89), (('v number', '0.3'), 98, 101)], 'For the sequential LSTM baseline for the language model, we also found an optimal dropout rate of 0.3'], [[(('parameter', 'learning rate'), 56, 69), (('v number', '0.1'), 73, 76)], 'For training we used stochastic gradient descent with a learning rate of 0.1']] \n",
      "\n",
      "[[[(('artifact', 'model'), 44, 49), (('artifact', 'model'), 91, 96)], 'The 3DDFA data provides projection and 3DMM model parameters for the Basel + FaceWarehouse model for each image of the 300W database']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 30, 36), (('parameter', 'layers'), 77, 83)], 'VGG16 has sixteen convolution layers and AlexNet has only five convolutional layers '], [[(('parameter', 'layer'), 62, 67), (('parameter', 'layers'), 150, 156), (('parameter', 'layers'), 177, 183), (('parameter', 'layer'), 208, 213)], 'Rather than using the convolution features of the last pooled layer as is done in the original faster R-CNN paper, we use features from convolutional layers that are the bottom layers of the previous pooling layer'], [[(('parameter', 'layer'), 146, 151), (('v number', '128'), 73, 76), (('v number', '128'), 90, 93)], 'As the standard procedure introduced in faster R-CNN, we randomly sample 128 positive and 128 negative roi proposals per batch to train the R-CNN layer'], [[(('parameter', 'learning rate'), 40, 53), (('v number', '0.0005'), 57, 63), (('v number', '50000'), 75, 80), (('v number', '0.9'), 94, 97)], 'For all the experiments, we use initial learning rate of 0.0005, step size 50000 and momentum 0.9']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 22, 26), (('parameter', 'learning rate'), 44, 57), (('v number', '1e-6'), 61, 65)], 'For learning, we used Adam optimizer with a learning rate of 1e-6 to avoid that the gradients explode'], [[(('artifact', 'model'), 16, 21), (('parameter', 'epochs'), 29, 35), (('v number', '50'), 26, 28)], 'We trained each model for 50 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 25, 38), (('parameter', 'weight decay'), 54, 66), (('v number', '0.01'), 89, 93), (('v number', '0.9'), 96, 99), (('v number', '5'), 106, 107), (('v number', '0.0001'), 108, 114)], ', with the values of the learning rate, momentum, and weight decay hyperparameters being 0.01 , 0.9 , and 5 0.0001 respectively'], [[(('parameter', 'learning rate'), 64, 77), (('v number', '10'), 104, 106)], 'Whenever the error on the validation set stopped decreasing the learning rate was decreased by a factor 10.']] \n",
      "\n",
      "[[[(('parameter', 'm'), 86, 87), (('v number', '0'), 60, 61), (('v number', '0'), 66, 67), (('v number', '1'), 72, 73), (('v number', '1'), 78, 79), (('v number', '0'), 82, 83), (('v number', '1'), 88, 89)], 'Location of point O varies and can be any pixel in set D_O= 0,y|y=0, ,n-1 x,n-1|x=0, ,m-1 '], [[(('parameter', 'm'), 62, 63), (('v number', '1'), 66, 67)], 'Thus, the number of difference state of parameter is equal to m+n-1 .']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 120, 133), (('v number', '4'), 118, 119)], 'REF has been used to visualize the values of the error function for the RNNSearch + Word2Vec and RNNMorph models with 4 hidden layers'], [[(('artifact', 'model'), 33, 38), (('parameter', 'steps'), 113, 118)], 'It can be seen that the RNNMorph model is consistently better in terms of the perplexity values through the time steps.']] \n",
      "\n",
      "[[[(('parameter', 'm'), 46, 47), (('v number', '0'), 23, 24), (('v number', '3.30'), 76, 80)], 'Let Q,Q ^_+ and assume 0 Q_ Q_ for every i,j [m] , then Q Q , see Corollary 3.30 '], [[(('parameter', 'T'), 78, 79), (('v number', '8.1'), 58, 61), (('v number', '26'), 62, 64)], 'Indeed, we know from the Collatz-Wielandt formula Theorem 8.1.26 in that A= A^T _A^Tv_iv_i for any v ^_ '], [[(('parameter', 'K'), 59, 60), (('v number', '1'), 10, 11), (('v number', '1'), 18, 19), (('v number', '1'), 24, 25), (('v number', '1'), 77, 78), (('v number', '1'), 108, 109), (('v number', '3'), 113, 114), (('v number', '1'), 123, 124), (('v number', '2'), 127, 128), (('v number', '1'), 132, 133)], 'Let v=p_w-1, ,p_w-1,p_u-1 , then A^Tv_i < v_i for every i [K+1] guarantees A<1 and is equivalent to p_w> 4K+1 _1+3 p_u >2K+1 _+2 _2-1,']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 28, 32), (('parameter', 'learning rate'), 84, 97), (('v number', '0.01'), 101, 105), (('v number', '0.001'), 107, 112), (('v number', '0.0001'), 113, 119), (('v number', '1e-05'), 120, 125)], 'For each experiment, we use Adam for optimization, and conduct a grid search on the learning rate in 0.01, 0.001,0.0001,1e-05 '], [[(('parameter', 'batch size'), 50, 60), (('v number', '50'), 71, 73)], 'We employ early stopping during training, and the batch size is set to 50.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 131, 136), (('v number', '0'), 37, 38), (('v number', '1'), 65, 66)], 'In that case the variance approaches 0 and the mean since px x = 1 and would normally dramatically increase the likelihood for the model – this is also known as singularity and one of the known drawbacks of normal EM']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 53, 66), (('v number', '0.00005'), 70, 77)], 'Specifically, we used the RMSProp optimizer with the learning rate of 0.00005 ']] \n",
      "\n",
      "[[[(('parameter', 'T'), 94, 95), (('v number', '200'), 111, 114)], 'Next, the dataset D is divided at each time interval d_t in which case D is defined as: D = _^T d_t where d_t= 200 '], [[(('parameter', 'T'), 23, 24), (('v number', '70'), 84, 86), (('v number', '10'), 97, 99), (('v number', '20'), 117, 119)], \"For each time interval t , we divide the available labeled dataset into a train set 70%, dev set 10%, and a test set 20% using ski-learn toolkit's module , which ensured that the class distribution remains reasonably balanced in each subset.\"]] \n",
      "\n",
      "[[[(('parameter', 'K'), 95, 96), (('parameter', 'K'), 117, 118), (('v number', '1'), 111, 112)], 'For each location q in the image matrix, extract its disk-shaped neighborhood from each of the K images _i , i=1,...,K ']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 64, 68), (('v number', '0.01'), 116, 120)], \"For optimization, we used the code base's implementation of the Adam optimizer , with a weight-decay coefficient of 0.01 for non-bias parameters\"], [[(('parameter', 'learning rate'), 45, 58), (('parameter', 'epochs'), 111, 117), (('v number', '3'), 62, 63), (('v number', '1e-05'), 64, 69)], 'To train our recurrent retriever, we set the learning rate to 3 1e-05 , and the maximum number of the training epochs to three'], [[(('artifact', 'model'), 20, 25), (('parameter', 'learning rate'), 38, 51), (('parameter', 'epochs'), 100, 106), (('v number', '3'), 55, 56), (('v number', '1e-05'), 57, 62)], 'To train our reader model, we set the learning rate to 3 1e-05 , and the maximum number of training epochs to two'], [[(('parameter', 'batch size'), 56, 66), (('parameter', 'batch size'), 126, 136), (('v number', '120'), 140, 143)], 'Empirically we observe better performance with a larger batch size as discussed in previous work , , and thus we set the mini-batch size to 120']] \n",
      "\n",
      "[[[(('artifact', 'model'), 143, 148), (('v number', '571'), 16, 19), (('v number', '3'), 27, 28), (('v number', '207'), 29, 32)], \"The rest of the 571 hrs of 3,207 lecture recordings excluding the same speaker's lectures in the evaluation sets were used for AM and language model LM training\"]] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 110, 123), (('parameter', 'batch size'), 125, 135), (('parameter', 'weight decay'), 137, 149)], 'For all the baselines and Generative Graph Transformer variations, we run the same hyper-parameter search on: learning rate, batch size, weight decay, and coefficient'], [[(('artifact', 'Adam'), 34, 38), (('v number', '0.0001'), 68, 74), (('v number', '5'), 76, 77), (('v number', '0.9'), 93, 96), (('v number', '0.999'), 104, 109), (('v number', '1e-08'), 116, 121)], 'In all the experiments we use the Adam optimizer with parameters [3 0.0001; 5 0.0001] , _1 = 0.9 , _2 = 0.999 and = 1e-08 '], [[(('parameter', 'batch size'), 24, 34), (('v number', '64'), 45, 47), (('v number', '16'), 82, 84)], 'For all the models, the batch size is set to 64, except for the RNN where we find 16 to be better']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 18, 23), (('parameter', 'dropout'), 27, 34), (('parameter', 'layer'), 35, 40), (('v number', '0.5'), 44, 47)], 'After each hidden layer, a dropout layer of 0.5 probability for regularization is added'], [[(('parameter', 'epoch'), 10, 15), (('artifact', 'Adam'), 61, 65)], 'Then each epoch of the training phase is optimized using the Adam optimizer '], [[(('parameter', 'epoch'), 61, 66), (('v number', '128'), 34, 37)], 'We are using a mini-batch of size 128 pairs in each training epoch'], [[(('parameter', 'layer'), 17, 22), (('parameter', 'layer'), 59, 64), (('v number', '512'), 27, 30), (('v number', '128'), 69, 72)], 'The first hidden layer has 512 units and the second hidden layer has 128 units'], [[(('parameter', 'activation'), 34, 44), (('parameter', 'hidden layers'), 67, 80), (('parameter', 'layer'), 106, 111)], 'We use Rectified Linear Unit relu activation functions in both the hidden layers and Sigmoid for the last layer.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 82, 88), (('parameter', 'layers'), 103, 109), (('parameter', 'layers'), 136, 142)], 'The network architecture used in our experiment is composed of four convolutional layers, four pooling layers and three fully connected layers, as shown in Fig'], [[(('parameter', 'weight decay'), 26, 38), (('v number', '0.0001'), 39, 45)], 'L_2 regularization with a weight decay 0.0001 was adopted to prevent overfitting'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 73, 79), (('v number', '0.001'), 29, 34), (('v number', '40'), 70, 72)], 'The learning rate was set as 0.001 and the training was stopped after 40 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 35, 40), (('parameter', 'epochs'), 48, 54), (('parameter', 'learning rate'), 60, 73), (('parameter', 'batch size'), 81, 91), (('v number', '20'), 45, 47), (('v number', '0.001'), 74, 79), (('v number', '512'), 92, 95)], 'For initial training, we train the model for 20 epochs with learning rate 0.001, batch size 512'], [[(('parameter', 'epochs'), 65, 71), (('parameter', 'learning rate'), 78, 91), (('parameter', 'batch size'), 101, 111), (('v number', '10'), 62, 64), (('v number', '0.01'), 92, 96), (('v number', '128'), 112, 115)], 'Each domain data will be trained independently one-by-one for 10 epochs, with learning rate 0.01 and batch size 128'], [[(('artifact', 'model'), 46, 51), (('parameter', 'epoch'), 65, 70)], 'The development data is used to pick the best model in different epoch runs']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 44, 49), (('v number', '64'), 58, 60)], 'The goals are encoded by GRU^ with a hidden layer of size 64'], [[(('artifact', 'method'), 30, 36), (('artifact', 'model'), 65, 70), (('parameter', 'batch size'), 83, 93), (('parameter', 'learning rate'), 136, 149), (('artifact', 'L'), 209, 210), (('v number', '32'), 97, 99), (('v number', '1.0'), 153, 156), (('v number', '0.1'), 173, 176), (('v number', '0.5'), 202, 205), (('v number', '2'), 211, 212)], 'A stochastic gradient descent method is employed to optimize the model with a mini-batch size of 32 for supervised learning, an initial learning rate of 1.0, momentum with =0.1 , and clipping gradients 0.5 in L^2 norm'], [[(('artifact', 'model'), 9, 14), (('artifact', 'model'), 61, 66), (('parameter', 'epochs'), 75, 81), (('v number', '400'), 71, 74)], 'The best model is chosen from the processing of training the model for 400 epochs'], [[(('parameter', 'learning rate'), 16, 29), (('parameter', 'epoch'), 64, 69), (('v number', '2'), 52, 53)], 'After that, the learning rate decays by a factor of 2 for every epoch']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 61, 65), (('parameter', 'batch size'), 179, 189), (('v number', '100'), 92, 95), (('v number', '40'), 140, 142), (('v number', '32'), 143, 145)], 'Training is carried out with a multinomial logistic loss and Adam optimizer over a batch of 100 MFCC samples since our input sample size is 40 32 , we opt to use a relatively big batch size'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '5'), 29, 30), (('v number', '0.001'), 31, 36)], 'The initial learning rate is 5 0.001 '], [[(('parameter', 'learning rate'), 35, 48), (('v number', '30'), 58, 60)], 'With every step of 10K iterations, learning rate drops to 30% of the previous step.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 25, 29), (('parameter', 'learning rate'), 54, 67)], 'For training, we use the Adam optimizer with decaying learning rate, as implemented in TensorFlow '], [[(('parameter', 'epoch'), 39, 44), (('v number', '0.99'), 60, 64)], 'Then, we decay the learning after each epoch by a factor of 0.99'], [[(('parameter', 'dropout'), 14, 21), (('parameter', 'weight decay'), 56, 68), (('v number', '0.1'), 44, 47)], 'We also apply dropout with a probability of 0.1, and L2 weight decay on all trainable variables with = '], [[(('artifact', 'model'), 14, 19), (('parameter', 'steps'), 28, 33), (('parameter', 'batch size'), 41, 51), (('v number', '64'), 55, 57)], 'We train each model for 42K steps with a batch size of 64']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 71, 77), (('parameter', 'batch size'), 128, 138), (('artifact', 'Adam'), 149, 153), (('parameter', 'learning rate'), 161, 174), (('v number', '350'), 67, 70), (('v number', '8'), 142, 143), (('v number', '1e-4'), 178, 182)], 'In both the cross-entropy and end-to-end experiments, we train for 350 epochs on a single GPU with image resolution of 256x512, batch size of 8, and Adam with a learning rate of 1e-4']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 30, 36), (('v number', '6'), 86, 87)], 'More precisely, the number of layers in the base encoder, base decoder and CADed is N=6 '], [[(('parameter', 'layers'), 35, 41), (('v number', '8'), 14, 15)], 'We employ h = 8 parallel attention layers, or heads'], [[(('parameter', 'layer'), 67, 72), (('v number', '512'), 47, 50), (('v number', '2048'), 122, 126)], 'The dimensionality of input and output is d_ = 512 , and the inner-layer of a feed-forward networks has dimensionality d_=2048 .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 90, 100), (('v number', '20'), 81, 83), (('v number', '32'), 104, 106)], 'For all experiments, we use a BPTT Backpropagation Through Time unroll of length 20 and a batch size of 32'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 85, 90), (('v number', '4e-4'), 36, 40), (('v number', '0'), 60, 61), (('v number', '1.5e9'), 67, 72)], 'The learning rate is initialized to 4e-4 and is annealed to 0 over 1.5e9 environment steps with polynomial annealing'], [[(('artifact', 'Adam'), 10, 14), (('v number', '0.9'), 43, 46), (('v number', '0.999'), 51, 56), (('v number', '1e-4'), 60, 64)], 'The other Adam optimizer parameters are _1=0.9, _2=0.999, = 1e-4'], [[(('parameter', 'layers'), 131, 137), (('v number', '2'), 17, 18), (('v number', '1e-3'), 46, 50), (('v number', '2'), 73, 74), (('v number', '1e-5'), 112, 116)], 'We also apply a ^2 norm cost with a weight of 1e-3 on the logits, and a ^2 regularization cost with a weight of 1e-5 to the linear layers that compute the baseline value and logits.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 27, 28), (('v number', '1'), 46, 47)], 'In -hot encoding, the size k is searched in [ 1, 2, 4, , 16]; the parameter is searched in [0.1, 0.2, , 1].']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 87, 93), (('parameter', 'learning rate'), 101, 114), (('parameter', 'batch size'), 129, 139), (('v number', '50'), 84, 86), (('v number', '2e-4'), 118, 122), (('v number', '2'), 143, 144)], 'For both the synthetic and real datasets, we train the Shadow Transfer networks for 50 epochs with a learning rate of 2e-4 and a batch size of 2'], [[(('parameter', 'epochs'), 24, 30), (('parameter', 'learning rate'), 36, 49), (('parameter', 'batch size'), 59, 69), (('v number', '20'), 21, 23), (('v number', '1e-5'), 53, 57), (('v number', '2'), 73, 74)], 'They are trained for 20 epochs at a learning rate of 1e-5, batch size of 2']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 19, 24), (('v number', '3'), 17, 18), (('v number', '512'), 57, 60), (('v number', '2048'), 77, 81), (('v number', '8'), 87, 88)], 'The encoder is a 3-layer Transformer with hidden size of 512, filter size of 2048, and 8 attention heads'], [[(('parameter', 'steps'), 17, 22), (('parameter', 'batch size'), 51, 61), (('parameter', 'K'), 62, 63), (('parameter', 'learning rate'), 72, 85), (('v number', '100'), 64, 67), (('v number', '0.003'), 86, 91)], 'We train for 40M steps using an SGD optimizer with batch size K=100 and learning rate 0.003 ']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 71, 78), (('parameter', 'layers'), 95, 101), (('parameter', 'epochs'), 197, 203), (('v number', '12'), 92, 94), (('v number', '50'), 178, 180), (('v number', '5'), 195, 196)], 'paragraph40ex plus.2ex minus.2ex-1emBERT We use the pretrained uncased version of BERT-Base 12 layers with mostly default parameters, except that we use a max sequence length of 50 and train for 5 epochs'], [[(('parameter', 'layer'), 54, 59), (('parameter', 'layer'), 107, 112), (('parameter', 'activation'), 128, 138), (('v number', '100'), 70, 73)], 'paragraph40ex plus.2ex minus.2ex-1emLSTM We use a two-layer LSTM with 100 units each, followed by a linear layer with a softmax activation'], [[(('artifact', 'model'), 75, 80), (('parameter', 'epochs'), 118, 124), (('v number', '20'), 115, 117)], 'For other parameters, we try to use values comparable to those of the BERT model, except that we need to train for 20 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 59, 64), (('parameter', 'layers'), 83, 89), (('v number', '121'), 79, 82)], 'As convolutional neural network, we use a densly-connected model DenseNet with 121 layers '], [[(('artifact', 'Adam'), 27, 31), (('parameter', 'learning rate'), 69, 82), (('v number', '0.9'), 47, 50), (('v number', '0.999'), 57, 62), (('v number', '0.0001'), 86, 92)], 'During training we use the Adam optimizer _1 = 0.9, _2 = 0.999 and a learning rate of 0.0001 '], [[(('parameter', 'epoch'), 47, 52), (('parameter', 'epochs'), 119, 125), (('v number', '8'), 117, 118)], 'We stop the training and jump back to the best epoch if the validation accuracy does not improve after a patience of 8 epochs']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 37, 42), (('v number', '1'), 27, 28), (('v number', '000'), 29, 32), (('v number', '000'), 33, 36)], 'The models are trained for 1,000,000 steps'], [[(('parameter', 'learning rate'), 9, 22), (('parameter', 'learning rate'), 32, 45), (('parameter', 'steps'), 75, 80), (('artifact', 'linear decay'), 95, 107), (('parameter', 'learning rate'), 115, 128), (('v number', '1e-4'), 26, 30), (('v number', '10'), 68, 70), (('v number', '000'), 71, 74)], 'We use a learning rate of 1e-4, learning rate warmup over the first 10,000 steps followed by a linear decay of the learning rate.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 42, 48), (('parameter', 'learning rate'), 70, 83), (('parameter', 'epoch'), 121, 126), (('v number', '110'), 38, 41), (('v number', '10'), 87, 89), (('v number', '35'), 97, 99), (('v number', '70'), 104, 106), (('v number', '95'), 115, 117)], 'All models are trained for a total of 110 epochs, and we decrease the learning rate by 10 at the 35-th, 70-th, and 95-th epoch.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 31, 44), (('v number', '0.0001'), 48, 54)], 'We use the Adam optimizer with learning rate of 0.0001 '], [[(('parameter', 'batch size'), 9, 19), (('v number', '64'), 30, 32)], 'The mini batch size is set to 64']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 89, 93), (('parameter', 'learning rate'), 111, 124), (('v number', '0.0001'), 128, 134)], 'On all training instances, we used a uniform distribution for weights initialization and Adam optimizer with a learning rate of 0.0001 for optimization'], [[(('parameter', 'batch size'), 50, 60), (('artifact', 'model'), 125, 130), (('v number', '115'), 64, 67)], 'Using single NVIDIA Titan X we were able to use a batch size of 115 and took approximately two days to train each individual model']] \n",
      "\n",
      "[[[(('artifact', 'L'), 22, 23), (('artifact', 'L'), 29, 30), (('parameter', 'm'), 31, 32)], 'minimizing the sum of L^ and L^m for the RNN-based methods, the effects of this were mixed: On the spirals dataset, performance dropped very sharply with the multimodal paradigm, whereas performance increased on the Weizmann video dataset'], [[(('artifact', 'L'), 42, 43), (('artifact', 'L'), 81, 82), (('parameter', 'm'), 83, 84), (('parameter', 'm'), 124, 125)], 'For the spirals dataset, we only minimize L^ , with all inputs provided, but not L^m , which is computed with only modality m provided as input'], [[(('parameter', 'learning rate'), 26, 39), (('v number', '0.02'), 40, 44), (('v number', '0.01'), 136, 140)], 'Finally, we used a higher learning rate 0.02 for BFVI on the spirals dataset because we noticed slow convergence with the lower rate of 0.01'], [[(('parameter', 'learning rate'), 15, 28), (('v number', '0.02'), 32, 36)], 'Increasing the learning rate to 0.02 for the other methods hurt their performance.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 21, 27), (('artifact', 'Adam'), 60, 64), (('parameter', 'weight decay'), 80, 92), (('v number', '200'), 17, 20), (('v number', '2e-4'), 96, 100), (('v number', '0.9'), 117, 120)], 'The networks run 200 epochs on single Tesla V100 GPU, using Adam optimizer with weight decay of 2e-4 and momentum of 0.9'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'learning rate'), 54, 67), (('v number', '5e-4'), 29, 33), (('v number', '0.9'), 89, 92)], 'The initial learning rate is 5e-4 and we adopt \"ploy\" learning rate policy with power of 0.9'], [[(('parameter', 'batch size'), 7, 17), (('v number', '16'), 21, 23)], 'We set batch size to 16 to fit our GPU memory']] \n",
      "\n",
      "[[[(('parameter', 'm'), 7, 8), (('artifact', 'Triplet Loss'), 13, 25), (('v number', '1'), 36, 37), (('v number', '128'), 82, 85)], 'Margin m for Triplet Loss is set to 1, and scale s for L2 normalization is set to 128'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'layer'), 24, 29), (('v number', '0.001'), 101, 106), (('v number', '0.0001'), 107, 113), (('v number', '1e-05'), 114, 119), (('v number', '1e-06'), 120, 125)], 'The learning rate of FC layer for “step< 20k”, “20k< step< 40k”, “40k< step< 60k” and “step> 60k” is 0.001,0.0001,1e-05,1e-06 respectively'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '1e-06'), 56, 61)], 'The learning rate of face embedding network is fixed to 1e-06 .']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 51, 64)], 'Adam is adopted to train the agent networks with a learning rate of 3e^ '], [[(('parameter', 'batch size'), 12, 22), (('v number', '96'), 25, 27), (('v number', '0.99'), 47, 51), (('v number', '5'), 114, 115), (('v number', '10'), 116, 118)], 'We use mini-batch size N=96 , discount factor =0.99 , soft update rate =1e^ , and size of replay buffer =10e^, _H=5 10^ ']] \n",
      "\n",
      "[[[(('parameter', 'T'), 26, 27), (('v number', '1000'), 28, 32)], 'We used temperature scale T=1000 for the ODIN approach since it was deemed as the optimal value based in the original paper '], [[(('parameter', 'layer'), 85, 90), (('parameter', 'layer'), 117, 122), (('v number', '16'), 18, 20), (('v number', '10'), 38, 40), (('v number', '100'), 51, 54)], 'The OODLs for VGG-16 trained on CIFAR-10 and CIFAR-100 were the second convolutional layer and the first max-polling layer, respectively'], [[(('parameter', 'layers'), 93, 99), (('v number', '10'), 37, 39), (('v number', '100'), 50, 53)], 'The OODL for ResNet trained on CIFAR-10 and CIFAR-100 were the thirteenth and ninth residual layers, respectively']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 0, 10), (('artifact', 'AdamW'), 18, 23), (('parameter', 'weight decay'), 89, 101), (('parameter', 'epochs'), 207, 213), (('parameter', 'dropout'), 316, 323), (('parameter', 'layer'), 371, 376), (('parameter', 'dropout'), 422, 429), (('parameter', 'dropout'), 458, 465), (('v number', '256'), 14, 17), (('v number', '0.01'), 61, 65), (('v number', '0.9'), 74, 77), (('v number', '0.999'), 83, 88), (('v number', '0.01'), 105, 109), (('v number', '10'), 204, 206), (('v number', '0.1'), 229, 232), (('v number', '5000'), 303, 307), (('v number', '0.5'), 344, 347), (('v number', '25'), 435, 437), (('v number', '0.5'), 481, 484)], 'batch size of 256 AdamW optimizer with initial learn rate of 0.01 and _ = 0.9, _ = 0.999 weight decay of 0.01 negative-log likelihood cross entropy loss reduce-on-plateau learn rate scheduler patience of 10 epochs with factor of 0.1 additional validation set is randomly extracted from the training set 5000 samples dropout with probability of 0.5 before the last linear layer was used in all models during training in MC dropout, N = 25 forward passes with dropout probability of 0.5 were performed']] \n",
      "\n",
      "[[[(('artifact', 'model'), 31, 36), (('v number', '512'), 113, 116)], 'The detailed parameters of the model are as follows: Both of the source embedding and the target embeddings have 512 dimensions and use the same BPE vocabulary'], [[(('parameter', 'layers'), 50, 56), (('v number', '6'), 37, 38), (('v number', '8'), 61, 62)], 'Both of the encoder and decoder have 6 multi-head layers and 8 attention heads'], [[(('parameter', 'layer'), 18, 23), (('parameter', 'layer'), 43, 48), (('v number', '2048'), 52, 56)], 'The size of inner layer at each multi-head layer is 2048'], [[(('artifact', 'Adam'), 7, 11), (('artifact', 'model'), 43, 48), (('parameter', 'learning rate'), 109, 122), (('parameter', 'steps'), 171, 176)], 'We use Adam optimizer to train transformer model with the inverse squared root schedule which will decay the learning rate based on the inverse square root of the warm-up steps'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 78, 83), (('v number', '5'), 34, 35), (('v number', '0.0001'), 36, 42), (('v number', '4'), 72, 73), (('v number', '000'), 74, 77)], 'The learning rate initialize with 5 0.0001 and warm-up during the first 4,000 steps'], [[(('parameter', 'batch size'), 51, 61), (('artifact', 'model'), 97, 102), (('parameter', 'steps'), 130, 135), (('v number', '32'), 65, 67), (('v number', '000'), 68, 71), (('v number', '30'), 123, 125), (('v number', '000'), 126, 129)], 'In order to train transformer adequately, we use a batch size of 32,000 tokens and fine-tune the model on labeled data for 30,000 steps'], [[(('parameter', 'dropout'), 0, 7), (('v number', '0.3'), 33, 36)], 'Dropout is applied at a ratio of 0.3']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rates'), 50, 64), (('v number', '1e-3'), 73, 77), (('v number', '1e-4'), 82, 86)], 'We use the Adam optimizer with default params and learning rates between 1e-3 and 1e-4']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 96, 100), (('v number', '0.9'), 135, 138)], 'We did not change any of the building blocks of draw and train the network as proposed with the Adam optimizer, the momentum set to _1=0.9 '], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.0005'), 28, 34)], 'The learning rate is set to 0.0005'], [[(('parameter', 'epoch'), 43, 48), (('v number', '300'), 39, 42)], 'Training converges after approximately 300 epoch'], [[(('parameter', 'epoch'), 3, 8), (('parameter', 'batch size'), 87, 97)], 'An epoch corresponds to gradient updates, S_T denotes dataset size and S_B is the mini-batch size'], [[(('parameter', 'steps'), 55, 60), (('v number', '64'), 72, 74)], 'The number of glimpses or equivalently defined as time steps, is set to 64']] \n",
      "\n",
      "[[[(('artifact', 'L'), 112, 113), (('v number', '2'), 110, 111), (('v number', '1'), 114, 115)], 'Uniformly distributed noise -, is added to the encoder output before it is passed on to the decoder, with a = 2 L-1 .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 59, 69), (('v number', '18'), 73, 75)], 'Since we cache the feature maps in memory, we can only set batch size as 18'], [[(('parameter', 'learning rate'), 66, 79), (('v number', '0.9'), 52, 55), (('v number', '0.1'), 80, 83)], 'The optimizer we use is SGD optimizer with momentum 0.9 and fixed learning rate 0.1']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 47, 52), (('parameter', 'm'), 66, 67), (('parameter', 'layer'), 89, 94), (('parameter', 'm'), 106, 107), (('v number', '200'), 68, 71)], 'The input and output sequence encoders are two-layer biLSTMs with m=200 hidden units per layer, producing m dimensional embeddings'], [[(('parameter', 'layer'), 28, 33), (('parameter', 'm'), 49, 50), (('v number', '200'), 51, 54)], 'The output decoder is a two-layer LSTM also with m=200 '], [[(('parameter', 'dropout'), 0, 7), (('v number', '0.5'), 36, 39)], 'Dropout is applied with probability 0.5 to each LSTM and symbol embedding']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 46, 59), (('parameter', 'epoch'), 95, 100), (('parameter', 'epoch'), 115, 120), (('v number', '0.15'), 63, 67), (('v number', '0.97'), 80, 84)], 'We used the Adagrad optimizer with an initial learning rate of 0.15, decayed by 0.97 for every epoch after the 4th epoch']] \n",
      "\n",
      "[[[(('artifact', 'LSTMs'), 33, 38), (('parameter', 'dropout'), 73, 80), (('v number', '0.5'), 84, 87)], 'For the initialization of the Bi-LSTMs, we used Xavier , and a recurrent dropout of 0.5'], [[(('artifact', 'Adam'), 52, 56), (('v number', '0.75'), 34, 38)], 'The threshold is initialized to = 0.75 , and we use Adam as our optimizer'], [[(('artifact', 'model'), 15, 20), (('parameter', 'epochs'), 28, 34), (('parameter', 'epochs'), 68, 74), (('v number', '50'), 25, 27), (('v number', '10'), 65, 67)], 'We trained the model for 50 epochs, with an early termination of 10 epochs']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 34, 44), (('parameter', 'epochs'), 95, 101), (('v number', '32'), 48, 50), (('v number', '128'), 85, 88), (('v number', '3'), 93, 94)], 'All models were fine-tuned with a batch size of 32, and a maximum sequence length of 128 for 3 epochs'], [[(('parameter', 'learning rate'), 10, 23), (('parameter', 'learning rate'), 34, 47), (('parameter', 'steps'), 79, 84), (('artifact', 'linear decay'), 90, 102), (('v number', '3'), 27, 28), (('v number', '10'), 72, 74)], 'We used a learning rate of 3 with learning rate warmup during the first 10% of steps, and linear decay afterwards'], [[(('parameter', 'dropout'), 20, 27), (('parameter', 'layer'), 40, 45), (('v number', '10'), 16, 18)], 'We also applied 10% dropout on the last layer']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 22, 35), (('v number', '0001'), 43, 47)], 'Adam optimizer with a learning rate of 5x0.0001 is used'], [[(('parameter', 'batch size'), 36, 46), (('parameter', 'epochs'), 79, 85), (('v number', '64'), 50, 52), (('v number', '600'), 75, 78)], 'Training is done on batches, with a batch size of 64, and is continued for 600 epochs'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epoch'), 51, 56)], 'The model used for testing is the one saved at the epoch with the highest validation accuracy.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 42, 47), (('artifact', 'LSTMs'), 48, 53), (('v number', '2'), 40, 41), (('v number', '300'), 59, 62)], 'All RNNs used as encoder or decoder are 2-layer LSTMs with 300-dimensional hidden states'], [[(('parameter', 'layer'), 84, 89), (('parameter', 'activation'), 124, 134), (('v number', '4'), 82, 83)], 'Action predictor and price decoder networks have the same network architecture, a 4-layer fully-connected network with ReLU activation functions'], [[(('parameter', 'dropout'), 18, 25), (('v number', '0.3'), 41, 44)], 'We also applied a dropout with a rate of 0.3 to all parts of our architecture.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 40, 45), (('v number', '100'), 49, 52)], 'The autoencoders are constituted of one layer of 100 LSTM neurons for the encoder and the decoder'], [[(('parameter', 'Version'), 15, 22), (('parameter', 'epochs'), 187, 193), (('v number', '50'), 64, 66), (('v number', '50'), 141, 143), (('v number', '30'), 184, 186)], 'For the KISSME version, the encodings are then projected into a 50-dimensional space, and the distance matrix, which thus has also dimension 50, was updated with the closed-form every 30 epochs'], [[(('parameter', 'learning rate'), 2, 15), (('parameter', 'epochs'), 99, 105), (('v number', '0.001'), 19, 24), (('v number', '10'), 49, 51), (('v number', '10'), 96, 98)], 'A learning rate of 0.001 was used and divided by 10 if the loss did not decrease anymore during 10 epochs'], [[(('parameter', 'batch size'), 2, 12), (('v number', '50'), 16, 18), (('v number', '1'), 32, 33), (('v number', '0.5'), 66, 69)], 'A batch size of 50, a margin of 1 for the contrastive loss and of 0.5 for the cosine loss were chosen']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 38, 43), (('parameter', 'K'), 55, 56), (('parameter', 'K'), 57, 58), (('parameter', 'K'), 158, 159)], 'For example, consider a convolutional layer with n_ n_ k k parameters, where n_ denotes the number of output channels and n_ the number of input channels and k the filter size'], [[(('parameter', 'K'), 70, 71), (('parameter', 'K'), 72, 73)], 'We treat it as n_ neurons, and each neuron has a parameter of size n_ k k '], [[(('parameter', 'layer'), 93, 98), (('v number', '32'), 112, 114), (('v number', '16'), 206, 208)], 'For MobileNet, we construct the initial network by keeping the size of the first convolution layer as the same =32 as the original MobileNet and setting the number of depthwise and pointwise channels to be 16'], [[(('parameter', 'layers'), 81, 87), (('v number', '16'), 70, 72)], 'For VGG19, we set the number of channels of the initial network to be 16 for all layers.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 97, 110), (('artifact', 'Adam'), 131, 135), (('v number', '200'), 42, 45), (('v number', '000'), 46, 49), (('v number', '64'), 87, 89), (('v number', '0.0001'), 114, 120)], 'We train on ImageNet32 and ImageNet64 for 200,000 iterations with mini batches of size 64, and a learning rate of 0.0001 using the Adam optimizer '], [[(('artifact', 'L'), 36, 37), (('parameter', 'K'), 51, 52), (('v number', '2'), 38, 39), (('v number', '8'), 53, 54)], 'The flow architecture is build with L=2 levels and K=8 .']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 36, 40), (('parameter', 'learning rate'), 58, 71), (('v number', '0.0002'), 75, 81), (('v number', '0.5'), 87, 90), (('v number', '0.9'), 96, 99)], 'Optimization was performed with the adam optimizer with a learning rate of 0.0002 , _1=0.5 , _2=0.9 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 51, 61), (('v number', '256'), 29, 32), (('v number', '256'), 33, 36)], 'All models are trained using 256 256 images with a batch size of eight'], [[(('artifact', 'Adam'), 4, 8), (('v number', '0'), 29, 30), (('v number', '0.9'), 40, 43)], 'The Adam optimiser with _1 = 0 and _2 = 0.9 is used'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0001'), 49, 55)], 'The initial learning rate of generator is set to 0.0001 '], [[(('parameter', 'learning rate'), 14, 27), (('v number', '10'), 42, 44)], 'In total, the learning rate is divided by 10 twice'], [[(('parameter', 'steps'), 57, 62), (('parameter', 'steps'), 132, 137), (('v number', '200'), 49, 52), (('v number', '000'), 53, 56), (('v number', '85'), 125, 127), (('v number', '000'), 128, 131)], 'In large datasets the convergence appears within 200,000 steps, while in small datasets the final convergence appears within 85,000 steps.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 80, 81), (('parameter', 'K'), 84, 85), (('parameter', 'K'), 130, 131), (('v number', '1'), 78, 79)], 'If using gradient descent as the update rule, latent representations s_k with 1 k < K are updated by s_k^ = s_k^ + _s _ _^ }} + _ k}}^ }}}} }} |_}']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 49, 54), (('parameter', 'activation'), 75, 85)], 'We perform batch normalization after each output layer, followed with ReLU activation function'], [[(('parameter', 'layer'), 44, 49), (('parameter', 'activation'), 88, 98)], 'In the relation kernel module, we use a two-layer MLP with batch normalization and ReLU activation for adjacency matrix building, whose input and output dimensions are consistent with the output of the aggregate network and adjacency matrix size, respectively'], [[(('parameter', 'layers'), 48, 54), (('v number', '2'), 26, 27), (('v number', '512'), 93, 96), (('v number', '128'), 101, 104)], 'GCN module is composed of 2 graph convolutional layers with output channel dimensionality of 512 and 128, respectively'], [[(('artifact', 'model'), 14, 19), (('artifact', 'Adam'), 46, 50), (('parameter', 'learning rate'), 66, 79), (('parameter', 'weight decay'), 90, 102), (('v number', '0.001'), 80, 85), (('v number', '0.0005'), 103, 109)], 'Our whole TGG model is trained end-to-end via ADAM optimizer with learning rate 0.001 and weight decay 0.0005'], [[(('parameter', 'batch size'), 4, 14), (('v number', '128'), 28, 31)], 'The batch size is set to be 128 for all datasets and we use validation sets for early stopping']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 49, 54), (('v number', '5'), 33, 34), (('v number', '10'), 35, 37), (('v number', '5'), 38, 39), (('v number', '1'), 42, 43), (('v number', '10'), 44, 46), (('v number', '6'), 47, 48)], 'The whole network is trained for 5 10^5 , 1 10^6 steps on indoor and outdoor images respectively'], [[(('artifact', 'Adam'), 7, 11), (('v number', '1'), 29, 30), (('v number', '2'), 35, 36), (('v number', '0.9'), 64, 67), (('v number', '0.999'), 72, 77)], 'We use Adam optimizer, where 1 and 2 take the default values of 0.9 and 0.999, respectively.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 52, 58), (('parameter', 'layers'), 83, 89), (('parameter', 'layer'), 182, 187), (('v number', '2'), 38, 39), (('v number', '3'), 69, 70), (('v number', '40'), 124, 126), (('v number', '24'), 159, 161), (('v number', '4'), 192, 193), (('v number', '512'), 247, 250)], 'To learn our descriptor, we use one S^2 convolution layers and three 3 convolution layers with constant number of channels, 40, while the bandwidths is set to 24 for the first three layer and 4 for the last one, which results in a descriptor with 512 entries'], [[(('parameter', 'layers'), 61, 67), (('parameter', 'layers'), 114, 120), (('parameter', 'layer'), 150, 155), (('v number', '4'), 43, 44)], 'The architecture of our decoder is made of 4 fully-connected layers, with ReLU non-linearities on the first three layers and tanh on the final output layer'], [[(('artifact', 'Adam'), 61, 65), (('v number', '32'), 49, 51)], 'The network is trained with mini-bacthes of size 32 by using ADAM '], [[(('parameter', 'learning rate'), 13, 26), (('v number', '0.001'), 37, 42), (('v number', '4000'), 64, 68)], 'The starting learning rate is set to 0.001 and is decayed every 4000 iterations'], [[(('parameter', 'epochs'), 28, 34), (('v number', '14'), 25, 27)], 'We train the network for 14 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 12, 16), (('v number', '0.9'), 58, 61), (('v number', '0.999'), 93, 98)], 'We used the Adam optimizer with the first momentum set to 0.9 and the second momentum set to 0.999'], [[(('parameter', 'weight decay'), 0, 12), (('v number', '5'), 24, 25), (('v number', '0.0001'), 26, 32), (('v number', '1e-08'), 61, 66)], 'Weight decay was set to 5*0.0001 for the convolutions and to 1e-08 for the decision trees'], [[(('parameter', 'batch size'), 4, 14), (('v number', '400'), 26, 29)], 'The batch size was set to 400 and each batch was always balanced in terms of available classes'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 76, 82), (('v number', '0.01'), 37, 41), (('v number', '0.1'), 57, 60), (('v number', '100'), 72, 75), (('v number', '0.0001'), 100, 106)], 'The initial learning rate was set to 0.01 and reduced by 0.1 after each 100 epochs until it reached 0.0001 '], [[(('parameter', 'learning rate'), 8, 21), (('parameter', 'epochs'), 78, 84), (('v number', '0.0001'), 25, 31), (('v number', '1000'), 73, 77)], 'For the learning rate of 0.0001 we continued the training for additional 1000 epochs and selected the best result']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 54, 60), (('v number', '1'), 70, 71), (('v number', '64'), 95, 97), (('v number', '16'), 127, 129)], 'The number of shared and specific graph convolutional layers are both 1, shared hidden size is 64, and specific hidden size is 16'], [[(('parameter', 'dropout'), 72, 79), (('parameter', 'layers'), 110, 116), (('parameter', 'layers'), 172, 178), (('parameter', 'batch size'), 184, 194), (('artifact', 'model'), 219, 224), (('parameter', 'epochs'), 245, 251), (('artifact', 'Adam'), 258, 262), (('v number', '2'), 34, 35), (('v number', '16'), 64, 66), (('v number', '0.3'), 120, 123), (('v number', '0.1'), 136, 139), (('v number', '256'), 198, 201), (('v number', '10'), 242, 244)], 'The number of negative samples is 2; the embedding dimension is 16; the dropout of shared graph convolutional layers is 0.3 and that is 0.1 of specific graph convolutional layers; the batch size is 256 and we train the model for a maximum of 10 epochs using Adam.']] \n",
      "\n",
      "[[[(('parameter', 'm'), 34, 35), (('parameter', 'm'), 68, 69), (('v number', '1'), 30, 31), (('v number', '0'), 63, 64), (('v number', '1'), 65, 66), (('v number', '1'), 79, 80), (('v number', '0'), 107, 108)], 'The latent parameter z_i at i 1, ,m has the fixed value e_i in 0,1 ^m that has 1 at the i -th position and 0 everywhere else']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 83, 87), (('parameter', 'learning rate'), 114, 127), (('parameter', 'batch size'), 145, 155), (('v number', '8'), 172, 173)], 'In an end-to-end training manner, we updated the weights of all networks using the Adam optimizer with an initial learning rate of 1e^ , and the batch size of training was 8'], [[(('parameter', 'epochs'), 36, 42), (('parameter', 'learning rate'), 54, 67), (('parameter', 'epochs'), 94, 100), (('v number', '100'), 32, 35), (('v number', '60'), 91, 93)], 'All networks were trained up to 100 epochs, where the learning rate was fixed in the first 60 epochs and then linearly reduced to 1e^ '], [[(('parameter', 'epoch'), 125, 130), (('v number', '0'), 70, 71), (('v number', '100'), 115, 118), (('v number', '60'), 122, 124)], 'In the early phase, the synthetic images were blurry, so _ was set to 0 at the beginning and linearly increased to 100 at 60 epoch'], [[(('artifact', 'model'), 24, 29), (('parameter', 'epoch'), 51, 56)], 'In the end, we used the model trained at the 100th epoch to perform on testing data.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 131, 136), (('v number', '18'), 145, 147), (('v number', '34'), 149, 151), (('v number', '101'), 153, 156), (('v number', '152'), 158, 161)], 'Models: R2+1D-d Source code: https:github.comdutranR2Plus1D is the fundamental architecture used for pre-training, where d denotes model depth = 18, 34, 101, 152 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('parameter', 'epoch'), 64, 69), (('v number', '100'), 25, 28), (('v number', '10'), 44, 46), (('v number', '15'), 49, 51)], 'The batch size is set to 100, and there are 10 - 15 batches per epoch for each task'], [[(('parameter', 'learning rate'), 43, 56), (('parameter', 'epochs'), 95, 101), (('v number', '5e-4'), 66, 70), (('v number', '300'), 91, 94)], 'To train the classifier, we use an initial learning rate equal to 5e-4 and we train it for 300 epochs by minimizing the cross-entropy loss for each task'], [[(('parameter', 'learning rate'), 61, 74), (('parameter', 'epochs'), 105, 111), (('v number', '1e-3'), 78, 82), (('v number', '1700'), 100, 104)], 'Moreover, in order to train the generator, we use an initial learning rate of 1e-3 and train it for 1700 epochs for each task']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 62, 67), (('parameter', 'epoch'), 72, 77), (('parameter', 'steps'), 78, 83), (('v number', '8'), 20, 21), (('v number', '1500'), 86, 90)], 'We set batch_size = 8 , from which we determine the number of steps per epoch steps = 1500 '], [[(('parameter', 'epochs'), 40, 46), (('v number', '55'), 37, 39)], 'For RA the process was stopped after 55 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('v number', '0.5'), 54, 57), (('v number', '0.999'), 62, 67)], 'We use the Adam optimizer with momentum parameters _1=0.5, _2=0.999 '], [[(('parameter', 'batch size'), 56, 66), (('v number', '8'), 77, 78)], 'We update one step for either G or D alternatively, and batch size is set to 8'], [[(('parameter', 'epochs'), 56, 62), (('parameter', 'learning rate'), 79, 92), (('parameter', 'epochs'), 193, 199), (('parameter', 'learning rates'), 213, 227), (('v number', '100'), 52, 55), (('v number', '0.0001'), 98, 104), (('v number', '0.0004'), 132, 138), (('v number', '50'), 190, 192), (('v number', '0.00001'), 233, 240), (('v number', '0.00004'), 250, 257)], 'Either the first or the second training stage lasts 100 epochs with an initial learning rate lr_G=0.0001 for the generator and lr_D=0.0004 for the discriminator, while the third stage lasts 50 epochs with initial learning rates lr_G=0.00001 and lr_D=0.00004 ']] \n",
      "\n",
      "[[[(('artifact', 'system'), 28, 34), (('v number', '2011'), 96, 100), (('v number', '16'), 131, 133)], 'We train the end-to-end TTS system with a high-quality American English speech database used in 2011 Blizzard Challenge, which has 16 hours of speech recorded by a single female speaker'], [[(('parameter', 'batch size'), 52, 62), (('artifact', 'Adam'), 140, 144), (('parameter', 'learning rate'), 184, 197), (('v number', '200'), 26, 29), (('v number', '000'), 30, 33), (('v number', '128'), 66, 69), (('v number', '4'), 89, 90), (('v number', '0.9'), 163, 166), (('v number', '0.999'), 172, 177), (('v number', '0.001'), 201, 206), (('v number', '1e-05'), 232, 237), (('v number', '50'), 244, 246), (('v number', '000'), 247, 250)], 'We train these models for 200,000 iterations with a batch size of 128 distributed across 4 GPUs with synchronous updates, using L1 loss and Adam optimizer with _1=0.9 , _2=0.999 and a learning rate of 0.001 exponentially decayed to 1e-05 after 50,000 iterations']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '80'), 23, 25), (('v number', '12.5'), 47, 51), (('v number', '50'), 68, 70)], 'The model output is an 80-channel Mel spectrum 12.5 ms frame shift, 50 ms frame length, one frame at a time'], [[(('artifact', 'model'), 4, 9), (('v number', '1536'), 79, 83), (('v number', '512'), 95, 98), (('v number', '1'), 120, 121)], 'The model structure of the discriminator has been shown in Fig.REF , which has 1536-dim input, 512-dim hidden size, and 1-dim output.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 26, 32), (('parameter', 'batch size'), 38, 48), (('v number', '50'), 23, 25), (('v number', '32'), 49, 51)], 'We train our model for 50 epochs with batch size 32'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 35, 48), (('v number', '0.01'), 49, 53), (('v number', '0.9'), 58, 61), (('v number', '0.999'), 69, 74)], 'We use Adam optimizer with initial learning rate 0.01, _1 0.9 and _2 0.999'], [[(('parameter', 'activation'), 38, 48), (('parameter', 'layers'), 66, 72), (('parameter', 'layer'), 89, 94)], 'Batch normalization is applied before activation functions in all layers except the last layer'], [[(('parameter', 'activation'), 59, 69), (('v number', '0.2'), 44, 47)], 'Leaky ReLU with a fixed leakiness parameter 0.2 is used as activation functions'], [[(('parameter', 'dropout'), 0, 7), (('parameter', 'layers'), 69, 75), (('v number', '0.7'), 24, 27)], 'Dropout with keep ratio 0.7 is applied on last three fully-connected layers.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 92, 97), (('artifact', 'model'), 162, 167)], 'Even though on the small datasets we work with we do not necessarily expect the Transformer model to perform better than recurrent neural networks, we chose this model for its significantly faster training, without sacrificing the performance'], [[(('parameter', 'layer'), 30, 35), (('v number', '2'), 28, 29), (('v number', '8'), 53, 54)], 'For our experiments a small 2-layer Transformer with 8 heads proved to be sufficient'], [[(('artifact', 'model'), 4, 9), (('parameter', 'dropout'), 30, 37), (('v number', '0.2'), 48, 51)], 'The model performed best with dropout values of 0.2'], [[(('artifact', 'Adam'), 51, 55), (('parameter', 'learning rate'), 80, 93)], 'For training of the Transformer models we used the Adam optimizer with a custom learning rate schedule including a brief linear warm-up and a cosine decay.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 60, 61), (('v number', '4'), 62, 63)], 'For WEAT T8, we augment the target and attribute lists with k=4 nearest neighbours of each term'], [[(('parameter', 'K'), 68, 69), (('v number', '2'), 70, 71)], 'As the initial lists of WEAT T1 are longer than those of T8, we use k=2 with T1']] \n",
      "\n",
      "[[[(('artifact', 'model'), 53, 58), (('parameter', 'batch size'), 77, 87), (('v number', '16'), 91, 93), (('v number', '5'), 148, 149), (('v number', '120'), 180, 183), (('v number', '160'), 184, 187)], 'For both synthetic and real experiments, our EMP-Net model is trained with a batch size of 16, where every instance within a batch is a sequence of 5 consecutive frames resized to 120 160 '], [[(('artifact', 'Adam'), 11, 15), (('v number', '0.9'), 79, 82), (('v number', '0.999'), 87, 92)], 'We use the ADAM optimiser , using the default first and second moment terms of 0.9 and 0.999 values respectively'], [[(('parameter', 'learning rate'), 9, 22), (('parameter', 'epochs'), 49, 55), (('v number', '0.001'), 26, 31), (('v number', '10'), 46, 48)], 'We use a learning rate of 0.001 and train for 10 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 50, 55), (('parameter', 'm'), 111, 112), (('v number', '6'), 126, 127)], 'According to the entire structure of the proposed model, the depth of our CSSFN network is given by: D = n[1 + m q + 1] + s + 6,']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 160, 166), (('v number', '0'), 141, 142), (('v number', '2'), 149, 150)], 'In order to avoid a degrading latent space, we employ a KL annealing strategy , where we gradually increase the weights of the KL terms from 0 after 2 training epochs'], [[(('artifact', 'Adam'), 88, 92), (('parameter', 'learning rate'), 119, 132), (('parameter', 'weight decay'), 149, 161), (('v number', '0.0001'), 136, 142), (('v number', '0.0001'), 165, 171)], 'In all of our experiments, we train the whole network in an end-to-end manner using the Adam optimiser with an initial learning rate of 0.0001 and a weight decay of 0.0001 '], [[(('artifact', 'method'), 115, 121), (('v number', '50'), 11, 13)], \"The ResNet-50 is initialised using ImageNet pre-trained weights, and all other weights are initialised using He 's method .\"]] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 31, 35), (('parameter', 'learning rate'), 64, 77), (('parameter', 'learning rate'), 98, 111), (('v number', '0.0001'), 78, 84), (('v number', '0.9'), 121, 124), (('v number', '50'), 130, 132)], 'The model is trained using the ADAM optimizer, with the initial learning rate 0.0001 , and with a learning rate decay of 0.9 each 50 iterations']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 29, 42), (('parameter', 'epochs'), 145, 151), (('v number', '0.1'), 96, 99), (('v number', '100'), 141, 144)], 'Moreover, we set the initial learning rate as 1e^ , periodically reduced it by multiplying with 0.1 , and stopped the learning process after 100 epochs'], [[(('artifact', 'Adam'), 16, 20), (('parameter', 'weight decay'), 99, 111), (('v number', '0.9'), 58, 61), (('v number', '0.999'), 86, 91), (('v number', '0.0001'), 115, 121)], 'We employed the Adam optimizer with the first momentum of 0.9, the second momentum of 0.999, and a weight decay of 0.0001 to minimize the loss see Eq']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 35, 42), (('v number', '100'), 59, 62), (('v number', '0.5'), 67, 70)], 'Character embedding dimensions and dropout rate are set to 100 and 0.5 respectively']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 40, 44), (('parameter', 'learning rate'), 62, 75), (('v number', '0.001'), 79, 84)], 'For the Bérard architecture, we use the Adam optimizer with a learning rate of 0.001'], [[(('parameter', 'learning rate'), 60, 73), (('v number', '1'), 77, 78)], 'For other architectures than Bérard, we use ADADELTA with a learning rate of 1 and we normalize the loss per utterance instead of per token']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 49, 53), (('parameter', 'learning rate'), 69, 82), (('parameter', 'learning rate'), 114, 127), (('v number', '0.001'), 86, 91), (('v number', '0.9'), 106, 109), (('v number', '0.7'), 137, 140)], 'We trained all models until saturation, and used Adam optimizer with learning rate of 0.001 , momentum of 0.9 and learning rate decay of 0.7 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 154, 164), (('v number', '0.9'), 144, 147), (('v number', '256'), 168, 171)], 'We train our models using Pytorch, and optimization is performed using synchronous Stochastic Gradient Descent SGD algorithm with a momentum of 0.9 and a batch size of 256'], [[(('parameter', 'learning rate'), 51, 64), (('parameter', 'learning rate'), 81, 94), (('parameter', 'epoch'), 118, 123), (('v number', '0.045'), 68, 73), (('v number', '0.98'), 109, 113)], 'Following the MobileNets , setup, we use a initial learning rate of 0.045, and a learning rate decay rate of 0.98 per epoch.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 42, 48), (('v number', '90'), 39, 41)], 'All models in this section are trained 90 epochs with random horizontal flip as data augmentation , for preprocessing, we normalize the data using the channel means and standard deviations as in '], [[(('parameter', 'learning rate'), 53, 66), (('parameter', 'epoch'), 110, 115), (('v number', '0.1'), 74, 77), (('v number', '0.01'), 95, 99)], 'The networks are updated with ADAGRAD optimizer with learning rate set to 0.1 and decreases to 0.01 from 45th epoch on wards.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 89, 96), (('v number', '1'), 81, 82), (('v number', '0.5'), 112, 115)], 'blackFurthermore, we applied an l_2 -regularization with a coefficient of black0.1 and a dropout with a rate of 0.5 to prevent over-fitting'], [[(('parameter', 'learning rate'), 42, 55), (('parameter', 'epoch'), 102, 107), (('parameter', 'batch size'), 139, 149), (('v number', '0.001'), 59, 64), (('v number', '0.99'), 93, 97), (('v number', '40'), 154, 156)], 'We trained models by using a RAdam with a learning rate of 0.001 by exponentially decreasing 0.99 per epoch, where the dimension of a mini-batch size was 40']] \n",
      "\n",
      "[[[(('artifact', 'model'), 57, 62), (('artifact', 'Adam'), 115, 119)], 'For each stage, we train different NN models and each NN model is trained via stochastic gradient descent with the Adam optimizer using TensorFlow '], [[(('parameter', 'batch size'), 4, 14), (('v number', '4096'), 18, 22), (('v number', '256'), 67, 70)], 'The batch size is 4096, and the projection step is performed every 256 iterations'], [[(('artifact', 'model'), 59, 64), (('artifact', 'model'), 77, 82), (('parameter', 'learning rate'), 96, 109), (('v number', '2'), 24, 25), (('v number', '10'), 26, 28), (('v number', '4'), 29, 30), (('v number', '0.001'), 115, 120), (('v number', '0.0001'), 124, 130)], 'We train for a total of 2 10^4 iterations for each sub-ADC model and residue model, varying the learning rate from 0.001 to 0.0001 across the iterations.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 49, 54), (('v number', '128'), 100, 103)], 'In experiments, we set word embedding dimension, model dimension and latent vector dimension all to 128'], [[(('parameter', 'layer'), 24, 29), (('artifact', 'L'), 30, 31), (('v number', '4'), 32, 33), (('v number', '8'), 54, 55), (('v number', '64'), 79, 81)], 'The number of attention layer L=4 , number of heads H=8 each with dimension d_=64 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 32, 42), (('parameter', 'epochs'), 66, 72), (('v number', '8'), 46, 47), (('v number', '150'), 62, 65)], 'For training STEP-Gen, we use a batch size of 8 and train for 150 epochs'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 42, 55), (('parameter', 'epochs'), 131, 137), (('v number', '0.1'), 59, 62), (('v number', '75'), 115, 117), (('v number', '113'), 119, 122), (('v number', '132'), 127, 130)], 'We use the Adam optimizer with an initial learning rate of 0.1 , which decreases to -th of its current value after 75, 113 and 132 epochs']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 20, 25), (('artifact', 'LSTMs'), 26, 31), (('parameter', 'layer'), 55, 60), (('parameter', 'layer'), 81, 86), (('v number', '2'), 18, 19)], 'We train separate 2-layer LSTMs with a embedding input layer and a linear output-layer to both the fixed- and variable- copy tasks by minimizing the cross-entropy loss'], [[(('parameter', 'layers'), 79, 85), (('v number', '6'), 34, 35), (('v number', '50'), 97, 99)], 'The embedding dimension is set to 6 and hidden and cell dimensions of the LSTM layers are set to 50'], [[(('parameter', 'learning rate'), 57, 70), (('parameter', 'K'), 97, 98), (('artifact', 'method'), 142, 148), (('v number', '64'), 42, 44), (('v number', '1.0'), 76, 79), (('v number', '10'), 103, 105), (('v number', '20'), 111, 113), (('v number', '100'), 171, 174), (('v number', '15'), 183, 185), (('v range', '[2,100]'), 201, 208)], 'We train using SGD using a batchsize of S=64 and a fixed learning rate of = 1.0 with fixed TBPTT K [5, 10, 15, 20, 30] and our adaptive TBPTT method [0.9, 0.5, 0.1] , W = 100 , K_0 = 15 and [K_, K_] = [2,100] ']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 26, 31), (('parameter', 'layer'), 58, 63), (('parameter', 'layer'), 95, 100), (('v number', '4'), 24, 25), (('v number', '4'), 56, 57)], 'The architecture uses a 4-layer transformer encoder and 4-layer transformer decoder, where one layer is language specific for both the encoder and decoder, the rest are shared'], [[(('parameter', 'steps'), 142, 147), (('parameter', 'steps'), 165, 170), (('v number', '0.1'), 130, 133), (('v number', '0'), 155, 156)], 'Initially, the auto-encoding loss and back-translation loss have equal weight, with the auto-encoding loss coefficient reduced to 0.1 by 100K steps and to 0 by 300k steps']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 29, 33), (('parameter', 'learning rate'), 49, 62), (('parameter', 'batch size'), 125, 135), (('v number', '3e-4'), 63, 67), (('v number', '40000'), 103, 108), (('v number', '32'), 136, 138)], 'All models are trained using Adam optimizer with learning rate 3e-4 and evaluate the performance after 40000 iterations with batch size 32']] \n",
      "\n",
      "[[[(('artifact', 'GNN'), 17, 20), (('artifact', 'L'), 144, 145)], 'For the proposed GNN with R-WDD, it also has three hyper parameters which are the regularizer , the parameter for GPS and the number of neurons L '], [[(('artifact', 'GNN'), 51, 54), (('v number', '1000'), 124, 128)], 'Also, the number of nodes for ELM and the proposed GNN with R-WDD for binary and multi-classification problems are fixed to 1000.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 19, 32), (('parameter', 'batch size'), 46, 56), (('parameter', 'learning rate'), 87, 100), (('parameter', 'epochs'), 128, 134), (('v number', '0.1'), 36, 39), (('v number', '4096'), 57, 61), (('v number', '128'), 62, 65), (('v number', '0.01'), 115, 119), (('v number', '3'), 126, 127)], 'We use the initial learning rate of 0.1, with batch size 4096 128 images per chip, and learning rate decay rate of 0.01 every 3 epochs'], [[(('parameter', 'dropout'), 7, 14), (('parameter', 'weight decay'), 30, 42), (('v number', '0.8'), 18, 21), (('v number', '1e-5'), 43, 47)], 'We use dropout of 0.8, and l2 weight decay 1e-5 and the same image preprocessing as Inception '], [[(('parameter', 'layers'), 22, 28), (('parameter', 'layers'), 53, 59), (('v number', '0.99'), 82, 86)], 'All our convolutional layers use batch-normalization layers with average decay of 0.99.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 16, 26), (('parameter', 'weight decay'), 79, 91), (('v number', '4096'), 30, 34), (('v number', '0.9'), 71, 74), (('v number', '8'), 95, 96), (('v number', '1e-05'), 97, 102), (('v number', '0.9999'), 215, 221)], 'We trained at a batch size of 4096 using SGD with Nesterov momentum of 0.9 and weight decay of 8 1e-05 and performed evaluation using an exponential moving average of the training weights computed with decay factor 0.9999'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 43, 49), (('artifact', 'linear warmup'), 53, 66), (('parameter', 'learning rate'), 80, 93), (('parameter', 'epoch'), 155, 160), (('v number', '10'), 40, 42), (('v number', '1.6'), 97, 100), (('v number', '0.975'), 145, 150)], 'The learning rate schedule consisted of 10 epochs of linear warmup to a maximum learning rate of 1.6, followed by exponential decay at a rate of 0.975 per epoch']] \n",
      "\n",
      "[[[(('artifact', 'model'), 60, 65), (('v number', '50'), 19, 21)], 'We use a buffer of 50 previously translated pairs to reduce model oscillations , '], [[(('artifact', 'Adam'), 17, 21), (('v number', '0.5'), 58, 61), (('v number', '0.999'), 63, 68)], 'All networks use Adam solver with its parameters _1, _2 = 0.5, 0.999 '], [[(('parameter', 'batch size'), 9, 19), (('parameter', 'learning rate'), 46, 59), (('parameter', 'epochs'), 126, 132), (('v number', '4'), 23, 24), (('v number', '0.0002'), 63, 69)], 'We use a batch size of 4, and keep a constant learning rate of 0.0002 for all the networks for the first-half of the training epochs, and then linearly decay it to zero over the second-half.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 88, 93), (('v number', '1'), 102, 103), (('v number', '6'), 115, 116)], 'A replica of the same network is created for value function estimation, with the output layer of size 1 instead of 6']] \n",
      "\n",
      "[[[(('artifact', 'model'), 12, 17), (('artifact', 'model'), 86, 91)], 'For the BOW model, we keep the hyper-parameters and features the same as the baseline model '], [[(('artifact', 'Adam'), 33, 37), (('v number', '20'), 53, 55)], 'Our models are trained using the Adam optimizer, and 20% of the training data is set aside as validation data']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 59, 65), (('v number', '128'), 51, 54), (('v number', '40'), 70, 72)], 'The best results were obtained with batch-size b = 128 and epochs e = 40 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 0, 5), (('parameter', 'layers'), 49, 55), (('v number', '5'), 47, 48), (('v number', '32'), 97, 99), (('v number', '64'), 100, 102), (('v number', '128'), 103, 106), (('v number', '256'), 107, 110)], 'Model Parameters: The Keypoint Encoder MLP has 5 layers, mapping positions to dimensions of size 32,64,128,256,D , yielding 100k parameters'], [[(('parameter', 'layers'), 29, 35), (('v number', '2'), 27, 28)], 'The message update MLP has 2 layers and maps to dimensions 2D,D '], [[(('parameter', 'layer'), 5, 10), (('v number', '0'), 15, 16)], 'Each layer has 0.66M parameters'], [[(('parameter', 'layers'), 17, 23), (('v number', '18'), 14, 16)], 'SuperGlue has 18 layers, with a total of 12M parameters.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '1.8'), 34, 37), (('v number', '3.6'), 77, 80)], 'The model was built on tensorflow 1.8 framework and implemented using Python 3.6'], [[(('artifact', 'model'), 146, 151), (('parameter', 'layer'), 212, 217), (('v number', '2017'), 121, 125)], 'In addition to original code to pre-process image, isolate objects, construct U-Net, and SegNet graph, we used the davis-2017 code repository for model evaluation and OSVOS code repository for constructing OSVOS layer.']] \n",
      "\n",
      "[[[(('artifact', 'LSTMs'), 42, 47), (('parameter', 'layers'), 53, 59), (('v number', '3'), 51, 52), (('v number', '256'), 81, 84)], 'The encoder and decoder are bidirectional LSTMs of 3-layers with a state size of 256'], [[(('parameter', 'hidden layers'), 21, 34), (('v number', '256'), 38, 41)], 'The dimension of all hidden layers is 256'], [[(('artifact', 'Adam'), 55, 59), (('parameter', 'learning rate'), 79, 92), (('v number', '0.005'), 96, 101)], 'We minimize the loss function in Equation REF with the Adam optimizer , with a learning rate of 0.005 '], [[(('parameter', 'dropout'), 24, 31), (('v number', '95.0'), 97, 101), (('v number', '50.0'), 116, 120)], 'We also applied regular dropout after the input and output of each LSTM with keep probability of 95.0% for SMTD and 50.0% for CamRest']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 73, 83), (('parameter', 'weight decay'), 111, 123), (('parameter', 'epochs'), 165, 171), (('v number', '256'), 87, 90), (('v number', '8'), 94, 95), (('v number', '0.0001'), 127, 133), (('v number', '0.9'), 153, 156), (('v number', '100'), 161, 164)], 'We use a traditional cross-entropy loss to train all the networks with a batch size of 256 on 8 GPUs by SGD, a weight decay of 0.0001, and a momentum of 0.9 for 100 epochs'], [[(('parameter', 'learning rate'), 16, 29), (('parameter', 'epochs'), 80, 86), (('v number', '0.1'), 33, 36), (('v number', '10'), 68, 70), (('v number', '30'), 77, 79)], 'We start from a learning rate of 0.1 and decrease it by a factor of 10 every 30 epochs'], [[(('parameter', 'layers'), 14, 20), (('parameter', 'learning rate'), 57, 70)], 'All the extra layers in Context-Gated Convolution have a learning rate ten times smaller than convolutional kernels.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 26, 31), (('artifact', 'LSTMs'), 32, 37)], 'For SEQ2SEQ, we adopt one layer LSTMs as encoder and decoder'], [[(('parameter', 'dropout'), 70, 77), (('v number', '512'), 58, 61), (('v number', '0.3'), 86, 89)], 'For Key-Value Retrieval dataset, hidden size is placed at 512 and the dropout rate is 0.3'], [[(('parameter', 'dropout'), 37, 44), (('v number', '128'), 54, 57), (('v number', '0.1'), 62, 65), (('v number', '3'), 75, 76), (('v number', '256'), 78, 81), (('v number', '0.1'), 86, 89), (('v number', '4'), 99, 100), (('v number', '5'), 105, 106)], 'On dataset bAbI, the hidden size and dropout rate are 128 and 0.1 for task 3, 256 and 0.1 for task 4 and 5'], [[(('parameter', 'learning rates'), 0, 14), (('v number', '0.001'), 26, 31), (('v number', '0.0001'), 45, 51), (('v number', '2'), 61, 62)], 'Learning rates are set to 0.001 for bAbI and 0.0001 for DSTC 2 and Key-Value Retrieval dataset'], [[(('parameter', 'dropout'), 37, 44), (('v number', '256'), 54, 57), (('v number', '0.1'), 62, 65), (('v number', '3'), 75, 76), (('v number', '4'), 81, 82), (('v number', '128'), 84, 87), (('v number', '0.1'), 92, 95), (('v number', '5'), 105, 106)], 'On dataset bAbI, hidden size and the dropout rate are 256 and 0.1 for task 3 and 4, 128 and 0.1 for task 5'], [[(('parameter', 'dropout'), 49, 56), (('v number', '512'), 66, 69), (('v number', '0.3'), 74, 77)], 'For Key-Value Retrieval dataset, hidden size and dropout rate are 512 and 0.3'], [[(('parameter', 'dropout'), 4, 11), (('v number', '0.2'), 27, 30), (('v number', '3'), 39, 40), (('v number', '4'), 42, 43), (('v number', '0.1'), 77, 80), (('v number', '5'), 89, 90), (('v number', '2'), 100, 101)], 'The dropout rate is set to 0.2 in task 3, 4 and Key-Value Retrieval dataset, 0.1 in task 5 and DSTC 2 dataset']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 34, 39), (('parameter', 'layer'), 51, 56), (('parameter', 'layer'), 82, 87), (('parameter', 'layer'), 107, 112), (('v number', '128'), 70, 73), (('v number', '2'), 80, 81), (('v number', '256'), 126, 129)], 'This implementation uses an input layer and linear layer dimension of 128 and a 2-layer LSTM with a hidden layer dimension of 256'], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'dropout'), 46, 53), (('v number', '0.3'), 62, 65)], 'The Adam optimizer is used for training and a dropout rate of 0.3 is enforced during training'], [[(('parameter', 'epochs'), 30, 36), (('v number', '10'), 27, 29)], 'The tagger was trained for 10 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 51, 64), (('v number', '500'), 40, 43)], 'All models have the dimensional size of 500 in the hidden layers'], [[(('parameter', 'learning rate'), 29, 42), (('v number', '0.0002'), 46, 52)], 'We train the models with the learning rate of 0.0002'], [[(('parameter', 'epochs'), 57, 63), (('v number', '400000'), 41, 47)], 'The best validated networks are saved in 400000 training epochs']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 57, 63), (('parameter', 'layers'), 77, 83), (('parameter', 'layer'), 119, 124), (('v number', '12'), 150, 152), (('v number', '14'), 156, 158)], 'The reduced AlexNet which consists of five convolutional layers and three FC layers, but the number of kernels in each layer are all reduced to about 12 to 14 of the ones in AlexNet ']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 177, 190), (('v number', '100'), 172, 175), (('v number', '0.00002'), 191, 198), (('v number', '0'), 210, 211), (('v number', '0.9'), 223, 226)], 'The network is trained using the RMSProp algorithm see for example http:www.cs.toronto.edu~ tijmencsc321slideslecture_slides_lec6.pdf as referred to in with minibatch size 100, learning rate 0.00002 , momentum 0, and decay 0.9 '], [[(('parameter', 'epochs'), 23, 29), (('v number', '100'), 33, 36)], 'The number of training epochs is 100.']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 120, 132), (('parameter', 'T'), 134, 135), (('v number', '1'), 172, 173)], 'where N is the number of images in the burst, S is the set of kernel sizes, and are the hyperparameters controlling the weight decay, t is the training step, and _1 + _2 = 1 ']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 16, 29), (('parameter', 'learning rate'), 89, 102), (('parameter', 'learning rate'), 155, 168), (('v number', '1e-4'), 33, 37), (('v number', '1e-3'), 106, 110), (('v number', '1'), 128, 129), (('v number', '000'), 130, 133), (('v number', '1e-3'), 172, 176)], 'We set the base learning rate as 1e-4, adopt a warm-up strategy to linearly increase the learning rate to 1e-3 during the first 1,000 iterations, keep the learning rate as 1e-3, and stop the learning after 40k iterations']] \n",
      "\n",
      "[[[(('artifact', 'model'), 15, 20), (('v number', '0.9'), 90, 93)], 'We train every model via stochastic gradient descent SGD with Nesterov momentum of weight 0.9 without dampening'], [[(('parameter', 'learning rate'), 22, 35), (('v number', '0.1'), 63, 66), (('v number', '0'), 94, 95)], 'We use a cosine shape learning rate schedule which starts from 0.1 and decreases gradually to 0 throughout the training'], [[(('parameter', 'weight decay'), 9, 21), (('v number', '0.0001'), 25, 31), (('v number', '1e-05'), 82, 87)], 'We set a weight decay of 0.0001 , except for the spatial shifting biases in which 1e-05 is used instead'], [[(('parameter', 'epoch'), 57, 62), (('parameter', 'epochs'), 89, 95)], 'During the training, we call dealloc and realloc at each epoch for the half of the total epochs']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 18, 23), (('parameter', 'layer'), 35, 40), (('v number', '4'), 16, 17), (('v number', '1'), 33, 34), (('v number', '768'), 54, 57)], 'We found that a 4 layer encoder, 1 layer decoder with 768 for the encoder hidden size and projections, decoder hidden size, and attention hidden size yielded equal-best results with deeper models']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('artifact', 'model'), 79, 84), (('parameter', 'epochs'), 108, 114), (('v number', '0.001'), 32, 37), (('v number', '0.1'), 68, 71), (('v number', '2'), 106, 107)], 'The initial learning rate is lr=0.001 and is reduced by a factor of 0.1 if the model does not improve for 2 epochs'], [[(('parameter', 'epochs'), 34, 40), (('v number', '5'), 32, 33)], 'The training process ends after 5 epochs of no improvements'], [[(('artifact', 'Adam'), 22, 26), (('v number', '0.9'), 64, 67), (('v number', '0.999'), 76, 81)], 'For optimisation, the Adam optimiser is selected with betas b_1=0.9 and b_2=0.999 '], [[(('parameter', 'batch size'), 13, 23), (('v number', '1'), 34, 35)], 'Finally, the batch size is set to 1.']] \n",
      "\n",
      "[[[(('parameter', 'hidden state'), 4, 16), (('v number', '256'), 33, 36)], 'The hidden state size was set to 256'], [[(('parameter', 'T'), 70, 71), (('parameter', 'K'), 93, 94), (('v number', '602'), 45, 48), (('v number', '2216'), 52, 56), (('v number', '1'), 95, 96), (('v number', '5'), 100, 101)], 'The vocabulary size is increased from around 602 to 2216 concepts w.r.t the different number k=1, , 5 of concept candidates for each word']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 4, 8), (('v number', '0.9'), 45, 48), (('v number', '0.999'), 55, 60), (('v number', '1e-08'), 68, 73)], 'The Adam solver is used with parameters _1 = 0.9, _2 = 0.999, and = 1e-08 '], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'learning rate'), 38, 51), (('v number', '32'), 25, 27), (('v number', '0.001'), 62, 67)], 'The batch size is set to 32, with the learning rate is set to 0.001'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 53, 59), (('v number', '80'), 50, 52)], 'The model is trained on the generated dataset for 80 epochs']] \n",
      "\n",
      "[[[(('parameter', 'activation'), 49, 59), (('v number', '6'), 43, 44)], 'The head number of multi-head attention is 6 and activation function inside is '], [[(('artifact', 'Adam'), 8, 12), (('parameter', 'learning rate'), 87, 100), (('v number', '1.5'), 77, 80), (('v number', '0.0001'), 111, 117)], 'We used Adam to optimize the loss function defined in Eq.REF where is set to 1.5 , and learning rate is set to 0.0001 '], [[(('parameter', 'batch size'), 13, 23), (('parameter', 'epochs'), 37, 43), (('v number', '128'), 27, 30), (('v number', '6'), 35, 36)], 'The training batch size is 128 for 6 epochs'], [[(('parameter', 'learning rate'), 21, 34), (('parameter', 'steps'), 62, 67), (('artifact', 'linear decay'), 72, 84), (('v number', '1'), 59, 60)], 'And we also employed learning rate warmup within the first 1% steps and linear decay within the rest']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 24, 31), (('v number', '150'), 18, 21), (('v number', '0.2'), 34, 37)], 'When HiddenSize = 150 , Dropout = 0.2 is a good choice'], [[(('artifact', 'model'), 36, 41), (('parameter', 'T'), 138, 139), (('v number', '150'), 121, 124), (('v number', '250'), 128, 131)], \"Increase will lead to high variance model and decrease will lead to over-fitting in early stage Increase HiddenSize from 150 to 250 doesn't improve the performance much\"], [[(('parameter', 'T'), 46, 47), (('artifact', 'model'), 55, 60), (('v number', '100'), 29, 32), (('v number', '200'), 36, 39)], \"Increase embedding size from 100 to 200 doesn't affect model performance much.\"]] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '300'), 51, 54)], 'The model word embeddings are initialized with the 300-dimensional pre-trained vectors provided by Glove '], [[(('artifact', 'Adam'), 39, 43), (('parameter', 'learning rate'), 59, 72), (('v number', '0.001'), 73, 78)], 'We update network weights by using the Adam optimizer with learning rate 0.001'], [[(('parameter', 'hidden layers'), 4, 17), (('v number', '100'), 50, 53)], 'The hidden layers of Bi-LSTM and Bi-GRU have size 100'], [[(('artifact', 'model'), 25, 30), (('parameter', 'dropout'), 32, 39), (('v number', '0.1'), 50, 53)], 'In the semantic matching model, Dropout is set to 0.1.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 20, 25), (('parameter', 'layers'), 120, 126), (('v number', '512'), 80, 83), (('v number', '6'), 118, 119), (('v number', '8'), 131, 132)], 'For the transformer model, we use token embeddings and hidden size of dimension 512, and the encoder and decoder have 6 layers and 8 attention heads'], [[(('parameter', 'layer'), 14, 19), (('v number', '4096'), 69, 73)], 'For the inner layer in the positionwise feed-forward network, we use 4096'], [[(('parameter', 'dropout'), 38, 45), (('v number', '0.2'), 49, 52)], 'Similar to previous models we set the dropout to 0.2']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 35, 41), (('v number', '50'), 18, 20), (('v number', '150'), 22, 25), (('v number', '150'), 27, 30), (('v number', '80'), 32, 34)], 'We train ATMC for 50, 150, 150, 80 epochs on MNIST, CIFAR10, CIFAR100 and SVHN respectively.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 25, 30), (('v number', '128'), 57, 60)], 'For all experiments, our model used word embeddings with 128 dimensions, pretrained using GloVe '], [[(('parameter', 'batch size'), 59, 69), (('v number', '256'), 47, 50), (('v number', '8'), 73, 74)], 'We set the dimensions of all hidden vectors to 256 and the batch size to 8'], [[(('parameter', 'dropout'), 11, 18), (('v number', '0.5'), 32, 35)], 'We applied dropout at a rate of 0.5'], [[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 32, 36), (('v number', '2'), 93, 94)], 'The model was trained using the Adam optimizer with default parameters and l_2 constraint of 2']] \n",
      "\n",
      "[[[(('parameter', 'K'), 48, 49), (('v number', '40'), 50, 52), (('v number', '50'), 70, 72)], 'Across all simulations we trained BCF to induce K=40 categories and G=50 feature types which are shared across categories'], [[(('parameter', 'K'), 121, 122), (('v number', '40'), 123, 125), (('v number', '0.7'), 156, 159), (('v number', '0.1'), 162, 165), (('v number', '0.1'), 173, 176)], 'We trained BayesCat on the same input stimuli as BCF, with the following parameters: the number of categories was set to K=40 , and the hyperparameters to =0.7, =0.1 , and =0.1 '], [[(('artifact', 'model'), 18, 23), (('parameter', 'K'), 32, 33), (('v number', '40'), 34, 36), (('v number', '5'), 70, 71)], 'The co-occurrence model induces K=40 categories, and, subsequently, G=5 feature types for each category.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 73, 78), (('artifact', 'model'), 119, 124), (('v number', '5'), 27, 28), (('v number', '19'), 66, 68)], 'We have extracted the CONV-5 image feature of the pre-trained VGG-19 CNN model for the LSTM + Q+ I+ Attention baseline model'], [[(('artifact', 'model'), 38, 43), (('artifact', 'model'), 112, 117), (('v number', '5'), 62, 63), (('v number', '152'), 104, 107)], 'Since submission, we have updated our model and used the CONV-5 image feature of the pre-trained Resnet-152 CNN model for MCB to get state of the art result'], [[(('artifact', 'model'), 45, 50), (('parameter', 'learning rate'), 117, 130), (('parameter', 'batch size'), 141, 151), (('v number', '0.0004'), 132, 138), (('v number', '200'), 154, 157), (('v number', '0.99'), 167, 171), (('v number', '1e-8'), 184, 188)], 'We have used RMSPROP optimizer to update the model parameter and configured hyper-parameter values to be as follows: learning rate =0.0004 , batch size = 200, alpha = 0.99 and epsilon=1e-8 to train the classification network '], [[(('artifact', 'model'), 28, 33), (('artifact', 'model'), 80, 85), (('artifact', 'model'), 86, 91), (('parameter', 'learning rate'), 146, 159), (('parameter', 'batch size'), 169, 179), (('v number', '0.001'), 161, 166), (('v number', '200'), 182, 185), (('v number', '0.9'), 195, 198), (('v number', '1e-8'), 211, 215)], 'In order to train a triplet model, we have used RMSPROP to optimize the triplet model model parameter and configure hyper-parameter values to be: learning rate =0.001 , batch size = 200, alpha = 0.9 and epsilon=1e-8'], [[(('parameter', 'learning rate'), 13, 26), (('parameter', 'learning rate'), 49, 62), (('parameter', 'epoch'), 72, 77)], 'We have used learning rate decay to decrease the learning rate on every epoch by a factor given by: Decay_factor=exp']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 72, 78), (('artifact', 'gradient clipping'), 80, 97), (('v number', '50'), 30, 32), (('v number', '50'), 34, 36), (('v number', '50'), 62, 64), (('v number', '1'), 101, 102), (('v number', '6'), 104, 105), (('v number', '2'), 123, 124), (('v number', '8'), 149, 150), (('v number', '7.5e-4'), 197, 203), (('v number', '1.3e-3'), 214, 220)], 'For example, for CIFAR10 with 50%:50% train-validation split, 50 search epochs, gradient clipping of 1, 6 normal cells and 2 reduction cells both of 8 experts for each forecaster, REF yields ^*_N= 7.5e-4 and ^*_R= 1.3e-3.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 65, 78), (('v number', '128'), 52, 55), (('v number', '0.1'), 79, 82)], 'We train these networks by two GPUs with mini-batch 128 and base learning rate 0.1'], [[(('parameter', 'learning rate'), 23, 36), (('v number', '10'), 59, 61)], 'As the same with , the learning rate decays by a factor of 10 after 32k and 48k iterations, and the training stops after 64k iterations']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 82, 92), (('v number', '96000'), 60, 65), (('v number', '32'), 93, 95)], 'GraphRNN and GraphRNN-S were trained using a single GPU for 96000 iterations with batch size 32']] \n",
      "\n",
      "[[[(('artifact', 'model'), 15, 20), (('artifact', 'model'), 63, 68)], 'The pretrained model is publicly available from the Tensorpack model zoo'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'weight decay'), 27, 39), (('v number', '0.02'), 21, 25), (('v number', '0.0001'), 43, 49), (('v number', '0.9'), 67, 70)], 'The learning rate is 0.02, weight decay is 0.0001, and momentum is 0.9']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('v number', '38'), 28, 30), (('v number', '780'), 31, 34), (('v number', '13'), 88, 90), (('v number', '233'), 91, 94), (('v number', '20'), 124, 126)], 'Our baseline model contains 38,780 embeddings in a 32D space, generated by discretizing 13,233 unique variable labels, with 20 bins per continuous variable'], [[(('parameter', 'activation'), 50, 60), (('parameter', 'layer'), 62, 67), (('parameter', 'dropout'), 118, 125), (('parameter', 'weight decay'), 216, 228), (('v number', '1'), 26, 27), (('v number', '0.5'), 143, 146), (('v number', '0.001'), 244, 249)], 'We utilize a GRU of depth 1 with a sigmoid output activation, layer normalization applied to all linear projections , dropout regularization p=0.5 on both the aggregated embedding and the hidden states, as well as a weight decay coefficient of 0.001 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 133, 143), (('v number', '2'), 37, 38)], 'The data set generated as in section 2 are of small batches we call them batch units which are able to fit in GPU memory, but larger batch size provides better performance for Transformer, “tokens_optm” is applied to support large batch sizes, the training script will forward and backward many batch units, and update parameters with an optimizer step until it has collected gradient with more than “tokens_optm” number of tokens on the target side.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 42, 52), (('v number', '16'), 7, 9), (('v number', '96'), 56, 58)], 'We use 16 GPU asynchronous workers, and a batch size of 96'], [[(('parameter', 'learning rate'), 18, 31), (('parameter', 'learning rate'), 46, 59), (('parameter', 'epoch'), 83, 88), (('v number', '0.045'), 35, 40), (('v number', '0.98'), 74, 78)], 'We use an initial learning rate of 0.045, and learning rate decay rate of 0.98 per epoch'], [[(('parameter', 'layers'), 22, 28), (('v number', '0.99'), 75, 79)], 'All the convolutional layers use batch normalization with average decay of 0.99'], [[(('parameter', 'learning rate'), 158, 171), (('parameter', 'learning rate'), 185, 198), (('parameter', 'epoch'), 216, 221), (('v number', '8'), 64, 65), (('v number', '1e-05'), 175, 180), (('v number', '0.9'), 208, 211)], 'Using the floating-point checkpoints, we then train models with 8-bit representation of weights and activations by using quantization-aware training , with a learning rate of 1e-05 and learning rate decay of 0.9 per epoch']] \n",
      "\n",
      "[[[(('parameter', 'activation'), 18, 28), (('parameter', 'layers'), 29, 35)], 'Secondly, all the activation layers used are PReLU, initialized with he_normal , '], [[(('parameter', 'dropout'), 13, 20), (('parameter', 'dropout'), 86, 93), (('parameter', 'epochs'), 141, 147), (('v number', '0.5'), 110, 113), (('v number', '0.0'), 127, 130), (('v number', '100'), 137, 140)], 'Thirdly, the dropout has been parametrized according to the strategy of the annealing dropout, from a rate of 0.5 to a rate of 0.0 after 100 epochs'], [[(('parameter', 'learning rate'), 51, 64), (('v number', '1.0'), 68, 71)], 'Adadelta was used for optimization with an initial learning rate of 1.0.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 83, 88), (('parameter', 'layer'), 185, 190), (('parameter', 'layer'), 199, 204), (('parameter', 'layer'), 281, 286), (('v number', '3'), 81, 82), (('v number', '3'), 197, 198)], 'We implement all decoders using a comparable number of GRU parameters, including 3-layer stacked-GRU subword and character-level decoders, where the attention is computed after the 1st layer and a 3-layer hierarchical decoder which implements the attention mechanism after the 2nd layer'], [[(('parameter', 'layer'), 66, 71), (('parameter', 'layer'), 99, 104), (('v number', '4'), 91, 92), (('v number', '256'), 122, 125)], 'LMM uses the same hierarchical GRU architecture, where the middle layer is augmented using 4 multi-layer perceptrons with 256 hidden units'], [[(('artifact', 'Adam'), 33, 37), (('parameter', 'batch size'), 55, 65), (('parameter', 'dropout'), 74, 81), (('parameter', 'learning rate'), 95, 108), (('parameter', 'learning rate'), 123, 136), (('parameter', 'epoch'), 208, 213), (('artifact', 'model'), 315, 320), (('v number', '100'), 69, 72), (('v number', '0.2'), 90, 93), (('v number', '0.0004'), 112, 118), (('v number', '0.8'), 146, 149)], 'All models are trained using the Adam optimizer with a batch size of 100, dropout rate of 0.2, learning rate of 0.0004 and learning rate decay of 0.8, applied when the perplexity does not decrease at a given epoch.Perplexity is the exponentiated average negative log-likelihood per segment BPE, or character that a model assigns to a dataset']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 65, 78), (('v number', '0.0001'), 82, 88)], 'We use ADAM to optimize the networks in Algorithm REF , with the learning rate of 0.0001 '], [[(('parameter', 'weight decay'), 9, 21), (('v number', '0.0001'), 151, 157)], 'We apply weight decay constraining the _2 -norm of the weight parameters of the network, where the hyper-parameter for the decay is empirically set to 0.0001 '], [[(('parameter', 'epochs'), 98, 104), (('v number', '1000'), 58, 62), (('v number', '3'), 96, 97)], 'To prevent the networks from overfitting, we train at max 1000 iterations per round, and at max 3 epochs for the samples in current experience memory']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 134, 140), (('v number', '128'), 31, 34), (('v number', '05'), 53, 55), (('v number', '0.05'), 56, 60), (('v number', '128'), 91, 94)], 'Embedding vectors of dimension 128 are drawn from U0.05,0.05 and the LSTM weights neurons: 128 and weights of the feed forward output layers are sampled from a Glorot uniform distribution '], [[(('artifact', 'model'), 11, 16), (('artifact', 'Adam'), 23, 27), (('parameter', 'learning rate'), 28, 41), (('parameter', 'epochs'), 78, 84), (('v number', '0.001'), 43, 48), (('v number', '20'), 75, 77), (('v number', '16'), 111, 113)], 'We fit our model using Adam learning rate: 0.001 on the training data over 20 epochs with mini batches of size 16']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 12, 18), (('v number', '800'), 24, 27)], 'Two Bi-LSTM layers with 800 cells each form the RNN'], [[(('parameter', 'dropout'), 24, 31), (('parameter', 'layers'), 83, 89), (('v number', '0.3'), 49, 52)], 'To avoid overfitting, a dropout with the rate of 0.3 is used between and after the layers'], [[(('artifact', 'LSTMs'), 34, 39), (('v number', '1'), 66, 67)], 'In addition, the gradients of the LSTMs are clipped to a value of 1']] \n",
      "\n",
      "[[[(('artifact', 'model'), 35, 40), (('v number', '650'), 147, 150)], 'Thus, we adopt a standard language model architecture as our primary baseline, an RNN with LSTM cells and hyper-parameters corresponding to medium 650 dimensional models '], [[(('parameter', 'epochs'), 40, 46), (('v number', '20'), 28, 30)], 'All models converged within 20 training epochs'], [[(('artifact', 'model'), 96, 101), (('v number', '103'), 92, 95), (('v number', '2'), 155, 156), (('v number', '500'), 157, 160)], 'During training, the softmax is computed using the full vocabulary, except for the Wikitext-103 model which uses a sampled-softmax with a sampling rate of 2,500']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('v number', '64'), 44, 46)], 'The batch size of the tested data is set as 64'], [[(('parameter', 'steps'), 14, 19), (('artifact', 'model'), 63, 68)], 'The number of steps of historical data incorporated in the GMN model will have an influence on the prediction performance'], [[(('parameter', 'steps'), 22, 27), (('parameter', 'steps'), 31, 36), (('parameter', 'steps'), 45, 50), (('v number', '6'), 20, 21), (('v number', '8'), 29, 30), (('v number', '10'), 42, 44)], 'Hence,the GMNs with 6-steps, 8-steps, and 10-steps of historical data are tested in the experiments, i.e'], [[(('parameter', 'steps'), 45, 50), (('v number', '6'), 71, 72), (('v number', '8'), 79, 80), (('v number', '10'), 90, 92)], 'The corresponding SGMN models with different steps are denoted as SGMN-6, SGMN-8,and SGMN-10, respectively'], [[(('parameter', 'm'), 87, 88), (('parameter', 'steps'), 129, 134), (('v number', '10'), 121, 123)], 'For the RNN-based baseline models, including GRU, GRU-I, GRU-D, LSTM, LSTM-I, and LSTM-M, their input sequences all have 10 time steps.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 79, 85), (('v number', '0.001'), 36, 41), (('v number', '80'), 61, 63), (('v number', '120'), 65, 68), (('v number', '160'), 70, 73), (('v number', '180'), 75, 78)], 'The initial learning rate is set to 0.001, and reduced after 80, 120, 160, 180 epochs'], [[(('parameter', 'epochs'), 4, 10), (('parameter', 'batch size'), 38, 48), (('v number', '200'), 0, 3), (('v number', '128'), 52, 55)], '200 epochs are used in total, and the batch size is 128'], [[(('parameter', 'epoch'), 62, 67), (('v number', '15000'), 27, 32)], 'Therefore, there are about 15000 iterations during one single epoch in total.']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 79, 91), (('v number', '9e-1'), 63, 67), (('v number', '1e-6'), 73, 77), (('v number', '0'), 92, 93)], 'Optimization We use ADADELTA with default hyper-parameters rho=9e-1, eps=1e-6, weight decay=0 to minimize the aforementioned loss function'], [[(('parameter', 'epochs'), 11, 17), (('v number', '6'), 9, 10)], 'We train 6 epochs in total'], [[(('parameter', 'learning rate'), 18, 31), (('parameter', 'epochs'), 55, 61), (('parameter', 'epoch'), 81, 86), (('parameter', 'epoch'), 110, 115), (('v number', '1.0'), 35, 38), (('v number', '4'), 53, 54), (('v number', '0.1'), 63, 66), (('v number', '0.01'), 91, 95)], 'We initialize the learning rate to 1.0 for the first 4 epochs, 0.1 for the fifth epoch and 0.01 for the sixth epoch']] \n",
      "\n",
      "[[[(('parameter', 'K'), 120, 121), (('v number', '300'), 104, 107), (('v number', '300'), 122, 125)], 'For all models using word embeddings i.e., Seg-*, Rev-*, MIL-*, we initialize the word embeddings using 300-dimensional k=300 pre-trained word2vec embeddings '], [[(('parameter', 'm'), 157, 158), (('v number', '50'), 61, 63), (('v number', '2'), 77, 78), (('v number', '50'), 79, 81), (('v number', '100'), 84, 87), (('v number', '100'), 142, 145), (('v number', '100'), 159, 162)], 'For the forward and backward GRUs we use hidden vectors with 50 dimensions n=2 50 = 100 , while for the attention mechanism we use vectors of 100 dimensions m=100 '], [[(('parameter', 'dropout'), 7, 14), (('v number', '0.5'), 25, 28)], 'We use dropout with rate 0.5 on the word embeddings and the internal GRU states']] \n",
      "\n",
      "[[[(('parameter', 'K'), 21, 22), (('v number', '20'), 33, 35)], 'The number of topics K is set to 20'], [[(('artifact', 'system'), 102, 108), (('artifact', 'system'), 156, 162), (('v number', '1'), 146, 147)], \"We follow the utterance “triples” structure as in our experiments, which means we aim to generate the system utterance S' by observing the former 1 turn of system utterance S and user utterance U.\"]] \n",
      "\n",
      "[[[(('artifact', 'L'), 71, 72), (('parameter', 'm'), 117, 118), (('parameter', 'm'), 170, 171), (('v number', '4'), 174, 175)], 'A local encoder E^lS encodes the source texture into a latent tensor z^l , which has a spatial size that is a factor m smaller than the size of the input texture: we use m = 4 '], [[(('artifact', 'L'), 17, 18), (('artifact', 'L'), 47, 48)], 'The generator Gz^l,z^g ningcolorconcatenates z^l and z^g , and can decode these latent tensors back into a texture patch, so that ideally GE^lS,E^gS=S , which encompasses the reconstruction task']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 185, 198), (('v number', '5e-3'), 202, 206), (('v number', '0.75'), 217, 221)], 'For PSST Gumbel softmax and for PSST Multinomial we used the same hyperparameters that were found to be best for ST Multinomial and ST Gumbel-Softmax, and we did not tuned them further learning rate of 5e-3 and decay 0.75 .']] \n",
      "\n",
      "[[[(('parameter', 'T'), 18, 19), (('v number', '360'), 44, 47)], 'At each time step t , the agent perceives a 360-degree panoramic view of its surroundings from the current location'], [[(('parameter', 'm'), 29, 30), (('parameter', 'm'), 43, 44), (('v number', '36'), 45, 47), (('v number', '3'), 71, 72), (('v number', '12'), 86, 88), (('v number', '30'), 101, 103)], 'The view is discretized into m view angles m=36 in our implementation, 3 elevations x 12 headings at 30-degree intervals'], [[(('artifact', 'model'), 104, 109), (('v number', '2018'), 12, 16)], 'As in Fried:2018:Speaker, the agent is trained using student forcing where actions are sampled from the model during training, and supervised using a shortest-path action to reach the goal state.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 36, 43), (('parameter', 'layers'), 86, 92), (('v number', '0.1'), 64, 67), (('v number', '0.1'), 132, 135)], 'In all abstractive models, we apply dropout with probability of 0.1 before all linear layers; label smoothing with smoothing factor 0.1 is also used'], [[(('artifact', 'Adam'), 18, 22), (('parameter', 'learning rate'), 28, 41), (('parameter', 'learning rate'), 92, 105), (('parameter', 'steps'), 134, 139), (('v number', '2'), 45, 46), (('v number', '0.9'), 53, 56), (('v number', '0.998'), 68, 73), (('v number', '8'), 128, 129), (('v number', '000'), 130, 133)], 'The optimizer was Adam with learning rate of 2, _1 = 0.9 , and _2 = 0.998 ; we also applied learning rate warmup over the first 8,000 steps, and decay as in '], [[(('parameter', 'layers'), 98, 104), (('v number', '256'), 33, 36), (('v number', '1'), 84, 85), (('v number', '024'), 86, 89)], 'All transformer-based models had 256 hidden units; the feed-forward hidden size was 1,024 for all layers'], [[(('parameter', 'steps'), 62, 67), (('v number', '4'), 27, 28), (('v number', '500'), 54, 57), (('v number', '000'), 58, 61)], 'All models were trained on 4 GPUs NVIDIA TITAN Xp for 500,000 steps']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 39, 43), (('artifact', 'method'), 44, 50), (('v number', '0.5'), 89, 92), (('v number', '0.9'), 123, 126), (('v number', '0.999'), 162, 167)], 'We train all of our networks using the Adam method for stochastic optimization , with _1=0.5 for all trained variables, _2=0.9 for our Boltzmann machines, and _2=0.999 for our generator and discriminator'], [[(('parameter', 'learning rate'), 46, 59), (('parameter', 'K'), 112, 113), (('parameter', 'steps'), 122, 127), (('v number', '0.001'), 98, 103), (('v number', '5'), 114, 115)], 'During training on synthetic data, we set the learning rate for both of our Boltzmann machines to 0.001 and use k=5 Gibbs steps']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 67, 73), (('parameter', 'layers'), 133, 139), (('parameter', 'layers'), 162, 168), (('parameter', 'layers'), 203, 209), (('parameter', 'layer'), 254, 259), (('v number', '2'), 46, 47), (('v number', '200'), 84, 87), (('v number', '2'), 141, 142), (('v number', '200'), 172, 175), (('v number', '1'), 233, 234), (('v number', '128'), 265, 268)], 'For multilingual adversarial training, we use 2 bidirectional LSTM layers each with 200 memory units in each direction as the shared layers, 2 bidirectional LSTM layers of 200 cells each direction as AM layers for each language, and 1 bidirectional LSTM layer with 128 cells each direction for LID'], [[(('artifact', 'model'), 41, 46), (('v number', '5'), 25, 26)], 'For decoding, a standard 5-gram language model is trained.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 30, 36), (('v number', '6'), 76, 77)], 'More precisely, the number of layers in the encoder and in the decoder is N=6 '], [[(('parameter', 'layers'), 35, 41), (('v number', '8'), 14, 15)], 'We employ h = 8 parallel attention layers, or heads'], [[(('parameter', 'layer'), 67, 72), (('v number', '512'), 47, 50), (('v number', '2048'), 122, 126)], 'The dimensionality of input and output is d_ = 512 , and the inner-layer of a feed-forward networks has dimensionality d_=2048 .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 55, 65), (('parameter', 'learning rate'), 78, 91), (('v number', '80'), 69, 71), (('v number', '0.1'), 95, 98)], 'The Bi-GRU and feed-forward models were trained with a batch size of 80 and a learning rate of 0.1'], [[(('parameter', 'layers'), 42, 48), (('parameter', 'dropout'), 79, 86), (('v number', '2'), 36, 37), (('v number', '512'), 52, 55), (('v number', '50'), 75, 77)], 'The Bi-GRU models were trained with 2 GRU layers of 512 neurons each, with 50% dropout']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 33, 46), (('v number', '0.0006'), 79, 85)], 'We use a variation of the \"poly\" learning rate decay policy proposed in : LR = 0.0006 }^']] \n",
      "\n",
      "[[[(('parameter', 'T'), 28, 29), (('parameter', 'K'), 40, 41), (('v number', '1'), 36, 37)], 'A set of instances of tasks T^i , i 1, ,K , and for each of them a set of demonstrations ^i = I_^i, I_t^i, ^i_t _ ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 124, 134), (('parameter', 'batch size'), 208, 218), (('v number', '82'), 111, 113), (('v number', '94'), 199, 201), (('v number', '20'), 222, 224)], 'For the impact of different batch sizes shown in Figure REF , we can observe that success rate hits the bottom 82% when the batch size is set to relatively small, and reaches a relatively high level 94% with batch size at 20'], [[(('parameter', 'batch size'), 16, 26), (('artifact', 'model'), 127, 132)], 'As we know, the batch size in training should not be set too small as it may lead to more calculation iterations and cause the model perform bad']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 50, 56), (('v number', '2'), 48, 49), (('v number', '1024'), 62, 66)], 'The language modelling LSTM network consists of 2-layers with 1024 hidden units and an equally sized character embedding space'], [[(('artifact', 'Adam'), 34, 38), (('parameter', 'learning rate'), 46, 59), (('parameter', 'dropout'), 124, 131), (('parameter', 'hidden layers'), 171, 184), (('v number', '2'), 63, 64), (('v number', '0.001'), 65, 70), (('v number', '0.95'), 84, 88), (('v number', '5'), 117, 118), (('v number', '0.2'), 144, 147)], 'The parameters were trained using Adam with a learning rate of 2 0.001 , a decay of 0.95 , gradient norm clipping of 5, and dropout probability 0.2 for the inputs and the hidden layers.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 127, 133), (('parameter', 'T'), 184, 185), (('parameter', 'T'), 186, 187), (('parameter', 'layer'), 287, 292), (('parameter', 'layer'), 336, 341), (('parameter', 'T'), 389, 390), (('parameter', 'T'), 391, 392), (('parameter', 'T'), 401, 402), (('parameter', 'T'), 403, 404)], 'For tasks with output at every timestep such as the language modeling problem in Subsection REF , the recurrent weights of all layers were initialized uniformly in the range [0, @root T-t ] , while for tasks such as classification where only the output at the last time step in the last layer is used, the recurrent weights of the last layer were initialized uniformly in the range [@root T-t , @root T-t ] as described in Subsection REF '], [[(('parameter', 'T'), 72, 73), (('parameter', 'T'), 74, 75)], 'The recurrent weights are constrained to be in the range of |u_n| @root T-t for IndRNN with ReLU as analysed in Subsection REF '], [[(('parameter', 'T'), 72, 73), (('parameter', 'T'), 74, 75), (('parameter', 'T'), 103, 104), (('v number', '1.0'), 37, 40), (('v number', '1'), 96, 97)], 'In our experiments, is simply set to 1.0 for long sequences since @root T-t is already close to 1 when T is very large'], [[(('parameter', 'steps'), 31, 36), (('v number', '20'), 28, 30)], 'For short sequences such as 20 steps, the constraint can also be removed as the gradient exploding problem is not very severe in such cases']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 39, 52), (('v number', '0.001'), 56, 61)], 'We use RMSprop as the optimizer with a learning rate of 0.001'], [[(('parameter', 'batch size'), 25, 35), (('parameter', 'epochs'), 50, 56), (('v number', '32'), 39, 41), (('v number', '100'), 46, 49)], 'We train the LSTM with a batch size of 32 and 100 epochs'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 36, 49), (('v number', '0.001'), 53, 58)], 'We use Adam as the optimizer with a learning rate of 0.001'], [[(('artifact', 'model'), 13, 18), (('parameter', 'batch size'), 26, 36), (('parameter', 'epochs'), 50, 56), (('v number', '32'), 40, 42), (('v number', '10'), 47, 49)], 'We train the model with a batch size of 32 for 10 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('parameter', 'batch size'), 46, 56), (('v number', '0.0001'), 31, 37), (('v number', '32'), 68, 70)], 'The learning rate was fixed to 0.0001 and the batch size was set to 32'], [[(('artifact', 'gradient clipping'), 11, 28), (('v number', '2.0'), 61, 64)], 'We adopted gradient clipping with a maximum gradient norm of 2.0 '], [[(('artifact', 'Adam'), 0, 4), (('v number', '0.9'), 61, 64), (('v number', '0.99'), 69, 73)], 'Adam algorithm was used for stochastic optimization, with _1=0.9, _2=0.99 ']] \n",
      "\n",
      "[[[(('artifact', 'L'), 46, 47), (('parameter', 'batch size'), 49, 59), (('parameter', 'learning rate'), 65, 78)], '[!b] Adversarial training of ROMark Combined [l] Batch size: b , Learning Rate: _ , _ , _ , Attack functions: N_1,...,N_K Randomly initialize the networks: D_ , E_ and C_ '], [[(('parameter', 'm'), 30, 31), (('parameter', 'batch size'), 35, 45)], 'Randomly sample message batch M of batch size b '], [[(('parameter', 'K'), 7, 8), (('parameter', 'K'), 44, 45)], 'Select K integers: k_,...,k_,...,k_ , where K is the number of types of attacks and _^k_=b Read minibatch B= x_1,...,x_b from training set'], [[(('parameter', 'm'), 61, 62), (('parameter', 'K'), 94, 95), (('parameter', 'K'), 205, 206), (('v number', '1'), 195, 196), (('v number', '2'), 198, 199)], 'Generate the watermarked minibatch B_= E_x_i,m_i: x_i B, m_i M Separate the minibatch B_ into K subsets B^_,...,B^_ where each contains k_ images Load severity ranges of attacks: S_1,...,S_K i = 1, 2,..., K Search the worst-case s^_ by: s^_ = _ _ B^} Lm, D_N_ix^, s Calculate the worst-case attacked image batch B^_ = N_x^_, s^_: x^_ B^_ Generate attacked minibatch B_= B^_ ,...,B^_ Feed B_ into decoder, and then do one step training step: Updating discriminator C:']] \n",
      "\n",
      "[[[(('artifact', 'model'), 23, 28), (('artifact', 'L'), 45, 46)], \"We fit each regression model's parameters by L-BFGS\"], [[(('artifact', 'model'), 44, 49), (('v number', '2'), 50, 51)], 'On Europarl data which has fewer languages, Model 2 performs best'], [[(('artifact', 'model'), 88, 93), (('artifact', 'model'), 131, 136), (('v number', '2'), 137, 138)], 'On the Bible corpora, all models are relatively close to one another, though the robust Model 2L gets more consistent results than Model 2 across data subsets'], [[(('artifact', 'model'), 27, 32), (('artifact', 'model'), 141, 146), (('artifact', 'model'), 157, 162), (('v number', '2'), 33, 34), (('v number', '3'), 147, 148), (('v number', '3'), 163, 164)], 'We use MAP estimates under Model 2 for all remaining experiments for speed and simplicity.Further enhancements are possible: we discuss our “Model 3” in app:model-3, but it did not seem to fit better.']] \n",
      "\n",
      "[[[(('parameter', 'm'), 64, 65), (('v number', '230'), 40, 43), (('v number', '3'), 83, 84)], 'The filters number of CNN d_c equals to 230 and the kernel size m in CNN equals to 3'], [[(('parameter', 'layer'), 10, 15), (('parameter', 'dropout'), 27, 34), (('v number', '0.5'), 92, 95)], 'In output layer, we employ dropout for regularization, where the drop probability is set to 0.5 '], [[(('parameter', 'learning rate'), 98, 111), (('parameter', 'learning rate'), 135, 148), (('parameter', 'steps'), 173, 178), (('v number', '0.1'), 115, 118)], 'To minimize the loss function defined in Eq.REF , we use stochastic gradient descent with initial learning rate of 0.1 , and decay the learning rate to one tenth every 100K steps.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 20, 33), (('v number', '0.0001'), 37, 43)], 'ADAM optimizer with learning rate of 0.0001 is used for training'], [[(('parameter', 'layer'), 34, 39), (('v number', '1'), 32, 33), (('v number', '8'), 44, 45)], 'Transformer encoder consists of 1 layer and 8 attention heads'], [[(('parameter', 'steps'), 19, 24), (('v number', '13'), 72, 74)], 'Number of decoding steps is limited to maximum input sequence length of 13 tokens'], [[(('parameter', 'epochs'), 26, 32), (('artifact', 'model'), 47, 52), (('v number', '20'), 23, 25)], 'Models are trained for 20 epochs, and the best model is chosen based on Max-BLEU score on the development set.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 39, 43), (('parameter', 'learning rate'), 135, 148), (('v number', '1e-05'), 152, 157), (('v number', '0.99'), 165, 169), (('v number', '0.99'), 177, 181), (('v number', '1e-08'), 185, 190), (('v number', '0.01'), 223, 227), (('v number', '0.5'), 235, 238), (('v number', '0.001'), 249, 254)], 'The surrogate has been trained with an Adam optimizer regarding to this adaptive loss with the following parameters for the optimizer: learning rate of 1e-05 , _1 = 0.99 , _2 = 0.99 , =1e-08 and for the adaptive loss: _k = 0.01 , _k = 0.5 and k_0 = 0.001 '], [[(('artifact', 'Adam'), 44, 48), (('parameter', 'learning rate'), 148, 161), (('v number', '1e-05'), 165, 170), (('v number', '0.99'), 178, 182), (('v number', '0.99'), 190, 194), (('v number', '1e-08'), 200, 205)], \"The generator has also been trained with an Adam optimizer configured similarly to the surrogate's with the following parameters for the optimizer: learning rate of 1e-05 , _1 = 0.99 , _2 = 0.99 and =1e-08 \"]] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 89, 102), (('v number', '100'), 54, 57), (('v number', '000'), 58, 61), (('v number', '0.0001'), 106, 112), (('v number', '10'), 135, 137), (('v number', '70'), 144, 146), (('v number', '000'), 147, 150)], 'For all our experiments, we train the TPP-PHOCNet for 100,000 iterations with an initial learning rate of 0.0001 , which is divided by 10 after 70,000 iterations'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'batch size'), 37, 47), (('parameter', 'weight decay'), 97, 109), (('v number', '10'), 51, 53), (('v number', '0.9'), 74, 77), (('v number', '0.999'), 85, 90), (('v number', '5'), 113, 114), (('v number', '1e-05'), 115, 120)], 'We use Adam optimization with a mini-batch size of 10, hyperparameters _1=0.9 and _2=0.999 and a weight decay of 5 1e-05 '], [[(('parameter', 'dropout'), 40, 47), (('parameter', 'layers'), 48, 54)], 'Analogue to subsec:sigtdo, we apply two dropout layers during training']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 28, 32), (('parameter', 'learning rate'), 59, 72), (('parameter', 'epochs'), 96, 102), (('parameter', 'epochs'), 147, 153), (('v number', '0.001'), 73, 78), (('v number', '80'), 93, 95), (('v number', '0.0001'), 124, 130), (('v number', '20'), 144, 146)], 'For the first stage, we use ADAM optimizer with an initial learning rate 0.001 for the first 80 epochs and then decay it to 0.0001 for the last 20 epochs'], [[(('parameter', 'epochs'), 34, 40), (('parameter', 'batch size'), 46, 56), (('v number', '50'), 31, 33), (('v number', '1'), 57, 58)], 'For the second stage, we train 50 epochs with batch size 1'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 55, 61), (('parameter', 'epochs'), 100, 106), (('v number', '0.001'), 36, 41), (('v number', '40'), 52, 54), (('v number', '0.1'), 85, 88), (('v number', '5'), 98, 99)], 'The learning rate is initialized as 0.001 for first 40 epochs and is then decayed by 0.1 in every 5 epochs'], [[(('artifact', 'model'), 209, 214), (('v number', '0.55'), 131, 135), (('v number', '0.45'), 180, 184)], 'For the box prediction network, a proposal is considered positive if its maximum 3D IoU with all ground-truth boxes is higher than 0.55 and negative if its maximum 3D IoU is below 0.45 during training the car model']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 56, 60), (('parameter', 'learning rate'), 67, 80)], 'The models are implemented in PyTorch and trained using Adam and a learning rate cosine decay schedule from 6e^ down to 3e^ ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 71, 77), (('parameter', 'batch size'), 84, 94), (('v number', '8'), 30, 31), (('v number', '50'), 68, 70), (('v number', '64'), 95, 97), (('v number', '16'), 181, 183)], 'A small network consisting of 8 cells is trained using RC-DARTS for 50 epochs, with batch size 64 for both the training and validation iterations and the initial number of channels 16'], [[(('artifact', 'Adam'), 13, 17), (('parameter', 'learning rate'), 41, 54), (('parameter', 'weight decay'), 96, 108), (('v number', '3e-4'), 58, 62), (('v number', '0.5'), 76, 79), (('v number', '0.999'), 81, 86)], 'REF , we use Adam optimizer with initial learning rate of 3e-4, momentum of 0.5, 0.999 , and no weight decay'], [[(('parameter', 'epoch'), 4, 9), (('v number', '0.5'), 0, 3), (('v number', '500'), 61, 64)], '0.5 epoch; The number of iteration in phase II e_p is set to 500']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 0, 4), (('v number', '0.9'), 13, 16), (('v number', '0.999'), 22, 27)], 'Adam with _1=0.9 , _2=0.999 is used for optimization'], [[(('parameter', 'learning rate'), 4, 17), (('artifact', 'linear warmup'), 32, 45), (('parameter', 'steps'), 68, 73), (('artifact', 'linear decay'), 78, 90), (('v number', '3e-5'), 21, 25), (('v number', '40'), 61, 63), (('v number', '000'), 64, 67)], 'The learning rate is 3e-5, with linear warmup over the first 40,000 steps and linear decay'], [[(('parameter', 'dropout'), 4, 11), (('v number', '0.1'), 20, 23)], 'The dropout rate is 0.1 '], [[(('parameter', 'weight decay'), 4, 16), (('v number', '0.01'), 20, 24)], 'The weight decay is 0.01 '], [[(('parameter', 'batch size'), 4, 14), (('v number', '330'), 18, 21)], 'The batch size is 330'], [[(('parameter', 'steps'), 50, 55), (('v number', '770'), 42, 45), (('v number', '000'), 46, 49)], 'The pre-training procedure runs for about 770,000 steps'], [[(('parameter', 'steps'), 34, 39), (('v number', '7'), 15, 16), (('v number', '10'), 27, 29), (('v number', '000'), 30, 33), (('v number', '8'), 46, 47)], 'It takes about 7 hours for 10,000 steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 118, 123), (('parameter', 'dropout'), 151, 158), (('v number', '64'), 46, 48), (('v number', '128'), 52, 55), (('v number', '96'), 127, 129), (('v number', '0.25'), 167, 171)], 'Different from the feature dimension value of 64 or 128 in prior works , , we set the middle feature dimension of the model to 96 for GraFormer with a dropout rate of 0.25'], [[(('artifact', 'Adam'), 9, 13), (('parameter', 'learning rate'), 57, 70), (('v number', '0.001'), 74, 79), (('v number', '64'), 100, 102)], 'We adopt Adam optimizer for optimization with an initial learning rate of 0.001 and mini-batches of 64'], [[(('parameter', 'learning rate'), 31, 44), (('parameter', 'steps'), 64, 69), (('v number', '0.9'), 48, 51), (('v number', '75000'), 58, 63)], 'For Human3.6M, we multiply the learning rate by 0.9 every 75000 steps'], [[(('parameter', 'learning rate'), 23, 36), (('parameter', 'epochs'), 60, 66), (('v number', '0.9'), 47, 50), (('v number', '30'), 57, 59)], 'For hand datasets, the learning rate decays by 0.9 every 30 epochs'], [[(('parameter', 'epochs'), 26, 32), (('parameter', 'epochs'), 51, 57), (('parameter', 'epochs'), 84, 90), (('v number', '50'), 23, 25), (('v number', '900'), 47, 50), (('v number', '3000'), 79, 83)], 'We train GraFormer for 50 epochs on Human3.6M, 900 epochs on Obman and GHD and 3000 epochs on FHAD.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 62, 66), (('parameter', 'learning rate'), 84, 97), (('parameter', 'dropout'), 113, 120), (('parameter', 'batch size'), 134, 144), (('parameter', 'layers'), 185, 191), (('v number', '1e-04'), 101, 106), (('v number', '0.3'), 109, 112), (('v number', '32'), 148, 150), (('v number', '0.2'), 156, 159)], 'We use the following hyper-parameters determined empirically: adam optimizer with a learning rate of 1e-04 , 0.3 dropout probability, batch size of 32, and 0.2 threshold for leaky ReLU layers'], [[(('parameter', 'epochs'), 28, 34), (('parameter', 'epochs'), 57, 63), (('v number', '100'), 24, 27), (('v number', '10'), 54, 56)], 'We train the models for 100 epochs and validate every 10 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 8, 12), (('parameter', 'learning rate'), 79, 92), (('v number', '0.9'), 41, 44), (('v number', '0.999'), 50, 55), (('v number', '0'), 59, 60), (('v number', '0.001'), 104, 109)], 'We used Adam with its hyperparameters _1=0.9,, _2=0.999,, =0 , and the initial learning rate was set to 0.001 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 55, 60), (('artifact', 'Adam'), 67, 71), (('parameter', 'weight decay'), 87, 99), (('artifact', 'AdamW'), 100, 105), (('parameter', 'weight decay'), 142, 154), (('parameter', 'epochs'), 170, 176), (('parameter', 'batch size'), 184, 194), (('v number', '0.9'), 113, 116), (('v number', '0.999'), 121, 126), (('v number', '1e-08'), 130, 135), (('v number', '0.01'), 158, 162), (('v number', '80'), 167, 169), (('v number', '32'), 198, 200)], 'A cross-entropy loss is employed to optimise the whole model using Adam with decoupled Weight decay AdamW with _=0.9 , _=0.999 , =1e-08 and a weight decay of 0.01 for 80 epochs with a batch size of 32'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0003'), 36, 42)], 'The initial learning rate is set to 0.0003'], [[(('parameter', 'layer'), 152, 157), (('v number', '12'), 51, 53), (('v number', '2048'), 86, 90), (('v number', '4'), 97, 98), (('v number', '256'), 105, 108), (('v number', '256'), 115, 118), (('v number', '256'), 125, 128)], 'For the task of sentence-level lip-reading, we use 12 multi-head attention blocks d^}=2048 , n^}=4 , d^}=256 , d^}=256 , d^}=256 together with a linear layer on the top of conformer blocks like '], [[(('parameter', 'epochs'), 114, 120), (('parameter', 'batch size'), 128, 138), (('v number', '50'), 111, 113), (('v number', '8'), 142, 143)], 'Following , we use a combination of CTC and cross-entropy loss to train a hybrid CTCAttention architecture for 50 epochs with a batch size of 8'], [[(('artifact', 'Adam'), 21, 25), (('parameter', 'steps'), 79, 84), (('v number', '0.9'), 33, 36), (('v number', '0.98'), 41, 45), (('v number', '1e-09'), 51, 56), (('v number', '25'), 72, 74), (('v number', '000'), 75, 78)], 'In this case, we use Adam with _=0.9 , _=0.98 and =1e-09 with the first 25 000 steps for warm-up'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0004'), 36, 42)], 'The initial learning rate is set to 0.0004'], [[(('artifact', 'model'), 60, 65), (('v number', '16.2'), 110, 114)], 'During decoding, we also apply a transformer-based language model trained on LRS2, LRS3, and Librispeech 960h 16.2 million words in total']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 55, 60), (('parameter', 'steps'), 117, 122), (('v number', '0'), 33, 34), (('v number', '10'), 48, 50), (('v number', '000'), 51, 54), (('v number', '0.1'), 88, 91), (('v number', '100'), 109, 112), (('v number', '000'), 113, 116)], 'We set the weight of KLD loss to 0 in the first 10,000 steps and gradually increased to 0.1 in the following 100,000 steps'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 79, 84), (('v number', '0.0005'), 28, 34), (('v number', '60'), 72, 74), (('v number', '000'), 75, 78)], 'The learning rate is set to 0.0005 with a cosine learning schedule with 60,000 steps warming up'], [[(('parameter', 'steps'), 80, 85), (('v number', '1'), 57, 58), (('v number', '0.0625'), 62, 68), (('v number', '100'), 72, 75), (('v number', '000'), 76, 79)], 'We decay the temperature in Gumble-softmax function from 1 to 0.0625 in 100,000 steps following '], [[(('parameter', 'steps'), 37, 42), (('parameter', 'batch size'), 50, 60), (('v number', '150'), 29, 32), (('v number', '000'), 33, 36), (('v number', '64'), 64, 66)], 'We train dVAE for a total of 150,000 steps with a batch size of 64.']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 14, 19), (('v number', '30'), 39, 41), (('v number', '1'), 53, 54)], 'Each training epoch for CE ranged from 30 minutes to 1 hour depending on number of GPUs utilized.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 70, 76), (('v number', '48'), 67, 69), (('v number', '4096'), 78, 82), (('v number', '64'), 100, 102)], 'For the universal representation module, we adopt a structure with 48 layers, 4096 hidden units and 64 heads'], [[(('parameter', 'layers'), 75, 81), (('v number', '12'), 72, 74), (('v number', '768'), 83, 86), (('v number', '12'), 104, 106)], 'For the task-specific representation modules, we adopt a structure with 12 layers, 768 hidden units and 12 heads'], [[(('parameter', 'batch size'), 10, 20), (('v number', '6144'), 57, 61)], 'The total batch size of all pre-training tasks is set to 6144'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 17, 30), (('parameter', 'weight decay'), 63, 75), (('parameter', 'learning rate'), 85, 98), (('parameter', 'steps'), 128, 133), (('artifact', 'linear decay'), 138, 150), (('parameter', 'learning rate'), 158, 171), (('v number', '1e-4'), 34, 38), (('v number', '0.9'), 43, 46), (('v number', '0.999'), 52, 57), (('v number', '0.01'), 79, 83), (('v number', '10'), 121, 123), (('v number', '000'), 124, 127)], 'We use Adam with learning rate of 1e-4, _1=0.9 , _2=0.999 , L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps and linear decay of the learning rate'], [[(('parameter', 'steps'), 20, 25), (('v number', '10'), 13, 15), (('v number', '000'), 16, 19)], 'In the first 10,000 steps, we also use the progressive learning to speedup convergence in the initial stage of pre-training'], [[(('artifact', 'model'), 4, 9), (('v number', '375'), 36, 39), (('v number', '384'), 60, 63)], 'The model is trained for a total of 375 billion tokens with 384 NVDIA v100 GPU cards and is implemented on PaddlePaddle framework'], [[(('artifact', 'model'), 88, 93), (('artifact', 'model'), 144, 149)], 'By virtue of parameter sharding used in , , we manage to reduce the memory usage of our model and address the problem of the total parameter of model exceeding the memory of a single GPU card.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 113, 118), (('v number', '5'), 94, 95), (('v number', '10'), 96, 98), (('v number', '8'), 99, 100)], 'We test all audio encoders in the Music Recognition scenario, where training converges within 5 10^8 environment steps'], [[(('parameter', 'steps'), 90, 95), (('parameter', 'steps'), 142, 147), (('v number', '10'), 85, 87), (('v number', '9'), 88, 89), (('v number', '2'), 135, 136), (('v number', '10'), 137, 139), (('v number', '9'), 140, 141)], 'We then choose the best-performing encoder for other experiments, where we train for 10^9 steps in Sound Instruction scenarios and for 2 10^9 steps in Duel scenario']] \n",
      "\n",
      "[[[(('artifact', 'model'), 103, 108), (('v number', '1000'), 11, 15), (('v number', '1'), 56, 57), (('v number', '10'), 58, 60)], 'Upwards of 1000 neural style transfer models trained on 1-10 style images each, can be blended through model interpolation, using an interface that is controlled by the user']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 76, 86), (('parameter', 'weight decay'), 122, 134), (('v number', '16'), 87, 89), (('v number', '0.9'), 113, 116), (('v number', '0.0001'), 135, 141)], 'Our network is trained end-to-end with stochastic gradient descent SGD with batch size 16 at most case, momentum 0.9, and weight decay 0.0001'], [[(('parameter', 'learning rate'), 77, 90), (('v number', '1'), 108, 109), (('v number', '0.9'), 140, 143)], 'Following previous works , , , we use the poly strategy in which the current learning rate is multiplied by 1 - ^ each iteration with power 0.9'], [[(('parameter', 'learning rate'), 9, 22), (('v number', '0.01'), 33, 37)], 'The base learning rate is set to 0.01']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 34, 47), (('v number', '0.02'), 72, 76), (('v number', '0.04'), 82, 86)], 'In smaller scale experiments, the learning rate was chosen to be either 0.02 , or 0.04 '], [[(('parameter', 'dropout'), 4, 11), (('parameter', 'weight decay'), 37, 49), (('v number', '80'), 55, 57), (('v number', '1e-05'), 63, 68)], 'The dropout keep probability and the weight decay were 80% and 1e-05 correspondingly']] \n",
      "\n",
      "[[[(('artifact', 'method'), 5, 11), (('v number', '86'), 35, 37), (('v number', '772'), 38, 41)], 'Both method are trained on the all 86,772 images in the dataset']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 33, 46), (('v number', '10'), 50, 52), (('v number', '10'), 66, 68), (('v number', '6'), 69, 70)], 'We use the Adam optimizer with a learning rate of 10^ , and up to 10^6 iterations of training'], [[(('parameter', 'layers'), 75, 81), (('v number', '0.25'), 21, 25)], 'Furthermore, we set =0.25 and a use a steepness of two times the number of layers s=2n for odd-even and s= _2n1+ _2n for bitonic'], [[(('parameter', 'batch size'), 18, 28), (('v number', '100'), 32, 35)], 'We use a constant batch size of 100 as in previous works unless denoted otherwise']] \n",
      "\n",
      "[[[(('artifact', 'model'), 106, 111), (('v number', '7'), 152, 153), (('v number', '3'), 167, 168)], 'Since documents can contain up to hundreds of sentences, for efficient training of our evidence retrieval model, we downsample the negative examples to 7 for IIRC and 3 for HotpotQA'], [[(('parameter', 'm'), 18, 19), (('parameter', 'm'), 34, 35), (('parameter', 'm'), 69, 70), (('v number', '4'), 20, 21), (('v number', '5'), 71, 72)], 'For IIRC, we take m=4 for the top-m context marginalization and take m=5 for HotpotQA'], [[(('artifact', 'model'), 66, 71), (('artifact', 'model'), 118, 123)], 'For memory and storage efficiency, we tie the pretrained language model weights among all the components in our joint model'], [[(('parameter', 'epochs'), 30, 36), (('parameter', 'epochs'), 52, 58), (('v number', '30'), 27, 29), (('v number', '5'), 50, 51)], 'The models are trained for 30 epochs for IIRC and 5 epochs for HotpotQA fullwiki']] \n",
      "\n",
      "[[[(('artifact', 'method'), 17, 23), (('artifact', 'Adam'), 70, 74), (('parameter', 'learning rate'), 101, 114), (('parameter', 'learning rate'), 132, 145), (('v number', '3'), 118, 119), (('v number', '0.0001'), 120, 126)], 'For our proposed method, we trained the segmentation network using an Adam optimizer with an initial learning rate of 3 0.0001 with learning rate decay'], [[(('artifact', 'method'), 17, 23), (('parameter', 'epoch'), 37, 42), (('parameter', 'learning rate'), 53, 66)], 'We evaluated our method at the 2k-th epoch where the learning rate decays to zero.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 28, 32), (('parameter', 'learning rate'), 59, 72), (('parameter', 'weight decay'), 85, 97), (('v number', '0.0002'), 76, 82), (('v number', '1e-05'), 98, 103), (('v number', '0.9'), 109, 112), (('v number', '0.999'), 120, 125)], 'Unless otherwise specified, Adam optimizer with an initial learning rate of 0.0002 , weight decay 1e-05 , _1=0.9 and _2=0.999 was used for training'], [[(('parameter', 'learning rate'), 0, 13), (('v number', '0.2'), 41, 44)], 'Learning rate was reduced by a factor of 0.2 when the validation loss stopped improving after a dataset-dependent number of iterations'], [[(('parameter', 'learning rate'), 30, 43), (('v number', '1e-06'), 58, 63)], 'Training was stopped when the learning rate dropped below 1e-06 or maximum number of iterations was reached']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 154, 160), (('parameter', 'layer'), 177, 182), (('v number', '1024'), 93, 97), (('v number', '4096'), 121, 125), (('v number', '6'), 152, 153), (('v number', '8'), 165, 166)], 'For target terminology extraction phase, we adopt the commonly used Transformer encoder with 1024 embeddinghidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer as the basic'], [[(('parameter', 'batch size'), 21, 31), (('v number', '128'), 42, 45), (('v number', '100'), 84, 87)], 'During training, the batch size is set to 128 and the sentence length is limited to 100 BPE tokens'], [[(('artifact', 'Adam'), 14, 18), (('parameter', 'dropout'), 61, 68), (('v number', '0.0001'), 39, 45), (('v number', '4000'), 52, 56), (('v number', '0.1'), 71, 74)], 'We employ the Adam optimizer with lr = 0.0001, t_ = 4000 and dropout = 0.1.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 36, 41), (('parameter', 'learning rate'), 65, 78), (('parameter', 'batch size'), 97, 107), (('v number', '1e-5'), 84, 88), (('v number', '2e-5'), 90, 94), (('v number', '4'), 113, 114), (('v number', '8'), 115, 116), (('v number', '4'), 148, 149), (('v number', '8'), 150, 151), (('v number', '16'), 152, 154)], 'We use grid search to fine-tune the model, and select select the learning rate from 1e-5, 2e-5 , batch size from 4,8 and gradient accumulation from 4,8,16 '], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 29, 35), (('v number', '4'), 27, 28)], 'The model is trained up to 4 epochs']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 9, 19), (('parameter', 'learning rate'), 32, 45), (('parameter', 'epochs'), 175, 181), (('v number', '4'), 23, 24), (('v number', '2'), 49, 50), (('v number', '1e-06'), 51, 56), (('v number', '3'), 173, 174)], 'We use a batch size of 4, and a learning rate of 2 1e-06 for fine-tuning ViLBERT and use RAdam optimizer and stop the training when the validation score does not change for 3 epochs'], [[(('artifact', 'Adam'), 54, 58), (('parameter', 'learning rate'), 81, 94), (('parameter', 'epochs'), 145, 151), (('v number', '10'), 50, 52), (('v number', '0.001'), 98, 103), (('v number', '0.01'), 131, 135), (('v number', '10'), 142, 144)], 'For COSMic Vanilla, we train with a batch-size of 10, Adam optimizer with a base learning rate of 0.001 that decays by a factor of 0.01 every 10 epochs'], [[(('parameter', 'epochs'), 59, 65), (('parameter', 'epochs'), 93, 99), (('v number', '100'), 55, 58), (('v number', '9'), 91, 92)], 'We observe that the Vanilla converges in approximately 100 epochs and ViLBERT converges in 9 epochs']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 66, 72), (('parameter', 'batch size'), 79, 89), (('v number', '200'), 62, 65), (('v number', '128'), 93, 96), (('v number', '2080'), 104, 108)], 'We train all our models using stochastic gradient descent for 200 epochs and a batch size of 128 on RTX 2080 GPUs'], [[(('parameter', 'learning rate'), 18, 31), (('parameter', 'weight decay'), 45, 57), (('v number', '0.1'), 35, 38), (('v number', '5.0e-4'), 61, 67)], 'We use a starting learning rate of 0.1 and a weight decay of 5.0e-4 '], [[(('parameter', 'learning rate'), 80, 93), (('parameter', 'epoch'), 97, 102), (('v number', '20'), 16, 18), (('v number', '10'), 19, 21), (('v number', '60'), 103, 105), (('v number', '120'), 107, 110), (('v number', '160'), 115, 118), (('v number', '0.2'), 122, 125)], 'For Wide ResNet-20-10 experiments, we use a step scheduler which multiplies the learning rate at epoch 60, 120 and 160 by 0.2.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('parameter', 'batch size'), 62, 72), (('v number', '32'), 76, 78)], 'We train our model on a single Nvidia 2080ti GPU with a total batch size of 32'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 33, 46), (('v number', '5'), 50, 51), (('v number', '0.0001'), 52, 58), (('v number', '0.9'), 63, 66), (('v number', '0.998'), 72, 77), (('v number', '0.001'), 104, 109)], 'We use the Adam optimizer with a learning rate of 5 0.0001 _1 =0.9, _2 =0.998, and the weight decays to 0.001 '], [[(('parameter', 'learning rate'), 23, 36), (('parameter', 'learning rate'), 60, 73), (('v number', '9'), 161, 162), (('v number', '0.5'), 167, 170)], 'We utilize the plateau learning rate schedule to update the learning rate, which tracks the BLEU score on the validation set, the patience and factor are set to 9 and 0.5, respectively'], [[(('parameter', 'steps'), 42, 47), (('v number', '100'), 38, 41)], 'The validation set is evaluated every 100 steps'], [[(('parameter', 'learning rate'), 23, 36), (('v number', '1e-07'), 50, 55)], 'Training ends when the learning rate is less than 1e-07 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 119, 124), (('v number', '15'), 40, 42)], 'In our experiments setup, Q is equal to 15 but may vary depending on the size of the output channels of the PixelHop++ model and also the number of PCA components.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 53, 66), (('v number', '0.9'), 37, 40), (('v number', '1e-4'), 70, 74)], 'We use a fixed Gumbel temperature of 0.9 and a fixed learning rate of 1e-4'], [[(('parameter', 'batch size'), 9, 19), (('parameter', 'epoch'), 103, 108), (('v number', '4'), 23, 24), (('v number', '4'), 61, 62), (('v number', '7'), 127, 128), (('v number', '1'), 138, 139), (('v number', '2080'), 152, 156)], 'We use a batch size of 4 and a gradient accumulation step of 4 and train on the collected data for one epoch which takes about 7 hours on 1 GeForce RTX 2080 11G.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 59, 65), (('v number', '60'), 56, 58), (('v number', '1000'), 69, 73)], 'We then train these models in an adversarial manner for 60 epochs of 1000 episodes each'], [[(('parameter', 'learning rate'), 118, 131), (('v number', '10'), 147, 149)], 'We use the same hyperparameters and experimental setup as we did for the standard training, except that we reduce the learning rate by a factor of 10'], [[(('parameter', 'learning rate'), 10, 23), (('parameter', 'epochs'), 65, 71), (('parameter', 'epochs'), 100, 106), (('parameter', 'epochs'), 128, 134), (('v number', '0.1'), 44, 47), (('v number', '20'), 62, 64), (('v number', '0.006'), 90, 95), (('v number', '20'), 107, 109), (('v number', '40'), 113, 115), (('v number', '0.0012'), 117, 123), (('v number', '40'), 135, 137), (('v number', '50'), 141, 143), (('v number', '0.0024'), 149, 155)], 'Thus, the learning rate is initially set to 0.1 for the first 20 epochs, then modified to 0.006 for epochs 20 to 40, 0.0012 for epochs 40 to 50, and 0.0024 thereafter']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 16, 26), (('v number', '32'), 30, 32)], 'Training uses a batch size of 32'], [[(('artifact', 'Adam'), 14, 18), (('parameter', 'learning rate'), 45, 58), (('v number', '0.001'), 62, 67)], 'We employ the Adam optimizer with an initial learning rate of 0.001'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 97, 103), (('v number', '0.1'), 56, 59), (('v number', '10'), 94, 96)], 'The learning rate is scheduled to reduce by a factor of 0.1 if the loss does not decrease for 10 epochs'], [[(('parameter', 'epochs'), 93, 99), (('v number', '15'), 90, 92)], 'We leverage an early stopping strategy, on the dev set perplexity, with a patience set to 15 epochs']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 46, 50), (('parameter', 'steps'), 77, 82), (('parameter', 'weight decay'), 91, 103), (('v number', '500'), 66, 69), (('v number', '500'), 83, 86), (('v number', '0.01'), 107, 111)], 'Our default parameters are as follows: We use Adam optimizer with 500 warmup steps=500 and weight decay of 0.01'], [[(('parameter', 'batch size'), 4, 14), (('v number', '16'), 18, 20)], 'Our batch size is 16']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('v number', '0.01'), 40, 44), (('v number', '0.03'), 49, 53)], 'The initial learning rate for GTA-IM is 0.01 and 0.03 for the PROX dataset'], [[(('parameter', 'epochs'), 23, 29), (('parameter', 'learning rate'), 57, 70), (('parameter', 'epochs'), 100, 106), (('v number', '450'), 33, 36), (('v number', '0.2'), 86, 89), (('v number', '200'), 96, 99)], 'The number of training epochs is 450 and we decrease the learning rate by a factor of 0.2 every 200 epochs'], [[(('parameter', 'batch size'), 9, 19), (('v number', '128'), 23, 26), (('v number', '1'), 31, 32), (('v number', '2'), 59, 60)], 'We use a batch size of 128 and 1 second of observation and 2 seconds for predictions following the settings of '], [[(('artifact', 'model'), 14, 19), (('v number', '8'), 33, 34), (('v number', '24'), 45, 47)], 'Training each model took between 8 hours and 24 hours depending on the used dataset.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 27, 32), (('artifact', 'model'), 48, 53)], 'We train the 2D CNN + LSTM model and the 3D CNN model on the same dataset to benchmark the performance against each other']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 9, 13), (('parameter', 'learning rate'), 21, 34), (('parameter', 'weight decay'), 50, 62), (('parameter', 'batch size'), 105, 115), (('v number', '0.001'), 38, 43), (('v number', '4096'), 119, 123)], 'We apply Adam with a learning rate of 0.001 and a weight decay of 1e^ to prevent overfitting, and a mini-batch size of 4096 across all tasks'], [[(('parameter', 'hidden layers'), 95, 108), (('parameter', 'layer'), 129, 134), (('v number', '100'), 113, 116)], 'For the fair competition, we set the default architecture of the dense neural network with two hidden layers and 100 neurons per layer for all models that involve DNN'], [[(('parameter', 'dropout'), 2, 9), (('artifact', 'method'), 10, 16), (('v number', '0.5'), 66, 69), (('v number', '0.2'), 103, 106)], 'A dropout method is also applied across all models with a rate of 0.5 for the MovieLens-1M dataset and 0.2 for the other two datasets to prevent overfitting'], [[(('parameter', 'layers'), 33, 39), (('v number', '2'), 54, 55)], 'More specifically, the number of layers in DCN set to 2'], [[(('artifact', 'model'), 32, 37), (('v number', '64'), 57, 59)], 'The attention embedding size of model AFM and AutoInt is 64'], [[(('artifact', 'model'), 8, 13), (('v number', '2'), 59, 60)], 'For our model DCAP, we set the maximum depth of network to 2 as a bounded order of feature interactions.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 46, 56), (('parameter', 'epochs'), 105, 111), (('v number', '64'), 68, 70), (('v number', '2'), 103, 104)], 'For all models, teacher forcing was used, the batch size was set to 64, and training was conducted for 2 epochs, when the losses converged'], [[(('artifact', 'Adam'), 4, 8), (('v number', '0.0005'), 71, 77), (('v number', '0.9'), 78, 81), (('v number', '0.999'), 82, 87)], 'The Adam optimizer was used for all models, with the parameters , _, _=0.0005,0.9,0.999 '], [[(('parameter', 'epochs'), 72, 78), (('v number', '0'), 58, 59), (('v number', '0'), 64, 65), (('v number', '2'), 70, 71)], 'For CLSM or VAE, we conducted KL-annealing linearly from =0 or =0 for 2 epochs .']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 52, 58), (('parameter', 'layers'), 77, 83), (('v number', '4'), 20, 21), (('v number', '768'), 22, 25), (('v number', '3'), 50, 51)], 'In FAN-b, we used a 4 768 LSTM in the encoder and 3 layers of self-attention layers in each decoder'], [[(('parameter', 'layers'), 36, 42), (('parameter', 'layers'), 76, 82), (('parameter', 'layers'), 131, 137), (('v number', '12'), 18, 20), (('v number', '5'), 59, 60), (('v number', '3'), 114, 115)], 'In FAN-c, we used 12 self-attention layers in the encoder, 5 self-attention layers in the slot value decoder, and 3 self-attention layers in the slot tag decoder'], [[(('artifact', 'model'), 22, 27), (('v number', '256'), 39, 42)], 'The input dimension d-model was set to 256'], [[(('parameter', 'layers'), 23, 29), (('v number', '4'), 39, 40), (('v number', '2'), 53, 54), (('v number', '048'), 55, 58)], 'For all self-attention layers, we used 4 heads and a 2,048-hidden unit feedforward net is used']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 52, 65), (('v number', '55000'), 18, 23), (('v number', '2'), 84, 85), (('v number', '0.0001'), 86, 92)], 'We train SRFN for 55000 iterations on CAVE, and the learning rate is initialized to 2 0.0001 '], [[(('parameter', 'learning rate'), 96, 109), (('v number', '20000'), 24, 29), (('v number', '10000'), 51, 56), (('v number', '4'), 120, 121), (('v number', '0.0001'), 122, 128)], 'The SRFN is trained for 20000 iteration on WV2 and 10000 iteration on Pavias, while the initial learning rate is set to 4 0.0001 ']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 143, 148), (('v number', '4000'), 103, 107), (('v number', '1500'), 152, 156)], 'For all test-time learning variants, we limit the maximum number of questions generated per context to 4000 and the maximum number of training steps to 1500'], [[(('parameter', 'steps'), 23, 28), (('parameter', 'batch size'), 67, 77), (('v range', '[16, 64]'), 78, 86)], 'The number of training steps is linearly dependent on the selected batch size [16, 64] '], [[(('parameter', 'K'), 8, 9), (('v number', '500'), 104, 107)], 'For our K -neighbor TTL setup that uses Context Expansion, we limit the number of retrieved contexts to 500'], [[(('parameter', 'learning rates'), 21, 35), (('v number', '1e-5'), 53, 57), (('v number', '5e-5'), 61, 65)], 'We evaluate multiple learning rates within the range 1e-5 to 5e-5'], [[(('artifact', 'Adam'), 11, 15), (('v number', '384'), 86, 89)], 'We use the Adam optimizer and truncate the paragraphs to a maximum sequence length of 384']] \n",
      "\n",
      "[[[(('parameter', 'T'), 135, 136), (('parameter', 'T'), 139, 140), (('parameter', 'T'), 148, 149), (('artifact', 'model'), 179, 184), (('v number', '0.2'), 141, 144), (('v number', '0.4'), 150, 153)], 'We trained all of the models with decimated key frames in the training split once i.e., every sequential example whose key frame is at t , t+0.2 s, t+0.4 s, , is used once during model training.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 207, 220), (('parameter', 'epochs'), 238, 244), (('v number', '32'), 42, 44), (('v number', '16'), 58, 60), (('v number', '0.9'), 175, 178), (('v number', '200'), 234, 237)], \"All models are trained in mixed-precision 32-bit weights, 16-bit gradients using Stochastic Gradient Descent SGD with Nesterov's acceleration with momentum coefficient set to 0.9 and cosine annealing of the learning rate schedule for 200 epochs\"]] \n",
      "\n",
      "[[[(('parameter', 'layers'), 36, 42), (('parameter', 'layer'), 67, 72), (('v number', '16'), 54, 56)], 'Our discriminator models have three layers each, with 16 nodes per layer'], [[(('parameter', 'layers'), 4, 10), (('parameter', 'layer'), 78, 83), (('v number', '0.2'), 52, 55)], 'All layers use LeakyReLU activations with a leak of 0.2 , except for the last layer, which is Sigmoid-activated']] \n",
      "\n",
      "[[[(('parameter', 'm'), 52, 53), (('v number', '224'), 56, 59)], 'In the experiments, all the images are resized with m = 224 '], [[(('parameter', 'K'), 126, 127), (('v number', '32'), 130, 132), (('v number', '16'), 137, 139)], 'Two variants of the proposed Vision Transformer Hashing VTS are used for experiments, i.e., VTS32 and VTS16 having patch size k = 32 and 16, respectively'], [[(('artifact', 'L'), 33, 34), (('v number', '12'), 84, 86)], 'The number of transformer blocks L as well as the number of attention heads A_h are 12 in the VTS models'], [[(('parameter', 'epochs'), 35, 41), (('parameter', 'batch size'), 49, 59), (('artifact', 'Adam'), 72, 76), (('parameter', 'learning rate'), 94, 107), (('v number', '150'), 31, 34), (('v number', '32'), 63, 65)], 'All the models are trained for 150 epochs with a batch size of 32 using Adam optimizer with a learning rate of 1e^ '], [[(('parameter', 'epochs'), 37, 43), (('v number', '30'), 34, 36)], 'The testing is performed at every 30 epochs and the best results are reported.']] \n",
      "\n",
      "[[[(('artifact', 'Triplet Loss'), 68, 80), (('v number', '50'), 127, 129)], 'The third and fourth instances, model3 and model4, are trained with triplet loss using feature embeddings obtained from ResNet-50 and MobileFaceNet, respectively as an ablation study to our proposed srt'], [[(('parameter', 'learning rate'), 56, 69), (('parameter', 'batch size'), 82, 92), (('v number', '1e-1'), 73, 77), (('v number', '512'), 96, 99)], 'All models are trained using SGD optimizer with initial learning rate of 1e-1 and batch size of 512'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '10'), 32, 34)], 'The learning rate is divided by 10 at 30k, 60k, ,90k training iterations']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 19, 32), (('parameter', 'K'), 106, 107), (('v number', '0.01'), 40, 44), (('v number', '0.6'), 102, 105)], 'We used an initial learning rate of _ = 0.01 and exponentially decayed that rate by computing: _ = _ -0.6 k,']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 47, 54), (('v number', '0.6'), 55, 58)], 'In order to prohibit overfitting, we have used dropout=0.6 along with Weight Regularization, Batch Normalization and Early Stopping']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 128, 141), (('parameter', 'epochs'), 149, 155), (('v number', '100'), 86, 89), (('v number', '3'), 108, 109), (('v number', '6'), 114, 115), (('v number', '0.05'), 143, 147), (('v number', '5'), 157, 158)], 'The skip-gram embeddings were trained using default parameters vector dimensionality: 100; subword: between 3 and 6 characters; learning rate: 0.05; epochs: 5'], [[(('parameter', 'epochs'), 26, 32), (('parameter', 'learning rate'), 162, 175), (('v number', '25'), 23, 25), (('v number', '100'), 41, 44), (('v number', '0.05'), 177, 181)], 'The NN was trained for 25 epochs for all 100 classes using the corpus specific pre-trained skip-gram embeddings from the step before, applying default parameters learning rate: 0.05.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 54, 67), (('parameter', 'batch size'), 77, 87), (('parameter', 'epoch'), 104, 109), (('v number', '2e-5'), 71, 75), (('v number', '16'), 91, 93), (('v number', '10'), 113, 115), (('v number', '128'), 144, 147)], 'For the fine-tuning of the BioBERT models, we use the learning rate of 2e-5, batch size of 16, training epoch of 10, and max sequence length of 128'], [[(('parameter', 'learning rate'), 49, 62), (('parameter', 'batch size'), 72, 82), (('parameter', 'epoch'), 98, 103), (('v number', '2e-5'), 66, 70), (('v number', '8'), 86, 87), (('v number', '10'), 107, 109), (('v number', '256'), 137, 140)], 'During the fine-tuning of PubMedBERT models, the learning rate of 2e-5, batch size of 8, training epoch of 10 and max sequence length of 256 are utilized.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 74, 87), (('parameter', 'weight decay'), 96, 108), (('v number', '0.1'), 91, 94), (('v number', '5e-4'), 112, 116), (('v number', '0.9'), 138, 141)], 'We train it using Stochastic Gradient Descent Optimizer with the constant learning rate of 0.1, weight decay of 5e-4, and the momentum of 0.9'], [[(('parameter', 'batch size'), 25, 35), (('parameter', 'epochs'), 61, 67), (('v number', '10'), 10, 12), (('v number', '128'), 39, 42), (('v number', '100'), 57, 60)], 'For CIFAR-10, we use the batch size of 128 and train for 100 epochs'], [[(('parameter', 'epochs'), 25, 31), (('parameter', 'batch size'), 52, 62), (('v number', '256'), 79, 82)], 'For GTSRB, the number of epochs is the same but the batch size is increased to 256.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 92, 97), (('v number', '90'), 33, 35), (('v number', '10'), 47, 49)], 'We splitted our training sets in 90% train and 10% validation, and used the best performing model on the validation set for the final test, following the validation strategy described in Section dsam:sec:methodology'], [[(('parameter', 'batch size'), 10, 20), (('parameter', 'learning rate'), 114, 127), (('v number', '96'), 24, 26), (('v number', '32'), 27, 29), (('v number', '0.9'), 98, 101), (('v number', '0.01'), 131, 135), (('v number', '0.007'), 140, 145)], \"We used a batch size of 96 32 images per source domain and trained using SGD with momentum set at 0.9 and initial learning rate at 0.01 and 0.007 for ResNet's and AlexNet's experiments respectively\"], [[(('parameter', 'epoch'), 17, 22), (('parameter', 'steps'), 48, 53), (('parameter', 'epochs'), 139, 145), (('parameter', 'learning rate'), 159, 172), (('parameter', 'epochs'), 201, 207), (('v number', '30'), 136, 138), (('v number', '0.2'), 188, 191), (('v number', '10'), 198, 200)], 'We considered an epoch as the minimum number of steps necessary to iterate over the largest source domain and we trained our models for 30 epochs, scaling the learning rate by a factor of 0.2 every 10 epochs']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 104, 117), (('v number', '100'), 121, 124)], 'In terms of models, all our experiments use the same three models: A multilayer perceptron MLP with two hidden layers of 100 neurons each, the standard LeNet5 from , and the standard ResNet18 ']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 9, 16), (('parameter', 'layer'), 50, 55), (('parameter', 'layer'), 78, 83), (('v number', '0.1'), 25, 28)], 'We use a dropout rate of 0.1 on the final encoder layer and fix the embedding layer during fine-tuning']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 34, 40), (('parameter', 'batch size'), 48, 58), (('parameter', 'weight decay'), 75, 87), (('v number', '200'), 30, 33), (('v number', '128'), 62, 65), (('v number', '1e-3'), 101, 105)], 'The fine-tuning process takes 200 epochs with a batch size of 128, and the weight decay is set to be 1e-3'], [[(('parameter', 'learning rates'), 4, 18), (('parameter', 'epochs'), 149, 155), (('v number', '16'), 26, 28), (('v number', '3856164'), 40, 47), (('v number', '0.006'), 63, 68), (('v number', '0.05'), 73, 77), (('v number', '0.28'), 97, 101), (('v number', '0.14'), 106, 110), (('v number', '40'), 114, 116), (('v number', '80'), 122, 124)], 'The learning rates of VGG-16 and ResNet-3856164 are started as 0.006 and 0.05, and multiplied by 0.28 and 0.14 at 40% and 80% of the total number of epochs']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 26, 32), (('artifact', 'Adam'), 43, 47), (('parameter', 'learning rate'), 69, 82), (('v number', '4'), 24, 25), (('v number', '0.001'), 63, 68)], 'Each MLP is trained for 4 epochs using the Adam optimizer with 0.001 learning rate'], [[(('parameter', 'batch size'), 4, 14), (('v number', '64'), 18, 20)], 'The batch size is 64']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 37, 50), (('v number', '0.25'), 51, 55), (('v number', '12'), 76, 78)], 'We use an SGD optimizer with initial learning rate 0.25 and a batch-size of 12'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 62, 67), (('v number', '0.99'), 46, 50), (('v number', '1000'), 57, 61)], 'The learning rate is decreased by a factor of 0.99 every 1000 steps']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 37, 41), (('parameter', 'learning rate'), 58, 71), (('parameter', 'steps'), 93, 98), (('parameter', 'steps'), 105, 110), (('parameter', 'batch size'), 123, 133), (('parameter', 'learning rate'), 194, 207), (('v number', '4'), 79, 80), (('v number', '2048'), 137, 141), (('v number', '4'), 217, 218), (('v number', '5'), 226, 227)], 'We pre-train VATT from scratch using Adam with an initial learning rate of 1e -4, 10k warmup steps, 500k steps in total, a batch size of 2048, and a quarter-period cosine schedule to anneal the learning rate from 1e -4 to 5e -5'], [[(('parameter', 'batch size'), 41, 51), (('v number', '512'), 55, 58)], 'In the exploration experiments, we use a batch size of 512 while keeping the rest of the training parameters the same']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 19, 23), (('parameter', 'learning rate'), 74, 87), (('v number', '4e-4'), 91, 95)], 'We train using the Adam optimizer with default parameters, and an initial learning rate of 4e-4 '], [[(('parameter', 'learning rate'), 14, 27), (('v number', '0.5'), 43, 46)], 'We reduce the learning rate by a factor of 0.5 every 100k iterations'], [[(('parameter', 'steps'), 31, 36), (('v number', '1000'), 60, 64)], 'We therefore skip all training steps with norm greater than 1000 entirely.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 0, 7), (('v number', '0.25'), 41, 45)], 'Dropout is applied with a probability of 0.25 everywhere'], [[(('artifact', 'Adam'), 51, 55), (('parameter', 'weight decay'), 64, 76), (('parameter', 'epochs'), 86, 92), (('parameter', 'epochs'), 109, 115), (('v number', '40'), 83, 85), (('v number', '10'), 106, 108)], 'We optimize the parameters by backpropagation with Adam without weight decay, over 40 epochs for DEFT and 10 epochs for GENIA'], [[(('parameter', 'steps'), 20, 25), (('artifact', 'model'), 113, 118), (('v number', '4'), 75, 76), (('v number', '1e-05'), 77, 82), (('v number', '9'), 135, 136), (('v number', '0.001'), 137, 142)], 'We use two learning steps: one for the Transformer weights, initialized at 4 1e-05 , and one for the rest of the model, initialized at 9 0.001 '], [[(('parameter', 'learning rate'), 4, 17), (('artifact', 'linear decay'), 28, 40), (('parameter', 'steps'), 79, 84), (('v number', '10'), 68, 70)], 'The learning rate follows a linear decay schedule with a warmup for 10% of the steps'], [[(('parameter', 'layer'), 52, 57), (('parameter', 'layers'), 82, 88), (('parameter', 'layers'), 113, 119), (('v number', '6'), 63, 64), (('v number', '12'), 79, 81), (('v number', '19'), 93, 95), (('v number', '24'), 110, 112)], 'We insert the tag embeddings in the Transformers at layer L_ = 6 for BERT with 12 layers and 19 for BERT with 24 layers']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 91, 96), (('parameter', 'learning rate'), 107, 120), (('parameter', 'batch size'), 133, 143), (('v number', '30'), 103, 105), (('v number', '1e-5'), 127, 131), (('v number', '64'), 150, 152), (('v number', '4'), 188, 189)], 'Our neural retrievers were trained on eight Nvidia RTX8000 GPUs, where we set the training epoch to be 30, learning rate lr be 1e-5, batch size bs be 64, gradient accumulation step gas be 4'], [[(('parameter', 'epoch'), 68, 73), (('v number', '3'), 77, 78), (('v number', '2e-5'), 86, 90), (('v number', '16'), 110, 112)], 'For both Image-DPR and Caption-DPR, In CReader, we set the training epoch as 3, lr as 2e-5, and batch-size as 16'], [[(('parameter', 'epoch'), 32, 37), (('v number', '3'), 41, 42), (('v number', '1e-5'), 50, 54), (('v number', '4'), 70, 71), (('v number', '4'), 102, 103)], 'In EReader, we set the training epoch as 3, lr as 1e-5, batch-size as 4, and gradient accumulation as 4.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('artifact', 'model'), 59, 64)], 'We use the Adam optimizer with _1 = , _2 = to optimize the model'], [[(('parameter', 'learning rate'), 18, 31), (('parameter', 'learning rate'), 81, 94), (('parameter', 'steps'), 104, 109), (('v number', '4000'), 99, 103), (('v number', '2e-4'), 113, 117)], 'Further, the same learning rate schedule as is used, i.e., linearly increase the learning rate for 4000 steps to 2e-4 and decay proportionally to the inverse square root of the step number'], [[(('parameter', 'batch size'), 18, 28), (('v number', '9'), 32, 33), (('v number', '600'), 34, 37)], 'We accumulate the batch size to 9,600 and adopt half-precision training implemented by apexhttps:github.comNVIDIAapex for faster convergence '], [[(('parameter', 'steps'), 97, 102), (('v number', '256'), 31, 34), (('v number', '100'), 93, 96)], \"As for our approach, we sample 256 candidates from each languages' development corpora every 100 steps to calculate the Self-evaluated Competence c for each language and HRLs-evaluated Competence for each LRL.\"]] \n",
      "\n",
      "[[[(('parameter', 'steps'), 54, 59), (('parameter', 'steps'), 65, 70), (('parameter', 'steps'), 96, 101), (('v number', '1000'), 60, 64), (('v number', '10000'), 90, 95)], 'The training process starts after some number of time steps 1000 steps for Reacher-v2 and 10000 steps for the other environments, and we use a purely exploratory policy in this initial phase to collect samples for the replay buffer for all the algorithms']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 100, 110), (('v number', '0.9'), 88, 91), (('v number', '256'), 121, 124)], 'We train our compact CNN models from scratch using the SGD optimizer with a momentum of 0.9 and the batch size is set to 256'], [[(('parameter', 'epochs'), 58, 64), (('parameter', 'weight decay'), 73, 85), (('v number', '10'), 9, 11), (('v number', '300'), 54, 57), (('v number', '5'), 96, 97), (('v number', '10'), 98, 100)], 'On CIFAR-10, we train the compact CNNs for a total of 300 epochs and the weight decay is set to 5 10^} '], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 104, 110), (('v number', '0.1'), 38, 41), (('v number', '10'), 63, 65), (('v number', '150'), 92, 95), (('v number', '225'), 100, 103)], 'The learning rate is initially set to 0.1, and then divided by 10 at the training points of 150 and 225 epochs'], [[(('parameter', 'epochs'), 19, 25), (('parameter', 'weight decay'), 72, 84), (('parameter', 'learning rate'), 116, 129), (('parameter', 'epochs'), 213, 219), (('v number', '2012'), 10, 14), (('v number', '90'), 16, 18), (('v number', '50'), 60, 62), (('v number', '1'), 92, 93), (('v number', '10'), 94, 96), (('v number', '0.1'), 140, 143), (('v number', '0.1'), 173, 176), (('v number', '30'), 194, 196), (('v number', '60'), 201, 203)], 'On ILSVRC-2012, 90 epochs are given to train compact ResNet-50 with the weight decay set to 1 10^ , and the initial learning rate is set to 0.1, which is then multiplied by 0.1 at the points of 30 and 60 training epochs'], [[(('parameter', 'learning rate'), 77, 90), (('parameter', 'weight decay'), 114, 126), (('v number', '50'), 102, 104), (('v number', '1'), 134, 135), (('v number', '10'), 136, 138)], 'Besides, following , , , we also consider the cosine scheduler to adjust the learning rate for ResNet-50 with the weight decay set to 1 10^ '], [[(('parameter', 'learning rate'), 12, 25), (('v number', '1'), 36, 37), (('v number', '10'), 38, 40), (('v number', '50'), 53, 55)], 'The initial learning rate is set to 1 10^ for ResNet-50.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 38, 43), (('parameter', 'layer'), 131, 136), (('v number', '2'), 58, 59), (('v number', '768'), 100, 103), (('v number', '3072'), 142, 146), (('v number', '12'), 151, 153)], 'The transformer network in GlobalPair model is made up of 2 transformer blocks, with hidden size of 768, feed-forward intermediate layer size 3072 and 12 attention heads'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'batch size'), 28, 38), (('v number', '2'), 42, 43)], 'We use Adam optimizer and a batch size of 2 for all three datasets'], [[(('parameter', 'learning rate'), 25, 38), (('parameter', 'epoch'), 83, 88), (('v number', '5'), 47, 48), (('v number', '1e-06'), 49, 54), (('v number', '0.2'), 75, 78)], 'For SIND and ROCStories, learning rate used is 5*1e-06 and decay factor is 0.2 per epoch'], [[(('parameter', 'learning rate'), 104, 117), (('parameter', 'epoch'), 160, 165), (('v number', '1'), 121, 122), (('v number', '1e-05'), 123, 128), (('v number', '1'), 142, 143), (('v number', '1e-06'), 144, 149)], 'For ACL, we found the training to be quite stochastic but the final results reported are for an initial learning rate of 1*1e-05 , decayed to 1*1e-06 after 1st epoch and constant afterwards']] \n",
      "\n",
      "[[[(('artifact', 'model'), 19, 24), (('v number', '1.2'), 39, 42)], 'We implemented the model using PyTorch 1.2 and trained it on four NVIDIA Tesla P100 GPUs'], [[(('artifact', 'Adam'), 12, 16), (('parameter', 'learning rate'), 34, 47), (('parameter', 'epoch'), 108, 113), (('v number', '0.001'), 51, 56), (('v number', '0.7'), 93, 96)], 'We used the Adam optimizer with a learning rate of 0.001, decreasing the rate by a factor of 0.7 after each epoch'], [[(('parameter', 'epochs'), 48, 54), (('v number', '9.5'), 65, 68)], 'Our test models generally converged after seven epochs, or about 9.5 hours of training.']] \n",
      "\n",
      "[[[(('artifact', 'method'), 122, 128), (('v number', '50'), 36, 38), (('v number', '20'), 71, 73)], 'By default, we assume there are n = 50 clients in total for each task, 20% of which are Byzantine nodes with fixed attack method, and the training data are IID among clients'], [[(('artifact', 'L'), 74, 75), (('v number', '0.1'), 78, 81), (('v number', '3.0'), 90, 93), (('v number', '10'), 116, 118)], 'In all experiments, we set the lower and upper bounds of gradient norm as L = 0.1 and R = 3.0 , and randomly select 10% of coordinates to compute sign statistics in our SignGuard-based algorithms'], [[(('parameter', 'epochs'), 38, 44), (('parameter', 'epochs'), 83, 89), (('v number', '60'), 35, 37), (('v number', '160'), 79, 82), (('v number', '10'), 100, 102), (('v number', '1'), 141, 142)], 'Each training procedure is run for 60 epochs for MNISTFashion-MNISTAG-News and 160 epochs for CIFAR-10, and local iteration is always set to 1'], [[(('parameter', 'weight decay'), 76, 88), (('v number', '0.9'), 67, 70), (('v number', '0.0005'), 99, 105)], 'We employ momentum in PS side and the momentum parameter is set to 0.9, and weight decay is set to 0.0005']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 57, 62), (('parameter', 'layer'), 94, 99), (('parameter', 'layer'), 205, 210), (('v number', '2'), 55, 56), (('v number', '4096'), 117, 121), (('v number', '256'), 233, 236)], 'Both the projection network and prediction network are 2-layer MLPs which consist of a linear layer with output size 4096 followed by batch normalization , rectified linear units ReLU , and a final linear layer with output dimension 256.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 43, 44), (('parameter', 'epochs'), 57, 63), (('parameter', 'epoch'), 118, 123), (('parameter', 'epoch'), 217, 222), (('v number', '64'), 47, 49), (('v number', '50'), 54, 56), (('v number', '16'), 99, 101), (('v number', '1600'), 136, 140), (('v number', '16'), 141, 143), (('v number', '100'), 144, 147), (('v number', '160'), 179, 182), (('v number', '16'), 183, 185), (('v number', '10'), 186, 188)], 'We train all agents on minibatches of size k = 64 for 50 epochs using MPI for parallelisation over 16 CPU cores; each epoch consists of 1600 16 100 episodes, with evaluation over 160 16 10 episodes at the end of each epoch'], [[(('artifact', 'method'), 4, 10), (('parameter', 'm'), 65, 66), (('v number', '2'), 59, 60), (('v number', '100'), 69, 72)], 'Our method, DTGSH, uses partial trajectories of length b = 2 and m = 100 as the number of candidate goals.']] \n",
      "\n",
      "[[[(('parameter', 'T'), 12, 13), (('v number', '2'), 7, 8), (('v number', '2'), 19, 20)], 'It is -2 }}^T ^b + 2 p '], [[(('parameter', 'T'), 37, 38), (('v number', '0'), 17, 18)], 'By setting it to 0, we will get = }}^T ^b} ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 25, 30), (('v number', '529'), 120, 123)], 'The training set for the model trainer contains these perturbed pictures, as well as the training pictures of all other 529 FaceScrub users.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 100, 106), (('v number', '6'), 78, 79), (('v number', '8'), 112, 113), (('v number', '512'), 148, 151), (('v number', '2048'), 178, 182)], 'We follow the Transformer base configuration in all our experiments, with N = 6 encoder and decoder layers, h = 8 attention heads, hidden size d_ = 512 and feedforward size d_ = 2048 '], [[(('artifact', 'Adam'), 19, 23), (('v number', '0.9'), 44, 47), (('v number', '0.98'), 54, 58)], 'We train using the Adam optimizer with _1 = 0.9, _2 = 0.98 .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 61, 66), (('v number', '1'), 33, 34), (('v number', '128'), 72, 75)], 'The baseline pointer network has 1 encoder and decoder BLSTM layer with 128 hidden size'], [[(('parameter', 'learning rate'), 18, 31), (('artifact', 'Adam'), 44, 48), (('v number', '0.0003'), 32, 38), (('v number', '32'), 64, 66)], 'We train it using learning rate 0.0003 with Adam and batch_size 32'], [[(('artifact', 'model'), 17, 22), (('v number', '2'), 33, 34), (('v number', '1'), 66, 67)], 'For our proposed model we employ 2 encoders one for each turn and 1 decoder with the same dimension, but modified attention head'], [[(('parameter', 'batch size'), 17, 27), (('parameter', 'learning rate'), 36, 49), (('artifact', 'Adam'), 62, 66), (('v number', '128'), 28, 31), (('v number', '0.0001'), 50, 56)], 'We train it with batch size 128 and learning rate 0.0001 with Adam']] \n",
      "\n",
      "[[[(('parameter', 'fold'), 34, 38), (('parameter', 'fold'), 77, 81), (('parameter', 'epochs'), 148, 154), (('parameter', 'learning rate'), 161, 174), (('parameter', 'batch size'), 184, 194), (('v number', '5'), 32, 33), (('v number', '10'), 74, 76), (('v number', '5'), 158, 159), (('v number', '1e-5'), 178, 182), (('v number', '2'), 198, 199)], 'We train the models by applying 5-Fold Cross ValidationWe experiment also 10-Fold, but the models show worse performance in the test set., with the epochs of 5, learning rate as 1e-5, batch size as 2']] \n",
      "\n",
      "[[[(('parameter', 'T'), 120, 121), (('parameter', 'layers'), 124, 130), (('parameter', 'layers'), 142, 148), (('v number', '6'), 122, 123)], 'We will approximate the posterior distribution P_ for arbitrary observations y using a conditional SNF X_0,...,X_T with T=6 layers, where the layers themselves are defined as follows:']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 8, 12), (('parameter', 'learning rate'), 29, 42), (('v number', '1'), 46, 47), (('v number', '0.001'), 48, 53)], 'We used Adam optimizer and a learning rate of 1 0.001 with an inverse square root scheduler'], [[(('parameter', 'learning rate'), 67, 80), (('v number', '2'), 90, 91), (('v number', '0.001'), 92, 97)], 'In ST trainings we use the same parameters as for ASR, but for the learning rate, that is 2 0.001 , as done in ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 44, 49), (('artifact', 'Adam'), 80, 84), (('parameter', 'learning rate'), 102, 115), (('parameter', 'batch size'), 128, 138), (('v number', '0.001'), 119, 124), (('v number', '64'), 142, 144), (('v number', '20'), 160, 162)], 'Training Process: Each configuration of the model architecture is trained using Adam optimizer with a learning rate of 0.001, a batch size of 64 for maximum of 20 iterations']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 39, 43), (('v number', '1'), 119, 120), (('v number', '0.9'), 123, 126), (('v number', '2'), 128, 129), (('v number', '0.999'), 131, 136)], 'The parameters are optimized using the Adam optimization algorithm with the suggested defaults for the hyperparameters 1 = 0.9, 2= 0.999'], [[(('parameter', 'epochs'), 84, 90), (('v number', '1'), 70, 71)], 'The training is stopped when the validation loss does not decrease by 1% in between epochs'], [[(('parameter', 'learning rate'), 15, 28), (('parameter', 'weight decay'), 42, 54), (('v number', '0.001'), 32, 37), (('v number', '0.0001'), 55, 61)], 'We use a fixed learning rate of 0.001 and weight decay 0.0001 '], [[(('parameter', 'epoch'), 123, 128), (('v number', '5'), 29, 30), (('v number', '2'), 72, 73), (('v number', '6350'), 102, 106)], 'At each iteration, we sample 5 patch pairs or triplets for pretext Task 2 from each image to generate 6350 patch pairs per epoch']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 22, 26), (('parameter', 'steps'), 78, 83)], 'Optimization: We used Adam optimizer with a linear scheduler that has warm-up steps'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.001'), 40, 45), (('v number', '1e-05'), 64, 69)], 'The initial learning rate was set to be 0.001 for BiMeanVAE and 1e-05 for Optimus.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 24, 37), (('parameter', 'epochs'), 56, 62), (('v number', '4'), 138, 139)], 'Hyperparameters such as learning rate and the number of epochs were tuned with the help of validation data with patience parameter set to 4'], [[(('parameter', 'learning rate'), 0, 13), (('v number', '0.001'), 42, 47), (('v number', '0.0001'), 50, 56), (('v number', '1e-05'), 59, 64)], 'Learning rate search space was limited to 0.001 , 0.0001 , 1e-05 values only'], [[(('artifact', 'system'), 21, 27), (('artifact', 'system'), 83, 89), (('v number', '56'), 98, 100)], \"We trained BERT on a system with a `Nvidia Tesla P100' GPU with 12GB GPU-RAM, 96GB system RAM and 56 cores\"], [[(('artifact', 'model'), 4, 9), (('v number', '20'), 57, 59)], 'Our model GLEN was trained on CPU only with 32GB RAM and 20 cores'], [[(('parameter', 'K'), 42, 43), (('v number', '664'), 38, 41), (('v number', '165'), 53, 56)], 'The number of parameters for GLEN was 664 k which is 165 times smaller than BERT in terms of number of parameters']] \n",
      "\n",
      "[[[(('parameter', 'T'), 3, 4), (('parameter', 'T'), 22, 23), (('v number', '1'), 16, 17)], 'St,t = 4St e-t2 1 - e-t ,']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 94, 99), (('v number', '100'), 78, 81)], 'Each actor generates trajectories, which are sent to the learner in chunks of 100 environment steps']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 11, 21), (('v number', '32'), 25, 27), (('v number', '96'), 135, 137)], 'We set the batch size to 32 images per source domain: since in all the testbed there are three source domains each data batch contains 96 images'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.001'), 64, 69), (('v number', '0.0001'), 74, 80)], 'The learning rate and the weigh decay are respectively fixed to 0.001 and 0.0001 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 62, 72), (('parameter', 'learning rate'), 85, 98), (('v number', '32'), 76, 78), (('v number', '1'), 102, 103), (('v number', '1e-05'), 104, 109)], 'We use the same training hyperparameters as CoOp, including a batch size of 32 and a learning rate of 1 1e-05 for all datasets except for the residual ratio '], [[(('parameter', 'layers'), 71, 77), (('v number', '256'), 88, 91)], 'The hidden embedding dimensionality of both visual and text bottleneck layers is set to 256, which is a quarter of the original embedding dimensionality']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 34, 39), (('parameter', 'epochs'), 58, 64), (('parameter', 'learning rate'), 94, 107), (('parameter', 'epochs'), 125, 131), (('v number', '300'), 54, 57), (('v number', '20'), 122, 124)], 'To be more specific, we employ an AdamW optimizer for 300 epochs together with a cosine decay learning rate scheduler and 20 epochs of linear warm-up'], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'learning rate'), 47, 60), (('v number', '1024'), 25, 29), (('v number', '0.001'), 64, 69)], 'The batch size is set to 1024, and the initial learning rate is 0.001'], [[(('parameter', 'weight decay'), 24, 36), (('artifact', 'method'), 65, 71), (('v number', '0.05'), 45, 49)], 'To avoid overfitting, a weight decay rate of 0.05 is used in our method']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 50, 56), (('parameter', 'activation'), 85, 95), (('v number', '64'), 70, 72)], 'For ProNet, we used an FNN-based encoder with two layers each of size 64 and ReLu as activation function'], [[(('parameter', 'layers'), 52, 58), (('parameter', 'activation'), 87, 97), (('v number', '64'), 72, 74)], 'For ProSeNet, we used an RNN-based encoder with two layers each of size 64 and tanh as activation function'], [[(('parameter', 'layers'), 25, 31), (('parameter', 'activation'), 60, 70), (('v number', '64'), 45, 47)], 'The FNN baseline had two layers each of size 64 and ReLu as activation function'], [[(('parameter', 'layers'), 23, 29), (('parameter', 'activation'), 58, 68), (('v number', '2'), 21, 22), (('v number', '64'), 43, 45)], 'The RNN baseline had 2 layers each of size 64 and tanh as activation function']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 39, 52), (('v number', '0.001'), 56, 61), (('v number', '5'), 96, 97)], 'An optimizer RMSprop was chosen with a learning rate of 0.001 , which was reduced with patience 5'], [[(('parameter', 'epochs'), 14, 20), (('parameter', 'steps'), 30, 35), (('parameter', 'epoch'), 40, 45), (('v number', '20'), 11, 13), (('v number', '100'), 26, 29)], 'There were 20 epochs with 100 steps per epoch'], [[(('parameter', 'batch size'), 4, 14), (('v number', '30'), 35, 37), (('v number', '256'), 60, 63), (('v number', '256'), 64, 67)], 'The batch size was specified to be 30 with an image size of 256*256 pixels'], [[(('parameter', 'batch size'), 4, 14), (('artifact', 'model'), 62, 67)], 'The batch size and image size were the same as for the simple model'], [[(('parameter', 'epochs'), 32, 38), (('parameter', 'steps'), 44, 49), (('parameter', 'epoch'), 54, 59), (('parameter', 'batch size'), 69, 79), (('v number', '600'), 28, 31), (('v number', '100'), 40, 43), (('v number', '30'), 83, 85)], 'The models were trained for 600 epochs, 100 steps per epoch, and the batch size of 30.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 147, 152), (('v number', '4.5'), 49, 52), (('v number', '0.01'), 130, 134)], 'For the ground truth occupancy points, we sample 4.5 million surface points and apply Gaussian noise with a standard deviation of 0.01 to help the model learn better surface details']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 49, 53), (('v number', '0.9'), 89, 92), (('v number', '0.999'), 98, 103)], 'Neural network training is always done using the Adam-optimizer with default settings _1=0.9 , _2=0.999 '], [[(('parameter', 'learning rate'), 32, 45), (('v number', '0.001'), 49, 54), (('v number', '10'), 71, 73), (('v number', '0.0001'), 77, 83), (('v number', '100'), 90, 93)], 'Depending on the benchmark, the learning rate is 0.001 MNIST and CIFAR-10 or 0.0001 CIFAR-100 and CORe50.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 157, 163), (('v number', '12'), 130, 132), (('v number', '12'), 142, 144), (('v number', '768'), 165, 168), (('v number', '12'), 188, 190)], 'For the intent classification, we employ the Bert-Base modelhttps:storage.googleapis.combert_models2020_02_20uncased_L-12_H-768_A-12.zip with 12 Transformer layers, 768 hidden states, and 12 self-attention heads'], [[(('parameter', 'layer'), 82, 87), (('v number', '512'), 54, 57), (('v number', '600'), 102, 105), (('v number', '5'), 145, 146)], 'The size of the hidden units in uni-direction LSTM is 512, inner-attention hidden layer d_w is set to 600, and the number of attention head r is 5'], [[(('artifact', 'Adam'), 20, 24), (('parameter', 'learning rate'), 89, 102), (('artifact', 'BiLSTM'), 137, 143), (('artifact', 'model'), 170, 175), (('v number', '0.9'), 63, 66), (('v number', '0.99'), 76, 80), (('v number', '1e-4'), 106, 110), (('v number', '2e-5'), 115, 119)], 'Furthermore, we use Adam optimizer with default values of _1 = 0.9 and _2 = 0.99 , and a learning rate of 1e-4 and 2e-5 for training the BiLSTM and fine-tuning the whole model respectively'], [[(('parameter', 'batch size'), 34, 44), (('parameter', 'epochs'), 92, 98), (('parameter', 'epochs'), 131, 137), (('v number', '8'), 48, 49), (('v number', '16'), 53, 55), (('v number', '32'), 113, 115), (('v number', '25'), 117, 119), (('v number', '16'), 121, 123), (('v number', '8'), 129, 130), (('v number', '10'), 142, 144), (('v number', '20'), 151, 153), (('v number', '30'), 160, 162)], 'Each update is computed through a batch size of 8 or 16 training examples and the number of epochs per batch are 32, 25, 16, and 8 epochs for 10-shot, 20-shot, 30-shot, and full-data settings, respectively'], [[(('parameter', 'dropout'), 13, 20), (('artifact', 'model'), 59, 64)], 'We apply the dropout as a regularization technique for our model to avoid over-fitting'], [[(('parameter', 'dropout'), 9, 16), (('artifact', 'BiLSTM'), 38, 44), (('parameter', 'layer'), 45, 50), (('parameter', 'layer'), 74, 79), (('parameter', 'layers'), 93, 99)], 'We apply dropout after output of each BiLSTM layer and output of each sub-layer BERT encoder layers'], [[(('parameter', 'dropout'), 11, 18), (('parameter', 'dropout'), 39, 46), (('parameter', 'layers'), 47, 53), (('v number', '0.1'), 27, 30)], 'We set the dropout rate as 0.1 for all dropout layers.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 47, 60), (('parameter', 'batch size'), 65, 75)], 'Empirically we have noticed that the design of learning rate and batch size will affect the training process of qKC'], [[(('artifact', 'Adam'), 8, 12), (('v number', '1'), 58, 59), (('v number', '0.9'), 147, 150), (('v number', '0.999'), 155, 160), (('v number', '2'), 176, 177), (('v number', '1e-8'), 192, 196), (('v number', '3'), 225, 226)], 'For the Adam optimiser, we use the default configuration: 1 the coefficients used for computing running mean of gradient and its square are set as 0.9 and 0.999, respectively, 2 eps is set as 1e-8 for numerical stability and 3 no L2 penalty term.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 142, 143), (('v number', '4'), 144, 145), (('v number', '2'), 150, 151), (('v number', '1'), 156, 157), (('v number', '0.05'), 162, 166), (('v number', '1e-08'), 172, 177)], 'Using the dataset and the architecture described above, two GNNs are trained using each of the techniques described in § REF and § REF , with k=4 , p=2 , q=1 , = 0.05 and =1e-08 '], [[(('artifact', 'Adam'), 28, 32), (('parameter', 'epochs'), 51, 57), (('parameter', 'learning rate'), 70, 83), (('v number', '60'), 48, 50), (('v number', '0.01'), 87, 91)], 'Training is performed using Adam algorithm with 60 epochs and a fixed learning rate of 0.01 '], [[(('artifact', 'model'), 14, 19), (('parameter', 'epoch'), 104, 109)], 'Regarding the model distillation, rather than re-evaluating the MPN F in REF for every sample X at each epoch, we precompute FX, X to save the computation time.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 3, 7), (('parameter', 'weight decay'), 25, 37), (('v number', '0.0005'), 46, 52)], 'An Adam optimizer with a weight decay rate of 0.0005 is used to optimize our networks'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 74, 80), (('v number', '0.00035'), 28, 35), (('v number', '10'), 71, 73)], 'The learning rate is set to 0.00035 with a warm-up scheme in the first 10 epochs'], [[(('parameter', 'epochs'), 75, 81), (('v number', '400'), 29, 32), (('v number', '40'), 72, 74)], 'We renew pseudo labels every 400 iterations and repeat this process for 40 epochs']] \n",
      "\n",
      "[[[(('parameter', 'm'), 24, 25), (('v number', '50'), 42, 44)], 'The embedding dimension m is chosen to be 50'], [[(('parameter', 'layer'), 35, 40), (('parameter', 'layers'), 61, 67)], 'Following the cerebellar embedding layer are fully connected layers having [32, 128, 32] hidden units with ReLU activations'], [[(('parameter', 'batch size'), 9, 19), (('parameter', 'epochs'), 50, 56), (('parameter', 'epoch'), 68, 73), (('v number', '32'), 23, 25), (('v number', '20'), 47, 49)], 'We use a batch size of 32 and run training for 20 epochs, with each epoch being one pass through the whole dataset']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 4, 8), (('parameter', 'weight decay'), 83, 95), (('parameter', 'learning rate'), 103, 116), (('parameter', 'epochs'), 171, 177), (('v number', '10'), 132, 134), (('v number', '0.5'), 157, 160), (('v number', '500'), 167, 170)], 'The Adam optimizer and L1 loss were adopted using default parameter values of zero weight decay, and a learning rate initialized to 10^ with step decay of = 0.5 after 500 epochs']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 50, 54), (('parameter', 'batch size'), 105, 115), (('parameter', 'epochs'), 137, 143), (('v number', '0.001'), 69, 74), (('v number', '1'), 76, 77), (('v number', '0.9'), 80, 83), (('v number', '2'), 85, 86), (('v number', '0.99'), 89, 93), (('v number', '1e-7'), 97, 101), (('v number', '25'), 119, 121), (('v number', '100'), 127, 130)], \"All models were trained with TensorFlow's default Adam optimizer lr= 0.001, 1 = 0.9, 2 = 0.99, = 1e-7, a batch size of 25, and 100 total epochs.\"]] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 29, 33), (('parameter', 'learning rate'), 41, 54), (('v number', '0.0001'), 58, 64)], 'As training algorithm we use Adam with a learning rate of 0.0001'], [[(('parameter', 'steps'), 46, 51), (('parameter', 'layers'), 79, 85), (('artifact', 'model'), 103, 108), (('v number', '5'), 31, 32), (('v number', '000'), 33, 36), (('v number', '30'), 39, 41), (('v number', '000'), 42, 45)], 'The models are trained between 5,000 - 30,000 steps depending on the number of layers of each specific model and the size of the dataset']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('artifact', 'method'), 23, 29)], 'The base VGG model and method for visualizing are implemented following , , which does not have batch normalization'], [[(('parameter', 'learning rate'), 43, 56), (('parameter', 'learning rate'), 87, 100), (('parameter', 'epochs'), 128, 134), (('parameter', 'learning rate'), 152, 165), (('parameter', 'learning rate'), 186, 199), (('parameter', 'epochs'), 220, 226), (('parameter', 'epochs'), 274, 280), (('v number', '0.1'), 111, 114), (('v number', '50'), 124, 126), (('v number', '0.1'), 169, 172), (('v number', '50'), 208, 210), (('v number', '90'), 216, 218), (('v number', '0.1'), 252, 255), (('v number', '10'), 270, 272)], 'For both SDP and sDprun we use the similar learning rate schedule adopted by : fix the learning rate equals to 0.1 at first 50% epochs, then reduce the learning rate to 0.1% of the base learning rate between 50% and 90% epochs, and keep reducing it to 0.1% for the last 10% epochs']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epoch'), 89, 94), (('v number', '2'), 36, 37), (('v number', '0.0001'), 38, 44), (('v number', '5'), 63, 64), (('v number', '1e-05'), 65, 70)], 'The initial learning rate is set as 2 0.0001 with a decay rate 5 1e-05 for each training epoch'], [[(('artifact', 'model'), 55, 60), (('parameter', 'epochs'), 72, 78), (('v number', '1000'), 67, 71)], 'For visualization of the training process, we test our model every 1000 epochs in the training process']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 135, 145), (('v number', '1'), 50, 51), (('v number', '843'), 52, 55)], 'We set the maximum number of tokens per sample to 1,843 to fit the memory constraints and apply gradient accumulation to keep the same batch size as ’s work']] \n",
      "\n",
      "[[[(('parameter', 'm'), 71, 72), (('v number', '32'), 73, 75)], 'After doing some experiments, we finalized the embedding dimensions to M=32 '], [[(('parameter', 'layers'), 17, 23), (('parameter', 'layers'), 58, 64), (('v number', '6'), 15, 16), (('v number', '3'), 56, 57)], 'We also employ 6-layers BERT for document embedding and 3-layers BERT for query embedding'], [[(('parameter', 'layers'), 6, 12), (('artifact', 'model'), 36, 41), (('artifact', 'model'), 85, 90), (('v number', '3'), 4, 5), (('v number', '6'), 67, 68)], 'The 3-layers BERT is called student model as it was trained from a 6-layerss teacher model.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 56, 62), (('parameter', 'dropout'), 128, 135), (('parameter', 'dropout'), 155, 162), (('v number', '8'), 54, 55), (('v number', '8'), 64, 65), (('v number', '32'), 124, 126), (('v number', '0.1'), 139, 142), (('v number', '0.1'), 166, 169), (('v number', '1024'), 224, 228)], 'The FDE is passed to a causal-linear transformer with 8 layers, 8 self-attention heads, a query and value dimensionality of 32, dropout of 0.1 , attention dropout of 0.1 , and a dimensionality of the feed-forward network of 1024.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 42, 52), (('v number', '16'), 56, 58)], 'The only difference is that we reduce the batch size to 16 so that the extra rotated images used during the training can still fit into the GPU memory.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 35, 41), (('v number', '8'), 33, 34)], 'We used a fixed mapping depth of 8 layers during all our experiments']] \n",
      "\n",
      "[[[(('artifact', 'model'), 23, 28), (('parameter', 'epoch'), 48, 53), (('parameter', 'learning rate'), 61, 74), (('parameter', 'batch size'), 89, 99), (('v number', '1e-5'), 78, 82), (('v number', '32'), 103, 105)], 'The mention extraction model is trained for one epoch with a learning rate of 1e-5 and a batch size of 32'], [[(('artifact', 'model'), 18, 23), (('parameter', 'epochs'), 45, 51), (('parameter', 'learning rate'), 59, 72), (('parameter', 'batch size'), 87, 97), (('v number', '3e-5'), 76, 80), (('v number', '32'), 101, 103)], 'The IS assignment model is trained for three epochs with a learning rate of 3e-5 and a batch size of 32']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('v number', '0.001'), 28, 33), (('v number', '0.9'), 55, 58)], 'The learning rate is set to 0.001, and the momentum is 0.9'], [[(('parameter', 'epoch'), 7, 12), (('parameter', 'epoch'), 56, 61), (('v number', '100'), 16, 19), (('v number', '3500'), 72, 76)], 'We set epoch to 100 on synthetic dataset, and fine-tune epoch is set to 3500 on LOL dataset'], [[(('parameter', 'epochs'), 43, 49), (('v number', '100'), 61, 64)], 'For ablation experiments, all the training epochs are set to 100 unless otherwise stated']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 28, 32), (('v number', '2'), 50, 51), (('v number', '0.0001'), 52, 58), (('v number', '0.5'), 66, 69), (('v number', '0.999'), 77, 82), (('v number', '1e-08'), 89, 94), (('v number', '32'), 137, 139)], 'We train the networks using Adam optimizer with = 2 0.0001 , _1 = 0.5 , _2 = 0.999 and = 1e-08 for all datasets with a minibatch size of 32']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 12, 18), (('parameter', 'layer'), 63, 68), (('parameter', 'activation'), 113, 123), (('parameter', 'layer'), 153, 158), (('parameter', 'dropout'), 166, 173), (('v number', '6'), 21, 22), (('v number', '260'), 28, 31), (('v number', '4'), 52, 53), (('v number', '1024'), 108, 112), (('v number', '0.5'), 176, 179)], 'N number of layers = 6 d_ = 260 h number of heads = 4 d_ inner-layer of positionwise feed-forward network = 1024 Activation function inside feed-forward layer = GELU dropout = 0.5']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 17, 22), (('parameter', 'learning rate'), 31, 44), (('v number', '1e-5'), 48, 52)], 'The optimizer is AdamW and the learning rate is 1e-5 as an initial value'], [[(('parameter', 'learning rate'), 4, 17), (('artifact', 'gradient clipping'), 126, 143), (('v number', '10'), 107, 109)], 'The learning rate scheduler used for training is get_linear_schedule_with_warmup, and the maximum value of 10 is used for the gradient clipping']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 122, 135), (('parameter', 'batch size'), 151, 161), (('v number', '1e-05'), 139, 144), (('v number', '8'), 149, 150)], 'We use the default hyperparameters as provided in the HuggingFace Transformers library , with two major changes: we use a learning rate of 1e-05 and 8 batch size in all experiments.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 39, 52), (('parameter', 'weight decay'), 65, 77), (('parameter', 'epoch'), 114, 119), (('v number', '0.01'), 56, 60), (('v number', '5e-4'), 81, 85), (('v number', '400'), 110, 113)], 'We use the Adam optimizer with initial learning rate of 0.01 and weight decay of 5e-4 to train all models for 400 epoch by minimizing the cross entropy loss, with early stopping on the validation set.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 81, 87), (('v number', '18'), 51, 53)], 'We use an encoder-decoder architecture with ResNet-18 without the classification layers as the encoder, two 1x1 convolutions in the bottleneck to reduce the number of channels and a simpler ResNet-based decoder'], [[(('parameter', 'activation'), 10, 20), (('parameter', 'activation'), 128, 138)], 'The final activation is sigmoid and we use binary-crossentropy loss for all models besides NSA continuous for which we use ReLU activation and mean squared error loss as the labels are unbounded'], [[(('artifact', 'Adam'), 51, 55), (('parameter', 'learning rate'), 80, 93), (('parameter', 'epochs'), 135, 141), (('v number', '64'), 42, 44), (('v number', '0.001'), 111, 116), (('v number', '1e-06'), 120, 125), (('v number', '320'), 131, 134)], 'The models are trained on batches of size 64 using Adam with a cosine-annealing learning rate that decays from 0.001 to 1e-06 over 320 epochs'], [[(('parameter', 'epochs'), 74, 80), (('v number', '560'), 70, 73)], 'For non-aligned objects, the loss takes longer to converge, so we use 560 epochs for the hazelnut, metal nut, and screw classes in the MVTec AD dataset'], [[(('parameter', 'epochs'), 21, 27), (('v number', '240'), 17, 20)], 'For rCXR, we use 240 epochs']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 31, 36), (('v number', '100'), 40, 43), (('v number', '100'), 56, 59), (('v number', '100'), 136, 139)], 'We use BiLSTMs with one hidden layer of 100 dimensions, 100-dimensional randomly initialised word embeddings, a label embedding size of 100'], [[(('parameter', 'learning rate'), 36, 49), (('parameter', 'batch size'), 63, 73), (('v number', '0.001'), 53, 58), (('v number', '128'), 77, 80), (('v number', '3'), 159, 160)], 'We train our models with RMSProp, a learning rate of 0.001 , a batch size of 128, and early stopping on the validation set of the main task with a patience of 3.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 64, 77), (('parameter', 'learning rate'), 243, 256), (('parameter', 'weight decay'), 273, 285), (('parameter', 'learning rates'), 323, 337), (('v number', '1'), 60, 61), (('v number', '0.1'), 92, 95), (('v number', '2'), 195, 196), (('v number', '0.0001'), 260, 266), (('v number', '0.05'), 289, 293)], 'We follow the same settings as to train our models, except: 1 a learning rate multiplier of 0.1 is applied to both CNN and Transformer backbones instead of only applying it to CNN backbones in , 2 both ResNet and Swin backbones use an initial learning rate of 0.0001 and a weight decay of 0.05 , instead of using different learning rates in .']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 27, 31), (('parameter', 'learning rate'), 50, 63), (('v number', '0.001'), 67, 72), (('v number', '4'), 95, 96), (('v number', '0.001'), 97, 102)], 'We train our networks with Adam and TTUR , with a learning rate of 0.001 for the generator and 4 0.001 for the discriminator'], [[(('parameter', 'batch size'), 51, 61), (('v number', '1'), 15, 16), (('v number', '0.5'), 38, 41), (('v number', '8'), 65, 66)], 'We also used R^1 regularization with =0.5 , with a batch size of 8'], [[(('parameter', 'Version'), 39, 46), (('v number', '0'), 14, 15)], 'REF , we use =0 to push the downscaled version of the generated image to be as close as possible to the LR target']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 49, 59), (('v number', '64'), 26, 28), (('v number', '8192'), 63, 67)], 'We conduct experiments on 64 TPU-v3 chips with a batch size of 8192'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 56, 69), (('v number', '0.9'), 27, 30), (('v number', '0.99'), 37, 41)], 'We use Adam optimizer _1 = 0.9, _2 = 0.99 with a linear learning rate decay'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '6.4e-4'), 29, 35)], 'The initial learning rate is 6.4e-4'], [[(('parameter', 'weight decay'), 4, 16), (('v number', '5e-5'), 27, 31)], 'The weight decay is set to 5e-5 throughout the training'], [[(('parameter', 'activation'), 37, 47), (('parameter', 'layers'), 72, 78), (('artifact', 'method'), 94, 100), (('v number', '0.9'), 142, 145)], 'To estimate the clipping bound B for activation of non-binary quantized layers, we follow the method in and use exponentially moving average =0.9 of maximum value in a batch.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 30, 34), (('parameter', 'learning rate'), 51, 64), (('v number', '0.0005'), 68, 74)], 'The model was optimized using Adam optimizer and a learning rate of 0.0005'], [[(('parameter', 'batch size'), 9, 19), (('v number', '32'), 31, 33)], 'The mini-batch size was set to 32']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 29, 35), (('v number', '12'), 26, 28), (('v number', '768'), 37, 40), (('v number', '12'), 59, 61)], 'We adopt a structure with 12 layers, 768 hidden units, and 12 heads for the task-specific representation modules'], [[(('parameter', 'layers'), 29, 35), (('v number', '48'), 26, 28), (('v number', '12288'), 37, 42), (('v number', '192'), 61, 64)], 'We adopt a structure with 48 layers, 12288 hidden units, and 192 heads for the universal representation modules'], [[(('parameter', 'layer'), 10, 15), (('artifact', 'model'), 126, 131), (('v number', '196608'), 77, 83), (('v number', '16'), 94, 96)], 'The inner layer of the universal representation modules has a dimensional of 196608, which is 16 times the hidden size of the model']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 92, 97), (('parameter', 'layers'), 230, 236), (('v number', '64'), 128, 130), (('v number', '64'), 181, 183)], 'Since the code of DUMA is not open-source, we reimplement it by only using one co-attention layer where the attention heads are 64 and the dimension of Query, Key and Value are all 64, because it is pointed that more co-attention layers do not improve the performance ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 35, 41), (('artifact', 'Adam'), 48, 52), (('parameter', 'learning rate'), 58, 71), (('v number', '100'), 31, 34), (('v number', '0.2'), 72, 75)], 'Specifically, we train DGC for 100 epochs using Adam with learning rate 0.2'], [[(('parameter', 'weight decay'), 4, 16), (('v number', '10'), 92, 94), (('v number', '000'), 95, 98)], 'For weight decay, as in SGC, we tune this hyperparameter on each dataset using hyperopt for 10,000 trails'], [[(('artifact', 'L'), 118, 119), (('parameter', 'epochs'), 141, 147), (('parameter', 'weight decay'), 156, 168), (('v number', '2'), 139, 140)], 'For the large-scale inductive learning task on the Reddit network, we also follow the protocols of SGC , where we use L-BFGS optimizer for 2 epochs with no weight decay.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 53, 54), (('v number', '1'), 55, 56)], 'Curvature constant of the hyperbolic space is set to K=1 '], [[(('artifact', 'Adam'), 33, 37), (('parameter', 'learning rate'), 53, 66), (('v number', '0.001'), 69, 74)], 'All the models are trained using Adam optimizer with learning rate = 0.001.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 45, 50), (('v number', '10'), 56, 58)], 'To reduce variance in learning, we train our model with 10 C-subheadsThe final _} in Eq']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 44, 54), (('parameter', 'steps'), 80, 85), (('parameter', 'learning rate'), 103, 116), (('artifact', 'linear warmup'), 135, 148), (('parameter', 'steps'), 155, 160), (('parameter', 'steps'), 217, 222), (('parameter', 'learning rate'), 236, 249), (('v number', '512'), 55, 58), (('v number', '1.5'), 117, 120), (('v number', '0.0001'), 121, 127), (('v number', '1'), 250, 251), (('v number', '1e-05'), 252, 257)], 'The first set follows the Megatron-LM work: batch size 512, 300K total training steps 157B tokens, and learning rate 1.5 0.0001 with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 297K steps with minimum learning rate 1 1e-05 '], [[(('parameter', 'batch size'), 68, 78), (('parameter', 'steps'), 113, 118), (('parameter', 'learning rate'), 136, 149), (('artifact', 'linear warmup'), 175, 188), (('parameter', 'steps'), 195, 200), (('parameter', 'steps'), 258, 263), (('parameter', 'learning rate'), 277, 290), (('v number', '8'), 82, 83), (('v number', '37'), 92, 94), (('v number', '6'), 150, 151), (('v number', '0.0001'), 152, 158), (('v number', '4'), 159, 160), (('v number', '34'), 252, 254)], 'The second parameter set tests a more aggressive training strategy: batch size 4K 8 larger, 37.5K total training steps 157B tokens, and learning rate 6 0.0001 4 larger with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 34.5K steps same minimum learning rate'], [[(('artifact', 'model'), 40, 45), (('parameter', 'batch size'), 51, 61), (('v number', '512'), 62, 65), (('v number', '3'), 111, 112)], 'But we also test 2K on the smaller 117M model with batch size 512 and 157B tokens which is the default for GPT-3 '], [[(('artifact', 'Adam'), 65, 69), (('parameter', 'weight decay'), 116, 128), (('parameter', 'activation'), 149, 159), (('artifact', 'gradient clipping'), 220, 237), (('v number', '0.9'), 85, 88), (('v number', '0.999'), 96, 101), (('v number', '1'), 106, 107), (('v number', '1e-08'), 108, 113), (('v number', '0.01'), 132, 136)], 'All experiments are performed with mixed precisionFP16 training, Adam optimizer _1 = 0.9 , _2 = 0.999 , = 1 1e-08 , weight decay of 0.01, checkpoint activation, same random seed for Python, NumPy, PyTorch, and CUDA, and gradient clipping.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 16, 20), (('parameter', 'learning rate'), 32, 45), (('parameter', 'batch size'), 56, 66), (('parameter', 'epochs'), 79, 85), (('v number', '0.001'), 49, 54), (('v number', '128'), 70, 73), (('v number', '50'), 89, 91)], 'On MNIST we use Adam optimizer, learning rate of 0.001, batch size of 128, and epochs of 50 for both the generator and the discriminator'], [[(('parameter', 'batch size'), 15, 25), (('parameter', 'epochs'), 36, 42), (('v number', '64'), 29, 31), (('v number', '40'), 46, 48)], 'On SVHN we use batch size of 64 and epochs of 40 for both the generator and the discriminator'], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 38, 51), (('parameter', 'learning rate'), 171, 184), (('parameter', 'learning rate'), 247, 260), (('parameter', 'epoch'), 310, 315), (('v number', '0.0002'), 55, 61), (('v number', '0.5'), 69, 72), (('v number', '0.999'), 84, 89), (('v number', '0.1'), 188, 191), (('v number', '0.9'), 208, 211), (('v number', '10'), 304, 306), (('v number', '20'), 316, 318), (('v number', '30'), 323, 325)], 'The Adam optimizer parameterized with learning rate of 0.0002, _1 of 0.5, and _2 of 0.999 is used for the generator, and the Momentum optimizer parameterized with initial learning rate of 0.1 and momentum of 0.9 is used for the discriminator, the learning rate of the discriminator decays by a factor of 10 at epoch 20 and 30'], [[(('parameter', 'batch size'), 19, 29), (('parameter', 'steps'), 41, 46), (('v number', '10'), 9, 11), (('v number', '128'), 33, 36), (('v number', '80000'), 50, 55)], 'On CIFAR-10 we use batch size of 128 and steps of 80000 for both the generator and the discriminator'], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 38, 51), (('parameter', 'learning rate'), 171, 184), (('parameter', 'learning rate'), 247, 260), (('parameter', 'epoch'), 310, 315), (('v number', '0.0002'), 55, 61), (('v number', '0.5'), 69, 72), (('v number', '0.999'), 84, 89), (('v number', '0.1'), 188, 191), (('v number', '0.9'), 208, 211), (('v number', '10'), 304, 306), (('v number', '100'), 316, 319), (('v number', '150'), 324, 327)], 'The Adam optimizer parameterized with learning rate of 0.0002, _1 of 0.5, and _2 of 0.999 is used for the generator, and the Momentum optimizer parameterized with initial learning rate of 0.1 and momentum of 0.9 is used for the discriminator, the learning rate of the discriminator decays by a factor of 10 at epoch 100 and 150']] \n",
      "\n",
      "[[[(('parameter', 'K'), 156, 157), (('parameter', 'K'), 191, 192), (('v number', '1'), 72, 73), (('v number', '2'), 130, 131), (('v number', '1'), 193, 194), (('v number', '2'), 195, 196), (('v number', '4'), 197, 198), (('v number', '8'), 199, 200), (('v number', '16'), 201, 203), (('v number', '3'), 210, 211)], 'We report experiments results of different training settings, including 1 zero-shot setting, where no training data is available, 2 few-shot setting, where K training instances are available K=1,2,4,8,16 , and 3 fully supervised setting, where full training set is available.']] \n",
      "\n",
      "[[[(('parameter', 'T'), 27, 28), (('parameter', 'T'), 61, 62), (('parameter', 'learning rate'), 98, 111), (('parameter', 'batch size'), 127, 137), (('v number', '10'), 29, 31), (('v number', '5'), 32, 33), (('v number', '3'), 63, 64), (('v number', '10'), 65, 67), (('v number', '5'), 68, 69), (('v number', '0.005'), 115, 120), (('v number', '1'), 141, 142)], 'All models are trained for T=10^5 iterations for GeoMNIST or T=3 10^5 iterations for CIFAR with a learning rate of 0.005 and a batch size of 1 under the softmax cross-entropy loss']] \n",
      "\n",
      "[[[(('artifact', 'model'), 51, 56), (('parameter', 'steps'), 69, 74), (('v number', '500'), 61, 64), (('v number', '000'), 65, 68), (('v number', '128'), 101, 104)], 'For the development experiments, we train our BERT model for 500,000 steps with a sequence length of 128'], [[(('artifact', 'model'), 62, 67), (('parameter', 'batch size'), 108, 118), (('v number', '32'), 122, 124), (('v number', '6000'), 157, 161), (('v number', '24'), 172, 174)], 'We use whole word masking and the default hyperparameters and model architecture of BERTBASE except a lower batch size of 32 in order to train on NVIDIA RTX 6000 GPUs with 24 GB RAM'], [[(('parameter', 'layers'), 56, 62), (('parameter', 'layer'), 93, 98), (('v number', '12'), 53, 55), (('v number', '12'), 64, 66), (('v number', '768'), 107, 110)], 'This corresponds to a bidirectional transformer with 12 layers, 12 attention heads, a hidden layer size of 768, and GELU activations .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 160, 165), (('v number', '50'), 119, 121)], 'We set the same experimental settings as the SOTA , and report the mean classiﬁcation results on the testing set after 50 runs of training followed by a linear model'], [[(('artifact', 'model'), 71, 76), (('artifact', 'Adam'), 83, 87), (('parameter', 'learning rate'), 114, 127), (('v number', '0.001'), 131, 136)], 'We initialize the parameters using Xavier initialization and train the model using Adam optimizer with an initial learning rate of 0.001'], [[(('parameter', 'epochs'), 62, 68), (('v number', '2000'), 72, 76)], 'We follow the same settings as DGI does and set the number of epochs to 2000'], [[(('parameter', 'batch size'), 16, 26), (('v number', '50'), 32, 34), (('v number', '2000'), 38, 42), (('v number', '20'), 86, 88)], 'We vary the the batch size from 50 to 2000, and the early stopping with a patience of 20 is adopted'], [[(('parameter', 'dropout'), 58, 65), (('v number', '25'), 34, 36), (('v number', '0.9'), 53, 56), (('v number', '0.7'), 74, 77)], 'We set the step of random walk as 25, soft-margin as 0.9, dropout rate as 0.7.']] \n",
      "\n",
      "[[[(('artifact', 'method'), 85, 91), (('v number', '1'), 17, 18)], 'A wrong such as =1 in the four-rooms domain could deteriorate the performance of the method']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 40, 47), (('artifact', 'Adam'), 51, 55), (('parameter', 'learning rates'), 154, 168)], 'We use the Ranger optimizer, an adapted version of Adam with improved stability at the beginning of training – by accounting for the variance in adaptive learning rates – and improved robustness and convergence speed , '], [[(('parameter', 'batch size'), 9, 19), (('parameter', 'learning rate'), 32, 45), (('v number', '16'), 23, 25), (('v number', '3e-5'), 49, 53)], 'We use a batch size of 16 and a learning rate of 3e-5 to which we apply cosine annealing'], [[(('parameter', 'epochs'), 34, 40), (('parameter', 'epoch'), 141, 146), (('v number', '100'), 30, 33), (('v number', '100'), 44, 47), (('v number', '5'), 85, 86)], 'For meta-training, we perform 100 epochs of 100 episodes and perform evaluation with 5 different seeds on the meta-validation set after each epoch'], [[(('parameter', 'epoch'), 4, 9), (('parameter', 'steps'), 33, 38), (('v number', '100'), 22, 25), (('v number', '4'), 85, 86)], 'One epoch consists of 100 update steps where each update step consists of a batch of 4 episodes'], [[(('parameter', 'epochs'), 36, 42), (('v number', '3'), 34, 35)], 'Early-stopping with a patience of 3 epochs is performed to avoid overfitting'], [[(('parameter', 'epochs'), 48, 54), (('parameter', 'epoch'), 110, 115), (('v number', '10'), 45, 47)], 'For the non-episodic baselines, we train for 10 epochs on the auxiliary languages while validating after each epoch']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 29, 33), (('parameter', 'epochs'), 53, 59), (('parameter', 'batch size'), 67, 77), (('parameter', 'learning rate'), 90, 103), (('v number', '16'), 81, 83), (('v number', '0.0005'), 107, 113)], 'We train all models with the Adam optimizer for four epochs with a batch size of 16 and a learning rate of 0.0005 '], [[(('parameter', 'weight decay'), 67, 79), (('v number', '1'), 18, 19), (('v number', '0.01'), 90, 94)], 'Focal loss with = 1 is used for classification of edges and nodes, weight decay is set to 0.01 and weights are initialized randomly'], [[(('parameter', 'T'), 32, 33), (('v number', '3'), 34, 35)], 'In all experiments, graphs with T=3 timesteps are considered.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 44, 49), (('v number', '4'), 69, 70), (('v number', '000'), 71, 74)], 'As in , , we use a pre-trained Faster R-CNN model that is trained on 4,000 video frames randomly sampled from the training set with object masks and attribute annotations to generate object proposals for each frame'], [[(('artifact', 'Adam'), 72, 76), (('parameter', 'epochs'), 94, 100), (('parameter', 'learning rate'), 133, 146), (('v number', '40'), 91, 93), (('v number', '8'), 104, 105), (('v number', '0.0001'), 157, 163)], 'All deep modules concept learner and program executor are trained using Adam optimizer for 40 epochs on 8 Nvidia 1080Ti GPUs and the learning rate is set to 0.0001 '], [[(('parameter', 'T'), 7, 8), (('parameter', 'K'), 29, 30), (('parameter', 'T'), 46, 47), (('parameter', 'T'), 68, 69), (('v number', '0.004'), 9, 14), (('v number', '256'), 18, 21), (('v number', '64'), 25, 27), (('v number', '10'), 31, 33), (('v number', '10'), 37, 39), (('v number', '128'), 48, 51), (('v number', '20'), 70, 72)], 'We set t=0.004, D=256, C=64, K=10, S=10 , and T=128 for CLEVRER and T=20 for Real-Billiard '], [[(('artifact', 'model'), 28, 33), (('artifact', 'model'), 276, 281), (('v number', '4000'), 236, 240)], 'In addition to our standard model that grounds object properties from question-answer pairs, we also train a variant VRDP on CLEVRER with an explicit rule-based program executor and object attribute supervisions attribute annotation in 4000 frames learned by the Faster R-CNN model.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 60, 66), (('parameter', 'learning rate'), 74, 87), (('parameter', 'steps'), 112, 117), (('parameter', 'dropout'), 122, 129), (('v number', '50'), 57, 59), (('v number', '5'), 91, 92), (('v number', '20'), 98, 100), (('v number', '000'), 101, 104), (('v number', '0.1'), 138, 141)], 'We trained the end-to-end ASR using a Noam optimizer for 50 epochs with a learning rate of 5 with 20,000 warmup steps and dropout-rate of 0.1']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 106, 110), (('parameter', 'learning rate'), 131, 144), (('parameter', 'batch size'), 149, 159), (('v number', '171'), 39, 42), (('v number', '12'), 70, 72), (('v number', '1e-3'), 126, 130), (('v number', '16'), 160, 162)], 'For the experiments which refer to all 171 tasks rather than just the 12 we focus on in NOOCh , we use an Adam optimizer with 1e-3 learning rate and batch size 16']] \n",
      "\n",
      "[[[(('artifact', 'method'), 87, 93), (('v number', '32'), 29, 31), (('v number', '32'), 32, 34)], 'CelebA images are resized to 32 32 resolution and we use the test set to evaluate each method'], [[(('parameter', 'K'), 39, 40), (('artifact', 'L'), 47, 48), (('parameter', 'K'), 67, 68), (('parameter', 'layers'), 112, 118), (('v number', '2'), 41, 42), (('v number', '5'), 49, 50), (('v number', '2'), 80, 81), (('v number', '2'), 82, 83), (('v number', '1'), 88, 89), (('v number', '1'), 90, 91), (('v number', '14'), 100, 102)], 'Additionally, our generative flow uses K=2 and L=5 , and duplicate K for scales 2 2 and 1 1 —we use 14 coupling layers in total'], [[(('parameter', 'batch size'), 36, 46), (('parameter', 'epochs'), 93, 99), (('v number', '32'), 50, 52), (('v number', '100'), 89, 92)], 'We train the generative flow with a batch size of 32 and stop the training process after 100 epochs'], [[(('parameter', 'learning rate'), 63, 76), (('v number', '1500'), 41, 45), (('v number', '0.005'), 80, 85)], 'Then, we solve each inverse problem with 1500 iterations and a learning rate of 0.005 ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 101, 107), (('v number', '32'), 98, 100)], 'For a fair comparison with CLIP, we train our DeCLIP-ResNet50 and DeCLIP-ViT-B32 from scratch for 32 epochs']] \n",
      "\n",
      "[[[(('parameter', 'learning rates'), 7, 21), (('v number', '0.001'), 25, 30), (('v number', '0.0001'), 32, 38)], 'We use learning rates of 0.001, 0.0001 for and BAR, respectively'], [[(('parameter', 'learning rates'), 43, 57), (('parameter', 'learning rate'), 79, 92), (('v number', '0.1'), 69, 72), (('v number', '3'), 73, 74)], 'Also, we use cosine annealing from initial learning rates lr to lr * 0.1^3 for learning rate scheduling for all datasets']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 2, 8), (('v number', '2'), 0, 1), (('v number', '3'), 63, 64)], '2 layers of the backbone were made trainable and the remaining 3 were frozen to retain the pre-trained ImageNet weights'], [[(('parameter', 'epochs'), 27, 33), (('v number', '15'), 24, 26), (('v number', '3.0'), 66, 69)], 'Models were trained for 15 epochs, though early stopping patience=3.0 was employed in order to avoid overfitting.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('v number', '0.01'), 33, 37)], 'The learning rate is set to be = 0.01 ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 28, 34), (('parameter', 'learning rate'), 73, 86), (('parameter', 'epoch'), 135, 140), (('parameter', 'weight decay'), 148, 160), (('v number', '200'), 24, 27), (('v number', '0.002'), 90, 95), (('v number', '0.98'), 126, 130), (('v number', '5e-4'), 164, 168)], 'We train all models for 200 epochs using RAdam optimizer with an initial learning rate of 0.002, an exponential decay rate of 0.98 per epoch, and a weight decay of 5e-4'], [[(('parameter', 'layer'), 51, 56), (('v number', '64'), 21, 23), (('v number', '128'), 25, 28), (('v number', '64'), 30, 32), (('v number', '64'), 38, 40), (('v number', '3'), 49, 50), (('v number', '18'), 69, 71), (('v number', '121'), 82, 85)], 'Mini-batch sizes are 64, 128, 64, and 64 for our 3-layer CNN, ResNet-18, DenseNet-121, and MobileNetV2, respectively.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 83, 89), (('parameter', 'batch size'), 97, 107), (('parameter', 'learning rate'), 127, 140), (('v number', '50'), 80, 82), (('v number', '128'), 111, 114), (('v number', '0.1'), 144, 147), (('v number', '0.00001'), 178, 185), (('v number', '0.1'), 259, 262)], 'For knowledge distillation on the ImageNet dataset, we run all distillation for 50 epochs with a batch size of 128, an initial learning rate of 0.1 for training from scratch and 0.00001 for fine-tuning, with milestones at [20, 30, 40] of a decreasing rate of 0.1 '], [[(('artifact', 'model'), 58, 63), (('parameter', 'weight decay'), 82, 94), (('v number', '0.9'), 23, 26), (('v number', '0.0001'), 98, 104)], 'The SGD optimizer with 0.9 momentum is used to update the model parameters, and a weight decay of 0.0001 is applied'], [[(('parameter', 'epochs'), 69, 75), (('parameter', 'batch size'), 83, 93), (('parameter', 'learning rate'), 113, 126), (('v number', '10'), 29, 31), (('v number', '200'), 65, 68), (('v number', '125'), 97, 100), (('v number', '0.1'), 130, 133), (('v number', '0.1'), 188, 191), (('v range', '[100, 150]'), 153, 163)], 'For experiments on the CIFAR-10 dataset, we run distillation for 200 epochs with a batch size of 125, an initial learning rate of 0.1 with milestones at [100, 150] of a decreasing rate of 0.1 '], [[(('parameter', 'weight decay'), 47, 59), (('v number', '0.9'), 37, 40), (('v number', '0.0002'), 63, 69)], 'The SGD optimizer with a momentum of 0.9 and a weight decay of 0.0002 is used to update the parameters'], [[(('parameter', 'batch size'), 164, 174), (('v number', '0.5'), 85, 88)], 'We set the coefficients of the cross-entropy loss and the KL-divergence loss both to 0.5 , and the coefficient for the input gradient alignment to , where B is the batch size.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 49, 55), (('parameter', 'layer'), 92, 97), (('parameter', 'activation'), 157, 167), (('v number', '256'), 66, 69)], 'The network is complete with two fully connected layers, one with 256 units, and the output layer with C units equal to the number of classes, and a softmax activation'], [[(('parameter', 'layers'), 4, 10), (('parameter', 'activation'), 40, 50), (('parameter', 'layers'), 86, 92), (('parameter', 'layers'), 131, 137)], 'All layers except the output use a ReLU activation, and we insert Batch Normalization layers between Convolutional and Max-Pooling layers.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 47, 52), (('artifact', 'Adam'), 94, 98), (('parameter', 'learning rate'), 122, 135), (('parameter', 'learning rate'), 187, 200), (('parameter', 'learning rate'), 306, 319), (('parameter', 'learning rate'), 331, 344), (('parameter', 'learning rate'), 353, 366), (('v number', '7.5'), 27, 30), (('v number', '10'), 31, 33), (('v number', '5'), 34, 35), (('v number', '1.5'), 38, 41), (('v number', '10'), 42, 44), (('v number', '6'), 45, 46), (('v number', '2'), 139, 140), (('v number', '0.0001'), 141, 147), (('v number', '1.0'), 254, 257), (('v number', '0.8'), 275, 278), (('v number', '0.9'), 296, 299), (('v number', '3'), 370, 371), (('v number', '0.0001'), 372, 378)], 'The network is trained for 7.5 10^5 , 1.5 10^6 steps on Haze4k and RESIDE respectively.We use Adam optimizer with initial learning rate of 2 0.0001 , and adopt the CyclicLR to adjust the learning rate, where on the triangular mode, the value of gamma is 1.0,base momentum is 0.8, max momentum is 0.9, base learning rate is initial learning rate and max learning rate is 3 0.0001 ']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('v number', '1e-3'), 43, 47), (('v number', '2e-5'), 77, 81)], 'The learning rate of the GCN component was 1e-3, and that of BART was set at 2e-5'], [[(('artifact', 'model'), 16, 21), (('parameter', 'epochs'), 29, 35), (('v number', '20'), 26, 28), (('v number', '2'), 103, 104)], 'We trained each model for 20 epochs and selected the best checkpoints on the validation set with ROUGE-2 score']] \n",
      "\n",
      "[[[(('artifact', 'model'), 36, 41), (('artifact', 'model'), 126, 131)], 'Following recent trends , , , , the model is trained on the MS1MV2 dataset , which is the same data used to train the teacher model'], [[(('parameter', 'Version'), 24, 31), (('v number', '5'), 60, 61)], 'The MS1MV2 is a refined version of MS-Celeb-1M and contains 5.8M images of 85k identities']] \n",
      "\n",
      "[[[(('artifact', 'method'), 13, 19), (('parameter', 'epochs'), 27, 33), (('parameter', 'batch size'), 39, 49), (('parameter', 'learning rate'), 61, 74), (('v number', '11'), 24, 26), (('v number', '1'), 53, 54), (('v number', '0.0031'), 78, 84)], 'We train the method for 11 epochs at a batch size of 1 and a learning rate of 0.0031'], [[(('parameter', 'weight decay'), 17, 29), (('v number', '0.9'), 48, 51), (('v number', '0.0001'), 56, 62)], 'The momentum and weight decay values are set to 0.9 and 0.0001 respectively'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 44, 50), (('v number', '10'), 32, 34), (('v number', '4'), 51, 52), (('v number', '8'), 54, 55), (('v number', '10'), 60, 62)], 'The learning rate is decayed by 10 times at epochs 4, 8 and 10']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 26, 36), (('parameter', 'epochs'), 52, 58), (('v number', '64'), 40, 42), (('v number', '100'), 48, 51)], 'Training was done using a batch size of 64 over 100 epochs for PCE-LSTM and DeepConvLSTM'], [[(('parameter', 'epochs'), 61, 67), (('v number', '200'), 57, 60)], 'FFNN showed slower convergence and hence was trained for 200 epochs']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 114, 120), (('parameter', 'learning rate'), 145, 158), (('parameter', 'epochs'), 186, 192), (('v number', '150'), 110, 113), (('v number', '10'), 174, 176), (('v number', '50'), 183, 185)], 'For both full and linear finetuning, we use the same training hyperparameters as ; specifically, we train for 150 epochs, decreasing the initial learning rate by a factor of 10 every 50 epochs'], [[(('parameter', 'learning rate'), 27, 40), (('parameter', 'learning rate'), 146, 159), (('v number', '0.01'), 7, 11), (('v number', '0.001'), 122, 127), (('v number', '0.01'), 252, 256)], 'We use 0.01 as the initial learning rate for all linear finetuning experiments; for full finetuning, we empirically found 0.001 to be the initial learning rate which gives comparable results for most datasets except Aircraft and Cars, for which we use 0.01']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 118, 123), (('parameter', 'layer'), 290, 295), (('parameter', 'hidden layers'), 310, 323)], 'We include a wide variety of benchmark algorithms in the experiment, including a support vector machine SVM , a multi-layer perceptron MLP , a convolutional network CNN-I , a LSTM , a variant convolutional network CNN-II , as well as an Attention-augmented-Bilinear-Network with one hidden layer BTABL and two hidden layers CTABL ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 69, 79), (('v number', '512'), 83, 86)], 'The experiments are performed on a single Tesla V100 32GB GPU with a batch size of 512'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 28, 34), (('artifact', 'Adam'), 78, 82), (('v number', '20'), 25, 27)], 'The model is trained for 20 epochs and its parameters are optimized using the Adam optimizer'], [[(('parameter', 'epoch'), 30, 35), (('v number', '8'), 46, 47), (('v number', '15'), 58, 60)], 'The average run time for each epoch is around 8 hours and 15 minutes'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epoch'), 76, 81), (('v number', '0.002'), 36, 41), (('v number', '0.5'), 68, 71)], 'The initial learning rate is set to 0.002 and decays by a factor of 0.5 per epoch'], [[(('parameter', 'dropout'), 2, 9), (('v number', '0.5'), 13, 16)], 'A dropout of 0.5 is also applied.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 16, 21), (('v number', '32'), 89, 91), (('v number', '4110'), 120, 124), (('v number', '2'), 129, 130)], 'We evaluate our model using the PyTorch deep learning library on a machine equipped with 32 CPU cores Intel Xeon Silver 4110 CPU@2.10GHz, 188GB RAM, and eight GPU cores GeForce GTX 1080TI with 11GB VRAM.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 16, 21), (('v number', '64'), 30, 32)], 'Within each RNN layer, we set 64 as the size of the hidden dimension']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 39, 45), (('artifact', 'Adam'), 51, 55), (('parameter', 'learning rate'), 86, 99), (('v number', '70'), 36, 38), (('v number', '5e-4'), 103, 107)], 'The proposed network is trained for 70 epochs with Adam optimizer and set the initial learning rate to 5e-4 '], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'epoch'), 36, 41), (('v number', '5'), 42, 43), (('v number', '20'), 48, 50), (('v number', '0.1'), 55, 58)], 'We then reduce the learning rate at epoch 5 and 20 by =0.1 ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 20, 26), (('parameter', 'epoch'), 59, 64), (('v number', '200'), 47, 50), (('v number', '1000'), 75, 79)], 'The total number of epochs during training are 200 and one epoch length is 1000'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '1e-5'), 28, 32)], 'The learning rate is set to 1e-5.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 133, 143), (('v number', '1'), 44, 45), (('v number', '64'), 144, 146)], 'These were determined based on two factors: 1 the commonly used settings across the RL literature for example 64x64 architecture and batch size 64 is most commonly used across many different problems and methods, and by sampling random combinations from a large grid of hyper parameters and comparing results trends to narrow down the set of hyper parameters to consider to consistently well-performing values and reasonable ranges.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 52, 56), (('parameter', 'learning rate'), 74, 87), (('parameter', 'batch size'), 102, 112), (('v number', '1e-4'), 91, 95), (('v number', '1'), 116, 117)], 'In the training stage, the optimization function is Adam, with a constant learning rate of 1e-4 and a batch size of 1'], [[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 27, 33), (('v number', '100'), 23, 26)], 'We train our model for 100 epochs and apply it to the testing data directly.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 23, 28), (('parameter', 'layer'), 46, 51), (('parameter', 'm'), 219, 220), (('v number', '2'), 205, 206), (('v number', '2'), 207, 208), (('v number', '5'), 209, 210), (('v number', '6'), 211, 212), (('v number', '4'), 213, 214), (('v number', '4'), 222, 223), (('v number', '4'), 224, 225), (('v number', '4'), 226, 227), (('v number', '4'), 228, 229), (('v number', '4'), 230, 231)], 'We employed the TT-RNN model in the recurrent layer to compress the weight matrix ^ ^ by reshaping and expressing it in the TT-format, in accordance to the dimensions of the input tensors _t , such that P=2 2 5 6 4 and M= 4 4 4 4 4 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('parameter', 'steps'), 28, 33), (('parameter', 'batch size'), 42, 52), (('v number', '140'), 56, 59)], 'We train the model for 200K steps using a batch size of 140'], [[(('parameter', 'steps'), 11, 16), (('parameter', 'steps'), 38, 43), (('v number', '512'), 86, 89)], 'We use 20K steps for BERT warmup, 10K steps for decoder warmup, and a max position of 512']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 39, 52), (('parameter', 'learning rate'), 85, 98), (('v number', '0.1'), 56, 59)], 'We train models using SGD with initial learning rate of 0.1 and use cosine annealing learning rate scheduling '], [[(('parameter', 'epochs'), 28, 34), (('artifact', 'model'), 65, 70), (('parameter', 'epoch'), 84, 89), (('v number', '100'), 24, 27)], 'We train all models for 100 epochs and perform evaluation on the model saved at the epoch which has the highest accuracy on the test set']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 26, 31), (('v number', '4'), 67, 68), (('v number', '80'), 75, 77), (('v number', '320'), 95, 98)], '[itemsep=0em] Time-stride layer: reduce input sequence length by R=4 times 80 input dimension, 320 output dimension'], [[(('parameter', 'layer'), 7, 12), (('v number', '320'), 14, 17), (('v number', '512'), 35, 38)], 'Linear layer: 320 input dimension, 512 output dimension.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 44, 50), (('v number', '12'), 41, 43), (('v number', '768'), 52, 55), (('v number', '12'), 73, 75)], 'We train CLSEBERT using Transformer with 12 layers, 768 hidden sizes and 12 attention heads'], [[(('parameter', 'batch size'), 4, 14), (('v number', '128'), 25, 28)], 'The batch size is set to 128'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '1e-4'), 28, 32)], 'The learning rate is set to 1e-4'], [[(('artifact', 'Adam'), 10, 14), (('artifact', 'model'), 55, 60)], 'We use an Adam optimizer to optimize parameters of the model'], [[(('artifact', 'model'), 13, 18), (('parameter', 'steps'), 40, 45)], 'Finally, the model is trained with 110K steps']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 52, 58), (('parameter', 'weight decay'), 119, 131), (('v number', '100'), 48, 51), (('v number', '0.9'), 111, 114), (('v number', '1e-4'), 132, 136)], 'We train all the architectures from scratch for 100 epochs using stochastic gradient descent SGD with momentum 0.9 and weight decay 1e-4'], [[(('parameter', 'learning rate'), 9, 22), (('parameter', 'epochs'), 81, 87), (('v number', '0.1'), 33, 36), (('v number', '0.1'), 58, 61), (('v number', '30'), 68, 70), (('v number', '60'), 71, 73), (('v number', '90'), 78, 80)], 'The base learning rate is set to 0.1 and is multiplied by 0.1 after 30,60 and 90 epochs'], [[(('parameter', 'learning rate'), 78, 91), (('v number', '0.01'), 102, 106)], 'The fine-tuning procedure uses the same configuration except that the initial learning rate is set to 0.01 ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 45, 51), (('parameter', 'learning rate'), 64, 77), (('v number', '360'), 41, 44)], 'Specifically, we train our supernets for 360 epochs with cosine learning rate decay'], [[(('parameter', 'batch size'), 9, 19), (('v number', '32'), 23, 25)], 'The mini-batch size is 32 per GPU'], [[(('parameter', 'weight decay'), 23, 35), (('parameter', 'dropout'), 47, 54), (('parameter', 'layer'), 75, 80), (('parameter', 'dropout'), 81, 88), (('v number', '0.9'), 18, 21), (('v number', '1e-05'), 39, 44), (('v number', '0.2'), 58, 61), (('v number', '0.2'), 92, 95)], 'We use momeutm of 0.9, weight decay of 1e-05 , dropout of 0.2 , stochastic layer dropout of 0.2 '], [[(('parameter', 'learning rate'), 9, 22), (('v number', '0.1'), 33, 36), (('v number', '256'), 73, 76)], 'The base learning rate is set as 0.1 and is linearly scaled up for every 256 training samples']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 59, 64), (('artifact', 'model'), 99, 104), (('artifact', 'model'), 135, 140), (('parameter', 'steps'), 180, 185), (('v number', '5000'), 49, 53), (('v number', '15000'), 169, 174)], 'We collect trajectories experiences in the first 5000 time steps and then pre-train the trajectory model along with the representation model and the conditional VAE for 15000 time steps, after which we start the training of the actor'], [[(('parameter', 'steps'), 13, 18), (('parameter', 'steps'), 57, 62)], 'All the time steps above are counted into the total time steps for a fair comparison'], [[(('artifact', 'model'), 22, 27), (('artifact', 'model'), 58, 63), (('parameter', 'steps'), 89, 94), (('parameter', 'steps'), 150, 155), (('v number', '10'), 81, 83), (('v number', '50'), 142, 144)], 'The trajectory return model along with the representation model is trained every 10 time steps for the pre-train process and is trained every 50 time steps in the rest of training, which already ensures a good performance in our experiments']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 21, 27), (('artifact', 'AdamW'), 34, 39), (('parameter', 'learning rate'), 62, 75), (('v number', '50'), 18, 20), (('v number', '0.0001'), 79, 85)], 'We train ViDT for 50 epochs using AdamW with the same initial learning rate of 0.0001 for its body, neck and head'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'batch size'), 54, 64), (('parameter', 'weight decay'), 72, 84), (('artifact', 'gradient clipping'), 103, 120), (('v number', '16'), 68, 70), (('v number', '1'), 88, 89), (('v number', '0.0001'), 90, 96), (('v number', '0.1'), 124, 127)], 'The learning rate is decayed by cosine annealing with batch size of 16, weight decay of 1 0.0001 , and gradient clipping of 0.1 '], [[(('parameter', 'epochs'), 24, 30), (('artifact', 'AdamW'), 37, 42), (('parameter', 'learning rate'), 60, 73), (('v number', '150'), 20, 23), (('v number', '5'), 77, 78), (('v number', '1e-05'), 79, 84)], 'Neck is trained for 150 epochs using AdamW with the initial learning rate of 5 1e-05 by cosine annealing']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 93, 99), (('v number', '100'), 89, 92)], 'For ResNet-based models, we follow the training schedule in and train all the models for 100 epochs'], [[(('parameter', 'learning rate'), 7, 20), (('parameter', 'learning rate'), 46, 59), (('v number', '0.1'), 67, 70)], 'Cosine learning rate is adopted with the base learning rate set to 0.1']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 79, 85), (('v number', '300'), 75, 78), (('v number', '640'), 130, 133), (('v number', '640'), 134, 137)], 'Following previous methods , , , , we pre-train our models on IC17-MLT for 300 epochs, in which images are cropped and resized to 640 640 pixels'], [[(('parameter', 'epochs'), 36, 42), (('v number', '600'), 32, 35)], 'We then finetune the models for 600 epochs'], [[(('artifact', 'Adam'), 28, 32), (('parameter', 'batch size'), 38, 48), (('v number', '16'), 49, 51), (('v number', '4'), 55, 56)], 'All models are optimized by Adam with batch size 16 on 4 GPUs'], [[(('parameter', 'learning rate'), 18, 31), (('parameter', 'learning rate'), 57, 70), (('v number', '1'), 74, 75), (('v number', '0.001'), 76, 81)], 'We adopt a “poly\" learning rate schedule with an initial learning rate of 1 0.001 ']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 25, 30), (('v number', '12'), 22, 24), (('v number', '768'), 55, 58), (('v number', '12'), 75, 77)], 'VLMo-Base consists of 12-layer Transformer blocks with 768 hidden size and 12 attention heads'], [[(('parameter', 'layer'), 19, 24), (('v number', '24'), 16, 18), (('v number', '1024'), 50, 54), (('v number', '16'), 71, 73)], 'VLMo-Large is a 24-layer Transformer network with 1024 hidden size and 16 attention heads']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 17, 22), (('parameter', 'layer'), 34, 39), (('artifact', 'Adam'), 148, 152), (('v number', '4'), 83, 84), (('v number', '1000'), 111, 115), (('v number', '64'), 141, 143)], 'We adopted a two-layer ReLU Multi-Layer Perceptron MLP for classification tasks on 4 UCI datasets, trained for 1000 episodes with batch-size 64 and Adam optimizer'], [[(('parameter', 'learning rate'), 67, 80), (('v number', '0.001'), 100, 105), (('v number', '0.005'), 107, 112), (('v number', '0.01'), 114, 118)], 'We report the best performance for each smooth rate under a set of learning rate settings, [0.0007, 0.001, 0.005, 0.01, 0.05] .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '3'), 35, 36)], 'Our model is implemented in Python 3, and mainly uses the following dependencies: torch as the machine learning library, nltk for text preprocessing, transformers for their BERT implementation, and numpy for high-level mathematical operations in CPU'], [[(('artifact', 'model'), 26, 31), (('parameter', 'learning rate'), 69, 82), (('v number', '1e-6'), 88, 92), (('v number', '3e-5'), 95, 99), (('v number', '1e-5'), 102, 106), (('v number', '3e-4'), 113, 117), (('v number', '32'), 142, 144), (('v number', '48'), 146, 148), (('v number', '64'), 150, 152), (('v number', '128'), 158, 161), (('v number', '2'), 200, 201), (('v number', '4'), 203, 204), (('v number', '6'), 206, 207), (('v number', '8'), 213, 214)], 'Using the accuracy of the model on the development set, we tuned the learning rate from 1e-6 , 3e-5 , 1e-5 , and 3e-4 , the adapter size from 32, 48, 64, and 128, and the hypercomplex dimensions from 2, 4, 6, and 8.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('v number', '1e-2'), 42, 46), (('v number', '1e-3'), 48, 52), (('v number', '1e-4'), 54, 58)], 'The learning rate is selected from [1e-1, 1e-2, 1e-3, 1e-4, 1e-5], for the value with the best performance in experiments'], [[(('parameter', 'epochs'), 13, 19), (('v number', '10'), 56, 58)], 'The training epochs are empirically set as multiples of 10 and are selected for each experiment'], [[(('parameter', 'epoch'), 44, 49), (('parameter', 'epochs'), 128, 134), (('v number', '20'), 125, 127)], 'We pre-run each experiment to determine the epoch value and stop training when the performance does not increase in the next 20 epochs to prevent over-fitting.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 0, 7), (('v number', '0.5'), 34, 37)], 'Dropout probability is set at p = 0.5 .']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 33, 37), (('parameter', 'learning rate'), 55, 68), (('v number', '0.0001'), 72, 78), (('v number', '0.5'), 106, 109), (('v number', '0.999'), 114, 119)], 'We trained the networks using an Adam optimizer with a learning rate of 0.0001 and beta1, beta2 values of 0.5 and 0.999'], [[(('parameter', 'learning rate'), 41, 54), (('parameter', 'epochs'), 87, 93), (('v number', '10'), 84, 86), (('v number', '10'), 110, 112)], 'Furthermore, we used a reduce on plateau learning rate scheduler with a patience of 10 epochs and a factor of 10'], [[(('parameter', 'epochs'), 66, 72), (('parameter', 'K'), 77, 78), (('parameter', 'steps'), 79, 84), (('v number', '8'), 11, 12), (('v number', '35'), 63, 65), (('v number', '220'), 73, 76), (('v number', '12'), 116, 118)], 'Batches of 8 patches were used and the models were trained for 35 epochs 220 k steps on an NVIDIA TITAN XP GPU with 12 GB of VRAM']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 51, 61), (('v number', '256'), 29, 32), (('v number', '256'), 33, 36), (('v number', '8'), 65, 66)], 'The network is trained using 256 256 images with a batch size of 8'], [[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 29, 33), (('v number', '0.0'), 54, 57), (('v number', '0.9'), 67, 70)], 'The model is optimized using Adam optimizer with _1 = 0.0 and _2 = 0.9'], [[(('parameter', 'learning rate'), 32, 45), (('parameter', 'learning rate'), 91, 104), (('v number', '0.0001'), 46, 52), (('v number', '1e-05'), 108, 113)], 'All generators are trained with learning rate 0.0001 , and discriminators are trained with learning rate of 1e-05 .']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 42, 47), (('v number', '1'), 23, 24), (('v number', '10'), 25, 27), (('v number', '9'), 28, 29)], 'Agents are trained for 1 10^9 environment steps which takes between three and eight days depending on the size of the training population']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 48, 53), (('parameter', 'layer'), 115, 120), (('artifact', 'model'), 177, 182), (('v number', '256'), 155, 158)], 'For unsupervised pretraining, we select a multi-layer convolutional network as the encoder _ , and we select a two-layer transformer with hidden dimension 256 as the sequential model _ '], [[(('parameter', 'K'), 41, 42), (('parameter', 'steps'), 65, 70)], 'Here, the positive pair is h_, c_t where k is the number of time steps ahead, and the negative pairs are h_i, c_t , where h_i hidden representations of a batch of random hidden representations assumed to be unrelated to c_t '], [[(('parameter', 'T'), 68, 69), (('parameter', 'K'), 75, 76), (('parameter', 'steps'), 77, 82), (('parameter', 'K'), 198, 199), (('parameter', 'K'), 206, 207), (('parameter', 'K'), 212, 213), (('parameter', 'steps'), 242, 247), (('v number', '1'), 41, 42), (('v number', '1'), 200, 201), (('v number', '12'), 234, 236)], 'The scoring function f based on Equation 1 in the main text at step t with k steps ahead is f_k = f_kh, c_t = h^ W_k c_t , where W_k is a learnable linear transformation defined separately for each k 1,...,K and K is predetermined as 12 time steps']] \n",
      "\n",
      "[[[(('parameter', 'K'), 88, 89), (('v number', '1'), 11, 12), (('v number', '1'), 76, 77)], 'where }_^ =1 indicates that }_^ is trained using Equation REF with setting =1 i.e., the k -th device is benign'], [[(('artifact', 'model'), 50, 55), (('parameter', 'T'), 134, 135)], 'A special case is }_t} } , which means the global model is optimized when all the malicious devices do not conduct attacks before the t -th round']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 41, 45), (('parameter', 'learning rate'), 52, 65), (('v number', '0.001'), 69, 74)], 'For all experiments, we train CoRGi with Adam and a learning rate of 0.001'], [[(('parameter', 'dropout'), 9, 16), (('parameter', 'layers'), 40, 46), (('v number', '0.1'), 113, 116), (('v number', '0.3'), 118, 121), (('v number', '0.5'), 123, 126), (('v number', '0.7'), 128, 131)], 'We apply dropout on the message passing layers, the prediction MLPs, as well as on edges, with rates chosen from 0.1, 0.3, 0.5, 0.7 with respect to the validation set performance'], [[(('parameter', 'layers'), 179, 185), (('parameter', 'learning rate'), 193, 206), (('v number', '1'), 76, 77)], \"For the baselines, the parameter settings are done in the following manner: 1 When the settings of the comparison models overlap with CoRGi's, e.g., the number of message passing layers or the learning rate, we used the same configurations as CoRGi\"], [[(('artifact', 'model'), 63, 68), (('v number', '2'), 0, 1)], '2 For the parameter settings that are unique to the comparison model, we followed the setting that is disclosed in the original paper']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 19, 23), (('parameter', 'learning rate'), 50, 63), (('v number', '0.0001'), 67, 73), (('v number', '0.7'), 91, 94), (('v number', '50'), 104, 106), (('v number', '000'), 107, 110)], 'Concretely, we use Adam optimizer with a starting learning rate at 0.0001, which decays by 0.7 in every 50,000 iterations'], [[(('artifact', 'model'), 13, 18), (('parameter', 'batch size'), 51, 61), (('v number', '32'), 62, 64), (('v number', '300'), 69, 72), (('v number', '000'), 73, 76)], 'We train the model on one NVIDIA TITAN XP GPU with batch size 32 for 300,000 iterations']] \n",
      "\n",
      "[[[(('artifact', 'model'), 39, 44), (('v number', '512'), 51, 54), (('v number', '0.1'), 82, 85)], 'We consider _ {} as the shared encoder model with =512 and the warm-up proportion 0.1 '], [[(('artifact', 'Adam'), 77, 81), (('parameter', 'batch size'), 99, 109), (('v number', '16'), 113, 115), (('v number', '1e-5'), 122, 126)], 'Both the explanation generation and task prediction models are trained using Adam optimizer with a batch size of 16, and =1e-5 '], [[(('parameter', 'epochs'), 26, 32), (('v number', '10'), 23, 25), (('v number', '3'), 89, 90)], 'Models are trained for 10 epochs with early-stopping criteria on the validation set and =3 '], [[(('parameter', 'dropout'), 50, 57), (('parameter', 'layer'), 58, 63), (('parameter', 'layer'), 137, 142), (('parameter', 'layer'), 179, 184), (('v number', '10'), 71, 73), (('v number', '256'), 108, 111)], 'The MLP for the task classification consists of a dropout layer with a 10% chance of masking, followed by a 256 dimensional hidden dense layer, again followed by a Sigmoid output layer']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 24, 37), (('artifact', 'model'), 70, 75), (('parameter', 'learning rate'), 84, 97), (('parameter', 'epochs'), 119, 125), (('v number', '5'), 41, 42), (('v number', '1e-05'), 43, 48), (('v number', '1000'), 114, 118)], 'Adam optimizer with the learning rate of 5 1e-05 is used to train our model and the learning rate is halved every 1000 epochs']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 37, 43), (('parameter', 'epochs'), 73, 79), (('artifact', 'Adam'), 103, 107), (('parameter', 'learning rate'), 122, 135), (('parameter', 'weight decay'), 146, 158), (('v number', '50'), 34, 36), (('v number', '200'), 69, 72), (('v number', '10'), 89, 91), (('v number', '0.001'), 139, 144), (('v number', '0.0001'), 162, 168), (('v number', '128'), 189, 192)], 'We run our training algorithm for 50 epochs on MNIST and F-MNIST and 200 epochs on CIFAR-10, using the Adam optimizer , a learning rate of 0.001, weight decay of 0.0001, and batch-sizes of 128']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 66, 72), (('parameter', 'layer'), 106, 111), (('v number', '4'), 58, 59), (('v number', '16'), 78, 80)], 'Then, for the causal mechanism approximation part, we use 4 dense layers with 16 variables in each hidden layer']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 9, 19), (('v number', '16'), 30, 32), (('v number', '32'), 37, 39)], 'The mini-batch size is set to 16 and 32 for Cityscapes and ADE20K respectively'], [[(('parameter', 'weight decay'), 37, 49), (('v number', '0.9'), 27, 30)], 'And we use the momentum of 0.9 and a weight decay of 5e^ '], [[(('parameter', 'learning rate'), 46, 59), (('parameter', 'learning rate'), 88, 101), (('v number', '1'), 131, 132), (('v number', '0.9'), 146, 149)], \"Similar to other works, , we apply the `poly' learning rate policy in which the initial learning rate is set to 1e^ and decayed by 1-^ with power=0.9\"]] \n",
      "\n",
      "[[[(('parameter', 'T'), 132, 133), (('parameter', 'T'), 158, 159), (('v number', '16'), 30, 32), (('v number', '16'), 57, 59)], 'As GCN has the hidden size of 16, we set each PE to have 16 multiply-accumulators and have the fixed relationship between tile size T and row grouping g that T=16g ']] \n",
      "\n",
      "[[[(('parameter', 'T'), 65, 66), (('parameter', 'K'), 71, 72), (('parameter', 'K'), 93, 94), (('v number', '1'), 55, 56), (('v number', '2'), 62, 63), (('v number', '3'), 69, 70), (('v number', '4'), 91, 92), (('v number', '5'), 120, 121)], 'For co-training, we have five hyper parameters namely: 1 _u , 2 ^T , 3 K mixing parameter, 4 k most used hashtags, and, 5 p popular retweets count'], [[(('parameter', 'K'), 113, 114), (('parameter', 'T'), 141, 142), (('parameter', 'K'), 149, 150), (('v number', '250'), 115, 118), (('v number', '1000'), 123, 127), (('v number', '0.7'), 135, 138), (('v number', '0.7'), 145, 148), (('v number', '0.2'), 153, 156)], 'By a uniform grid search on SM dataset using F1-macro as criterion, we find the following values that work well: k=250 , p=1000 , _u = 0.7, ^T = 0.7,K = 0.2 '], [[(('parameter', 'K'), 4, 5), (('v number', '100'), 45, 48), (('v number', '10'), 52, 54), (('v number', '000'), 55, 58)], 'For k and p , the parameter search range was 100 to 10,000'], [[(('parameter', 'T'), 10, 11), (('parameter', 'K'), 16, 17), (('v number', '0'), 41, 42), (('v number', '1'), 46, 47)], 'For _u , ^T and K , the search range was 0 to 1']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 91, 104), (('parameter', 'batch size'), 122, 132), (('v number', '1'), 108, 109), (('v number', '1e-05'), 110, 115), (('v number', '32'), 136, 138)], 'Our question type classifiers and template exemplar classifiers are trained with a maximum learning rate of 1 1e-05 and a batch size of 32'], [[(('parameter', 'learning rate'), 44, 57), (('v number', '3'), 61, 62), (('v number', '1e-05'), 63, 68), (('v number', '32768'), 101, 106)], 'For training generation models, the maximum learning rate is 3 1e-05 and each batch contains at most 32768 tokens']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 25, 29), (('parameter', 'learning rate'), 54, 67), (('parameter', 'batch size'), 81, 91), (('v number', '0.0002'), 47, 53), (('v number', '32'), 95, 97)], 'This experiment uses the Adam optimizer with a 0.0002 learning rate and sets the batch size to 32 in our training process'], [[(('parameter', 'layers'), 99, 105), (('v number', '40'), 96, 98)], 'We find that the loss and accuracy curves of the training set are converging after training for 40 layers'], [[(('parameter', 'epochs'), 91, 97), (('v number', '50'), 88, 90)], 'Therefore, considering the computational performance of the workstation, we finally set 50 epochs for training.']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 95, 107), (('v number', '0.9'), 71, 74), (('v number', '0.0001'), 88, 94)], 'For all experiments shown in the main paper, we use SGD optimizer with 0.9 momentum and 0.0001 weight decay'], [[(('artifact', 'model'), 31, 36), (('parameter', 'learning rate'), 83, 96), (('v number', '0.01'), 100, 104), (('v number', '10'), 136, 138)], 'For semantic segmentation, the model is trained for 65K iterations starting with a learning rate of 0.01 that is reduced by a factor of 10 at 40K and 55K'], [[(('artifact', 'model'), 48, 53), (('parameter', 'learning rate'), 109, 122), (('v number', '0.02'), 126, 130), (('v number', '0.002'), 150, 155), (('v number', '0.0002'), 171, 177)], 'For the other three dense prediction tasks, the model is trained for 90K or 270K iterations with the initial learning rate of 0.02 that is reduced to 0.002 at 60K210K and 0.0002 at 80K250K']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 8, 21), (('parameter', 'batch size'), 26, 36), (('v number', '0.04'), 48, 52), (('v number', '64'), 57, 59)], 'Initial learning rate and batch size are set to 0.04 and 64, respectively'], [[(('artifact', 'model'), 20, 25), (('v number', '128'), 30, 33)], 'We train the entire model for 128 epoches.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 41, 46), (('v number', '1.5e6'), 26, 31), (('v number', '10'), 55, 57), (('v number', '000'), 58, 61), (('v number', '25'), 66, 68), (('v number', '000'), 69, 72)], 'Training is performed for 1.5e6 training steps between 10,000 and 25,000 placement episodes depending on placement episode length and the best solution found during training is reported']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 133, 139), (('parameter', 'layer'), 165, 170), (('parameter', 'layer'), 191, 196), (('parameter', 'layer'), 225, 230), (('parameter', 'activation'), 242, 252), (('parameter', 'activation'), 262, 272), (('artifact', 'model'), 317, 322), (('parameter', 'layers'), 357, 363), (('parameter', 'layer'), 389, 394), (('parameter', 'layer'), 414, 419), (('parameter', 'activation'), 431, 441), (('parameter', 'activation'), 451, 461), (('parameter', 'layer'), 493, 498), (('parameter', 'batch size'), 499, 509), (('parameter', 'learning rate'), 514, 527), (('parameter', 'epochs'), 545, 551), (('artifact', 'Adam'), 591, 595), (('v number', '4'), 92, 93), (('v number', '2'), 119, 120), (('v number', '16'), 511, 513), (('v number', '0.001'), 529, 534), (('v number', '500'), 553, 556), (('v number', '2000'), 562, 566)], 'Runtime Environment: Nvidia GP107CL Quadro P620 Architecture: Autoencoder Encoder: Contains 4 VGG Blocks VGG Block has 2 Convolution Layers followed by a maxpooling layer Batch normalization layer was used at the end of each layer before the activation function Activation Function: ReLU Decoder: Decoder part of the model consists of convolution transpose layers with batch normalization layer at the end of each layer before the activation function Activation Function: ReLU, Sigmoid Output Layer Batch Size: 16 Learning Rate: 0.001 Number of Epochs: 500 CAE, 2000 DEC and IDEC Optimizer: Adam']] \n",
      "\n",
      "[[[(('artifact', 'L'), 57, 58), (('v number', '5000'), 61, 65), (('v number', '5'), 99, 100), (('v number', '1.5'), 134, 137), (('v number', '1'), 164, 165)], 'For both experiments, we set the total training episodes L = 5000 , trajectory generation size B = 5 , exploratory policy parameter = 1.5 , and resample constant = 1 '], [[(('parameter', 'batch size'), 53, 63), (('parameter', 'T'), 153, 154), (('parameter', 'T'), 157, 158), (('v number', '1000'), 71, 75), (('v number', '1'), 159, 160)], 'We note that such setup of B and then implies a mini-batch size |}_| = 1000 after appending past experiences and including samples from all time periods t < T-1 as specified in Section REF '], [[(('parameter', 'learning rate'), 11, 24), (('artifact', 'EMA'), 66, 69), (('parameter', 'learning rate'), 70, 83), (('v number', '2'), 56, 57), (('v number', '1'), 95, 96)], 'We fix the learning rate for actor parameter update _ = 2 and use EMA learning rate ^_} = 2l + 1 for our critic parameter update.']] \n",
      "\n",
      "[[[(('artifact', 'L'), 75, 76), (('v number', '4'), 23, 24), (('v number', '4'), 25, 26), (('v number', '4'), 37, 38), (('v number', '500'), 77, 80)], 'In our defense, we use 4 4 patches n=4 and an overcomplete dictionary with L=500 atoms'], [[(('parameter', 'm'), 61, 62), (('artifact', 'L'), 68, 69), (('v number', '2'), 13, 14), (('v number', '15'), 44, 46), (('v number', '15'), 47, 49), (('v number', '500'), 50, 53), (('v number', '15'), 63, 65), (('v number', '500'), 70, 73)], 'The stride S=2 , so the encoder output is a 15 15 500 tensor m=15 , L=500 '], [[(('artifact', 'L'), 31, 32), (('v number', '10'), 49, 51)], 'The number of dictionary atoms L is chosen to be 10 times the ambient dimension of patches.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 12, 17), (('v number', '12'), 9, 11), (('v number', '768'), 35, 38), (('v number', '12'), 56, 58)], 'We use a 12-layer Transformer with 768 hidden size, and 12 attention heads']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 81, 86), (('artifact', 'model'), 179, 184)], 'With the analytical framework that can map the weight distribution of the hidden layer to the order-chaos phase diagram, we can then study the back-propagation process during the model training process in the phase diagram'], [[(('parameter', 'layer'), 95, 100), (('parameter', 'layer'), 169, 174), (('v number', '28'), 54, 56), (('v number', '28'), 57, 59), (('v number', '784'), 62, 65), (('v number', '784'), 106, 109), (('v number', '784'), 139, 142), (('v number', '784'), 143, 146)], 'As each sample image in the Fashion-MNIST dataset has 28 28 = 784 pixels, we design the hidden layer with 784 neurons, such that there are 784 784 weights in the hidden layer'], [[(('parameter', 'activation'), 4, 14), (('parameter', 'layer'), 38, 43)], 'The activation function of the hidden layer is in line with the theoretical framework'], [[(('parameter', 'epoch'), 12, 17), (('parameter', 'layer'), 111, 116), (('parameter', 'layer'), 168, 173)], 'After every epoch during the training process, we calculate the mean and variance of the weights in the hidden layer to identify the orderedchaotic phase of the hidden layer.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 176, 181), (('v number', '50'), 120, 122)], 'Sentences which were consented by both the annotators as “claim-worthy” were finalized as ground truth claims for these 50 articles, and used as testing set for evaluating the model performance on DNF datasets'], [[(('artifact', 'model'), 131, 136), (('parameter', 'fold'), 147, 151), (('parameter', 'fold'), 211, 215), (('v number', '475'), 14, 17), (('v number', '250'), 37, 40), (('v number', '300'), 59, 62), (('v number', '650'), 68, 71), (('v number', '700'), 90, 93), (('v number', '5'), 110, 111), (('v number', '5'), 145, 146), (('v number', '4'), 184, 185), (('v number', '1'), 209, 210)], 'The remaining 475 articles from CDC, 250 articles from DNF-300, and 650 articles from DNF-700 were split into 5 folds to train the model using a 5-Fold cross validation , where we use 4 folds for training and 1 fold for validation']] \n",
      "\n",
      "[[[(('artifact', 'model'), 10, 15), (('v number', '0.01'), 55, 59)], 'To reduce model size, all weights below a threshold of 0.01 have been clipped to zero.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 47, 52), (('v number', '20'), 126, 128)], 'We allocate one thread per one channel of each layer by using the OpenMP library to improve the execution speed of the ResNet-20.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('v number', '32'), 42, 44)], 'The batch size used in our experiments is 32'], [[(('artifact', 'model'), 14, 19), (('parameter', 'epochs'), 27, 33), (('v number', '15'), 24, 26)], 'We train each model for 15 epochs'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'learning rate'), 100, 113), (('v number', '1.0'), 50, 53), (('v number', '1e-05'), 54, 59), (('v number', '5.0'), 124, 127), (('v number', '0.0001'), 128, 134)], 'The initial learning rate for the text encoder is 1.0 1e-05 , and for other parameters, the initial learning rate is set to 5.0 0.0001 '], [[(('artifact', 'model'), 16, 21), (('parameter', 'epoch'), 52, 57)], 'We evaluate our model on the validation set at each epoch'], [[(('parameter', 'T'), 28, 29), (('parameter', 'learning rate'), 58, 71), (('v number', '0.1'), 87, 90)], \"If the macro F1 score doesn't increase, we then decay the learning rate by a factor of 0.1\"], [[(('parameter', 'learning rate'), 12, 25), (('v number', '5.0'), 29, 32), (('v number', '1e-07'), 33, 38)], 'The minimum learning rate is 5.0 1e-07 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 63, 73), (('parameter', 'epochs'), 87, 93), (('v number', '15'), 77, 79), (('v number', '60'), 84, 86)], 'For the ellipse dataset, we train the unrolled network using a batch size of 15 for 60 epochs'], [[(('parameter', 'epochs'), 58, 64), (('v number', '15'), 48, 50), (('v number', '50'), 55, 57)], 'The F-FPN network training used a batch-size of 15 for 50 epochs'], [[(('parameter', 'layers'), 52, 58), (('v number', '20'), 43, 45)], 'The unrolled network architecture contains 20 total layers i.e'], [[(('parameter', 'steps'), 7, 12), (('parameter', 'layers'), 29, 35)], 'update steps – the number of layers was chosen on the memory capacity of the GPU.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 39, 44), (('v number', '25'), 69, 71)], 'Additionally, for the expression-based model we train our network on 25 expressions and test on the remaining expressions'], [[(('artifact', 'model'), 13, 18), (('v number', '4'), 40, 41)], 'We train our model with a batch-size of 4'], [[(('artifact', 'model'), 4, 9), (('v number', '24'), 23, 25), (('v number', '4'), 47, 48)], 'Our model takes around 24 hours to converge on 4 Nvidia Tesla V100s.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 65, 70), (('parameter', 'learning rate'), 103, 116), (('parameter', 'weight decay'), 130, 142), (('artifact', 'Adam'), 163, 167), (('v number', '0.1'), 19, 22), (('v number', '0.1'), 43, 46), (('v number', '16'), 58, 60), (('v number', '000'), 61, 64), (('v number', '2048'), 72, 76), (('v number', '0.001'), 120, 125), (('v number', '0.00001'), 146, 153)], 'We use dropouts of 0.1, label smoothing of 0.1, warmup of 16,000 steps, 2048 tokens per batch per GPU, learning rate of 0.001 and weight decay of 0.00001 with the ADAM optimizer for training'], [[(('parameter', 'steps'), 54, 59), (('parameter', 'learning rate'), 95, 108), (('parameter', 'learning rate'), 128, 141), (('artifact', 'model'), 209, 214), (('v number', '0.3'), 32, 35), (('v number', '16'), 47, 49), (('v number', '000'), 50, 53), (('v number', '512'), 61, 64), (('v number', '0.00003'), 112, 119)], 'For mBART50, we use dropouts of 0.3, warmup of 16,000 steps, 512 tokens per batch per GPU, and learning rate of 0.00003.A small learning rate is needed since we can train on very small batches given the large model size']] \n",
      "\n",
      "[[[(('artifact', 'model'), 35, 40), (('v number', '2'), 50, 51), (('v number', '3'), 53, 54), (('v number', '4'), 56, 57)], 'The datasets used for training the model are Data 2, 3, 4 presented in Section ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 38, 44), (('parameter', 'batch size'), 50, 60), (('parameter', 'learning rate'), 77, 90), (('parameter', 'epochs'), 119, 125), (('v number', '20'), 35, 37), (('v number', '512'), 61, 64), (('v number', '0.1'), 91, 94), (('v number', '0.1'), 107, 110), (('v number', '5'), 117, 118)], 'Models are trained on MS1Mv2 , for 20 epochs with batch size 512 and initial learning rate 0.1, dropped by 0.1 every 5 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 5, 10), (('v number', '1'), 27, 28), (('v number', '2'), 30, 31), (('v number', '4'), 33, 34), (('v number', '8'), 36, 37), (('v number', '16'), 43, 45)], 'Each model is trained with 1, 2, 4, 8, and 16 few-shot training sets, and tested on the full test sets'], [[(('parameter', 'batch size'), 47, 57), (('parameter', 'learning rate'), 115, 128), (('v number', '256'), 61, 64), (('v number', '0.001'), 129, 134)], 'To fine-tune Tip-Adapter-F, we train it with a batch size of 256, and use Stochastic Gradient Descent SGD , with a learning rate 0.001 and a cosine scheduler'], [[(('parameter', 'epoch'), 23, 28), (('parameter', 'epochs'), 95, 101), (('v number', '200'), 19, 22), (('v number', '20'), 92, 94)], 'In contrast to the 200-epoch training in CoOp and CLIP-Adapter, Tip-Adapter-F only requires 20 epochs for fine-tuning and has super-fast convergence speed, saving much computational cost for training.']] \n",
      "\n",
      "[[[(('parameter', 'hidden state'), 9, 21), (('parameter', 'hidden state'), 67, 79)], 'The last hidden state of the encoder RNN is used to initialise the hidden state of the decoder'], [[(('artifact', 'model'), 14, 19), (('parameter', 'epochs'), 28, 34), (('artifact', 'Adam'), 45, 49), (('v number', '400'), 24, 27)], 'We train each model for 400 epochs using the Adam optimizer '], [[(('parameter', 'epoch'), 39, 44), (('artifact', 'model'), 54, 59), (('v number', '3'), 122, 123)], 'We choose the hyperparameters and best epoch for each model by obtaining results on the validation set using beam size of 3 and not enforcing executability']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 28, 34), (('parameter', 'batch size'), 43, 53), (('artifact', 'Adam'), 61, 65), (('parameter', 'learning rate'), 81, 94), (('artifact', 'linear warmup'), 122, 135), (('parameter', 'steps'), 151, 156), (('artifact', 'linear decay'), 166, 178), (('v number', '10'), 25, 27), (('v number', '16'), 57, 59), (('v number', '2e-5'), 98, 102), (('v number', '10000'), 145, 150)], 'We train our models upto 10 epochs, with a batch size of 16, Adam optimizer with learning rate of 2e-5 with a schedule of linear warmup of first 10000 steps and then linear decay']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 67, 71), (('v number', '165'), 38, 41), (('v number', '000'), 42, 45), (('v number', '0.9'), 132, 135), (('v number', '0.999'), 191, 196)], 'The training process is performed for 165,000 iterations using the Adam optimizer , with a decay rate of gradient moving average _1=0.9 and a decay rate of squared gradient moving average _2=0.999 '], [[(('parameter', 'learning rate'), 10, 23), (('parameter', 'epochs'), 65, 71), (('v number', '0.0001'), 27, 33), (('v number', '0.5'), 52, 55), (('v number', '25'), 62, 64)], 'We used a learning rate of 0.0001 and reduced it by 0.5 every 25 epochs'], [[(('parameter', 'batch size'), 9, 19), (('v number', '32'), 24, 26)], 'The mini-batch size was 32 training patches per iteration.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 59, 64), (('v number', '8'), 10, 11)], 'And using 8 NVIDIA Tesla V100 for training the Transformer model'], [[(('artifact', 'AdamW'), 7, 12), (('parameter', 'learning rate'), 43, 56), (('artifact', 'model'), 60, 65), (('parameter', 'learning rate'), 136, 149), (('v number', '0.0001'), 85, 91)], 'We use Adamw as the optimizer, the initial learning rate of model finetune is set to 0.0001, and cosine scheduler is used to adjust the learning rate'], [[(('parameter', 'epochs'), 29, 35), (('parameter', 'epochs'), 75, 81), (('v number', '50'), 26, 28), (('v number', '200'), 71, 74), (('v number', '1'), 127, 128)], 'For most models, we train 50 epochs on the training dataset, and train 200 epochs on the small number of labeled data in Phase 1']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 31, 37), (('parameter', 'batch size'), 45, 55), (('v number', '40'), 28, 30), (('v number', '32'), 59, 61)], 'We fine-tune BERT for up to 40 epochs with a batch size of 32'], [[(('parameter', 'learning rate'), 71, 84), (('parameter', 'weight decay'), 100, 112), (('v number', '5e-5'), 88, 92)], \"We optimize BERT parameters using gluonnlp's bertadam optimizer with a learning rate of 5e-5 and no weight decay\"]] \n",
      "\n",
      "[[[(('parameter', 'steps'), 39, 44), (('v number', '64'), 48, 50)], 'Further, models are pre-trained for 1M steps on 64 TPU cores using the LAMB optimizer .']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 34, 39), (('artifact', 'Adam'), 68, 72), (('v number', '1000'), 17, 21), (('v number', '200'), 44, 47)], 'DLC trains every 1000 environment steps for 200 iterations with the Adam optimizer '], [[(('parameter', 'batch size'), 4, 14), (('v number', '50'), 25, 27)], 'The batch size is set to 50'], [[(('artifact', 'model'), 36, 41), (('parameter', 'learning rates'), 72, 86), (('v number', '6e-4'), 87, 91), (('v number', '6e-4'), 93, 97), (('v number', '8e-5'), 103, 107)], 'The representation, value and actor model are respectively trained with learning rates 6e-4, 6e-4, and 8e-5'], [[(('artifact', 'model'), 57, 62), (('v number', '0.1'), 108, 111)], 'The prior _^ and posterior _^ variance in the transition model are bounded from below to a minimum value of 0.1'], [[(('artifact', 'model'), 4, 9), (('artifact', 'model'), 72, 77), (('v number', '2'), 125, 126), (('v number', '1'), 137, 138), (('v number', '2'), 144, 145)], 'The model loss on true observations J_ is weighted twice as much as the model losses on predicted opponent observations J__t^2} and J__t^1, s_t^2} '], [[(('artifact', 'model'), 4, 9), (('artifact', 'L'), 30, 31), (('v number', '50'), 32, 34), (('v number', '15'), 72, 74)], 'The model learning horizon is L=50 whereas the imagination horizon is H=15 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 16, 26), (('artifact', 'model'), 108, 113), (('parameter', 'epochs'), 123, 129), (('v number', '1e-6'), 46, 50), (('v number', '1e-5'), 52, 56), (('v number', '1e-4'), 58, 62), (('v number', '1e-3'), 64, 68), (('v number', '5e-3'), 70, 74), (('v number', '5e-2'), 76, 80), (('v number', '5e-1'), 82, 86), (('v number', '5'), 92, 93), (('v number', '500'), 119, 122)], 'Next, for fixed batch size b = 2L , we vary = 1e-6, 1e-5, 1e-4, 1e-3, 5e-3, 5e-2, 5e-1, and 5 and train the model with 500 epochs']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 82, 86), (('parameter', 'epochs'), 104, 110), (('parameter', 'batch size'), 120, 130), (('v number', '60'), 101, 103), (('v number', '128'), 134, 137)], 'We train both the attribute prediction and the latent manipulation networks using Adam optimizer for 60 epochs with the batch size of 128'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 69, 75), (('v number', '0.01'), 36, 40), (('v number', '0.1'), 56, 59), (('v number', '10'), 66, 68)], 'The initial learning rate is set to 0.01 and decayed by 0.1 every 10 epochs'], [[(('parameter', 'batch size'), 151, 161), (('parameter', 'learning rate'), 199, 212), (('parameter', 'learning rate'), 237, 250), (('v number', '32'), 124, 126), (('v number', '000'), 127, 130), (('v number', '16'), 165, 167), (('v number', '0.002'), 254, 259)], 'To train the domain-specific StyleGAN2 generator ^* for each out-of-domain domain, we finetune the pretrained StyleGAN2 for 32,000 iterations with the batch size of 16 on each dataset using the same learning rate scheduler but a smaller learning rate of 0.002 ']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 68, 75), (('v number', '0.9'), 38, 41), (('v number', '0.999'), 46, 51), (('v number', '1e-08'), 54, 59), (('v number', '0.1'), 91, 94)], 'Standard hyperparameter choices of _1=0.9, _2=0.999, =1e-08 , and a dropout probability of 0.1 were chosen'], [[(('artifact', 'model'), 0, 5), (('parameter', 'epochs'), 96, 102), (('v number', '8020'), 37, 41), (('v number', '1'), 103, 104), (('v number', '10'), 108, 110)], 'Model training and validating with a 8020 traintest split of the training data, across training epochs 1, , 10 '], [[(('parameter', 'epoch'), 13, 18), (('artifact', 'model'), 19, 24)], 'The selected epoch model was that with the highest pronoun prediction accuracy on the validation set.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 134, 139), (('v number', '7'), 159, 160)], 'Once the document has been processed and visibility graph has been generated, we feed them to our Graph Recurrent Neural Network GRNN model framework with the 7-dimensional node input space to get projected to a higher order space encoding with individual node features preserving the structural content information of the document'], [[(('parameter', 'hidden state'), 77, 89), (('v number', '4'), 42, 43), (('v number', '128'), 61, 64)], 'The graph-level RNN used in our work uses 4 layered GRU with 128 dimensional hidden state'], [[(('parameter', 'hidden state'), 96, 108), (('parameter', 'activation'), 158, 168), (('parameter', 'activation'), 228, 238), (('v number', '16'), 81, 83), (('v number', '8'), 114, 115)], 'To get the predicted adjacency vector in the output, the edge-level RNN maps the 16 dimensional hidden state to a 8 dimensional vector through a MLP and ReLU activation, then another MLP maps the vector to a scalar with sigmoid activation'], [[(('parameter', 'layer'), 19, 24), (('parameter', 'hidden state'), 25, 37), (('parameter', 'layer'), 89, 94)], 'We use the highest layer hidden state of the graph-level RNN to initialize with a linear layer to match the dimensionality'], [[(('artifact', 'Adam'), 4, 8), (('v number', '32'), 55, 57)], 'The Adam Optimizer has been used for minibatch size of 32'], [[(('parameter', 'learning rate'), 11, 24), (('parameter', 'epoch'), 76, 81), (('v number', '0.001'), 31, 36), (('v number', '0.2'), 57, 60)], 'We set the learning rate to be 0.001 which is decayed by 0.2 at every 100th epoch in all experiments.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 38, 44), (('v number', '25'), 35, 37), (('v number', '0.15'), 74, 78)], 'We trained the resulting fBERT for 25 epochs using the MLM objective with 0.15 probability to randomly mask tokens in the input'], [[(('artifact', 'model'), 13, 18), (('parameter', 'batch size'), 37, 47), (('artifact', 'Adam'), 95, 99), (('parameter', 'learning rate'), 117, 130), (('v number', '32'), 51, 53), (('v number', '512'), 60, 63), (('v number', '5e-5'), 134, 138)], 'The language model is trained with a batch size of 32 and a 512 maximum token length using the Adam optimizer with a learning rate of 5e-5 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 9, 19), (('parameter', 'learning rate'), 34, 47), (('v number', '1536'), 23, 27), (('v number', '1.95'), 51, 55), (('v number', '6'), 68, 69)], 'We use a batch size of 1536 and a learning rate of 1.95 to train on 6 NVIDIA V100 GPUs']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 40, 44), (('parameter', 'batch size'), 89, 99), (('v number', '0.0001'), 78, 84), (('v number', '32'), 103, 105)], 'For the optimization of NTL, we utilize Adam as the optimizer, with learning =0.0001 and batch size of 32'], [[(('artifact', 'Adam'), 67, 71), (('parameter', 'learning rate'), 88, 101), (('v number', '0.0002'), 106, 112), (('v number', '0.5'), 138, 141), (('v number', '0.999'), 146, 151)], 'In the training of adversarial augmentation, the optimizer is also Adam, and we set the learning rate to =0.0002 with two decay momentums 0.5 and 0.999 '], [[(('parameter', 'batch size'), 4, 14), (('v number', '64'), 18, 20), (('v number', '256'), 84, 87)], 'The batch size is 64, and the dimension of the latent space fed to the generator is 256.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('parameter', 'learning rate'), 49, 62), (('parameter', 'learning rate'), 96, 109), (('v number', '0.01'), 36, 40)], 'The initial learning rate is set as 0.01 and the learning rate at each iteration is the initial learning rate multiplied by }^ '], [[(('parameter', 'weight decay'), 17, 29), (('v number', '0.9'), 47, 50), (('v number', '0.0001'), 55, 61)], 'The momentum and weight decay rates are set to 0.9 and 0.0001, respectively'], [[(('parameter', 'epochs'), 11, 17), (('v number', '150'), 7, 10)], 'We set 150 epochs for training']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 35, 40), (('parameter', 'learning rate'), 49, 62)], 'The optimizer used for training is AdamW and the learning rate scheduler is get_linear_schedule_with_warmup '], [[(('parameter', 'epochs'), 16, 22), (('v number', '5'), 28, 29), (('v number', '10'), 33, 35)], 'We use training epochs from 5 to 10 depending on the subtask'], [[(('parameter', 'learning rate'), 10, 23), (('v number', '1e-5'), 27, 31), (('v number', '1'), 44, 45), (('v number', '1e-6'), 50, 54)], 'Also, the learning rate is 1e-5 in subtask #1 and 1e-6 in other subtasks']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 30, 36), (('parameter', 'K'), 54, 55), (('v number', '8'), 26, 27), (('v number', '2'), 44, 45), (('v number', '2'), 56, 57), (('v number', '8'), 99, 100), (('v number', '4'), 108, 109)], \"We use hidden size task_H=8 , layers task_L=2 , heads K=2 , structure learner's hidden size func_H=8 and d_=4 \"], [[(('parameter', 'hidden layers'), 28, 41), (('v number', '2'), 26, 27), (('v number', '16'), 58, 60)], 'We compare to an MLP with 2 hidden layers and hidden size 16'], [[(('parameter', 'epochs'), 28, 34), (('v number', '250'), 24, 27)], 'We train the models for 250 epochs'], [[(('artifact', 'model'), 16, 21), (('artifact', 'Adam'), 30, 34), (('parameter', 'learning rate'), 50, 63), (('v number', '0.001'), 64, 69)], 'To optimize the model, we use Adam optimizer with learning rate 0.001 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 48, 53), (('v number', '256'), 64, 67)], 'We use a standard Transformer architecture with model dimension 256'], [[(('parameter', 'layers'), 26, 32), (('parameter', 'layer'), 91, 96), (('v number', '2'), 24, 25), (('v number', '4'), 38, 39), (('v number', '1'), 89, 90), (('v number', '1'), 102, 103), (('v number', '1'), 128, 129)], 'The encoder consists of 2 layers with 4 self-attention heads and the decoder consists of 1 layer with 1 self-attention head and 1 attention head over the slots'], [[(('artifact', 'model'), 62, 67), (('v number', '128'), 40, 43), (('v number', '64'), 104, 106)], 'We feed in the sentences with less than 128 characters to our model and consider the number of slots as 64 half of the maximum input length']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 39, 45), (('parameter', 'batch size'), 58, 68), (('parameter', 'epochs'), 121, 127), (('parameter', 'batch size'), 140, 150), (('v number', '100'), 35, 38), (('v number', '100'), 72, 75), (('v number', '350'), 117, 120), (('v number', '50'), 154, 156)], 'The MNIST networks are trained for 100 epochs each with a batch size of 100 while the CIFAR networks are trained for 350 epochs each with a batch size of 50'], [[(('parameter', 'steps'), 74, 79), (('parameter', 'steps'), 104, 109), (('parameter', 'steps'), 205, 210), (('parameter', 'steps'), 236, 241), (('v number', '0'), 38, 39), (('v number', '2000'), 60, 64), (('v number', '5000'), 90, 94), (('v number', '10000'), 190, 195), (('v number', '50000'), 221, 226)], 'Following , the schedule of starts at 0 for a warmup period 2000 training steps on MNIST, 5000 training steps on CIFAR, followed by a linear increase to the desired target perturbation size 10000 training steps on MNIST, 50000 training steps on CIFAR, after which is fixed at the target level']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 25, 30), (('parameter', 'learning rate'), 43, 56), (('parameter', 'weight decay'), 68, 80), (('v number', '5e-5'), 38, 42), (('v number', '0.01'), 63, 67)], 'As an optimiser, we used AdamW with a 5e-5 learning rate and a 0.01 weight decay'], [[(('parameter', 'dropout'), 33, 40), (('v number', '10'), 29, 31)], 'For regularisation, we set a 10% dropout probability'], [[(('parameter', 'batch size'), 9, 19), (('v number', '128'), 24, 27)], 'Training batch size was 128'], [[(('parameter', 'epochs'), 54, 60), (('parameter', 'batch size'), 68, 78), (('v number', '32'), 82, 84)], 'For finetuning on labelled data, we trained for three epochs with a batch size of 32, which corresponds to default settings recommended by .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 34, 39), (('parameter', 'layer'), 52, 57), (('v number', '12'), 31, 33), (('v number', '12'), 49, 51)], 'We carry out pre-training with 12-layer encoder, 12-layer decoder ProphetNet models']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 42, 46), (('parameter', 'learning rate'), 64, 77), (('v number', '0.001'), 81, 86)], 'The hyperparameters are trained using the Adam optimizer with a learning rate of 0.001 and the cosine embedding loss function from the PyTorch framework.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 141, 151), (('v number', '8'), 155, 156)], 'The tumor types or organ labels were evenly separated in the three training, validation, and testing groups and the data were sampled with a batch size of 8'], [[(('parameter', 'epochs'), 160, 166), (('v number', '4'), 27, 28), (('v number', '5'), 59, 60), (('v number', '50'), 157, 159), (('v number', '0.001'), 189, 194)], 'The FiLMed U-Nets of depth 4 for the spinal cord tumor and 5 for the chest CT were trained with a Dice loss function until the validation loss plateaued for 50 epochs early stopping with = 0.001 '], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'learning rate'), 86, 99), (('v number', '0.001'), 30, 35)], 'The initial learning rate was 0.001 and was modulated according to a cosine annealing learning rate.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 54, 60), (('parameter', 'learning rate'), 127, 140), (('v number', '10100'), 40, 45), (('v number', '350'), 50, 53), (('v number', '0.9'), 107, 110), (('v number', '0.1'), 144, 147)], 'We train the softmax baselines on CIFAR-10100 for 350 epochs using SGD as the optimiser with a momentum of 0.9, and an initial learning rate of 0.1'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 45, 51), (('v number', '10'), 39, 41), (('v number', '150'), 52, 55), (('v number', '250'), 60, 63)], 'The learning rate drops by a factor of 10 at epochs 150 and 250'], [[(('parameter', 'epochs'), 48, 54), (('parameter', 'learning rate'), 86, 99), (('parameter', 'weight decay'), 113, 125), (('v number', '90'), 45, 47), (('v number', '0.1'), 103, 106), (('v number', '1e-4'), 129, 133)], 'For models trained on ImageNet, we train for 90 epochs with SGD optimizer, an initial learning rate of 0.1 and a weight decay of 1e-4'], [[(('parameter', 'learning rate'), 9, 22), (('v number', '0.01'), 39, 43), (('v number', '30'), 90, 92), (('v number', '0.1'), 114, 117)], 'We use a learning rate warmup decay of 0.01 along with a step scheduler with step size of 30 and a step factor of 0.1.']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 105, 117), (('v number', '0.3'), 27, 30), (('v number', '0.001'), 126, 131), (('v number', '0.9'), 145, 148)], 'We used a margin value of =0.3 for computing the loss which was then minimized using SGD optimizer, with weight decay rate of 0.001 and momentum 0.9 '], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 90, 96), (('v number', '0.0001'), 37, 43), (('v number', '0.5'), 77, 80), (('v number', '50'), 87, 89)], 'The initial learning rate was set to 0.0001 which was reduced by a factor of 0.5 every 50 epochs'], [[(('parameter', 'epochs'), 61, 67), (('parameter', 'epochs'), 164, 170), (('v number', '60'), 58, 60), (('v number', '200'), 160, 163), (('v number', '10'), 282, 284)], 'For the Oxford Robotcar dataset, we ran training for only 60 epochs and for other larger datasets, Brisbane City Loop, Nordland and MSLS, training was done for 200 epochs this is due to the increased number of negatives in proportion to the size of the database as we only consider 10 negatives for each query '], [[(('artifact', 'Triplet Loss'), 42, 54), (('v number', '520'), 93, 96), (('v number', '1040'), 130, 134)], 'For generating positivesnegatives for the triplet loss, we used a maximumminimum distance of 520 meters for the city datasets and 1040 frames for the Nordland dataset']] \n",
      "\n",
      "[[[(('artifact', 'BiLSTM'), 34, 40), (('v number', '100'), 52, 55)], 'We select the hidden units of the BiLSTM network as 100'], [[(('parameter', 'batch size'), 12, 22), (('artifact', 'Adam'), 34, 38), (('parameter', 'learning rate'), 52, 65), (('v number', '10'), 26, 28), (('v number', '0.0002'), 69, 75)], 'We choose a batch size of 10, and Adam with initial learning rate of 0.0002 '], [[(('artifact', 'L'), 42, 43), (('v number', '35'), 47, 49)], 'When using GTNs, the number of edge-types L is 35, which is determined by the number of unique types of dependency relations, e.g., nsubj, case, etc., as obtained from the dependency parser.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 9, 14), (('artifact', 'model'), 63, 68)], 'When the model is transparent and the explanation displays the model parameters in an intelligible way, humans can directly adjust the parameters based on their judgements'], [[(('parameter', 'steps'), 22, 27), (('v number', '2'), 28, 29), (('v number', '3'), 34, 35)], 'In this special case, steps 2 and 3 in Figure REF are combined into a single step'], [[(('artifact', 'model'), 56, 61), (('v number', '20'), 65, 67), (('v number', '20'), 163, 165)], 'For example, increased a word weight in the Naive Bayes model by 20% for the class that the word supported, according to human feedback, and reduced the weight by 20% for the opposite class binary classification']] \n",
      "\n",
      "[[[(('artifact', 'system'), 18, 24), (('parameter', 'K'), 77, 78), (('parameter', 'batch size'), 85, 95), (('v number', '5'), 79, 80), (('v number', '1'), 96, 97)], 'Additionally, the system is trained on a single machine with a 12GB GPU with k=5 and batch size 1'], [[(('parameter', 'batch size'), 33, 43), (('v number', '1'), 49, 50), (('v number', '16'), 54, 56), (('v number', '8'), 71, 72)], 'We further increase the training batch size from 1 to 16 by leveraging 8 TPUv3 cores on Google Cloud for distributed training'], [[(('parameter', 'K'), 69, 70), (('v number', '10'), 71, 73)], 'Finally, we increase the number of documents passed to the reader to k=10 during training.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 71, 76), (('v number', '1'), 48, 49)], 'In the experiment, the videos are re-sampled at 1 fps and input to the model'], [[(('artifact', 'model'), 7, 12), (('parameter', 'epochs'), 108, 114), (('v number', '20'), 105, 107)], 'During model training, the generator encoder is pre-trained with the surgical phase recognition task for 20 epochs'], [[(('parameter', 'epochs'), 34, 40), (('parameter', 'epoch'), 121, 126), (('parameter', 'epoch'), 156, 161), (('v number', '64'), 135, 137), (('v number', '2000'), 165, 169)], 'During GAN training, we use small epochs to train the generator and the discriminator in an iterative fashion, where the epoch size is 64 and the number of epoch is 2000.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('artifact', 'Adam'), 25, 29), (('parameter', 'learning rate'), 56, 69), (('artifact', 'linear decay'), 92, 104), (('v number', '0.001'), 73, 78)], 'We train the model as an Adam optimizer with an initial learning rate of 0.001 and follow a linear decay strategy'], [[(('parameter', 'dropout'), 17, 24), (('parameter', 'weight decay'), 45, 57), (('v number', '0.2'), 33, 36), (('v number', '0.001'), 66, 71)], 'We use the input dropout rate of 0.2 and the weight decay rate of 0.001 to regularize.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 14, 20), (('parameter', 'K'), 32, 33), (('v number', '3'), 34, 35)], 'For the TAGCN layers, we choose k=3 fixed-size learnable filters as recommended in '], [[(('parameter', 'layers'), 12, 18), (('parameter', 'K'), 30, 31), (('v number', '8'), 32, 33)], 'For the GAT layers, we choose K=8 parallel attention mechanisms to produce rich node features with multi-head attention']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'steps'), 41, 46), (('parameter', 'batch size'), 57, 67), (('parameter', 'learning rate'), 81, 94), (('parameter', 'weight decay'), 127, 139), (('v number', '2048'), 71, 75), (('v number', '1e-4'), 98, 102), (('v number', '0.9'), 109, 112), (('v number', '0.98'), 120, 124), (('v number', '0.01'), 143, 147)], 'We use Adam to train the models for 500k steps, with the batch size of 2048, the learning rate of 1e-4, _1 = 0.9 , _2 = 0.98 , weight decay of 0.01'], [[(('parameter', 'learning rate'), 14, 27), (('parameter', 'steps'), 45, 50), (('artifact', 'linear decay'), 59, 71), (('v number', '10'), 38, 40), (('v number', '000'), 41, 44)], 'We warmup the learning rate for first 10,000 steps then do linear decay']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 60, 73), (('v number', '3'), 77, 78), (('v number', '0.0001'), 79, 85), (('v number', '0.0001'), 113, 119)], 'Adam optimizer and MSE loss are used in both models, with a learning rate of 3 0.0001 in the 3D ResNet case, and 0.0001 in the CNN-GRU'], [[(('parameter', 'batch size'), 49, 59), (('parameter', 'epochs'), 82, 88), (('artifact', 'model'), 116, 121), (('v number', '5'), 96, 97), (('v number', '100'), 102, 105), (('v number', '3'), 127, 128), (('v number', '150'), 133, 136)], 'Another difference between both models is in the batch size, and in the number of epochs, being 5 and 100 in the 3D model, and 3 and 150 in the RNN, respectively'], [[(('artifact', 'model'), 20, 25), (('parameter', 'epoch'), 96, 101), (('v number', '7'), 70, 71), (('v number', '25100'), 102, 107)], 'Also, in the 3D-CNN model, early stopping is used, with a patience of 7, so the training end in epoch 25100']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 70, 76), (('artifact', 'L'), 80, 81)], 'In the pre-training step, we denote the number of Transformer encoder layers as L, the size of hidden vectors as H, and the number of self-attention heads as A'], [[(('artifact', 'L'), 36, 37), (('parameter', 'steps'), 105, 110), (('v number', '12'), 38, 40), (('v number', '768'), 44, 47), (('v number', '12'), 51, 53)], 'We followed the setting of BERTBASE L=12, H=768, A=12, total parameters=110M and continued to train 200K steps from cased BERTBASE checkpoint'], [[(('parameter', 'batch size'), 54, 64), (('v number', '512'), 41, 44), (('v number', '128'), 76, 79)], 'The maximum sequence length was fixed to 512, and the batch size was set to 128'], [[(('artifact', 'Adam'), 8, 12), (('parameter', 'learning rate'), 20, 33), (('v number', '2e-5'), 37, 41), (('v number', '1e-8'), 57, 61)], 'We used Adam with a learning rate of 2e-5 and epsilon of 1e-8 and employed cased BERTBASE vocabulary with 30K tokens.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 35, 48), (('parameter', 'learning rate'), 84, 97), (('parameter', 'steps'), 124, 129), (('v number', '0'), 103, 104), (('v number', '8000'), 119, 123)], 'Following , we also use the “Noam“ learning rate scheduler, linearly increasing the learning rate from 0 for the first 8000 steps, then decaying afterward'], [[(('artifact', 'Adam'), 12, 16), (('v number', '0.998'), 24, 29), (('v number', '0.1'), 66, 69)], \"We also set Adam's _2 = 0.998 and use a label smoothing factor of 0.1 .\"]] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 57, 61), (('parameter', 'learning rate'), 67, 80), (('parameter', 'epochs'), 94, 100), (('v number', '0.001'), 81, 86), (('v number', '10'), 91, 93)], 'All models are trained from multiple random starts using Adam with learning rate 0.001 for 10 epochs'], [[(('parameter', 'batch size'), 7, 17), (('v number', '256'), 21, 24), (('v number', '512'), 29, 32)], 'We set batch size to 256 and 512 for YNOC and POS, respectively.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 27, 32), (('parameter', 'K'), 87, 88), (('v number', '10'), 101, 103)], 'And the number of gradient steps of policy and critic parameters in each update, i.e., K , is set as 10'], [[(('parameter', 'batch size'), 0, 10), (('artifact', 'Adam'), 30, 34), (('v number', '1024'), 21, 25)], 'Batch size is set to 1024 and Adam is used as the optimizer'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0003'), 36, 42)], 'The initial learning rate is set to 0.0003 ']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 132, 139), (('v number', '0'), 115, 116)], 'It consists of the image caption data from MS COCO , Visual Genome , and image question answering data from VQA v2.0 , GQA balanced version and VG-QA ']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 60, 67), (('v number', '3'), 40, 41)], 'All experiments were performed with the 3-billion parameter version of T5-UQA'], [[(('parameter', 'epochs'), 26, 32), (('v number', '30'), 23, 25), (('v number', '1'), 69, 70)], 'Models were trained to 30 epochs, where generation performance ROUGE-1 plateaued'], [[(('parameter', 'batch size'), 57, 67), (('v number', '1'), 71, 72), (('v number', '64'), 93, 95), (('v number', '10'), 258, 260)], 'To improve inference quality, at inference time we use a batch size of 1, a beam search over 64 beams, and given the diversity of generations, and the preference for shorter generations even after considerable training combine all facts generated in the top 10 beams after splicing on the fact delimiter into a candidate list of generated facts.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 66, 71), (('parameter', 'epochs'), 108, 114), (('parameter', 'steps'), 152, 157), (('v number', '2'), 106, 107), (('v number', '10000'), 146, 151)], 'For every run of training, including pre-training and fine-tuning steps wherever applicable, we train for 2 epochs and checkpoint at intervals of 10000 steps']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 48, 52), (('parameter', 'epochs'), 71, 77), (('parameter', 'batch size'), 91, 101), (('v number', '100'), 67, 70), (('v number', '8'), 105, 106)], 'All the networks are trained from scratch using Adam optimizer for 100 epochs with a total batch size of 8'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 115, 121), (('v number', '0.0001'), 42, 48), (('v number', '5'), 64, 65)], 'The learning rate is first initialized to 0.0001 and divided by 5 following milestones at the 30th, 50th, and 80th epochs'], [[(('parameter', 'K'), 25, 26), (('parameter', 'layer'), 58, 63), (('parameter', 'layer'), 90, 95), (('v number', '71'), 55, 57), (('v number', '33'), 87, 89)], 'The output channel width k is [14,16,20,20,40] for the 71-layer and [14,16,40] for the 33-layer']] \n",
      "\n",
      "[[[(('artifact', 'model'), 18, 23), (('artifact', 'model'), 57, 62), (('v number', '2'), 16, 17), (('v number', '1'), 55, 56), (('v number', '2'), 80, 81)], 'ASR-Text-Speech-2 Model Similar to the ASR-Text-Speech-1 model, ASR-Text-Speech-2 also improves upon Text-Speech by taking ASR transcripts as its input'], [[(('artifact', 'model'), 26, 31), (('parameter', 'learning rate'), 104, 117), (('v number', '2e-5'), 121, 125)], 'We domain-adapt this BERT model on the ground truth and ASR transcripts from our target datasets with a learning rate of 2e-5']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('parameter', 'dropout'), 48, 55)], 'The learning rate was selected from , while the dropout rate was selected from for ConvE and for TuckER'], [[(('parameter', 'dropout'), 14, 21), (('v number', '5'), 4, 5)], 'For 5^ E, the dropout rate was not used, but N3 regularization was , its weight selected from '], [[(('artifact', 'model'), 83, 88), (('v number', '300'), 44, 47), (('v number', '500'), 52, 55)], 'For ConvE, models with embedding dimensions 300 and 500 were trained, and the best model for each dimension was saved for fine-tuning'], [[(('parameter', 'layer'), 43, 48), (('v number', '32'), 54, 56), (('v number', '3'), 70, 71), (('v number', '3'), 72, 73)], 'Following , we use a single 2d convolution layer with 32 channels and 3 3 kernel size'], [[(('artifact', 'model'), 82, 87), (('v number', '5'), 4, 5), (('v number', '200'), 43, 46), (('v number', '500'), 51, 54)], 'For 5^ E, models with embedding dimensions 200 and 500 were trained, and the best model for each dimension was saved for fine-tuning'], [[(('artifact', 'model'), 28, 33), (('parameter', 'epochs'), 42, 48), (('v number', '100'), 38, 41)], 'Following , we trained each model for 100 epochs'], [[(('parameter', 'epochs'), 51, 57), (('artifact', 'model'), 67, 72), (('v number', '20'), 48, 50)], 'Testing on the validation set is performed each 20 epochs, and the model with the best overall mean reciprocal rank MRR is selected.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 129, 134), (('v number', '128'), 61, 64)], 'For the kg proposed in this work, the best embedding size is 128 fig:embDim since it is the best compromise between accuracy and model complexity'], [[(('artifact', 'Adam'), 32, 36), (('parameter', 'learning rate'), 42, 55), (('v number', '0.0001'), 58, 64)], 'The best optimizer proved to be ADAM with learning rate = 0.0001 , coherently to fig:optimizer,fig:lr.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 39, 49), (('v number', '128'), 68, 71), (('v number', '512'), 91, 94)], 'For convergence analysis, we set total batch size as 64K for seqlen 128 and 32K for seqlen 512']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 33, 37), (('parameter', 'learning rate'), 68, 81)], 'Our models are trained using the Adam optimizer with the triangular learning rate schedule from ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('artifact', 'Adam'), 25, 29), (('parameter', 'batch size'), 67, 77), (('v number', '0.5'), 38, 41), (('v number', '0.9'), 51, 54), (('v number', '128'), 81, 84)], 'We train the model using Adam with _1=0.5 and _2 = 0.9 with a mini-batch size of 128'], [[(('parameter', 'learning rates'), 34, 48), (('parameter', 'learning rate'), 83, 96)], 'Such hyper-parameters include the learning rates of the encoder and generator, the learning rate of the discriminator using two time-scale update rule TTUR , the gradient penalty coefficient in WGAN-GP, the dimensions of latent vectors from the encoder, in Eq']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'steps'), 108, 113), (('parameter', 'batch size'), 123, 133), (('parameter', 'learning rate'), 146, 159), (('v number', '0.9'), 32, 35), (('v number', '0.999'), 43, 48), (('v number', '4096'), 137, 141), (('v number', '2e-4'), 163, 167)], 'We use Adam optimizer with _1 = 0.9 , _2 = 0.999 to pre-train ElasticBERTBASE and ElasticBERTLARGE for 125K steps with the batch size of 4096 and learning rate of 2e-4']] \n",
      "\n",
      "[[[(('artifact', 'model'), 43, 48), (('parameter', 'layers'), 109, 115), (('v number', '256'), 64, 67), (('v number', '1024'), 97, 101), (('v number', '2'), 107, 108)], 'The individual models are trained with the model hidden size of 256, feed-forward hidden size of 1024, and 2 layers'], [[(('artifact', 'model'), 131, 136), (('parameter', 'layers'), 197, 203), (('v number', '512'), 152, 155), (('v number', '1024'), 185, 189), (('v number', '6'), 195, 196)], 'All multilingual models either cluster-based or universal MNMT models with or without knowledge distillation were trained with the model hidden size of 512, feed-forward hidden size of 1024, and 6 layers'], [[(('artifact', 'Adam'), 11, 15), (('v number', '0.0005'), 85, 91)], 'We use the Adam optimizer and an inverse square root schedule with warmup maximum LR 0.0005'], [[(('parameter', 'dropout'), 9, 16), (('v number', '0.3'), 52, 55), (('v number', '0.1'), 60, 63)], 'We apply dropout and label smoothing with a rate of 0.3 and 0.1 for bilingual and multilingual models respectively.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('v number', '0.3'), 36, 39)], 'The initial learning rate is set to 0.3 which is then adjusted by cosine annealing'], [[(('parameter', 'weight decay'), 32, 44), (('v number', '1e-4'), 48, 52)], 'SGD optimizer is adopted with a weight decay of 1e-4'], [[(('parameter', 'epochs'), 40, 46), (('parameter', 'batch size'), 54, 64), (('v number', '250'), 36, 39), (('v number', '1024'), 68, 72)], 'We train our PSS-Net for a total of 250 epochs with a batch size of 1024.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 39, 44), (('parameter', 'batch size'), 62, 72), (('v number', '64'), 75, 77)], 'For all experiments, we initialize our model with _0= and SGD batch size b=64 '], [[(('parameter', 'K'), 35, 36), (('parameter', 'steps'), 68, 73)], 'In each round, we uniformly sample K devices at random, which run E steps of SGD in parallel'], [[(('parameter', 'learning rate'), 39, 52), (('v number', '0.1'), 56, 59)], 'For all experiments, we use an initial learning rate _0=0.1 with decay rate _0} , where r is communication round index']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 7, 13), (('parameter', 'epochs'), 50, 56), (('v number', '90'), 4, 6), (('v number', '90'), 47, 49)], 'The 90 epochs training setup trains models for 90 epochs using standard preprocessing and allows for fair comparisons with classic works'], [[(('parameter', 'epochs'), 8, 14), (('parameter', 'epochs'), 52, 58), (('v number', '350'), 4, 7), (('v number', '350'), 48, 51)], 'The 350 epochs training setup trains models for 350 epochs using improved data augmentation and regularization and is closer to training methodologies used in modern works with state-of-the-art accuracies.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('v number', '0.01'), 51, 55), (('v number', '0.001'), 60, 65)], 'The initial learning rate is set to values between 0.01 and 0.001 and the decay leads to reduction between one and two orders of magnitude at the end of the training'], [[(('parameter', 'hidden layers'), 39, 52), (('parameter', 'layer'), 71, 76), (('v number', '150'), 57, 60)], 'For all variants of the NNs we use two hidden layers and 150 nodes per layer']] \n",
      "\n",
      "[[[(('parameter', 'm'), 89, 90), (('v number', '64'), 82, 84), (('v number', '0.4'), 91, 94)], 'For experiments with Prototype Memory hyperparameters we used CosFace loss with s=64 and m=0.4 '], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.1'), 31, 34), (('v number', '10'), 50, 52)], 'The learning rate started from 0.1 and divided by 10 at 100k , 200k , 250k , 275k iterations finishing at 300K iterations'], [[(('parameter', 'm'), 105, 106), (('parameter', 'm'), 149, 150), (('v number', '100'), 53, 56), (('v number', '100'), 65, 68), (('v number', '200'), 107, 110), (('v number', '000'), 111, 114), (('v number', '0.2'), 119, 122), (('v number', '64'), 142, 144), (('v number', '0.4'), 151, 154)], 'For the experiments with larger networks, we used PM-100: ResNet-100, pre-trained using Prototype Memory M=200,000 , r=0.2 and CosFace with s=64 and m=0.4 '], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.1'), 31, 34), (('v number', '10'), 50, 52)], 'The learning rate started from 0.1 and divided by 10 at 200k , 400k , and 500k iterations, finishing at 540K '], [[(('parameter', 'batch size'), 5, 15), (('v number', '512'), 19, 22)], 'Mini-batch size is 512 for all models, mini-batch sampling is group-based iterate-and-shuffle.']] \n",
      "\n",
      "[[[(('artifact', 'BiLSTM'), 69, 75), (('parameter', 'layers'), 84, 90)], \"During training, XLM-RoBERTa's weights were kept frozen and only the BiLSTM and CRF layers were updated\"], [[(('artifact', 'BiLSTM'), 16, 22), (('parameter', 'layers'), 79, 85), (('parameter', 'layer'), 166, 171), (('v number', '16'), 28, 30), (('v number', '32'), 32, 34), (('v number', '64'), 36, 38), (('v number', '128'), 40, 43), (('v number', '256'), 45, 48), (('v number', '512'), 53, 56), (('v number', '64'), 144, 146), (('v number', '1'), 164, 165)], 'We experimented BiLSTM with 16, 32, 64, 128, 256 and 512 hidden units and more layers, but in the end, having a small segmentation module, with 64 hidden units and 1 layer, generically yielded the best performances in the validation splits'], [[(('parameter', 'dropout'), 10, 17), (('parameter', 'layer'), 18, 23), (('artifact', 'BiLSTM'), 50, 56), (('parameter', 'learning rate'), 109, 122), (('v number', '0.25'), 33, 37), (('v number', '0.001'), 126, 131)], 'We used a dropout layer of value 0.25 between the BiLSTM and the CRF, and the RMSprop optimizer with a fixed learning rate of 0.001.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 31, 36), (('v number', '21'), 138, 140), (('v number', '21'), 141, 143), (('v range', '[0.2, 4.0]'), 182, 192), (('v range', '[0, 75]'), 223, 230)], 'Specifically, we first train a model by applying degradations of isotropic Gaussian kernels and noises, where the kernel size is fixed at 21 21 , the kernel width is set to the range[0.2, 4.0] and the noise level is set at [0, 75] as in , '], [[(('artifact', 'model'), 18, 23), (('v range', '[0, 25]'), 276, 283)], 'We also train our model on degradations with anisotropic Gaussian kernels and noises, where the kernels have a Gaussian density function N0, , the covariance matrix is determined by a random rotation angle and two random eigenvalues _1, _2 , and the noise is set to the range [0, 25] , as in']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 89, 101), (('v number', '0.9'), 81, 84), (('v number', '0.0001'), 105, 111)], 'The optimizer is the same as for the fitness evaluation SGD with momentum set at 0.9 and weight decay of 0.0001 '], [[(('parameter', 'learning rates'), 4, 18), (('v number', '0.035'), 30, 35), (('v number', '0.05'), 77, 81), (('v number', '40100'), 95, 100)], 'The learning rates are set at 0.035 for the network with FEMNIST as in , and 0.05 for DenseNet-40100'], [[(('parameter', 'layers'), 92, 98), (('parameter', 'activation'), 152, 162), (('v number', '55'), 75, 77), (('v number', '32'), 104, 106), (('v number', '64'), 111, 113), (('v number', '22'), 186, 188)], 'The NN for FEMNIST is also similar to the one used in , i.e., CNN with two 55 convolutional layers with 32 and 64 filters, respectively, each with ReLU activation and each followed by a 22 max pooling'], [[(('parameter', 'layers'), 58, 64), (('v number', '512'), 70, 73), (('v number', '62'), 78, 80)], 'The convolutional part is followed by two fully connected layers with 512 and 62 neurons number of classes, respectively']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 92, 97), (('parameter', 'activation'), 143, 153), (('v number', '10'), 109, 111)], 'All static non-linearities are modeled as feed-forward Neural Networks with a single hidden layer containing 10 neurons and hyperbolic tangent activation function.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 34, 39), (('parameter', 'batch size'), 48, 58), (('parameter', 'epochs'), 93, 99), (('v number', '32'), 45, 47), (('v number', '128'), 63, 66), (('v number', '3'), 91, 92)], 'For MNLI finetuning, we train the model with 32 batch size and 128 max sequence length for 3 epochs'], [[(('artifact', 'AdamW'), 7, 12), (('parameter', 'learning rate'), 33, 46), (('v number', '2e-5'), 28, 32)], 'We use AdamW optimizer with 2e-5 learning rate'], [[(('artifact', 'model'), 35, 40), (('parameter', 'batch size'), 49, 59), (('parameter', 'epochs'), 115, 121), (('v number', '12'), 46, 48), (('v number', '384'), 61, 64), (('v number', '128'), 89, 92), (('v number', '2'), 113, 114)], 'For SQuAD finetuning, we train the model with 12 batch size, 384 max sequence length and 128 document stride for 2 epochs'], [[(('artifact', 'AdamW'), 7, 12), (('parameter', 'learning rate'), 33, 46), (('v number', '3e-5'), 28, 32)], 'We use AdamW optimizer with 3e-5 learning rate']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 59, 65), (('parameter', 'steps'), 77, 82), (('parameter', 'batch size'), 86, 96), (('parameter', 'steps'), 173, 178), (('parameter', 'epochs'), 189, 195), (('v number', '100'), 55, 58), (('v number', '100'), 69, 72), (('v number', '000'), 73, 76), (('v number', '8'), 97, 98), (('v number', '256'), 127, 130), (('v number', '30'), 166, 168), (('v number', '000'), 169, 172), (('v number', '100'), 185, 188)], 'For both SFTs and adapters, we train for the lesser of 100 epochs or 100,000 steps of batch size 8 and maximum sequence length 256, subject to an absolute minimum of 30,000 steps since 100 epochs seemed insufficient for some languages with very small corpora'], [[(('artifact', 'model'), 0, 5), (('parameter', 'steps'), 44, 49), (('v number', '1'), 38, 39), (('v number', '000'), 40, 43), (('v number', '5'), 50, 51), (('v number', '000'), 52, 55), (('v number', '5'), 105, 106), (('v number', '1'), 122, 123)], 'Model checkpoints are evaluated every 1,000 steps 5,000 for high-resource languages on a held-out set of 5% of the corpus 1% for high-resource languages, and the one with the smallest loss is selected at the end of training'], [[(('artifact', 'AdamW'), 11, 16), (('parameter', 'learning rate'), 43, 56), (('v number', '5'), 64, 65), (('v number', '0'), 95, 96)], 'We use the AdamW optimizer with an initial learning rate of 5e -5 which is linearly reduced to 0 over the course of training.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 26, 31), (('v number', '1'), 62, 63), (('v number', '8'), 67, 68)], 'Depending on the task and model, each training run took about 1 to 8 days to complete.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 55, 68), (('v number', '100'), 11, 14), (('v number', '0.01'), 70, 74), (('v number', '0.98'), 98, 102)], 'We perform 100 training iterations starting with a TD0 learning rate =0.01, and multiplying it by 0.98 at each iteration']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('parameter', 'weight decay'), 52, 64), (('v number', '0.00002'), 36, 43), (('v number', '0.05'), 68, 72)], 'The initial learning rate is set as 0.00002 and the weight decay is 0.05 '], [[(('parameter', 'batch size'), 55, 65), (('v number', '512'), 43, 46), (('v number', '512'), 47, 50), (('v number', '16'), 69, 71)], 'We set the crop size of the input image as 512 512 and batch size as 16 by default'], [[(('parameter', 'epochs'), 45, 51), (('v number', '240'), 41, 44)], 'Besides, the networks are fine-tuned for 240 epochs on the train set']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 36, 42), (('v number', '500'), 11, 14), (('v number', '1000'), 16, 20), (('v number', '2000'), 22, 26), (('v number', '5000'), 31, 35)], 'Meanwhile, 500, 1000, 2000 and 5000 epochs are performed in each network']] \n",
      "\n",
      "[[[(('artifact', 'model'), 40, 45), (('v number', '1.0'), 64, 67), (('v number', '0.25'), 73, 77), (('v number', '10'), 85, 87)], 'We further consider the _2 -norm threat model, in which we set =1.0 and =0.25 in the 10-step PGD adversary']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 57, 63), (('parameter', 'dropout'), 159, 166), (('parameter', 'activation'), 199, 209), (('v number', '5'), 55, 56), (('v number', '512'), 85, 88), (('v number', '300'), 115, 118), (('v number', '5'), 145, 146), (('v number', '128'), 148, 151), (('v number', '0.1'), 176, 179)], 'In addition, we implemeneted the Text-CNN and GRU with 5 epochs, batch_size equal to 512, sequence_length equal to 300, conv_layer_size equal to 5, 128 units, dropout equal to 0.1, and using sigmoid activation function'], [[(('parameter', 'epochs'), 44, 50), (('v number', '5'), 42, 43), (('v number', '16'), 78, 80), (('v number', '8'), 111, 112)], 'Finally, we implement the Toxic-BERT with 5 epochs, train_batch_size equal to 16, and test_batch_size equal to 8']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('v number', '6'), 25, 26)], 'The batch size is set to 6 during training'], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'epochs'), 53, 59), (('v number', '3000'), 48, 52)], 'The Adam solver is used to train our models for 3000 epochs'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0001'), 36, 42), (('v number', '0.5'), 66, 69), (('v number', '500'), 87, 90)], 'The initial learning rate is set to 0.0001, the decay rate set to 0.5 and step size is 500']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 35, 45), (('parameter', 'batch size'), 79, 89), (('v number', '128'), 49, 52)], 'For low resolution dataset, we use batch size of 128 by default because larger batch size even makes models slower to converge'], [[(('parameter', 'batch size'), 36, 46), (('v number', '32'), 50, 52)], 'For high resolution dataset, we use batch size of 32 to pre, because of the memory limit of experiment equipment']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 201, 211), (('v number', '60'), 215, 217)], 'We use gradient accumulation for cases where the same number of training batches could not be loaded on the GPUs due to the varying memory consumption required for different methods and set the global batch size to 60'], [[(('parameter', 'learning rate'), 43, 56), (('v number', '1e-3'), 60, 64)], 'We use Adafactor optimizer with an initial learning rate of 1e-3'], [[(('parameter', 'learning rate'), 7, 20), (('parameter', 'learning rate'), 82, 95), (('parameter', 'learning rate'), 119, 132), (('v number', '10'), 43, 45)], 'We use learning rate warm-up for the first 10% of training and linearly decay the learning rate to half of the initial learning rate towards the end of training'], [[(('artifact', 'method'), 75, 81), (('v number', '4'), 35, 36), (('v number', '16'), 114, 116)], 'For all of the experiments, we use 4 32GB V100 GPUs for training with each method except Mix-Review, where we use 16 32GB V100 GPUs']] \n",
      "\n",
      "[[[(('parameter', 'K'), 13, 14), (('parameter', 'K'), 28, 29), (('v number', '4'), 40, 41), (('v number', '1'), 46, 47)], 'For CUB, the k value of top-k is set as 4 and 1 in SCS and SCE cases, respectively'], [[(('parameter', 'K'), 19, 20), (('parameter', 'K'), 34, 35), (('v number', '3'), 39, 40), (('v number', '1'), 45, 46)], 'While for NAB, the k value of top-k is 3 and 1 in the case of SCS and SCE, respectively'], [[(('parameter', 'K'), 4, 5), (('parameter', 'K'), 32, 33)], 'top-k represents the sharing of k classes of text.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 11, 21), (('artifact', 'Adam'), 43, 47), (('parameter', 'learning rate'), 75, 88), (('v number', '2'), 25, 26)], 'We set the batch size as 2 and adopted the Adam optimizer with a one-cycle learning rate policy'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epoch'), 67, 72), (('v number', '0.002'), 37, 42), (('v number', '30'), 84, 86)], 'The maximum learning rate was set to 0.002, and the total training epoch was set to 30']] \n",
      "\n",
      "[[[(('artifact', 'model'), 20, 25), (('v number', '50'), 46, 48)], 'We use a DeepLab-v3 model with dilated ResNet-50 backbone '], [[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 26, 32), (('v number', '60'), 23, 25), (('v number', '64'), 55, 57)], 'We train the model for 60 epochs using batches of size 64'], [[(('artifact', 'model'), 4, 9), (('parameter', 'weight decay'), 64, 76), (('v number', '0.9'), 56, 59)], 'The model weights are updated through SGD with momentum 0.9 and weight decay 1e^ '], [[(('parameter', 'learning rate'), 61, 74), (('v number', '0.004'), 31, 36)], 'The initial learning is set to 0.004 and decayed with a poly learning rate scheme'], [[(('parameter', 'K'), 70, 71), (('v number', '128'), 79, 82)], 'The features of negatives __}, , __} are saved in a memory bank, with K set to 128']] \n",
      "\n",
      "[[[(('parameter', 'learning rates'), 66, 80), (('v number', '8'), 57, 58), (('v number', '16'), 59, 61), (('v number', '1'), 81, 82), (('v number', '1e-05'), 83, 88), (('v number', '3'), 89, 90), (('v number', '1e-05'), 91, 96), (('v number', '5'), 97, 98), (('v number', '1e-05'), 99, 104), (('v number', '8'), 105, 106), (('v number', '1e-05'), 107, 112)], 'We search over the hyperparameter space with batch sizes 8,16 and learning rates 1 1e-05,3 1e-05,5 1e-05,8 1e-05 ']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 0, 4), (('v number', '0.9'), 23, 26), (('v number', '0.98'), 31, 35), (('v number', '1e-09'), 41, 46)], 'Adam optimizer with _1=0.9, _2=0.98 and =1e-09 '], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 56, 61), (('parameter', 'learning rate'), 78, 91), (('v number', '4'), 62, 63), (('v number', '000'), 64, 67), (('v number', '0.0005'), 95, 101)], 'The learning rate scheduler is inverse_sqrt with warmup steps 4,000 , default learning rate is 0.0005 ']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 27, 33), (('v number', '2'), 65, 66), (('v number', '1'), 131, 132), (('v number', '1'), 137, 138)], 'As for the three trainable layers, we set the initial value of = 2 as it is originally defined as the scaling factor, and we set = 1 , = 1 .']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 44, 49), (('parameter', 'batch size'), 57, 67), (('v number', '2048'), 71, 75)], 'We trained all unsupervised methods for 25k steps with a batch size of 2048'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'steps'), 79, 84), (('v number', '1e-5'), 56, 60), (('v number', '1e-3'), 65, 69), (('v number', '2'), 74, 75)], 'We used an Adam optimizer with a linear warm-up between 1e-5 and 1e-3 for 2.5k steps followed by cosine decay schedule as introduced by '], [[(('artifact', 'method'), 38, 44), (('v number', '0.3'), 65, 68), (('v number', '16'), 77, 79), (('v number', '0.4'), 109, 112), (('v number', '12'), 121, 123), (('v number', '2019'), 137, 141)], 'Concerning parameters specific to our method, for n_w we chose = 0.3 and w = 16 on MIMIC-III Benchmark and = 0.4 and w = 12 on Physionet 2019']] \n",
      "\n",
      "[[[(('parameter', 'fold'), 25, 29), (('parameter', 'fold'), 36, 40), (('artifact', 'cross-validation'), 41, 57), (('v number', '3'), 23, 24), (('v number', '5'), 34, 35)], 'To do so, we have made 3-fold and 5-fold cross-validation strategies'], [[(('parameter', 'K'), 62, 63), (('parameter', 'fold'), 66, 70), (('artifact', 'cross-validation'), 71, 87)], 'In addition to the resourceful training approach based on the k - fold cross-validation, we also evaluated our experiments with a frugal training approach based on few-shot learning'], [[(('artifact', 'model'), 26, 31), (('v number', '2'), 35, 36)], 'Here, we only trained the model on 2 samples per class for each dataset'], [[(('parameter', 'learning rates'), 20, 34), (('v number', '0.0001'), 50, 56), (('v number', '0.001'), 61, 66)], 'For all models, the learning rates ranged between 0.0001 and 0.001'], [[(('parameter', 'layers'), 109, 115), (('v number', '256'), 130, 133), (('v number', '512'), 135, 138), (('v number', '0.1'), 140, 143), (('v number', '3'), 148, 149)], 'For DiffPool, the hidden dimension, the output dimension, the assignment ratio and the number of convolution layers were equal to 256, 512, 0.1 and 3, respectively'], [[(('parameter', 'layers'), 28, 34), (('parameter', 'layer'), 59, 64), (('v number', '3'), 90, 91), (('v number', '512'), 93, 96), (('v number', '48'), 101, 103)], 'For g-U-Nets, the number of layers, hidden and convolution layer dimensions were equal to 3, 512 and 48, respectively']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 11, 16), (('parameter', 'learning rate'), 34, 47), (('v number', '1e-6'), 51, 55)], 'We use the AdamW optimizer with a learning rate of 1e-6 for the parameters of the transformer models used in extracting node embeddings'], [[(('artifact', 'Adam'), 71, 75), (('parameter', 'learning rate'), 93, 106), (('v number', '1e-4'), 110, 114)], 'For the parameters of the RGCN encoder and edge classifier, we use the Adam optimizer with a learning rate of 1e-4'], [[(('parameter', 'epochs'), 27, 33), (('parameter', 'batch size'), 41, 51), (('v number', '10'), 24, 26), (('v number', '8'), 55, 56)], 'We train our models for 10 epochs with a batch size of 8 documents']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 36, 46), (('v number', '2'), 50, 51), (('v number', '180'), 58, 61), (('v number', '180'), 62, 65), (('v number', '4'), 66, 67), (('v number', '92'), 129, 131), (('v number', '20'), 157, 159)], 'All models are trained using a mini-batch size of 2 using 180 180 4 image patches that are padded reflect mode on each side with 92 voxels along x and y and 20 along z spatial dimension to account for valid convolutions'], [[(('artifact', 'Adam'), 31, 35), (('parameter', 'learning rate'), 57, 70), (('v number', '1e-05'), 51, 56)], 'Optimization is done using the Adam optimizer with 1e-05 learning rate'], [[(('artifact', 'model'), 4, 9), (('artifact', 'model'), 79, 84), (('v number', '1000'), 50, 54)], 'The model is applied to the validation data every 1000 iterations and the best model according to the Jaccard index is used for the final evaluation.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 34, 38), (('parameter', 'learning rate'), 60, 73), (('parameter', 'steps'), 120, 125), (('v number', '500'), 116, 119)], 'The network was trained using the ADAM optimizer , with the learning rate starting at zero and warming up to 3e^ in 500 steps before remaining at 3e^ '], [[(('parameter', 'batch size'), 11, 21), (('v number', '16'), 64, 66)], 'We set the batch size to one, but accumulated the gradient over 16 batches']] \n",
      "\n",
      "[[[(('artifact', 'method'), 4, 10), (('artifact', 'model'), 38, 43)], 'SGD method is adopted to optimize the model'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epoch'), 129, 134), (('v number', '0.005'), 39, 44), (('v number', '4'), 83, 84), (('v number', '0.1'), 105, 108)], 'The initial learning rate is set to be 0.005 in a single GTX 1080Ti with batchsize 4 and is decreased by 0.1 at the 8th and 11th epoch, respectively'], [[(('parameter', 'epochs'), 30, 36), (('v number', '12'), 18, 20)], 'Totally there are 12 training epochs.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 24, 29), (('v number', '3'), 22, 23), (('v number', '2048'), 56, 60)], 'After the backbone, a 3-layer MLP with hidden dimension 2048 is added as the projection head'], [[(('artifact', 'model'), 31, 36), (('parameter', 'K'), 54, 55), (('parameter', 'epochs'), 111, 117), (('v number', '100'), 107, 110)], 'When evaluating our pretrained model, we both use the k-NN algorithm and train a linear classification for 100 epochs as former works']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '2012'), 60, 64)], 'The model for Places-LT is pre-trained on the full ImageNet-2012 dataset while models for other datasets are trained from scratch'], [[(('parameter', 'epochs'), 72, 78), (('parameter', 'epochs'), 136, 142), (('parameter', 'batch size'), 192, 202), (('v number', '90'), 56, 58), (('v number', '30'), 60, 62), (('v number', '200'), 68, 71), (('v number', '10'), 121, 123), (('v number', '10'), 125, 127), (('v number', '30'), 133, 135), (('v number', '256'), 206, 209), (('v number', '128'), 211, 214), (('v number', '256'), 220, 223)], 'For ImageNet-LT, Places-LT, and iNaturalist18, we train 90, 30, and 200 epochs in the first standard training stage; and 10, 10, and 30 epochs in the second margin calibration stage, with the batch size of 256, 128, and 256, respectively'], [[(('parameter', 'batch size'), 86, 96), (('v number', '10'), 10, 12), (('v number', '100'), 26, 29), (('v number', '13'), 61, 63), (('v number', '000'), 64, 67), (('v number', '512'), 100, 103)], 'For CIFAR-10-LT and CIFAR-100-LT, the models are trained for 13,000 iterations with a batch size of 512'], [[(('parameter', 'weight decay'), 47, 59), (('parameter', 'weight decay'), 117, 129), (('v number', '0.9'), 39, 42), (('v number', '5e-4'), 60, 64), (('v number', '1e-4'), 133, 137)], 'We use the SGD optimizer with momentum 0.9 and weight decay 5e-4 for all datasets except for iNaturalist18 where the weight decay is 1e-4 '], [[(('parameter', 'learning rate'), 48, 61), (('v number', '0.05'), 96, 100), (('v number', '0.1'), 115, 118), (('v number', '0'), 165, 166)], 'In the standard training stage, we use a cosine learning rate schedule with an initial value of 0.05 for CIFAR and 0.1 for other datasets, which gradually decays to 0'], [[(('parameter', 'learning rate'), 49, 62), (('parameter', 'learning rate'), 88, 101), (('v number', '0.05'), 116, 120), (('v number', '0'), 124, 125)], 'In the margin calibration stage, we use a cosine learning rate schedule with an initial learning rate starting from 0.05 to 0 for all datasets']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 124, 130), (('v number', '300'), 120, 123)], 'For the ablation experiments, we follow the standard training schedule and train our models on the ImageNet dataset for 300 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 7, 12), (('v number', '1e-06'), 82, 87)], 'In our model, the top level hyperparameters including _0, _0 , a_0,b_0 are set to 1e-06 , resulting in a noninformative prior']] \n",
      "\n",
      "[[[(('parameter', 'T'), 48, 49), (('parameter', 'T'), 111, 112), (('parameter', 'T'), 117, 118), (('v number', '1'), 113, 114)], 'Each utterance, x^ , is a time-series of length T^ where every time-slice is a vector of audio features, x_t^, t=1, ,T^ ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 48, 54), (('v number', '200'), 40, 43), (('v number', '000'), 44, 47), (('v number', '2'), 76, 77), (('v number', '048'), 78, 81)], 'In each experiment we train the GAN for 200,000 epochs over mini-batches of 2,048 samples, with the generator performing one gradient update per mini-batch and the adversary performing three'], [[(('parameter', 'learning rate'), 11, 24), (('v number', '0.02'), 44, 48)], 'We set the learning rate of the networks to 0.02']] \n",
      "\n",
      "[[[(('parameter', 'K'), 9, 10), (('parameter', 'K'), 95, 96)], 'A set of k tasks with a common source language, a shared encoder across all tasks and a set of k task specific decoders '], [[(('artifact', 'model'), 16, 21), (('artifact', 'L'), 187, 188), (('v number', '1'), 128, 129)], \"Let denote each model's parameters, a probability vector p_1 p_k denoting the probability of sampling a task such that _^ p_i = 1 , datasets for each task I!P_1 I!P_k and a loss function L .\"]] \n",
      "\n",
      "[[[(('parameter', 'layer'), 23, 28), (('parameter', 'layer'), 59, 64)], 'In the GCN, the hidden layer ^ is computed from a previous layer ^ as follows, ^ = ^} ^} ^ ^.']] \n",
      "\n",
      "[[[(('artifact', 'system'), 19, 25), (('v number', '32'), 59, 61)], 'Embedding size for system action and slot values is set as 32'], [[(('parameter', 'layer'), 7, 12), (('v number', '100'), 50, 53)], 'Hidden layer size of the policy network is set as 100'], [[(('artifact', 'Adam'), 7, 11), (('artifact', 'method'), 25, 31), (('parameter', 'learning rate'), 45, 58), (('v number', '1e-3'), 62, 66)], 'We use Adam optimization method with initial learning rate of 1e-3'], [[(('parameter', 'dropout'), 0, 7), (('artifact', 'model'), 73, 78), (('v number', '0.5'), 16, 19)], 'Dropout rate of 0.5 is applied during supervised training to prevent the model from over-fitting.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 67, 80), (('parameter', 'epochs'), 126, 132), (('parameter', 'epochs'), 138, 144), (('parameter', 'epochs'), 153, 159), (('v number', '0.1'), 81, 84), (('v number', '0.1'), 115, 118), (('v number', '200'), 122, 125), (('v number', '300'), 134, 137), (('v number', '350'), 149, 152)], 'We use SGD with Nesterov momentum to update network, starting from learning rate 0.1 and multiplying with a factor 0.1 at 200 epochs, 300 epochs and 350 epochs'], [[(('parameter', 'weight decay'), 0, 12), (('v number', '0.0001'), 23, 29), (('v number', '0.9'), 46, 49)], 'Weight decay is set as 0.0001 and momentum as 0.9'], [[(('parameter', 'batch size'), 26, 36), (('parameter', 'epochs'), 51, 57), (('v number', '64'), 40, 42), (('v number', '400'), 47, 50)], 'We train the network with batch size as 64 for 400 epochs and report the accuracy at the final iteration'], [[(('parameter', 'epochs'), 103, 109), (('parameter', 'learning rate'), 127, 140), (('parameter', 'epochs'), 166, 172), (('parameter', 'epochs'), 178, 184), (('parameter', 'epochs'), 193, 199), (('v number', '200'), 99, 102), (('v number', '0.1'), 155, 158), (('v number', '100'), 162, 165), (('v number', '150'), 174, 177), (('v number', '175'), 189, 192)], 'For Tiny ImageNet, we use the similar training settings as CIFAR, except that we train for totally 200 epochs and multiply the learning rate with a factor 0.1 at 100 epochs, 150 epochs and 175 epochs'], [[(('parameter', 'layer'), 102, 107), (('v number', '2'), 111, 112)], 'To adapt Tiny ImageNet to the networks designed for CIFAR, we set the stride of the first convolution layer as 2, which is adopted in as well.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 205, 211), (('parameter', 'batch size'), 223, 233), (('v number', '1050'), 52, 56), (('v number', '20'), 214, 216), (('v number', '128'), 236, 239), (('v number', '50'), 286, 288), (('v number', '000'), 289, 292), (('v number', '99.5'), 319, 323), (('v number', '10'), 369, 371), (('v number', '000'), 372, 375), (('v number', '98.73'), 401, 406)], 'Hardware: Notebook PC with I7-7700HQ, 16GB RAM, GTX 1050 GPU Software: Matlab 2018a, Neural Network Toolbox, Image Processing Toolbox, Parallel Computing Toolbox Parameter Optimization Settings: SGDM, Max Epochs = 20, Mini-Batch Size = 128 Training Dataset: MNIST training dataset with 50,000 images Training Accuracy: 99.5% Testing Dataset: MNIST testing dataset with 10,000 images Testing Accuracy: 98.73%']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 74, 79), (('artifact', 'method'), 138, 144), (('v number', '0'), 179, 180)], 'We set the network parameters as follows: The weights of each convolution layer of the stacked hourglass network are initialized with the method as described in , the biases with 0'], [[(('parameter', 'layers'), 45, 51), (('parameter', 'dropout'), 55, 62), (('v number', '0.00001'), 110, 117)], 'The networks do not employ any normalization layers or dropout, but use an L2 weight regularization factor of 0.00001'], [[(('parameter', 'batch size'), 132, 142), (('v number', '1'), 146, 147)], 'Due to the demanding training of recurrent neural networks, in terms of both memory and computational requirements, we set the mini-batch size to 1'], [[(('parameter', 'batch size'), 53, 63), (('v number', '10'), 67, 69)], 'For the non-recurrent neural networks, we use a mini-batch size of 10'], [[(('artifact', 'Adam'), 27, 31), (('parameter', 'learning rate'), 65, 78), (('parameter', 'learning rate'), 100, 113), (('v number', '40000'), 42, 47), (('v number', '0.0001'), 82, 88), (('v number', '0.00001'), 128, 135), (('v number', '20000'), 142, 147)], 'We train all networks with ADAM for total 40000 iterations and a learning rate of 0.0001, while the learning rate is reduced to 0.00001 after 20000 iterations']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 115, 120), (('v number', '256'), 147, 150)], 'All embeddings, including the initialized embedding and the output produced by the decoder before the final linear layer, have a dimensionality of 256'], [[(('parameter', 'learning rate'), 9, 22), (('parameter', 'epoch'), 128, 133), (('parameter', 'learning rate'), 144, 157), (('v number', '0.25'), 26, 30), (('v number', '0.1'), 64, 67), (('v number', '1e-05'), 170, 175)], 'We use a learning rate of 0.25 and reduce it by a decay rate of 0.1 once the validation ROUGE score stops increasing after each epoch until the learning rate falls below 1e-05 '], [[(('artifact', 'model'), 51, 56), (('v number', '1'), 302, 303)], 'We first train the basic topic-aware convolutional model with respect to a standard maximum likelihood objective, and then switch to further minimize a mixed training objective , incorporating the reinforcement learning objective L_ and the original maximum likelihood L_ , which is given as L_ = L_ + 1 - L_,']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 4, 11), (('v number', '0.4'), 41, 44)], 'The dropout ratio for the LSTM is set to 0.4~ '], [[(('artifact', 'model'), 11, 16), (('artifact', 'model'), 37, 42), (('v number', '1124'), 74, 78)], 'Unlike the model it is based on, our model uses word embeddings of length 1124'], [[(('artifact', 'model'), 172, 177), (('v number', '100'), 34, 37), (('v number', '1024'), 79, 83)], 'These result from concatenating a 100 dimension learned word embedding, with a 1024 dimension learned linear combination of the internal states of a bidirectional language model run on the input sentence as described in '], [[(('parameter', 'layer'), 15, 20), (('parameter', 'layer'), 42, 47), (('artifact', 'model'), 70, 75)], 'Using a linear layer over the last hidden layer of the classification model, part-of-speech tags are predicted for spans containing single words.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0001'), 36, 42), (('v number', '10'), 58, 60), (('v number', '70'), 67, 69), (('v number', '000'), 70, 73)], 'The initial learning rate is set to 0.0001 and divided by 10 after 70,000 training iterations for the gw and Botany']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 55, 62), (('parameter', 'layers'), 82, 88), (('parameter', 'learning rate'), 130, 143), (('v number', '64'), 51, 53), (('v number', '0.3'), 71, 74), (('v number', '4'), 76, 77), (('v number', '1000'), 97, 101), (('v number', '600'), 125, 128), (('v number', '0.8'), 147, 150)], 'In all our experiments, we use a minibatch size of 64, dropout rate of 0.3, 4 RNN layers of size 1000, a word vector size of 600, learning rate of 0.8 across all LSTM-based multilingual experiments'], [[(('parameter', 'layers'), 58, 64), (('parameter', 'learning rate'), 109, 122), (('v number', '2'), 52, 53), (('v number', '500'), 73, 76), (('v number', '500'), 100, 103), (('v number', '1.0'), 126, 129)], 'For single-source single-target translation, we use 2 RNN layers of size 500, a word vector size of 500, and learning rate of 1.0'], [[(('parameter', 'learning rates'), 4, 18), (('parameter', 'epoch'), 106, 111), (('v number', '0.7'), 47, 50), (('v number', '9'), 112, 113)], 'All learning rates are decaying at the rate of 0.7 if the validation score is not improving or it is past epoch 9'], [[(('parameter', 'epoch'), 6, 11), (('parameter', 'T'), 12, 13), (('parameter', 'T'), 42, 43), (('parameter', 'T'), 45, 46), (('v number', '100'), 34, 37), (('v number', '1'), 38, 39)], 'GL at epoch t is defined as GLt = 100 1- ^t}^t} , modified by us to suit our objective using BLEU scores '], [[(('parameter', 'T'), 3, 4), (('parameter', 'epoch'), 32, 37), (('parameter', 'T'), 38, 39), (('parameter', 'T'), 47, 48), (('parameter', 'epoch'), 76, 81), (('parameter', 'T'), 82, 83)], 'E_^t is the validation score at epoch t and E_^t is the optimal score up to epoch t ']] \n",
      "\n",
      "[[[(('parameter', 'fold'), 95, 99), (('artifact', 'cross-validation'), 100, 116), (('v number', '10'), 92, 94)], 'To train and evaluate all of the methods in a fair and consistent way, we used the standard 10 fold Cross-Validation in FLU2013 dataset, and within each topic of PHM2017 dataset']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 174, 184), (('v number', '32'), 188, 190)], 'For both the MLP and the CNN, we train the network to minimize negative log-likelihood loss, using stochastic gradient descent SGD with the RMSprop update rule and a typical batch size of 32'], [[(('parameter', 'batch size'), 35, 45), (('parameter', 'batch size'), 98, 108), (('v number', '5'), 138, 139)], 'There are a few exceptions to this batch size: when the training set is very small, we adjust the batch size to ensure there are at least 5 training batches'], [[(('parameter', 'K'), 47, 48), (('parameter', 'K'), 84, 85), (('parameter', 'batch size'), 112, 122)], 'Thus, for a training set with N categories and K examples per category a total of N*K training points, we use a batch size of min32, ']] \n",
      "\n",
      "[[[(('artifact', 'L'), 49, 50), (('parameter', 'learning rate'), 84, 97), (('artifact', 'Adam'), 116, 120), (('artifact', 'method'), 134, 140), (('artifact', 'model'), 154, 159), (('artifact', 'model'), 194, 199), (('artifact', 'method'), 244, 250), (('v number', '1'), 51, 52), (('v number', '0.0001'), 101, 107)], 'Our models are trained using Tensorflow with the L^1 norm loss function, we set the learning rate as 0.0001 and use Adam optimisation method to train the model, all weights to be trained in the model are initialised using Xavier initialisation method.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('parameter', 'epochs'), 59, 65), (('v number', '50'), 25, 27), (('v number', '60'), 56, 58)], 'The batch size is set to 50, and models are trained for 60 epochs'], [[(('parameter', 'dropout'), 13, 20), (('parameter', 'activation'), 59, 69), (('parameter', 'layers'), 101, 107), (('v number', '0.5'), 29, 32)], 'The original dropout rate of 0.5 is retained, and the ReLU activation function is used in all weight layers']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('artifact', 'model'), 64, 69), (('parameter', 'learning rate'), 193, 206), (('v number', '40'), 74, 76), (('v number', '000'), 77, 80), (('v number', '1e-05'), 210, 215)], 'The model is fine-tuned from the Microsoft COCO pre-trained SSD model for 40,000 iterations using the standard stochastic gradient descent SGD algorithm and the back-propagation algorithm at a learning rate of 1e-05 .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 142, 148), (('v number', '8'), 60, 61), (('v number', '150'), 138, 141)], 'In the searching stage, we train a small network stacked by 8 cells parent graphs using SNAS with three levels of resource constraint for 150 epochs'], [[(('parameter', 'batch size'), 29, 39), (('v number', '32'), 17, 19), (('v number', '64'), 51, 53), (('v number', '160'), 74, 77)], 'The search takes 32 hoursThe batch size of SNAS is 64 and that of ENAS is 160']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 24, 28), (('parameter', 'learning rate'), 55, 68), (('parameter', 'epochs'), 95, 101), (('parameter', 'learning rate'), 121, 134), (('parameter', 'epochs'), 154, 160), (('v number', '0.001'), 72, 77), (('v number', '90'), 92, 94), (('v number', '0.1'), 138, 141), (('v number', '10'), 151, 153)], 'During training, we use ADAM optimizer with an initial learning rate of 0.001 for the first 90 epochs and then decay the learning rate by 0.1 in every 10 epochs'], [[(('parameter', 'epochs'), 13, 19), (('v number', '120'), 9, 12)], 'We train 120 epochs in total'], [[(('artifact', 'model'), 24, 29), (('v number', '0.55'), 129, 133), (('v number', '0.55'), 177, 181)], 'During training the car model, a proposal is considered positive if its PointsIoU with a certain ground-truth box is higher than 0.55 and negative if its PointsIoU is less than 0.55 with all ground-truth boxes'], [[(('artifact', 'model'), 94, 99), (('v number', '0.5'), 51, 54), (('v number', '0.5'), 59, 62)], 'The positive and negative PointsIoU thresholds are 0.5 and 0.5 for the pedestrian and cyclist model.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 72, 77), (('parameter', 'epochs'), 108, 114), (('parameter', 'dropout'), 142, 149), (('v number', '25'), 20, 22), (('v number', '100'), 104, 107), (('v number', '1'), 136, 137), (('v number', '0.5'), 158, 161)], 'We choose dimension 25 for character-based embeddings and the final CRF-layer, and train the network in 100 epochs with a batch-size of 1 and dropout rate of 0.5'], [[(('artifact', 'method'), 19, 25), (('parameter', 'learning rate'), 73, 86), (('v number', '0.005'), 90, 95)], 'As an optimization method, we use the stochastic gradient descent with a learning rate of 0.005'], [[(('artifact', 'model'), 121, 126), (('v number', '300'), 41, 44), (('v number', '300'), 61, 64)], 'Apart from fitting the LSTM dimension to 300 while using the 300-dimensional pretrained German fastText embeddings , the model is fixed throughout our experiments to these settings']] \n",
      "\n",
      "[[[(('parameter', 'K'), 41, 42), (('v number', '1'), 35, 36), (('v number', '4'), 45, 46)], 'In all our experiments, we set N = 1 and k = 4 , and used SGD with Nesterov momentum , , and cross-entropy loss in training'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'weight decay'), 42, 54), (('v number', '0.1'), 37, 40), (('v number', '5.00'), 58, 62), (('v number', '0001'), 63, 67), (('v number', '0.9'), 82, 85), (('v number', '64'), 108, 110)], 'The initial learning rate was set at 0.1, weight decay to 5.00.0001 , momentum to 0.9 and minibatch size to 64'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 68, 74), (('parameter', 'epochs'), 110, 116), (('v number', '0.2'), 45, 48), (('v number', '60'), 52, 54), (('v number', '120'), 56, 59), (('v number', '160'), 64, 67), (('v number', '200'), 106, 109)], 'The learning rate was dropped by a factor of 0.2 at 60, 120 and 160 epochs, and we trained for a total of 200 epochs, following the settings used in .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '1080'), 42, 46)], 'The model was trained and tested on a GTX 1080 Ti GPU and also tested on a NVIDIA DRIVE PX2'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 126, 132), (('v number', '5e-6'), 32, 36), (('v number', '1e-1'), 69, 73), (('v number', '100'), 122, 125)], 'The learning rate was set to be 5e-6 at the beginning and decayed by 1e-1 when the validation error stopped improving for 100 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 21, 26), (('v number', '64'), 42, 44)], 'For hidden units per layer, we use size = 64'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 64, 70), (('v number', '0.01'), 37, 41)], 'The learning rate was initialized as 0.01 and then decayed over epochs'], [[(('parameter', 'epochs'), 29, 35), (('v number', '5'), 27, 28)], 'We ran our experiments for 5 epochs after which the training starts to convert as the dataset is very large'], [[(('parameter', 'K'), 16, 17), (('artifact', 'L'), 46, 47), (('v number', '50'), 20, 22), (('v number', '3'), 50, 51), (('v number', '60'), 97, 99), (('v number', '40'), 100, 102)], 'Further, we use K = 50 random walks of length l = 3 for each entity We used a traintest split of 60%40% for both the triples set and labels set']] \n",
      "\n",
      "[[[(('artifact', 'model'), 26, 31), (('parameter', 'layers'), 95, 101), (('parameter', 'activation'), 132, 142), (('v number', '320'), 110, 113), (('v number', '512'), 118, 121)], 'For the response side, we model g^Rs_i^R as gs_i^R followed by two fully-connected feedforward layers of size 320 and 512 with tanh activation']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 49, 62), (('artifact', 'gradient clipping'), 131, 148), (('v number', '0.15'), 63, 67), (('v number', '0.1'), 104, 107), (('v number', '2'), 181, 182)], 'We apply the optimization technique Adagrad with learning rate 0.15 and an initial accumulator value of 0.1, as well as employ the gradient clipping with a maximum gradient norm of 2.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 51, 57), (('v number', '30'), 48, 50)], 'Stopping criteria – all models were trained for 30 epochs'], [[(('parameter', 'learning rate'), 0, 13), (('parameter', 'learning rate'), 101, 114), (('v number', '0.01'), 118, 122)], 'Learning rate and optimizers – D1 was optimized using a stochastic gradient descent optimizer with a learning rate of 0.01'], [[(('artifact', 'Adam'), 34, 38), (('parameter', 'learning rate'), 56, 69), (('v number', '0.001'), 73, 78)], 'D2 and G were optimized using the Adam optimizer with a learning rate of 0.001'], [[(('parameter', 'dropout'), 0, 7), (('parameter', 'dropout'), 56, 63), (('parameter', 'layer'), 82, 87), (('v number', '10'), 47, 49)], 'Dropout and batch normalization – G contains a 10% rate dropout after each hidden layer']] \n",
      "\n",
      "[[[(('parameter', 'T'), 56, 57), (('v number', '0.01'), 43, 47)], 'We set the two hyperparameters to =1T and =0.01 , where T is the number of topics, following '], [[(('artifact', 'model'), 130, 135), (('v number', '0.01'), 139, 143)], 'In the re-estimation process, at each step of the EM algorithm, we set the threshold for removing unnecessary components from the model to 0.01 and remove terms with an estimated probability less than this threshold from the language models, as in .']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 70, 83), (('parameter', 'batch size'), 85, 95)], 'The main hyperparameters involved in the training of a 3D network are learning rate, batch size, and the number of iterations']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 30, 43), (('v number', '0.001'), 47, 52)], 'Adam optimizer is used with a learning rate of 0.001'], [[(('parameter', 'layer'), 12, 17), (('v number', '128'), 61, 64), (('v number', '256'), 69, 72)], 'We used one layer LSTM models with hidden dimensions of size 128 and 256'], [[(('artifact', 'model'), 12, 17), (('v number', '10'), 41, 43)], 'For the GEM model, we keep one minibatch 10 examples of data per task for obtaining the projected gradients']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 101, 106), (('v number', '40'), 95, 97)], 'Moreover, in the rightmost plot the learning curve for the smaller dataset goes down after the 40-th epoch, which does not happen when the larger dataset is used'], [[(('artifact', 'model'), 55, 60), (('v number', '20'), 18, 20)], 'This shows that N=20 is a reasonable trade-off between model performance and cost of labelling.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 45, 50), (('v number', '0.0004'), 118, 124), (('v number', '200'), 128, 131), (('v number', '0.99'), 135, 139), (('v number', '1e-8'), 142, 146)], 'We have used RMSPROP optimizer to update the model parameter and configured hyper-parameter values to be as follows: =0.0004, = 200, = 0.99, =1e-8 to train the classification network '], [[(('artifact', 'model'), 28, 33), (('artifact', 'model'), 80, 85), (('artifact', 'model'), 86, 91), (('v number', '0.001'), 147, 152), (('v number', '200'), 156, 159), (('v number', '0.9'), 163, 166), (('v number', '1e-8'), 169, 173)], 'In order to train a triplet model, we have used RMSPROP to optimize the triplet model model parameter and configure hyper-parameter values to be: =0.001, = 200, = 0.9, =1e-8 '], [[(('parameter', 'learning rate'), 13, 26), (('parameter', 'learning rate'), 49, 62), (('parameter', 'epoch'), 72, 77)], 'We also used learning rate decay to decrease the learning rate on every epoch by a factor given by: Decay_factor=exp']] \n",
      "\n",
      "[[[(('parameter', 'K'), 85, 86), (('v number', '1'), 88, 89)], 'showed that for a single independent variable having q unique values, a mixture with k -1 can be identified, at least if each of the logistic regressions is itself also identified in the same way as a standard non-mixture logistic regression cannot be fit without additional constraints if the number of parameters is larger than the number of observations']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 9, 19), (('v number', '64'), 23, 25), (('v number', '7'), 57, 58), (('v number', '35'), 89, 91), (('v number', '35'), 92, 94)], 'We use a batch size of 64 and set the depth of SCAN as d=7 with the receptive field size 35 35 '], [[(('artifact', 'Adam'), 22, 26), (('parameter', 'learning rate'), 61, 74), (('v number', '5'), 75, 76), (('v number', '0.001'), 77, 82)], 'For optimization, the ADAM algorithm is adopted with a start learning rate 5 0.001 '], [[(('parameter', 'learning rate'), 21, 34), (('v number', '10'), 49, 51), (('v number', '15'), 55, 57), (('v number', '000'), 58, 61), (('v number', '17'), 66, 68), (('v number', '500'), 69, 72)], 'During training, the learning rate is divided by 10 at 15,000 and 17,500 iterations.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 19, 24), (('parameter', 'layers'), 34, 40)], 'For the hyperbolic model, the two layers of parameters were identified as this resulted in better performance in informal experiments']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 8, 12), (('parameter', 'T'), 41, 42)], 'We used Adam to minimize the L2 loss w.r.t']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 94, 100), (('parameter', 'batch size'), 109, 119), (('v number', '200'), 90, 93), (('v number', '32'), 123, 125)], 'Across all experiments, for all three models, training is performed for a fixed number of 200 epochs using a batch size of 32'], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 101, 114), (('parameter', 'weight decay'), 149, 161), (('v number', '0.001'), 115, 120), (('v number', '1'), 122, 123), (('v number', '0.9'), 124, 127), (('v number', '2'), 129, 130), (('v number', '0.999'), 131, 136), (('v number', '1e-08'), 142, 147), (('v number', '0'), 167, 168)], 'The Adam optimizer is used through the learning process with the following values for its parameters—learning rate=0.001, 1=0.9, 2=0.999, eps=1e-08, weight decay rate=0, amsgrad=False']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 24, 29), (('parameter', 'layer'), 74, 79), (('parameter', 'layer'), 129, 134), (('v number', '512'), 163, 166)], 'The encoder is a single-layer bidirectional LSTM, the decoder is a single-layer unidirectional LSTM, and the classifier is a two layer feed-forward network with a 512 hidden dimension'], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'dropout'), 33, 40), (('v number', '64'), 18, 20), (('v number', '0.2'), 62, 65), (('v number', '0.05'), 67, 71), (('v number', '0.0'), 73, 76)], 'The batch size is 64, and we use dropout with probability p = 0.2, 0.05, 0.0 for Toys, Sports, and Movies datasets, respectively.']] \n",
      "\n",
      "[[[(('parameter', 'T'), 27, 28), (('artifact', 'model'), 33, 38), (('parameter', 'learning rate'), 55, 68), (('parameter', 'batch size'), 70, 80), (('parameter', 'epoch'), 91, 96), (('parameter', 'hidden layers'), 116, 129)], 'The hyperparameters of the T-GCN model mainly include: learning rate, batch size, training epoch, and the number of hidden layers'], [[(('parameter', 'learning rate'), 50, 63), (('parameter', 'batch size'), 78, 88), (('parameter', 'epoch'), 113, 118), (('v number', '0.001'), 67, 72), (('v number', '64'), 92, 94), (('v number', '3000'), 122, 126)], 'In the experiment, we manually adjust and set the learning rate to 0.001, the batch size to 64, and the training epoch to 3000.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 44, 49), (('artifact', 'model'), 91, 96)], 'The 3DDFA data provides projection and 3DMM model parameters for the Basel + FaceWarehouse model for each image of the 300W database']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 23, 29), (('v number', '40'), 20, 22)], 'Training is run for 40 epochs'], [[(('parameter', 'learning rates'), 0, 14), (('v number', '24'), 24, 26), (('v number', '14'), 46, 48)], 'Learning rates begin at 24 for generators and 14 for discriminators, are constant for the first half of training and decreasing linearly to zero during the second half']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 25, 29), (('parameter', 'learning rate'), 39, 52), (('parameter', 'batch size'), 75, 85), (('v number', '0.001'), 56, 61), (('v number', '64'), 89, 91), (('v number', '32'), 106, 108)], 'We train our models with Adam , with a learning rate of 0.001 , and a mini-batch size of 64 for BTEC, and 32 for LibriSpeech because of memory constraints'], [[(('parameter', 'variational dropout'), 7, 26), (('parameter', 'dropout'), 44, 51), (('parameter', 'steps'), 107, 112), (('v number', '0.2'), 129, 132), (('v number', '0.4'), 153, 156)], 'We use variational dropout , i.e., the same dropout mask is applied to all elements in a batch at all time steps, with a rate of 0.2 for LibriSpeech and 0.4 for BTEC']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 19, 23), (('parameter', 'learning rate'), 50, 63), (('v number', '0.0001'), 67, 73)], 'We make use of the ADAM optimiser with an initial learning rate of 0.0001 '], [[(('parameter', 'epochs'), 27, 33), (('v number', '6'), 25, 26), (('v number', '1e-06'), 70, 75)], 'We train the network for 6 epochs while reducing the learning-rate to 1e-06 .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 58, 63), (('artifact', 'LSTMs'), 64, 69), (('v number', '2'), 56, 57)], 'Specifically, both the encoder and decoder consisted of 2-layer LSTMs'], [[(('parameter', 'learning rate'), 49, 62), (('v number', '1.0'), 66, 69)], 'Networks were trained using Adadelta with a base learning rate of 1.0'], [[(('parameter', 'dropout'), 0, 7), (('parameter', 'layers'), 16, 22), (('v number', '0.3'), 34, 37)], 'Dropout between layers was set at 0.3.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 26, 32), (('parameter', 'epochs'), 109, 115)], 'We selected the number of epochs for stopping training by contrasting training loss and validation loss over epochs'], [[(('parameter', 'epochs'), 69, 75), (('v number', '100'), 65, 68)], 'Figure REF shows the curves of training and validation loss over 100 epochs'], [[(('parameter', 'epochs'), 91, 97), (('v number', '50'), 88, 90)], 'It could be observed that the validation loss did not show a descending trend at around 50 epochs'], [[(('parameter', 'epochs'), 24, 30), (('v number', '50'), 21, 23), (('v number', '1'), 59, 60), (('v number', '2'), 109, 110)], 'The reason to choose 50 epochs rather than a higher one is 1 to avoid over fitting on the training data, and 2 keep low computational cost.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 102, 112), (('v number', '220'), 46, 49), (('v number', '000'), 50, 53), (('v number', '24'), 116, 118)], 'The proposed Tiny SSD network was trained for 220,000 iterations in the Caffe framework with training batch size of 24'], [[(('parameter', 'learning rate'), 54, 67), (('v number', '0.00001'), 75, 82), (('v number', '0.5'), 89, 92)], 'RMSProp was utilized as the training policy with base learning rate set to 0.00001 and = 0.5 .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 38, 43), (('v number', '500'), 48, 51)], 'Our experiment, our pointer-generator model has 500-dimensional hidden states and word embeddings'], [[(('artifact', 'model'), 17, 22), (('v number', '1'), 83, 84), (('v number', '3'), 94, 95), (('v number', '5'), 139, 140)], 'We take the best model as our generator and during the decoding stage, we generate 1-best and 3-best using beam search with a beam size of 5']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 17, 21), (('v number', '0.9'), 55, 58), (('v number', '0.999'), 67, 72)], 'During training, Adam optimization is applied with _1 =0.9 and _2 =0.999'], [[(('parameter', 'batch size'), 113, 123), (('v number', '1024'), 96, 100), (('v number', '512'), 101, 104), (('v number', '1'), 127, 128)], 'Due to the GPU memory limitation, the images used in our experiments are resized and cropped to 1024 512 and the batch size is 1']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 29, 34), (('artifact', 'BiLSTM'), 35, 41), (('parameter', 'layer'), 63, 68), (('parameter', 'layer'), 144, 149), (('v number', '200'), 106, 109)], 'For all models, we use a two-layer biLSTM as encoder and a two-layer unidirectional LSTM as decoder, with 200-dimensional hidden states in each layer'], [[(('parameter', 'dropout'), 9, 16), (('v number', '0.8'), 57, 60)], 'We apply dropout on RNN cells with a keep probability of 0.8'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 28, 41), (('v number', '0.001'), 45, 50)], 'We use Adam with an initial learning rate of 0.001 to optimize the cross-entropy loss'], [[(('artifact', 'gradient clipping'), 0, 17), (('v number', '2'), 59, 60)], 'Gradient clipping is also applied with the maximum norm of 2']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 43, 55), (('v number', '1e-05'), 59, 64), (('v number', '0.95'), 90, 94)], 'Each of the training procedures includes a weight decay of 1e-05 and a discounted factor =0.95 .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 5, 10), (('parameter', 'hidden layers'), 36, 49), (('parameter', 'layer'), 71, 76), (('v number', '80'), 54, 56)], 'Each model is shallow with only two hidden layers and 80 units in each layer'], [[(('artifact', 'model'), 54, 59), (('v number', '35'), 115, 117)], 'Early stopping was used to prevent overfitting during model training, and not all models were trained for the full 35 iterations.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 13, 26), (('parameter', 'm'), 31, 32), (('parameter', 'batch size'), 45, 55)], 'where is the learning rate and M is the mini-batch size'], [[(('parameter', 'learning rate'), 40, 53), (('artifact', 'Adam'), 78, 82), (('parameter', 'm'), 167, 168), (('v number', '16'), 169, 171)], 'In the mini-batch gradient descent, the learning rate is controlled using the Adam optimizer with the hyperparameters recommended in , and the mini-batches are set as M=16 examples'], [[(('parameter', 'K'), 45, 46), (('artifact', 'L'), 69, 70)], 'The detailed settings of the hyperparameters K , T_k , , , l_0 , and l are described for each experimental task in Section .']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 102, 114), (('v number', '0.9'), 94, 97), (('v number', '0.0005'), 115, 121)], 'We optimize Tube-CNN and TPN with the stochastic gradient descent SGD algorithm with momentum 0.9 and weight decay 0.0005 on mini-batches '], [[(('parameter', 'T'), 19, 20), (('v number', '10'), 21, 23)], 'We fix tube length T=10 for all tube models']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 17, 21), (('v number', '0.5'), 55, 58), (('v number', '0.999'), 67, 72)], 'During training, Adam optimization is applied with _1 =0.5 and _2 =0.999'], [[(('artifact', 'model'), 13, 18), (('parameter', 'learning rate'), 48, 61), (('v number', '0.0001'), 62, 68)], 'We train the model on a single Titan X GPU with learning rate=0.0001'], [[(('artifact', 'cross-validation'), 75, 91), (('artifact', 'method'), 92, 98), (('v number', '10'), 43, 45)], 'For the weighted factor _1 , we apply _1 = 10 , which is chosen by using a cross-validation method.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 15, 20), (('parameter', 'epochs'), 29, 35), (('v number', '250'), 25, 28), (('v number', '0.9'), 71, 74)], 'We trained the model for 250 epochs using SGD with a momentum value of 0.9 '], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'learning rate'), 43, 56), (('parameter', 'epochs'), 93, 99), (('parameter', 'batch size'), 107, 117), (('v number', '0.1'), 36, 39), (('v number', '0.5'), 80, 83), (('v number', '20'), 90, 92), (('v number', '128'), 121, 124)], 'We used an initial learning rate of 0.1, a learning rate multiplicative drop of 0.5 every 20 epochs, and a batch size of 128']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('v number', '0.1'), 36, 39)], 'The learning rate is initialized as 0.1, and decreases with the operation with the staircase in tensorflow'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '3e-4'), 64, 68)], 'The learning rate for generator and discriminator is fixed with 3e-4 .']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 74, 78), (('parameter', 'weight decay'), 84, 96), (('parameter', 'batch size'), 115, 125), (('v number', '8'), 15, 16), (('v number', '1e-06'), 100, 105), (('v number', '64'), 129, 131)], 'We spend about 8 hours on training the SRR-net by using the Caffe and use Adam with weight decay of 1e-06 and mini-batch size of 64'], [[(('parameter', 'K'), 32, 33), (('artifact', 'L'), 71, 72), (('v number', '12'), 37, 39), (('v number', '3'), 76, 77)], 'Besides, we set the growth rate K to 12, the number of the denseblocks L is 3'], [[(('artifact', 'Adam'), 52, 56), (('parameter', 'weight decay'), 62, 74), (('parameter', 'batch size'), 94, 104), (('v number', '0.0001'), 78, 84), (('v number', '10'), 108, 110)], 'We use the pytorch to construct the network and use Adam with weight decay of 0.0001 and mini-batch size of 10'], [[(('parameter', 'learning rate'), 16, 29), (('parameter', 'learning rate'), 47, 60), (('v number', '0.001'), 33, 38), (('v number', '0.95'), 70, 74)], 'We start with a learning rate of 0.001 and the learning rate decay of 0.95.']] \n",
      "\n",
      "[[[(('parameter', 'm'), 758, 759), (('v number', '2014'), 365, 369)], 'We also tried other types of word embeddings, such as the word2vec embeddings pre-trained on the Google News datasethttps:code.google.comarchivepword2vec denoted as “Word2vec-News”, word2vec embeddings pre-trained on the Wikipedia corpushttps:github.comjind11word2vec-on-wikipedia denoted as “Word2vec-wiki”, GloVe embeddings pre-trained on the corpus of Wikipedia 2014 + Gigaword 5http:nlp.stanford.edudataglove.6B.zip denoted as “Glove-wiki”, fastText embeddings pre-trained on Wikipediahttps:github.comfacebookresearchfastTextblobmaster pretrained-vectors.md denoted as “FastText-wiki”, and fastText embeddings initialized with the standard GloVe Common Crawl embeddings and then fine-tuned on PubMed abstracts plus MIMIC-III notes denoted as “FastText-P.M.+MIMIC”']] \n",
      "\n",
      "[[[(('artifact', 'model'), 10, 15), (('parameter', 'layer'), 42, 47)], 'A smaller model with only one LSTM or GRU layer was not performing well on this task'], [[(('parameter', 'layers'), 42, 48), (('parameter', 'layers'), 59, 65)], 'Experiments with a larger number of gated layers and dense layers also failed to give improvements in performance'], [[(('parameter', 'layers'), 53, 59), (('v number', '2'), 45, 46)], 'We got the best classification results using 2 gated layers for both LSTM and GRU models'], [[(('parameter', 'layer'), 57, 62), (('parameter', 'activation'), 80, 90)], 'Therefore, our LSTM and BLSTM models consist of two LSTM layer with function as activation'], [[(('parameter', 'layers'), 49, 55), (('parameter', 'layer'), 80, 85), (('parameter', 'layer'), 110, 115), (('parameter', 'layer'), 142, 147)], 'For each heartbeat, the outputs of LSTM or BLSTM layers were given to the dense layer and the outputs of this layer were given to the softmax layer for classification refer to Section .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 31, 36), (('v number', '100'), 40, 43), (('v number', '100'), 56, 59), (('v number', '100'), 136, 139)], 'We use BiLSTMs with one hidden layer of 100 dimensions, 100-dimensional randomly initialised word embeddings, a label embedding size of 100'], [[(('parameter', 'learning rate'), 36, 49), (('parameter', 'batch size'), 63, 73), (('v number', '0.001'), 53, 58), (('v number', '128'), 77, 80), (('v number', '3'), 159, 160)], 'We train our models with RMSProp, a learning rate of 0.001 , a batch size of 128, and early stopping on the validation set of the main task with a patience of 3.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 15, 20), (('parameter', 'epochs'), 28, 34), (('parameter', 'batch size'), 49, 59), (('v number', '10'), 25, 27), (('v number', '64'), 60, 62)], 'We trained our model for 10 epochs with the mini-batch size 64'], [[(('artifact', 'LSTMs'), 127, 132), (('v number', '300'), 38, 41), (('v number', '300'), 94, 97)], 'The size of word embedding was set as 300, and the size of LSTM hidden states was also set as 300 for the forward and backward LSTMs, respectively'], [[(('parameter', 'dropout'), 0, 7), (('parameter', 'hidden state'), 26, 38), (('v number', '0.3'), 53, 56)], 'Dropout is applied to the hidden state with its rate 0.3'], [[(('parameter', 'learning rate'), 37, 50), (('v number', '1.0'), 51, 54)], 'We used SGD as an optimizer with the learning rate 1.0'], [[(('artifact', 'gradient clipping'), 94, 111), (('v number', '5.0'), 145, 148)], 'Our parameters, which include word embeddings, were uniformly initialized in [-0.1, 0.1], and gradient clipping was used with the clipping value 5.0']] \n",
      "\n",
      "[[[(('artifact', 'model'), 120, 125), (('parameter', 'layers'), 184, 190), (('v number', '200'), 72, 75), (('v number', '200'), 199, 202)], 'We use the convolutional and deconvolutional networks from , a GRU with 200 units as deterministic path in the dynamics model, and implement all other functions as two fully connected layers of size 200 with ReLU activations ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 19, 24), (('parameter', 'learning rate'), 270, 283), (('v number', '32'), 159, 161), (('v number', '5'), 197, 198), (('v number', '100'), 230, 233), (('v number', '000'), 234, 237), (('v number', '0.0001'), 286, 292), (('v number', '0.8'), 308, 311), (('v number', '5000'), 318, 322), (('v number', '60'), 340, 342), (('v number', '000'), 343, 346)], 'For our short-term model, the VGRU-r1 MA, we trained on all action classes using our proposed multi-objective cost, calculating gradients over mini-batches of 32 samples clipping gradient norms to 5 and optimizing parameters over 100,000 iterations RMSprop with initial learning rate = 0.0001 and decayed by 0.8 every 5000 iterations until 60,000 iterations'], [[(('artifact', 'model'), 4, 9), (('v number', '50'), 20, 22), (('v number', '10'), 71, 73), (('v number', '400'), 92, 95)], 'The model was given 50 seed frames and tasked with predicting the next 10 subsequent frames 400 milliseconds'], [[(('parameter', 'steps'), 117, 122), (('v number', '60'), 114, 116)], 'When training for this, the VTLN-RNN is unrolled backwards while the Body-RNN is unrolled forwards, in time, over 60 steps']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('v number', '0.0003'), 21, 27), (('v number', '0.9'), 48, 51)], 'The learning rate is 0.0003 and the momentum is 0.9 ']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 66, 70), (('parameter', 'learning rate'), 97, 110), (('v number', '0.001'), 114, 119)], 'For IBP, across all datasets, the networks were trained using the Adam algorithm with an initial learning rate of 0.001 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 49, 54), (('parameter', 'layers'), 62, 68), (('v number', '5'), 60, 61)], 'DeepRNNSearch GRU: a deep GRU-equipped RNNSearch model with 5 layers'], [[(('parameter', 'hidden state'), 43, 55), (('v number', '620'), 59, 62), (('v number', '1000'), 67, 71)], 'We set the dimension of word embedding and hidden state to 620 and 1000 respectively'], [[(('artifact', 'model'), 4, 9), (('v number', '256'), 47, 50)], 'The model was trained with a minibatch size of 256.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 46, 56), (('artifact', 'model'), 70, 75)], 'Lowering max_length allows us to use a higher batch size andor bigger model but biases the translation towards shorter sentences'], [[(('parameter', 'steps'), 23, 28), (('parameter', 'epochs'), 49, 55), (('parameter', 'batch size'), 78, 88)], 'The number of training steps can be converted to epochs by multiplying by the batch size and dividing by the number of subwords in the training data'], [[(('artifact', 'model'), 0, 5), (('artifact', 'model'), 53, 58)], 'Model size is number of trainable parameters of each model'], [[(('artifact', 'model'), 29, 34), (('artifact', 'model'), 98, 103)], 'Because of the difference in model structures, it is almost certain that two models with the same model size will not have the same training time.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 28, 34), (('v number', '5'), 39, 40), (('v number', '9'), 45, 46), (('v number', '5'), 58, 59), (('v number', '9'), 71, 72)], 'The number of convolutional layers are 5 and 9 for IntNet-5 and IntNet-9, respectively, and we have adopted the same weight initialization as that of ResNet'], [[(('artifact', 'LSTMs'), 37, 42), (('v number', '256'), 53, 56)], 'The state size of the bi-directional LSTMs is set to 256']] \n",
      "\n",
      "[[[(('artifact', 'model'), 92, 97), (('v number', '90'), 33, 35), (('v number', '10'), 47, 49)], 'We splitted our training sets in 90% train and 10% validation, and used the best performing model on the validation set for the final test, following the validation strategy described in Section '], [[(('parameter', 'batch size'), 10, 20), (('parameter', 'learning rate'), 114, 127), (('v number', '96'), 24, 26), (('v number', '32'), 27, 29), (('v number', '0.9'), 98, 101), (('v number', '0.01'), 131, 135), (('v number', '0.007'), 140, 145)], \"We used a batch size of 96 32 images per source domain and trained using SGD with momentum set at 0.9 and initial learning rate at 0.01 and 0.007 for ResNet's and AlexNet's experiments respectively\"], [[(('parameter', 'epoch'), 17, 22), (('parameter', 'steps'), 48, 53), (('parameter', 'epochs'), 139, 145), (('parameter', 'learning rate'), 159, 172), (('parameter', 'epochs'), 201, 207), (('v number', '30'), 136, 138), (('v number', '0.2'), 188, 191), (('v number', '10'), 198, 200)], 'We considered an epoch as the minimum number of steps necessary to iterate over the largest source domain and we trained our models for 30 epochs, scaling the learning rate by a factor of 0.2 every 10 epochs']] \n",
      "\n",
      "[[[(('parameter', 'm'), 72, 73), (('v number', '1505'), 67, 71), (('v number', '2.80'), 82, 86), (('v number', '128'), 117, 120)], 'The hardware device is workstation configured with a Intel Xeon E3-1505 M v5 CPU @2.80 GHz, a NVIDIA Titan X GPU and 128 GB DDR3 Memory'], [[(('parameter', 'layer'), 20, 25), (('v number', '0.01'), 77, 81), (('v number', '0'), 118, 119)], 'The weights in each layer are initialized based on a normal distribution N0, 0.01 , and the biases are initialized to 0'], [[(('parameter', 'learning rates'), 4, 18), (('v number', '0.01'), 30, 34)], 'The learning rates start with 0.01 and are gradually decreased along the training progress'], [[(('parameter', 'learning rates'), 65, 79), (('parameter', 'learning rates'), 138, 152), (('v number', '110'), 95, 98), (('v number', '0.001'), 157, 162)], 'That is, if the objective function is convergent at a stage, the learning rates are reduced to 110 of the current values, and the minimum learning rates are 0.001.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 87, 91), (('parameter', 'learning rate'), 107, 120), (('v number', '0.001'), 121, 126), (('v number', '0.9'), 132, 135), (('v number', '0.999'), 141, 146)], 'Training setups: The loss function is the mean squared error MSE and is optimized with Adam optimizer with learning rate=0.001, _1 =0.9, _2 =0.999'], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'epochs'), 72, 78), (('v number', '256'), 36, 39), (('v number', '1000'), 67, 71)], 'The batch size used in this work is 256 and models are trained for 1000 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 30, 36), (('v number', '6'), 69, 70)], 'More precisely, the number of layers in the encoder and decoder is N=6 '], [[(('parameter', 'layers'), 35, 41), (('v number', '8'), 14, 15)], 'We employ h = 8 parallel attention layers, or heads'], [[(('parameter', 'layer'), 67, 72), (('v number', '512'), 47, 50), (('v number', '2048'), 122, 126)], 'The dimensionality of input and output is d_ = 512 , and the inner-layer of a feed-forward networks has dimensionality d_=2048 .']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 77, 81), (('parameter', 'epochs'), 112, 118), (('v number', '60'), 109, 111)], 'We trained CRsAE using mini-batch gradient descent back-propagation with the ADAM optimizer for a maximum of 60 epochs'], [[(('parameter', 'K'), 38, 39), (('v number', '45'), 44, 46), (('v number', '3'), 47, 48), (('v number', '135'), 51, 54)], 'The number of training parameters was K C = 45 3 = 135 '], [[(('artifact', 'L'), 34, 35), (('parameter', 'learning rate'), 42, 55), (('artifact', 'Adam'), 63, 67), (('parameter', 'T'), 110, 111), (('parameter', 'batch size'), 127, 137)], 'The parameters to be tuned were , L , the learning rate of the ADAM optimizer, the number of FISTA iterations T , and the mini-batch size B ']] \n",
      "\n",
      "[[[(('parameter', 'activation'), 21, 31), (('artifact', 'model'), 180, 185), (('v number', '1'), 230, 231)], 'Figure REF shows the activation patterns and the corresponding informative time-windows of the components with the maximum contribution to the decoding of each class in the LF-CNN model, trained on the pooled data from Experiment 1 and updated using the pseudo-real time update procedure described above on single held-out subject']] \n",
      "\n",
      "[[[(('artifact', 'model'), 8, 13), (('artifact', 'model'), 87, 92)], 'As TNRD model serves as a strong baseline, we initialize the parameters using the TNRD model']] \n",
      "\n",
      "[[[(('artifact', 'model'), 25, 30), (('v number', '16'), 11, 13)], 'We use VGG-16 pretrained model on ImageNet in two parallel CNN pipelines'], [[(('parameter', 'batch size'), 59, 69), (('v number', '2'), 73, 74), (('v number', '0.9'), 105, 108)], 'The stochastic gradient descent SGD are used with the mini-batch size of 2 and the momentum parameter of 0.9 '], [[(('parameter', 'learning rate'), 19, 32), (('v number', '0.0005'), 36, 42)], 'We set the initial learning rate to 0.0005 '], [[(('parameter', 'weight decay'), 11, 23), (('v number', '0.0005'), 71, 77)], 'We set the weight decay parameter applied to L2 regularization term to 0.0005 .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 36, 41), (('v number', '100'), 15, 18), (('v number', '000'), 19, 22), (('v number', '98.23'), 74, 79)], 'We use the top 100,000 words as the model vocabulary since they can cover 98.23% of the training data']] \n",
      "\n",
      "[[[(('artifact', 'model'), 15, 20), (('artifact', 'model'), 70, 75)], 'We propose the model to work for various resolutions, so we train the model with materials of various scales'], [[(('artifact', 'model'), 54, 59), (('v number', '3'), 25, 26), (('v number', '000'), 27, 30), (('v number', '000'), 31, 34)], 'We randomly sample about 3,000,000 pairs to train the model']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 18, 31), (('parameter', 'weight decay'), 40, 52), (('v number', '0.9'), 67, 70), (('v number', '0.0001'), 72, 78), (('v number', '0.001'), 83, 88)], 'The momentum, the learning rate and the weight decay are set to be 0.9, 0.0001 and 0.001 respectively'], [[(('parameter', 'epochs'), 9, 15), (('artifact', 'model'), 21, 26), (('v number', '25'), 6, 8)], 'After 25 epochs, the model can reach a local optimum.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 11, 24), (('v number', '0.001'), 28, 33)], 'We apply a learning rate of 0.001 and RMSProp for optimization'], [[(('parameter', 'hidden layers'), 32, 45), (('parameter', 'm'), 92, 93), (('v number', '100'), 10, 13), (('v number', '3'), 104, 105)], 'A size of 100 is chosen for the hidden layers d_ and d_ , and the convolutional window size m is set to 3'], [[(('parameter', 'dropout'), 0, 7), (('v number', '0.3'), 100, 103)], 'Dropout is applied for regularization to the output of the convolutional operation with probability 0.3 '], [[(('parameter', 'epochs'), 30, 36), (('artifact', 'model'), 106, 111), (('v number', '60'), 27, 29)], 'The network is trained for 60 epochs and performance is monitored on the development sets – we select the model that yields the highest PRA value.Our implementation is available at https:github.comYoumna-HCoherence_AES']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('v number', '36'), 25, 27)], 'The batch size is set to 36'], [[(('parameter', 'steps'), 36, 41), (('parameter', 'learning rate'), 58, 71), (('parameter', 'steps'), 117, 122), (('v number', '200'), 28, 31), (('v number', '000'), 32, 35), (('v number', '1e-3'), 75, 79), (('v number', '0.8'), 97, 100), (('v number', '10000'), 111, 116)], 'The network is trained with 200,000 steps and the initial learning rate is 1e-3, which decays to 0.8 times per 10000 steps']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 78, 83), (('v number', '13'), 40, 42), (('v number', '13'), 43, 45), (('v number', '50'), 46, 48), (('v number', '5'), 49, 50), (('v number', '4'), 51, 52), (('v number', '1'), 55, 56), (('v number', '5'), 59, 60), (('v number', '50'), 63, 65)], 'Therefore, we change the filter size to 13 13 50 5 4 + 1 + 5 = 50 in the last layer'], [[(('parameter', 'learning rate'), 19, 32), (('v number', '1e-05'), 36, 41)], 'We also change the learning rate to 1e-05 to avoid divergence'], [[(('parameter', 'batch size'), 9, 19), (('v number', '64'), 23, 25), (('v number', '0.9'), 41, 44), (('v number', '0.0005'), 60, 66)], 'We use a batch size of 64, a momentum of 0.9 and a decay of 0.0005, which are the same as those in the original YOLOv2 configuration.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 108, 112), (('parameter', 'learning rate'), 157, 170), (('v number', '0.02'), 97, 101), (('v number', '0.5'), 142, 145)], 'Weights are initialized from a zero-centered Gaussian distribution, with a standard deviation of 0.02 ; The Adam optimizer is used, with _1 = 0.5 ; The base learning rate is initialized at 2e^ .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 47, 51)], 'The model is trained using mini-batch SGD with Adam optimizer with standard parameters to minimize the class weighted cross-entropy loss'], [[(('parameter', 'hidden state'), 96, 108), (('v number', '128'), 27, 30), (('v number', '256'), 73, 76)], 'In our experiments, we use 128 dimensions for the LSTM hidden states and 256 dimensions for the hidden state in the decoder'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 36, 42), (('v number', '40'), 33, 35)], 'All model setups are trained for 40 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 340, 345), (('v number', '64'), 146, 148), (('v number', '64'), 149, 151)], 'We defined the network architectures and training settings while referring to the source code provided by the authors of SN-GAN which is used for 64 64 dog and cat image generation.https:github.compfnet-researchsngan_projection The reason why we refer to this source code is that there is no previous study attempting to learn a generative model using Clothing1M, to the best of our knowledge']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 29, 42), (('v number', '1e-05'), 46, 51), (('v number', '0.9'), 60, 63), (('v number', '0.999'), 74, 79)], 'We use Adam optimizer with a learning rate of 1e-05 , _1 of 0.9 and _2 of 0.999 '], [[(('parameter', 'layers'), 96, 102), (('v number', '2'), 282, 283), (('v number', '0'), 349, 350), (('v number', '0.001'), 360, 365)], 'The CNNs are initialized with the pre-trained VGG16 weights on ImageNet and the fully connected layers which includes the weights of the binary random variables are initialized by sampling from a truncated normal distribution a normal distribution in which values sampled more than 2 standard deviations from the mean are dropped and re-picked with 0 mean and 0.001 standard deviation.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 14, 21), (('parameter', 'weight decay'), 76, 88), (('artifact', 'model'), 107, 112)], 'We also apply Dropout with keep probability p_ , and L2 regularization with weight decay factor to all the model for avoiding overfitting'], [[(('parameter', 'activation'), 16, 26), (('parameter', 'layers'), 61, 67)], 'The unspecified activation functions for all fully-connected layers appearing in models are set to ']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 39, 43), (('artifact', 'model'), 44, 49)], 'As explained in the main paper, we use Adam model introduced in for total body motion capture'], [[(('artifact', 'model'), 4, 9), (('artifact', 'model'), 189, 194), (('parameter', 'T'), 278, 279), (('v number', '30'), 64, 66), (('v number', '62'), 146, 148), (('v number', '22'), 199, 201), (('v number', '20'), 218, 220), (('v number', '3'), 281, 282), (('v number', '200'), 333, 336)], 'The model parameters include the shape parameters ^ , where K_ =30 is the dimension of shape deformation space, the pose parameters ^ where the J=62 is the number of joints in the modelThe model has 22 body joints and 20 joints for each hand., the global translation parameters t ^3 , and the facial expression parameter ^ where K_ =200 is the number of facial expression bases.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 4, 9), (('artifact', 'model'), 60, 65), (('v number', '200'), 80, 83)], 'The layer sizes for both the LSTM and BLSTM networks in our model are chosen as 200'], [[(('parameter', 'hidden layers'), 48, 61), (('artifact', 'Adam'), 81, 85), (('v number', '2'), 75, 76)], 'Based on the size of our dataset, the number of hidden layers is chosen as 2 and Adam optimization is used as in ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 61, 71), (('parameter', 'dropout'), 86, 93), (('v number', '128'), 53, 56), (('v number', '16'), 75, 77), (('v number', '0.5'), 131, 134)], 'The randomly initialized word embedding size is also 128.The batch size is 16 and the dropout rate for non-recurrent connection is 0.5.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 47, 53), (('v number', '10'), 44, 46), (('v number', '0.95'), 75, 79), (('v number', '1'), 85, 86)], 'For training configurations, we trained for 10 epochs using AdaDelta with =0.95 and =1'], [[(('parameter', 'epoch'), 58, 63), (('v number', '50'), 8, 10), (('v number', '50'), 11, 13), (('v number', '100'), 35, 38)], 'For the 50:50 scenario, we queried 100 sentences for each epoch'], [[(('parameter', 'epoch'), 70, 75), (('parameter', 'epochs'), 177, 183), (('v number', '85'), 8, 10), (('v number', '15'), 11, 13), (('v number', '10'), 51, 53), (('v number', '10'), 174, 176)], 'For the 85:15 scenario, we used a smaller query of 10 sentences in an epoch to keep the number of queries less than the number of available fully supervised training data in 10 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 22, 27), (('artifact', 'model'), 61, 66), (('v number', '9'), 82, 83), (('v number', '1'), 84, 85)], 'For both the baseline model and our adversarial auto-encoder model, we used a,b = 9,1 for the weighted classification loss'], [[(('parameter', 'learning rate'), 74, 87), (('v number', '0'), 66, 67), (('v number', '0.0001'), 91, 97)], 'The CNN was trained using an RMSProp Optimizer with a momentum of 0 and a learning rate of 0.0001 '], [[(('parameter', 'learning rate'), 65, 78), (('v number', '0.2'), 118, 121)], 'When training the discriminator training, the same optimizer and learning rate were used, but the momentum was set to 0.2 .']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 59, 72), (('v number', '600'), 100, 103)], 'We did not tune the dimensions of word embeddings and LSTM hidden layers; we used the same value of 600 reported in Wiseman et al'], [[(('parameter', 'layer'), 12, 17), (('parameter', 'layer'), 68, 73), (('artifact', 'LSTMs'), 74, 79)], 'We used one-layer pointer networks during content planning, and two-layer LSTMs during text generation'], [[(('parameter', 'epochs'), 27, 33), (('v number', '25'), 24, 26)], 'Models were trained for 25 epochs with the Adagrad optimizer Duchi et al'], [[(('parameter', 'learning rate'), 14, 27), (('parameter', 'learning rate'), 39, 52), (('parameter', 'batch size'), 93, 103), (('v number', '0.15'), 32, 36), (('v number', '0.5'), 77, 80), (('v number', '0.97'), 82, 86), (('v number', '5'), 108, 109)], '; the initial learning rate was 0.15 , learning rate decay was selected from 0.5, 0.97 , and batch size was 5']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 25, 29), (('v number', '0.9'), 48, 51), (('v number', '0.98'), 57, 61), (('v number', '1e-09'), 69, 74)], 'Specifically, we use the Adam optimizer with _1=0.9 , _2=0.98 , and =1e-09 '], [[(('parameter', 'learning rate'), 20, 33), (('v number', '16000'), 64, 69), (('v number', '2'), 86, 87)], 'We follow a similar learning rate schedule with warmup_steps of 16000: LearningRate = 2 * d^ * step_num^, step_num * warmup_steps^ .']] \n",
      "\n",
      "[[[(('parameter', 'T'), 43, 44), (('parameter', 'T'), 53, 54), (('parameter', 'T'), 67, 68), (('v number', '1'), 49, 50), (('v number', '1'), 55, 56)], 'The loss function is defined as loss = - _^t o + 1 - t 1 - o where t and o are the target and the predicted outputs respectively']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 0, 6), (('parameter', 'layer'), 42, 47), (('v number', '300'), 34, 37), (('v number', '100'), 59, 62)], 'Layers m_1 and m_2 also have size 300 and layer h has size 100'], [[(('parameter', 'dropout'), 29, 36), (('v number', '0.5'), 62, 65)], 'For regularisation, we apply dropout to the embeddings with p=0.5 '], [[(('artifact', 'model'), 4, 9), (('parameter', 'learning rate'), 43, 56), (('v number', '1.0'), 57, 60)], 'The model is optimised using AdaDelta with learning rate 1.0 ']] \n",
      "\n",
      "[[[(('artifact', 'GNN'), 71, 74), (('artifact', 'model'), 75, 80), (('v number', '2'), 129, 130)], 'Based on the pre-training dataset, we designed an architecture for the GNN model comprising an encoder with a receptive field of 2 as described in Section REF , obtained from the range [1, ,10] '], [[(('artifact', 'GNN'), 105, 108), (('parameter', 'layers'), 109, 115)], 'Validation accuracy and validation loss on the pre-training dataset used to obtain the optimal number of GNN layers is depicted in Figure REF '], [[(('parameter', 'hidden layers'), 70, 83), (('parameter', 'layer'), 143, 148), (('v number', '1'), 104, 105), (('v number', '2'), 106, 107), (('v number', '3'), 108, 109), (('v number', '4'), 110, 111), (('v number', '8'), 161, 162), (('v number', '4'), 175, 176), (('v number', '8'), 177, 178), (('v number', '16'), 179, 181), (('v number', '256'), 184, 187)], 'Each of the MLPs, g_ , used in the encoder in Equations REF – has two hidden layers chosen from the set 1,2,3,4 and the number of channels per layer parameter E=8 chosen from 4,8,16, ,256 '], [[(('parameter', 'dropout'), 2, 9), (('parameter', 'layer'), 44, 49), (('v number', '0.5'), 18, 21), (('v number', '0'), 83, 84), (('v number', '0.1'), 86, 89), (('v number', '0.9'), 93, 96)], 'A dropout rate of 0.5 was used between each layer in the MLPs, chosen from the set 0, 0.1, , 0.9 '], [[(('parameter', 'epochs'), 23, 29), (('artifact', 'GNN'), 38, 41), (('artifact', 'model'), 42, 47), (('v number', '500'), 59, 62)], 'The number of training epochs for the GNN model was set to 500'], [[(('parameter', 'batch size'), 0, 10), (('v number', '12'), 14, 16)], 'Batch size of 12 was used during training'], [[(('artifact', 'GNN'), 14, 17), (('artifact', 'model'), 18, 23), (('artifact', 'model'), 161, 166)], 'Note that the GNN model can handle entire graphs utilising efficient sparse matrix operations and we do not require to subsample the graph as in the case of MFN model, as described in Section REF .']] \n",
      "\n",
      "[[[(('artifact', 'system'), 4, 10), (('artifact', 'system'), 31, 37)], 'Our system is based on the NMT system in '], [[(('artifact', 'model'), 21, 26), (('v number', '4'), 41, 42)], 'We built an ensemble model consisting of 4 independent models, which are the cross product of two different deep RNN architectures, i.e., deep stacked RNN and deep transition RNN, and two different recurrent unit functions, i.e., GRU and LSTM.']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 7, 12), (('artifact', 'method'), 29, 35), (('parameter', 'learning rate'), 43, 56), (('v number', '0.001'), 60, 65)], 'We use AdamW as optimization method with a learning rate of 0.001 ']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 29, 34), (('v number', '512'), 58, 61), (('v number', '512'), 94, 97)], 'We use a BiRNN with a single layer in each direction with 512-dimensional word embeddings and 512-dimensional recurrent states'], [[(('parameter', 'layer'), 11, 16), (('v number', '256'), 56, 59)], 'The hidden layer of the feed-forward neural network has 256 hidden units'], [[(('artifact', 'Adam'), 28, 32), (('parameter', 'learning rate'), 40, 53), (('v number', '0.0002'), 57, 63), (('v number', '128'), 83, 86)], 'To train our models, we use Adam with a learning rate of 0.0002 and a minibatch of 128 examples'], [[(('parameter', 'epochs'), 37, 43), (('v number', '15'), 34, 36)], 'Models are trained for a total of 15 epochs'], [[(('artifact', 'gradient clipping'), 39, 56), (('v number', '5'), 111, 112)], 'To avoid exploding gradients, we apply gradient clipping such that the norm of all gradients is no larger than 5 '], [[(('parameter', 'dropout'), 9, 16), (('v number', '0.2'), 62, 65), (('v number', '0.3'), 70, 73)], 'We apply dropout to prevent overfitting with a probability of 0.2 and 0.3 for the non-recurrent input and output connections respectively .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('parameter', 'batch size'), 17, 27), (('parameter', 'epochs'), 38, 44), (('parameter', 'epochs'), 78, 84), (('parameter', 'learning rate'), 164, 177), (('v number', '75'), 31, 33), (('v number', '10'), 35, 37), (('v number', '5'), 76, 77), (('v number', '0.001'), 181, 186)], 'The model uses a batch size of 75, 10 epochs for dict2vec and fasttext, and 5 epochs for GloVe the autoencoder converges faster due to the smaller vocabulary and a learning rate of 0.001']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 90, 94), (('parameter', 'learning rate'), 112, 125), (('parameter', 'batch size'), 163, 173), (('v number', '0.0002'), 129, 135), (('v number', '0.5'), 142, 145), (('v number', '0.999'), 153, 158), (('v number', '16'), 177, 179)], 'In the video generation and segmentation experiments, we performed gradient-descent using ADAM, with an initial learning rate of 0.0002, _1 = 0.5 , _2 = 0.999 and batch size of 16'], [[(('parameter', 'epochs'), 46, 52), (('parameter', 'epochs'), 93, 99), (('v number', '25'), 43, 45), (('v number', '200'), 89, 92)], 'We trained the video generation models for 25 epochs and the video segmentation ones for 200 epochs'], [[(('parameter', 'weight decay'), 78, 90), (('parameter', 'learning rate'), 104, 117), (('parameter', 'epochs'), 211, 217), (('v number', '0.9'), 73, 76), (('v number', '0.001'), 94, 99), (('v number', '0.1'), 121, 124), (('v number', '10'), 149, 151), (('v number', '10'), 208, 210)], 'For video action recognition, we used SGD with momentum and dampening of 0.9, weight decay of 0.001 and learning rate of 0.1, reduced by a factor of 10 when no improvement in validation accuracy occurred for 10 epochs'], [[(('parameter', 'batch size'), 0, 10), (('parameter', 'epochs'), 37, 43), (('v number', '128'), 15, 18), (('v number', '130'), 48, 51)], 'Batch size was 128 and the number of epochs was 130.']] \n",
      "\n",
      "[[[(('parameter', 'learning rates'), 12, 26), (('v number', '3'), 62, 63), (('v number', '1e-05'), 64, 69)], 'The initial learning rates for both architectures were set to 3 1e-05 in order to protect the pre-trained weights'], [[(('parameter', 'batch size'), 30, 40), (('v number', '48'), 44, 46)], 'Training was performed with a batch size of 48']] \n",
      "\n",
      "[[[(('parameter', 'K'), 121, 122), (('v number', '35'), 123, 125)], 'The subsequent labelling phase was performed by means of the standard MATLAB kmeans function after specifying a total of K=35 labels to assign'], [[(('artifact', 'model'), 122, 127), (('artifact', 'model'), 153, 158), (('v number', '1000'), 188, 192), (('v number', '1000'), 246, 250)], 'Once the detection of the inpainting domain is completed, in order to provide a good initialisation to the exemplar-based model we use the TV inpainting model REF with REF with the value =1000 and a maximum number of iterations equal to maxiter2=1000 with a stopping criterion on the relative error between iterates depending on a default tolerance'], [[(('artifact', 'model'), 77, 82), (('v number', '12'), 106, 108)], 'Finally, we followed for the implementation of the exemplar-based inpainting model: for this we specified 12 propagation of iterations and tested different size of the patches']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 106, 116), (('parameter', 'weight decay'), 147, 159), (('v number', '256'), 120, 123), (('v number', '0.9'), 137, 140), (('v number', '0.0001'), 163, 169)], 'For ResNet, we kept the same training schedule as AIG, and follow the standard ResNet training procedure: batch size of 256, momentum of 0.9 , and weight decay of 0.0001 '], [[(('parameter', 'weight decay'), 8, 20), (('v number', '0.0001'), 52, 58)], 'For the weight decay for gate parameters, we use |} 0.0001 '], [[(('parameter', 'epochs'), 17, 23), (('artifact', 'model'), 42, 47), (('parameter', 'learning rate'), 95, 108), (('parameter', 'epochs'), 159, 165), (('v number', '100'), 13, 16), (('v number', '0.1'), 121, 124), (('v number', '0.1'), 140, 143), (('v number', '30'), 156, 158)], 'We train for 100 epochs from a pretrained model of the appropriate architecture with step-wise learning rate starting at 0.1 , and decay by 0.1 after every 30 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 83, 88), (('v number', '100'), 106, 109)], 'We will develop a simple framework that we will use to examine what happens to the model parameters after 100 iterations of extended Baum-Welch']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 62, 72), (('v number', '100'), 24, 27), (('v number', '8'), 76, 77)], 'We train E2E-Spot using 100 frame long clips by default and a batch size of 8 clips'], [[(('parameter', 'steps'), 28, 33), (('parameter', 'epoch'), 71, 76), (('v number', '625'), 15, 18)], 'We group every 625 training steps into a training cycle i.e., a pseudo-epoch of 500K frames'], [[(('artifact', 'model'), 19, 24), (('artifact', 'model'), 54, 59), (('v number', '100'), 29, 32), (('v number', '150'), 64, 67)], 'We train the 200MF model for 100 cycles and the 800MF model for 150 cycles on FineGym and SoccerNet-v2, due to the larger dataset sizes see sec:suppdataset'], [[(('artifact', 'AdamW'), 27, 32), (('parameter', 'learning rate'), 50, 63), (('artifact', 'linear warmup'), 82, 95), (('v number', '0.001'), 67, 72), (('v number', '3'), 80, 81)], 'Training is performed with AdamW , setting a base learning rate of 0.001 , with 3 linear warmup cycles followed by cosine decay .']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 24, 37), (('parameter', 'batch size'), 75, 85), (('parameter', 'batch size'), 101, 111), (('v number', '2.4447e-4'), 7, 16), (('v number', '1'), 62, 63), (('v number', '15'), 87, 89), (('v number', '3'), 147, 148)], 'We use 2.4447e-4 as the learning rate for training HaRT, with 1 user train batch size, 15 users eval batch size and early stopping patience set to 3'], [[(('parameter', 'batch size'), 67, 77), (('v number', '60'), 85, 87), (('v number', '3'), 123, 124)], 'For GPT-2HLC, we use the default settings from with train and eval batch size set to 60 and early stopping patience set to 3.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 51, 56), (('parameter', 'weight decay'), 126, 138), (('parameter', 'batch size'), 156, 166), (('v number', '0.9'), 119, 122), (('v number', '0.0005'), 142, 148), (('v number', '128'), 170, 173)], 'For all the experiments on CIFAR, we train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 128'], [[(('parameter', 'epochs'), 31, 37), (('v number', '450'), 27, 30)], 'The network is trained for 450 epochs'], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'epochs'), 84, 90), (('v number', '0.02'), 36, 40), (('v number', '10'), 71, 73), (('v number', '225'), 80, 83)], 'We set the initial learning rate as 0.02, and reduce it by a factor of 10 after 225 epochs'], [[(('parameter', 'epochs'), 37, 43), (('v number', '10'), 34, 36)], 'The warm up period for the DNN is 10 epochs'], [[(('artifact', 'model'), 17, 22), (('parameter', 'weight decay'), 92, 104), (('parameter', 'batch size'), 121, 131), (('v number', '0.9'), 85, 88), (('v number', '0.001'), 108, 113), (('v number', '32'), 135, 137)], 'We train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.001, and a batch size of 32'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 28, 34), (('v number', '80'), 25, 27)], 'The model is trained for 80 epochs'], [[(('parameter', 'epoch'), 36, 41), (('v number', '1'), 34, 35)], 'The warm up period for the DNN is 1 epoch'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 81, 87), (('v number', '0.002'), 36, 41), (('v number', '10'), 69, 71), (('v number', '40'), 78, 80)], 'The initial learning rate is set as 0.002 and reduced by a factor of 10 after 40 epochs'], [[(('parameter', 'epoch'), 9, 14), (('v number', '1000'), 26, 30)], 'For each epoch, we sample 1000 mini-batches from the training data'], [[(('artifact', 'model'), 17, 22), (('parameter', 'weight decay'), 92, 104), (('parameter', 'batch size'), 121, 131), (('v number', '0.9'), 85, 88), (('v number', '0.001'), 108, 113), (('v number', '32'), 135, 137)], 'We train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.001, and a batch size of 32'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 29, 35), (('v number', '100'), 25, 28)], 'The model is trained for 100 epochs'], [[(('parameter', 'epoch'), 36, 41), (('v number', '1'), 34, 35)], 'The warm up period for the DNN is 1 epoch'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 80, 86), (('v number', '0.01'), 36, 40), (('v number', '10'), 68, 70), (('v number', '50'), 77, 79)], 'The initial learning rate is set as 0.01 and reduced by a factor of 10 after 50 epochs'], [[(('parameter', 'epoch'), 9, 14), (('v number', '1000'), 26, 30)], 'For each epoch, we sample 1000 mini-batches from the training data']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 24, 30), (('parameter', 'learning rate'), 47, 60), (('parameter', 'epoch'), 130, 135), (('v number', '100'), 20, 23), (('v number', '0.2'), 64, 67), (('v number', '10'), 100, 102)], 'We trained them for 100 epochs with a starting learning rate of 0.2 which is reduced by a factor of 10 at the 30th, 60th and 90th epoch'], [[(('parameter', 'steps'), 82, 87), (('parameter', 'learning rate'), 102, 115), (('parameter', 'learning rate'), 141, 154), (('v number', '0.01'), 158, 162)], 'As ViTs are compute-intensive, we finetune ImageNet pretrained ViT models for 20k steps with a cosine learning rate schedule with a starting learning rate of 0.01 '], [[(('parameter', 'batch size'), 46, 56), (('v number', '8'), 27, 28), (('v number', '512'), 60, 63), (('v number', '256'), 77, 80), (('v number', '128'), 112, 115)], 'We train all our models on 8 A100 GPUs with a batch size of 512 for ResNets, 256 for ViT-Small and ViT-Base and 128 for ViT-Large']] \n",
      "\n",
      "[[[(('parameter', 'K'), 7, 8), (('parameter', 'K'), 34, 35), (('artifact', 'method'), 45, 51), (('v number', '10'), 9, 11)], 'We set k=10 when applying the top-k sampling method to find candidate tokens'], [[(('parameter', 'batch size'), 34, 44), (('v number', '4'), 48, 49)], 'For a fair comparison, we set the batch size to 4 for all experiments.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 25, 29), (('v number', '1024'), 67, 71)], 'Models are trained using Adam optimizer with a fixed batch-size of 1024'], [[(('parameter', 'steps'), 33, 38), (('parameter', 'steps'), 121, 126), (('v number', '530038'), 26, 32), (('v number', '397529'), 114, 120)], 'En models are trained for 530038 steps while the rest of models due to smaller training data size are trained for 397529 steps'], [[(('parameter', 'steps'), 29, 34), (('parameter', 'learning rate'), 68, 81), (('parameter', 'T'), 108, 109)], 'For all the runs, we use 40K steps of linear warm-up and then use a learning rate schedule of the form }, : t:']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 23, 28), (('v number', '64'), 80, 82)], 'We maintain the hidden layer size H to the number of attention heads A ratio of 64 which is in line with prior works , '], [[(('parameter', 'layer'), 29, 34), (('v number', '39'), 160, 162), (('v number', '16'), 209, 211)], 'Second, across larger hidden layer sizes Tempo consistently demonstrates a clear improvement over the baseline as shown in the figure, this can be as high as a 39% speedup over Baseline which corresponds to a 16% speedup over Checkpoint']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('parameter', 'steps'), 28, 33), (('artifact', 'Adam'), 97, 101)], 'We train our model for 800k steps on the provided training data on a standard MSE loss using the ADAM optimizer '], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 39, 44)], 'The learning rate schedule contains 2k steps of warm-up'], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'steps'), 106, 111), (('v number', '0.002'), 36, 41), (('v number', '0.98'), 91, 95), (('v number', '100'), 102, 105)], 'After warm-up, the learning rate is 0.002 and decays from there exponentially at a rate of 0.98 every 100 steps'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0002'), 29, 35)], 'The minimal learning rate is 0.0002 '], [[(('artifact', 'model'), 69, 74), (('v number', '00'), 133, 135), (('v number', '00'), 136, 138), (('v number', '22'), 143, 145), (('v number', '00'), 146, 148)], \"At each step, we sample valid starting frames for the seed sequences model input, which are frames that correspond to a time between 00:00 and 22:00 o'clock\"], [[(('artifact', 'model'), 77, 82), (('v number', '16'), 41, 43)], 'We take the average over the gradient of 16 successive samples to update the model parameters'], [[(('parameter', 'batch size'), 41, 51), (('v number', '16'), 55, 57)], 'This effectively corresponds to taking a batch size of 16']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 49, 55), (('v number', '2'), 47, 48), (('v number', '256'), 77, 80), (('v number', '784'), 85, 88)], 'On MNIST, we use LeNet5 for the classifier and 2 layers MLP with hidden size 256 and 784 for the encoder and decoder of conditional VAE'], [[(('parameter', 'epochs'), 51, 57), (('parameter', 'batch size'), 59, 69), (('parameter', 'learning rate'), 75, 88), (('parameter', 'weight decay'), 101, 113), (('v number', '30'), 48, 50), (('v number', '128'), 70, 73), (('v number', '0.001'), 89, 94), (('v number', '5'), 114, 115), (('v number', '0.0001'), 116, 122)], 'For standard training of the classifier, we use 30 epochs, batch size 128, learning rate 0.001 , and weight decay 5 0.0001 '], [[(('parameter', 'epochs'), 24, 30), (('parameter', 'learning rate'), 32, 45), (('parameter', 'batch size'), 54, 64), (('v number', '20'), 21, 23), (('v number', '0.001'), 46, 51), (('v number', '64'), 65, 67), (('v number', '10'), 85, 87)], 'For the CVAE, we use 20 epochs, learning rate 0.001 , batch size 64, and latent size 10'], [[(('parameter', 'steps'), 18, 23), (('v number', '40'), 15, 17)], 'in PGD, we use 40 steps for the inner part'], [[(('parameter', 'epochs'), 36, 42), (('v number', '10'), 33, 35)], 'Adversarial training start after 10 epochs standard training'], [[(('parameter', 'steps'), 10, 15), (('v number', '40'), 7, 9)], 'We use 40 steps PGD for latent space adversarial training']] \n",
      "\n",
      "[[[(('artifact', 'model'), 94, 99), (('v number', '50'), 69, 71)], 'For all the baselines and our methods, we use DeepLab-v3 with Resnet-50 as the image backbone model'], [[(('parameter', 'learning rate'), 30, 43), (('v number', '0.01'), 44, 48), (('v number', '0.9'), 60, 63), (('v number', '0.0001'), 81, 87)], 'We use the SGD optimizer with learning rate 0.01 , momentum 0.9 and weight-decay 0.0001 '], [[(('parameter', 'learning rate'), 18, 31), (('v number', '0.9'), 49, 52)], 'We use polynomial learning rate decay with power 0.9 '], [[(('parameter', 'batch size'), 7, 17), (('parameter', 'epochs'), 53, 59), (('v number', '64'), 18, 20), (('v number', '200'), 49, 52)], 'We use batch size 64 for all tasks and train for 200 epochs']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 69, 82), (('parameter', 'batch size'), 111, 121), (('v number', '0.01'), 83, 87), (('v number', '0.9'), 103, 106), (('v number', '32'), 122, 124)], 'We use stochastic gradient descent as the optimizer with the initial learning rate 0.01 , the momentum 0.9 and batch size 32'], [[(('parameter', 'epochs'), 26, 32), (('parameter', 'epochs'), 46, 52), (('v number', '50'), 23, 25), (('v number', '5'), 44, 45)], 'The models are trained 50 epochs, and every 5 epochs the validation datasets is tested and IoU , PA and OII are estimated']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 26, 31), (('parameter', 'layer'), 59, 64), (('v number', '12'), 23, 25), (('v number', '4096'), 108, 112), (('v number', '16'), 118, 120), (('v number', '1024'), 159, 163)], 'Specifically, we use a 12-layer Transformer, in which each layer has a feed-forward MLP with hidden size of 4096, and 16 attention heads with a hidden size of 1024.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 40, 45), (('v number', '50'), 56, 58)], 'The network architecture of the encoder model is ResNet-50 , which is one of the widely used architectures in recent SOTA FR , , , '], [[(('parameter', 'learning rate'), 66, 79), (('v number', '0.1'), 83, 86)], 'An optimizer Stochastic Gradient Descent SGD is used with initial learning rate of 0.1'], [[(('parameter', 'weight decay'), 35, 47), (('v number', '0.9'), 23, 26), (('v number', '5e-4'), 51, 55)], 'The momentum is set to 0.9 and the weight decay to 5e-4'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 59, 65), (('v number', '10'), 32, 34), (('v number', '8'), 41, 42), (('v number', '16'), 44, 46), (('v number', '24'), 48, 50), (('v number', '32'), 56, 58)], 'The learning rate is divided by 10 after 8, 16, 24, and 32 epochs'], [[(('parameter', 'epochs'), 74, 80), (('parameter', 'batch size'), 97, 107), (('v number', '40'), 71, 73), (('v number', '512'), 111, 114)], 'The models presented in Sections REF , REF , REF , REF are trained for 40 epochs in total with a batch size of 512 on 100K synthetic images']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 41, 47), (('parameter', 'batch size'), 116, 126), (('v number', '10'), 38, 40), (('v number', '8'), 130, 131)], 'We fine-tune XLM and mBERT models for 10 epochs over the parallel fine-tuning corpus for each language pair, with a batch size of 8'], [[(('artifact', 'AdamW'), 7, 12), (('parameter', 'learning rate'), 18, 31), (('v number', '1e-5'), 35, 39)], 'We use AdamW with learning rate of 1e-5'], [[(('parameter', 'dropout'), 4, 11), (('v number', '0.1'), 27, 30)], 'The dropout rate is set to 0.1']] \n",
      "\n",
      "[[[(('artifact', 'model'), 21, 26), (('parameter', 'batch size'), 139, 149), (('v number', '23200'), 153, 158)], 'We build our Chinese model following the similar settings except initializing the textual encoder with ERNIE , , and training with a total batch size of 23200']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('parameter', 'learning rate'), 63, 76), (('v number', '2'), 80, 81), (('v number', '1e-06'), 82, 87)], 'We train our model using the HuggingFace implementation , on a learning rate of 2 1e-06 '], [[(('parameter', 'K'), 63, 64), (('v number', '32'), 67, 69)], 'The question matching pool retrieved by TF-IDF is comprised of k = 32 knowledge base FAQs']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 150, 162), (('parameter', 'batch size'), 175, 185), (('v number', '0.9'), 138, 141), (('v number', '0.0001'), 163, 169), (('v number', '8'), 186, 187)], 'In all settings, we train segmentation networks from ImageNet pre-trained weights using stochastic gradient descent SGD with the momentum 0.9 and the weight decay 0.0001 with batch size 8'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.01'), 39, 43), (('v number', '1'), 65, 66)], 'The learning rate is initialized to be 0.01 and is multiplied by 1-^ ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 45, 50), (('v number', '1'), 99, 100), (('v number', '485'), 101, 104), (('v number', '700'), 138, 141)], 'We follow previous works , , , and train our model on the conventional training set which contains 1,485 samples from the NJU2K-train and 700 samples from the NLPR-train ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 41, 51), (('parameter', 'learning rate'), 68, 81), (('artifact', 'linear warmup'), 98, 111), (('parameter', 'steps'), 171, 176), (('v number', '256'), 36, 39), (('v number', '32'), 55, 57), (('v number', '0.00002'), 85, 92), (('v number', '10'), 145, 147)], 'We set a maximum sequence length of 256, batch size of 32, and peak learning rate of 0.00002 with linear warmup scheduling, increasing it during 10% of the total training steps'], [[(('parameter', 'epochs'), 40, 46), (('v number', '3'), 38, 39), (('v number', '10'), 79, 81)], 'On ToxiGen, we train for a maximum of 3 epochs, on TweetEval, for a maximum of 10'], [[(('parameter', 'steps'), 22, 27), (('v number', '500'), 18, 21)], 'We evaluate every 500 steps and use the checkpoint with the best validation loss for testing']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 41, 54), (('parameter', 'learning rate'), 115, 128), (('v number', '0.002'), 132, 137)], 'We found it beneficial to use one-cyclic learning rate scheduling, following recommendations of , with the maximum learning rate of 0.002'], [[(('parameter', 'epochs'), 28, 34), (('artifact', 'Adam'), 40, 44), (('parameter', 'batch size'), 59, 69), (('artifact', 'model'), 117, 122), (('parameter', 'epochs'), 160, 166), (('v number', '350'), 24, 27), (('v number', '30'), 73, 75), (('v number', '100'), 156, 159)], 'We train all models for 350 epochs with Adam optimizer and batch size of 30 with early stopping enabled for when the model does not improve for consecutive 100 epochs']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 30, 36), (('artifact', 'L'), 68, 69), (('parameter', 'layers'), 93, 99), (('v number', '1.0'), 53, 56)], 'Standard deviation for output layers are scaled by a 1.0 term where L is the total number of layers'], [[(('parameter', 'activation'), 74, 84), (('v number', '0'), 34, 35), (('v number', '2048'), 110, 114)], 'All bias terms are initialized as 0, and all models are trained with ReLU activation and a sequence length of 2048.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 11, 21), (('v number', '32'), 25, 27)], 'We apply a batch size of 32'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.005'), 29, 34)], 'The initial learning rate is 0.005 and decays under cosine rule'], [[(('parameter', 'weight decay'), 4, 16), (('parameter', 'weight decay'), 53, 65), (('v number', '4'), 23, 24), (('v number', '1e-05'), 25, 30)], 'The weight decay of is 4 1e-05 , while we do not use weight decay for .']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 37, 44), (('v number', '0.8'), 82, 85), (('v number', '1'), 89, 90)], 'We also trained models with a weaker version of random crop and resize range from 0.8 to 1, which we call “w.a.\" in the results'], [[(('artifact', 'AdamW'), 7, 12), (('artifact', 'model'), 26, 31), (('parameter', 'epochs'), 41, 47), (('parameter', 'batch size'), 53, 63), (('parameter', 'batch size'), 87, 97), (('artifact', 'L'), 114, 115), (('v number', '1600'), 36, 40), (('v number', '4096'), 67, 71), (('v number', '2048'), 101, 105)], 'We use AdamW to train the model for 1600 epochs with batch size of 4096 for ViT-B, and batch size of 2048 for ViT-L'], [[(('parameter', 'learning rate'), 16, 29), (('parameter', 'epoch'), 50, 55), (('v number', '80'), 47, 49)], 'We use a cosine learning rate schedule with an 80-epoch warmup'], [[(('parameter', 'learning rate'), 9, 22), (('artifact', 'L'), 60, 61), (('v number', '1.5'), 26, 29), (('v number', '0.0001'), 30, 36)], 'The base learning rate is 1.5 0.0001 for both ViT-B and ViT-L, and is further scaled by batchsize256']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 221, 231), (('v number', '5.0'), 92, 95), (('v number', '0.5'), 128, 131), (('v number', '0.25'), 171, 175), (('v number', '0.1'), 211, 214), (('v number', '32'), 234, 236), (('v number', '4'), 237, 238)], 'We used grid search to choose training parameters, that include: view synthesis weight _s = 5.0 , virtual camera loss weight _v=0.5 , virtual camera projection noise _v = 0.25 , canonical jittering noise _t= _r=0.1 , and batch size b=32 4 per GPU'], [[(('artifact', 'AdamW'), 11, 16), (('parameter', 'weight decay'), 76, 88), (('parameter', 'learning rate'), 118, 131), (('v number', '0.9'), 57, 60), (('v number', '0.999'), 66, 71), (('v number', '0.0001'), 94, 100), (('v number', '2'), 138, 139), (('v number', '0.0001'), 140, 146)], 'We use the AdamW optimizer , with standard parameters _1=0.9 , _2=0.999 , a weight decay of w=0.0001 , and an initial learning rate of lr=2 0.0001 '], [[(('parameter', 'epochs'), 45, 51), (('parameter', 'learning rate'), 65, 78), (('parameter', 'epochs'), 88, 94), (('v number', '200'), 41, 44), (('v number', '80'), 85, 87)], 'For our stereo experiments, we train for 200 epochs, halving the learning rate every 80 epochs'], [[(('parameter', 'epochs'), 44, 50), (('parameter', 'learning rate'), 64, 77), (('parameter', 'epochs'), 87, 93), (('v number', '100'), 40, 43), (('v number', '40'), 84, 86)], 'For our video experiments, we train for 100 epochs, halving the learning rate every 40 epochs'], [[(('parameter', 'epochs'), 50, 56), (('parameter', 'epochs'), 88, 94), (('v number', '50'), 47, 49), (('v number', '10'), 85, 87), (('v number', '2'), 126, 127), (('v number', '1e-05'), 128, 133)], 'Higher-resolution fine-tuning is performed for 50 epochs for stereo experiments, and 10 epochs for video experiments, with lr=2 1e-05 .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 24, 29), (('artifact', 'model'), 97, 102), (('parameter', 'layer'), 113, 118), (('parameter', 'Version'), 119, 126), (('artifact', 'model'), 151, 156), (('v number', '12'), 21, 23), (('v number', '2'), 134, 135)], 'We use GPT2–small, a 12 layer transformer-based LM comprising of 124M parameters, as the teacher model and a six-layer version of GPT–2 as the student model']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 69, 82), (('parameter', 'learning rate'), 110, 123), (('v number', '8'), 64, 65), (('v number', '1e4'), 86, 89)], 'We use the Ranger optimiser yong2020gradient and batch sizes of 8, a learning rate of 1e4 with an exponential learning rate decay'], [[(('parameter', 'epochs'), 39, 45), (('v number', '50'), 36, 38)], 'We train all considered methods for 50 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 75, 88), (('v number', '0.001'), 92, 97)], 'The neural networks have been trained with an RMSprop optimizer and with a learning rate of 0.001 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 8, 13), (('artifact', 'model'), 33, 38)], 'Cascade Model: In CM sec:cascade model, the value of a list depends only on the attraction probabilities of its items'], [[(('parameter', 'K'), 57, 58), (('parameter', 'K'), 72, 73), (('v number', '1'), 51, 52), (('v number', '1'), 59, 60), (('v number', '1'), 66, 67), (('v number', '1'), 74, 75)], 'If we apply LCB on attraction probabilities , then 1 - _^K 1 - L_ 1 - _^K 1 - _ and we get the LCB for the whole list A as we have an upper bound on each term in the product'], [[(('artifact', 'model'), 43, 48), (('v number', '1'), 25, 26)], 'Now suppose that we have 1 - LCBs for each model parameter'], [[(('artifact', 'model'), 30, 35), (('v number', '1'), 90, 91)], 'Then, by the union bound, all model parameter LCBs hold jointly with probability at least 1 - | | , and so do the LCBs for all lists.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 121, 126), (('v number', '0.9'), 52, 55), (('v number', '8255'), 84, 88), (('v number', '10'), 118, 120), (('v number', '10'), 137, 139), (('v number', '2.0'), 188, 191), (('v number', '1e-05'), 192, 197)], 'We used the SGD optimizer with a momentum factor of 0.9 , a perturbation bound of = 8255 for both FBF and TRADES, N = 10 steps of size = 10 for TRADES, and a regularization weighting of = 2.0 1e-05 that was tuned through experimentation']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 12, 16), (('v number', '1e-06'), 34, 39), (('v number', '0.9'), 45, 48), (('v number', '0.98'), 58, 62)], 'We used the Adam optimizer with = 1e-06 , _1=0.9 , and _2=0.98 following '], [[(('parameter', 'learning rate'), 52, 65), (('v number', '6'), 21, 22), (('v number', '0'), 90, 91), (('v number', '0.0006'), 95, 101), (('v number', '0'), 136, 137)], 'We use a warm up for 6% of the pretraining were the learning rate linearly increases from 0 to 0.0006, then decreases linearly to reach 0 at the end of the pretraining'], [[(('parameter', 'dropout'), 48, 55), (('parameter', 'epochs'), 76, 82), (('parameter', 'epochs'), 120, 126), (('v number', '2'), 33, 34), (('v number', '0.1'), 56, 59), (('v number', '20'), 73, 75), (('v number', '0'), 104, 105), (('v number', '5'), 118, 119)], 'We fixed the update frequency to 2 and we use a dropout 0.1 in the first 20 epochs and we changed it to 0 in the last 5 epochs']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 22, 29), (('v number', '0.1'), 38, 41), (('v number', '0.1'), 69, 72)], 'We additionally use a dropout rate of 0.1 and label smoothing set to 0.1.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 66, 76), (('parameter', 'batch size'), 110, 120), (('v number', '16'), 89, 91), (('v number', '5'), 125, 126)], 'All the experiments were ran with the following hyper parameters, batch size was kept at 16 where as the eval batch size was 5'], [[(('parameter', 'epoch'), 22, 27), (('v number', '5'), 57, 58), (('v number', '0.06'), 84, 88)], 'The maximum number of epoch ran for the experiments were 5 with the warm-up kept at 0.06'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'weight decay'), 42, 54), (('v number', '1.5e-5'), 27, 33), (('v number', '0.01'), 59, 63)], 'The learning rate used was 1.5e-5 and the weight decay was 0.01.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 47, 52), (('parameter', 'layer'), 79, 84)], 'We set up one discriminator for the foreground layer, and one for the residual layer'], [[(('parameter', 'epochs'), 72, 78), (('v number', '1200'), 67, 71), (('v number', '1'), 93, 94), (('v number', '3'), 99, 100), (('v number', '3'), 127, 128)], 'We use all aforementioned losses and train on NVIDIA RTX A6000 for 1200 epochs in both Stage 1 and 3, and only use L_ in Stage 3'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 91, 104), (('parameter', 'batch size'), 115, 125), (('v number', '0.001'), 105, 110), (('v number', '16'), 126, 128)], 'We use the ADAM optimizer when training the decomposition network and discriminators, with learning rate 0.001 and batch size 16'], [[(('parameter', 'T'), 91, 92), (('parameter', 'T'), 108, 109), (('parameter', 'T'), 114, 115), (('parameter', 'T'), 124, 125), (('v number', '3'), 49, 50), (('v number', '1'), 110, 111), (('v number', '4'), 116, 117), (('v number', '8'), 126, 127)], 'When applying strategies in Section REF , we use 3 temporal scales: the frame at time step t is paired with t+1 , t+4 , and t+8 as inputs']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 6, 12), (('v number', '200'), 17, 20)], 'Total epochs are 200'], [[(('artifact', 'EMA'), 7, 10), (('artifact', 'model'), 27, 32), (('v number', '0.99'), 43, 47)], 'The in EMA for the teacher model is set to 0.99 ']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 257, 262), (('v number', '30000'), 266, 271), (('v number', '35000'), 292, 297)], 'For training on all datasets with BART, we first follow the hyperparameter setting provided by the original BART training script for XSumhttps:github.compytorchfairseqblobmainexamplesbartREADME.summarization.md except that we set the total number of update steps to 30000 for TempWikiBio and 35000 for the content transfer dataset'], [[(('parameter', 'batch size'), 39, 49), (('v number', '65536'), 86, 91)], 'In addition, we adjust the accumulated batch size for training on TempWikiBio to have 65536 tokens in each batch'], [[(('parameter', 'learning rates'), 17, 31), (('artifact', 'model'), 142, 147), (('v number', '1'), 101, 102), (('v number', '1e-05'), 103, 108), (('v number', '3'), 111, 112), (('v number', '1e-05'), 113, 118), (('v number', '5'), 125, 126), (('v number', '1e-05'), 127, 132)], 'We then tune the learning rates on TempWikiBio and the content transfer dataset by searching through 1 1e-05 , 3 1e-05 , and 5 1e-05 with the model without prompts']] \n",
      "\n",
      "[[[(('artifact', 'model'), 53, 58), (('v number', '256'), 68, 71), (('v number', '256'), 72, 75)], 'The original input and output size of the inpainting model are both 256 256 as many other inpainting methods'], [[(('artifact', 'model'), 13, 18), (('parameter', 'steps'), 27, 32), (('parameter', 'steps'), 56, 61), (('artifact', 'Adam'), 76, 80), (('v number', '0'), 97, 98), (('v number', '0.9'), 106, 109)], 'We train the model in 240k steps in Celeba-HQ and 1000k steps in Places2 in Adam optimizer of _1=0 and _2=0.9 '], [[(('parameter', 'learning rates'), 12, 26), (('v number', '2e-4'), 31, 35), (('v number', '2e-5'), 40, 44)], 'The initial learning rates are 2e-4 and 2e-5 for the generator and discriminator respectively'], [[(('parameter', 'learning rate'), 10, 23), (('parameter', 'steps'), 64, 69), (('v number', '0.5'), 40, 43), (('v number', '15'), 48, 50)], 'Then, the learning rate is decayed with 0.5 for 15 of the total steps'], [[(('artifact', 'model'), 4, 9), (('v number', '3.1'), 35, 38), (('v number', '1'), 56, 57), (('v number', '5'), 88, 89)], 'Our model is trained in Pytorch v1.3.1, and costs about 1 day to train in Celeba-HQ and 5 days to train in Places2 with a single NVIDIAR TeslaR V100 16GB GPU.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 25, 31), (('parameter', 'epochs'), 51, 57), (('parameter', 'learning rate'), 66, 79), (('v number', '4'), 23, 24), (('v number', '15'), 48, 50), (('v number', '32'), 129, 131)], 'Models are trained for 4 epochs except LEAM for 15 epochs, with a learning rate of 1e^ , cross-entropy loss, and a batch-size of 32'], [[(('artifact', 'Adam'), 0, 4), (('parameter', 'activation'), 41, 51)], 'Adam is used as the optimiser, with Tanh activation function'], [[(('artifact', 'model'), 5, 10), (('parameter', 'layer'), 104, 109), (('v number', '300'), 49, 52)], 'LEAM model uses FastText embeddings of dimension 300 as the encoder, and softmax function in the output layer'], [[(('artifact', 'model'), 55, 60), (('v number', '10'), 30, 32)], 'We report average results for 10 separate runs of each model.']] \n",
      "\n",
      "[[[(('artifact', 'method'), 108, 114), (('parameter', 'T'), 216, 217), (('v number', '0.01'), 220, 224)], 'Data is generated by solving the problem REF using a high precision numerical scheme with a pseudo-spectral method for spatial discretization and 4th order Runge-Kutta for temporal discretization with time step size t = 0.01 ']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 65, 69), (('parameter', 'learning rate'), 105, 118), (('v number', '0.9'), 82, 85), (('v number', '0.999'), 92, 97), (('v number', '1e-06'), 122, 127), (('v number', '1e-05'), 141, 146), (('v number', '0.0001'), 198, 204)], 'During this training period, the entire network is trained using Adam optimizer _=0.9 and _=0.999 with a learning rate of 1e-06 for the FEM, 1e-05 for the ACM and APM decoder, and the MCM is set as 0.0001 '], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'weight decay'), 36, 48), (('v number', '8'), 25, 26), (('v number', '0'), 52, 53)], 'The batch size is set to 8, and the weight decay is 0'], [[(('artifact', 'Adam'), 19, 23), (('parameter', 'learning rate'), 58, 71), (('v number', '1e-07'), 75, 80), (('v number', '1e-06'), 83, 88), (('v number', '1e-05'), 95, 100)], 'In this stage, the Adam optimizer is used with an initial learning rate of 1e-07 , 1e-06 , and 1e-05 for each of the above-mention modules, respectively']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 33, 46), (('v number', '0.0001'), 50, 56)], 'We use the Adam optimizer with a learning rate of 0.0001 '], [[(('parameter', 'epochs'), 33, 39), (('parameter', 'batch size'), 47, 57), (('parameter', 'epoch'), 122, 127), (('v number', '1'), 27, 28), (('v number', '000'), 29, 32), (('v number', '50'), 61, 63)], 'We run each experiment for 1,000 epochs with a batch size of 50 unless stated differently and report results for the best epoch']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 27, 33), (('v number', '15'), 24, 26)], 'We train the models for 15 epochs'], [[(('parameter', 'fold'), 77, 81), (('artifact', 'cross-validation'), 82, 98), (('v number', '8'), 130, 131)], 'It takes approximately one week to run the hyper-parameter search using five-fold cross-validation for all language models, using 8 GPUs in total.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 33, 46), (('parameter', 'weight decay'), 56, 68), (('v number', '0.01'), 47, 51), (('v number', '0.0005'), 72, 78)], 'We use the Adam optimizer with a learning rate 0.01 and weight decay of 0.0005'], [[(('parameter', 'epochs'), 31, 37), (('v number', '200'), 27, 30)], 'All models are trained for 200 epochs with no weight scheduling'], [[(('parameter', 'dropout'), 9, 16), (('parameter', 'layer'), 17, 22), (('artifact', 'GNN'), 50, 53), (('parameter', 'layer'), 54, 59), (('v number', '0.5'), 35, 38)], 'We add a dropout layer with rate p=0.5 after each GNN layer during training']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 120, 124), (('parameter', 'weight decay'), 142, 154), (('parameter', 'batch size'), 171, 181), (('v number', '0.0005'), 158, 164), (('v number', '50'), 185, 187)], 'For self-supervised learning for generating brain networks, the autoencoder adopts mean squared error loss MSELoss, the Adam optimizer with a weight decay of 0.0005 and a batch size of 50'], [[(('parameter', 'epochs'), 58, 64), (('v number', '1000'), 44, 48)], 'We implement the following hyperparameters: 1000 training epochs'], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'learning rate'), 51, 64), (('parameter', 'epochs'), 98, 104), (('v number', '0.001'), 36, 41), (('v number', '90'), 79, 81), (('v number', '50'), 95, 97)], 'We set the initial learning rate as 0.001, and the learning rate is reduced to 90% after every 50 epochs'], [[(('parameter', 'weight decay'), 59, 71), (('parameter', 'batch size'), 88, 98), (('parameter', 'epochs'), 124, 130), (('v number', '0.0005'), 75, 81), (('v number', '50'), 102, 104), (('v number', '1000'), 110, 114)], 'For edge reconstruction, we adopt the SGD optimizer with a weight decay of 0.0005 and a batch size of 50 with 1000 training epochs'], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'learning rate'), 51, 64), (('parameter', 'epochs'), 98, 104), (('v number', '0.001'), 36, 41), (('v number', '90'), 79, 81), (('v number', '50'), 95, 97)], 'We set the initial learning rate as 0.001, and the learning rate is reduced to 90% after every 50 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 9, 22), (('parameter', 'batch size'), 40, 50), (('artifact', 'model'), 65, 70), (('v number', '5'), 26, 27), (('v number', '1e-06'), 28, 33), (('v number', '4'), 54, 55)], 'We use a learning rate of 5 1e-06 and a batch size of 4 for task model training'], [[(('parameter', 'learning rate'), 25, 38), (('parameter', 'batch size'), 45, 55), (('v number', '64'), 59, 61)], 'For MoE, we use the same learning rate and a batch size of 64 instead'], [[(('parameter', 'epochs'), 35, 41), (('parameter', 'epochs'), 80, 86), (('v number', '10'), 32, 34), (('v number', '50'), 77, 79)], 'The task models are trained for 10 epochs and the MoE models are trained for 50 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 28, 33), (('artifact', 'model'), 128, 133), (('v number', '125'), 58, 61), (('v number', '1.3'), 94, 97), (('v number', '1'), 117, 118)], 'For the pretrained language model checkpoints, we use the 125 million parameters 125M and the 1.3 billion parameters 1.3B dense model from ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 131, 141), (('v number', '101'), 39, 42), (('v number', '3090'), 193, 197)], 'First, we adopt the weights of ResNeXt-101-BiFPN , trained on ImageNet to initialize the backbone network parameters, set the mini-batch size as two, and optimize our network on one NVidia RTX 3090 GPU'], [[(('parameter', 'learning rate'), 24, 37), (('parameter', 'learning rate'), 99, 112), (('parameter', 'learning rate'), 175, 188), (('v number', '0.001'), 41, 46), (('v number', '0.0001'), 118, 124), (('v number', '0.001'), 128, 133), (('v number', '1'), 147, 148), (('v number', '00'), 149, 151), (('v number', '0.0001'), 192, 198), (('v number', '40'), 205, 207), (('v number', '000'), 208, 211), (('v number', '45'), 252, 254), (('v number', '000'), 255, 258)], 'Second, we set the base learning rate as 0.001 , adopt a warm-up strategy to linearly increase the learning rate from 0.0001 to 0.001 in the first 1,00 iterations, reduce the learning rate to 0.0001 after 40,000 iterations, and stop the learning after 45,000 iterations']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 7, 12), (('parameter', 'layers'), 33, 39), (('parameter', 'layer'), 63, 68), (('v number', '24'), 4, 6)], 'For 24-layer transformer encoder layers and the text embedding layer, we use the RoBERTa-Large to initialize'], [[(('parameter', 'layer'), 20, 25), (('artifact', 'model'), 64, 69)], 'The cross attention layer and the rest of the parameters in the model are randomly initialized'], [[(('artifact', 'Adam'), 33, 37), (('parameter', 'learning rate'), 57, 70), (('artifact', 'linear decay'), 85, 97), (('parameter', 'learning rate'), 98, 111), (('v number', '2e-5'), 74, 78)], 'The Bi-VLDoc is trained by using Adam optimizer with the learning rate of 2e-5 and a linear decay learning rate schedule'], [[(('parameter', 'batch size'), 7, 17), (('parameter', 'epochs'), 47, 53), (('v number', '384'), 18, 21), (('v number', '10'), 44, 46)], 'We use batch size 384 to train Bi-VLDoc for 10 epochs on the IIT-CDIP dataset.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 71, 77), (('parameter', 'epochs'), 109, 115), (('v number', '50'), 68, 70), (('v number', '100'), 96, 99)], 'We use early stopping on the validation-accuracy with a patience of 50 epochs, and a maximum of 100 training epochs'], [[(('artifact', 'model'), 56, 61), (('artifact', 'model'), 109, 114)], 'Validation accuracy and loss is either reported for the model that triggered the early stopping, or the best model found']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 34, 40), (('parameter', 'learning rate'), 76, 89), (('parameter', 'batch size'), 97, 107), (('parameter', 'weight decay'), 120, 132), (('v number', '4000'), 29, 33), (('v number', '0.9'), 64, 67), (('v number', '1'), 90, 91), (('v number', '3'), 93, 94), (('v number', '128'), 108, 111)], 'All ConvNets are trained for 4000 epochs with SGD with momentum 0.9 , fixed learning rate 1 -3 , batch size 128, and no weight decay'], [[(('parameter', 'layers'), 12, 18), (('parameter', 'Version'), 80, 87), (('v number', '1.11'), 88, 92), (('v number', '0'), 93, 94)], \"All learned layers are initialized with Pytorch's default weight initialization version 1.11.0\"], [[(('parameter', 'learning rate'), 78, 91), (('parameter', 'learning rate'), 139, 152), (('parameter', 'epochs'), 202, 208), (('v number', '1'), 130, 131), (('v number', '4'), 133, 134), (('v number', '1'), 178, 179), (('v number', '3'), 181, 182), (('v number', '5'), 200, 201), (('v number', '1'), 257, 258), (('v number', '3'), 260, 261)], 'To stabilize prolonged training in the absence of batch normalization, we use learning rate warmup: starting from a base value of 1 -4 the learning rate is linearly increased to 1 -3 during the first 5 epochs of training, after which it remains constant at 1 -3 .']] \n",
      "\n",
      "[[[(('artifact', 'L'), 33, 34), (('parameter', 'epochs'), 42, 48), (('parameter', 'batch size'), 56, 66), (('v number', '90'), 39, 41), (('v number', '4096'), 70, 74), (('v number', '224'), 96, 99), (('v number', '224'), 100, 103)], 'We train FocalNet-B and FocalNet-L for 90 epochs with a batch size of 4096 and input resolution 224 224 '], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 62, 68), (('v number', '0.001'), 36, 41), (('v number', '5'), 60, 61)], 'The initial learning rate is set to 0.001 after a warmup of 5 epochs'], [[(('parameter', 'layers'), 67, 73), (('v number', '0.0001'), 52, 58)], 'For stability, we use LayerScale with initial value 0.0001 for all layers'], [[(('parameter', 'epochs'), 68, 74), (('parameter', 'learning rate'), 88, 101), (('parameter', 'learning rate'), 122, 135), (('artifact', 'AdamW'), 150, 155), (('v number', '30'), 65, 67), (('v number', '3'), 105, 106), (('v number', '1e-05'), 107, 112)], 'After the pretraining, we finetune the models on ImageNet-1K for 30 epochs with initial learning rate of 3 1e-05 , cosine learning rate scheduler and AdamW optimizer']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 84, 97), (('v number', '768'), 61, 64), (('v number', '2e-5'), 101, 105), (('v number', '1e-5'), 158, 162)], 'We use the pre-trained uncased BERT-base as the encoder with 768-dim embedding, the learning rate is 2e-5, and the coefficient of L2-regularization is set to 1e-5'], [[(('parameter', 'dropout'), 76, 83), (('v number', '0.07'), 60, 64), (('v number', '0.1'), 68, 71), (('v number', '0.1'), 97, 100)], 'For contrastive learning loss, we set the hyper-parameters =0.07 , =0.1 and dropout probablity p=0.1 '], [[(('parameter', 'T'), 64, 65), (('parameter', 'K'), 74, 75), (('v number', '6'), 68, 69), (('v number', '5'), 78, 79)], 'We use LDA to generate topic words for masking sentences, where T = 6 and K = 5']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 29, 36), (('artifact', 'model'), 49, 54), (('parameter', 'layer'), 104, 109)], 'We use the bert-base-uncased version of the BERT model as our encoder, and a single transformer decoder layer to reconstruct the sentence, following , '], [[(('artifact', 'model'), 81, 86), (('artifact', 'model'), 134, 139)], 'While our approach is generally applicable for both encoder-only Masked Language Model MLM and the encoder-decoder Denoising Language Model e.g']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 120, 126), (('v number', '200'), 116, 119)], 'With high computational costs for contrastive pretraining, these experiments consider a fixed pretraining budget of 200 epochs'], [[(('parameter', 'batch size'), 48, 58), (('parameter', 'learning rate'), 66, 79), (('v number', '64'), 59, 61), (('v number', '0.03'), 80, 84), (('v number', '4'), 88, 89), (('v number', '5000'), 108, 112), (('v number', '16'), 130, 132)], 'For COCO, pretraining is performed with per-GPU batch size 64 and learning rate 0.03 on 4 NVIDIA Quadro RTX 5000 GPUs with memory 16 GB'], [[(('parameter', 'batch size'), 59, 69), (('parameter', 'learning rate'), 77, 90), (('v number', '32'), 70, 72), (('v number', '0.015'), 91, 96), (('v number', '2'), 100, 101), (('v number', '1080'), 121, 125), (('v number', '11'), 146, 148)], 'For ImageNet-Subset, pretraining is performed with per-GPU batch size 32 and learning rate 0.015 on 2 NVIDIA GeForce GTX 1080 Ti GPUs with memory 11 GB'], [[(('parameter', 'K'), 42, 43), (('v number', '8'), 44, 45), (('v number', '192'), 46, 49)], 'All pretraining uses a dictionary size of K=8,192'], [[(('parameter', 'layers'), 23, 29), (('parameter', 'learning rate'), 108, 121), (('parameter', 'batch size'), 137, 147), (('v number', '0.02'), 125, 129), (('v number', '4'), 151, 152), (('v number', '2'), 156, 157), (('v number', '1080'), 177, 181)], 'Full finetuning of all layers is performed within the Detectron2 framework with a 24k iteration schedule, a learning rate of 0.02, and a batch size of 4 on 2 NVIDIA GeForce GTX 1080 Ti GPUs, unless otherwise noted.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 81, 87), (('parameter', 'layers'), 176, 182), (('v number', '5'), 79, 80), (('v number', '100'), 101, 104)], 'In 1D examples, we use the modified fully connected architecture with depth of 5 layers and width of 100 neurons for both branch and trunk nets, and the design of the optional layers P and D in Figure REF will be detailed in each of the following examples'], [[(('parameter', 'batch size'), 4, 14), (('artifact', 'Adam'), 40, 44), (('parameter', 'learning rate'), 74, 87), (('parameter', 'steps'), 133, 138), (('v number', '100'), 31, 34), (('v number', '0.001'), 91, 96), (('v number', '0.95'), 103, 107), (('v number', '5000'), 128, 132)], 'The batch size is chosen to be 100 with ADAM optimizer, where the initial learning rate lr=0.001 and a 0.95 decay rate in every 5000 steps'], [[(('parameter', 'layers'), 66, 72), (('v number', '6'), 64, 65)], 'Same architecture is used in 2D examples except that a depth of 6 layers is used']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 17, 30), (('parameter', 'dropout'), 63, 70), (('v number', '512'), 50, 53), (('v number', '0.2'), 87, 90)], 'The dimension of hidden layers in FFNs was set to 512, and the dropout rate was set to 0.2'], [[(('parameter', 'batch size'), 9, 19), (('v number', '5'), 23, 24)], 'The mini-batch size is 5 spansaction'], [[(('parameter', 'learning rate'), 10, 23), (('parameter', 'learning rate'), 89, 102), (('v number', '1e-5'), 27, 31), (('v number', '52e-4'), 59, 64)], 'We used a learning rate of 1e-5 for language models and 1e-52e-4 for other parametersThe learning rate was determined by using the development dataset'], [[(('parameter', 'learning rate'), 17, 30), (('parameter', 'learning rate'), 70, 83), (('parameter', 'epoch'), 110, 115), (('parameter', 'epoch'), 168, 173), (('v number', '0'), 150, 151)], 'We scheduled the learning rate by linear warm-up, which increases the learning rate linearly during the first epoch and then decreases it linearly to 0 until the final epoch'], [[(('artifact', 'model'), 15, 20), (('parameter', 'epochs'), 30, 36), (('v number', '20'), 27, 29), (('v number', '5'), 83, 84)], 'We trained the model up to 20 epochs and applied early stopping with a patience of 5 by monitoring the fully labeled span F1 score on the development dataset']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 71, 77), (('v number', '128'), 45, 48), (('v number', '20'), 81, 83)], 'The node embedding dimensionality was set to 128 and the number of GCN layers to 20'], [[(('parameter', 'hidden layers'), 38, 51), (('v number', '2'), 36, 37), (('v number', '256'), 57, 60)], 'The MLP edge classifier consists of 2 hidden layers with 256 neurons'], [[(('artifact', 'Adam'), 32, 36), (('parameter', 'learning rate'), 43, 56), (('parameter', 'weight decay'), 67, 79), (('parameter', 'batch size'), 104, 114), (('v number', '0.001'), 37, 42), (('v number', '5'), 58, 59), (('v number', '0.0001'), 60, 66), (('v number', '0.9'), 81, 84), (('v number', '0.999'), 86, 91), (('v number', '200'), 118, 121)], 'Optimization was performed with Adam 0.001 learning rate, 5 0.0001 weight decay, 0.9, 0.999 betas and a batch size of 200']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 68, 74), (('v number', '6'), 78, 79), (('v number', '768'), 123, 126), (('v number', '12'), 159, 161)], 'For the multi-modal fusion module, we set the number of Transformer layers L_=6 , and the dimension of the hidden states D=768 with the number of heads set to 12'], [[(('artifact', 'AdamW'), 50, 55), (('parameter', 'steps'), 78, 83), (('parameter', 'learning rates'), 93, 107), (('v number', '100'), 70, 73), (('v number', '000'), 74, 77), (('v number', '1e-5'), 171, 175), (('v number', '5e-5'), 180, 184)], 'For the optimization, the models are trained with AdamW optimizer for 100,000 steps with the learning rates for the uni-modal encoders and the remaining parameters set to 1e-5 and 5e-5, respectively'], [[(('parameter', 'learning rate'), 41, 54), (('v number', '10'), 28, 30), (('v number', '0'), 78, 79)], 'The warm-up ratio is set to 10%, and the learning rate is linearly decayed to 0 after warm-up']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 22, 27), (('parameter', 'epochs'), 45, 51), (('parameter', 'weight decay'), 81, 93), (('v number', '50'), 42, 44), (('v number', '0.9'), 61, 64), (('v number', '0.999'), 71, 76), (('v number', '1'), 94, 95), (('v number', '0.0001'), 96, 102)], 'It is trained with an AdamW optimizer for 50 epochs, with _1=0.9, _2 = 0.999 and weight decay 1 0.0001 '], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epoch'), 76, 81), (('v number', '2'), 29, 30), (('v number', '0.0001'), 31, 37), (('v number', '0.1'), 57, 60)], 'The initial learning rate is 2 0.0001 , and it decays by 0.1 after the 40th epoch']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('v number', '3e-5'), 28, 32), (('v number', '1e-5'), 53, 57)], 'The learning rate is set to 3e-5 for base models and 1e-5 for large models, respectively'], [[(('parameter', 'learning rate'), 11, 24), (('parameter', 'batch size'), 97, 107), (('v number', '64'), 111, 113), (('v number', '32'), 140, 142)], 'Except for learning rate, We use the same training hyper-parameters for all experiments with the batch size of 64 and the maximum length of 32'], [[(('parameter', 'dropout'), 50, 57), (('v number', '0.05'), 36, 40), (('v number', '0.1'), 80, 83)], 'The temperature parameter is set to 0.05, and the dropout probability is set to 0.1'], [[(('artifact', 'model'), 13, 18), (('parameter', 'epoch'), 25, 30), (('artifact', 'model'), 48, 53), (('parameter', 'steps'), 92, 97), (('v number', '1'), 23, 24), (('v number', '125'), 88, 91)], 'We train our model for 1 epoch and evaluate the model on the STSb development set every 125 steps, and keep the best checkpoint by following .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 33, 38), (('v number', '50'), 59, 61)], 'We use a DeepLab-v3 segmentation model with dilated ResNet-50 backbone to facilitate a fair comparison with '], [[(('artifact', 'model'), 26, 31), (('parameter', 'epochs'), 39, 45), (('v number', '45'), 36, 38), (('v number', '16'), 68, 70)], 'We train the segmentation model for 45 epochs using batches of size 16'], [[(('parameter', 'weight decay'), 58, 70), (('v number', '0.9'), 50, 53), (('v number', '0.0001'), 71, 77)], 'The weights are updated through SGD with momentum 0.9 and weight decay 0.0001 '], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 75, 81), (('v number', '2'), 21, 22), (('v number', '0.001'), 23, 28), (('v number', '2'), 57, 58), (('v number', '0.0001'), 59, 65), (('v number', '40'), 72, 74)], 'The learning rate is 2 0.001 at the start and reduced to 2 0.0001 after 40 epochs'], [[(('artifact', 'model'), 98, 103), (('v number', '0.9'), 39, 42)], 'Further, we use confidence threshold = 0.9 to select the most confident masks from our Mask R-CNN model see Section REF ']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 14, 20), (('artifact', 'L'), 24, 25), (('parameter', 'layer'), 36, 41), (('v number', '24'), 26, 28), (('v number', '32'), 56, 58), (('v number', '2048'), 105, 109)], 'The number of layers is L=24 , each layer consists of A=32 attention heads and the hidden dimension is H=2048 '], [[(('artifact', 'L'), 81, 82), (('v number', '16'), 67, 69), (('v number', '1024'), 74, 78), (('v number', '24'), 83, 85)], 'For the non-causal part, we use encoder-only Transformers, where A=16 , H=1024 , L=24 ']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 35, 41), (('parameter', 'layer'), 110, 115), (('v number', '512'), 49, 52), (('v number', '2'), 77, 78), (('v number', '048'), 79, 82)], 'Both the Conformer and Transformer layers have a 512 embedding dimension and 2,048 hidden units in the linear layer'], [[(('parameter', 'dropout'), 7, 14), (('parameter', 'layers'), 50, 56), (('v number', '0.1'), 18, 21)], 'We set dropout at 0.1 in the linear and attention layers'], [[(('parameter', 'dropout'), 12, 19), (('parameter', 'layers'), 58, 64), (('v number', '0.1'), 23, 26), (('v number', '31'), 86, 88)], 'We also set dropout at 0.1 in the Conformer Convolutional layers and a kernel size of 31 for the point- and depth-wise convolutions'], [[(('artifact', 'model'), 72, 77), (('v number', '7'), 166, 167)], 'For the constrained data condition, we train a one-to-many multilingual model that prepends a token representing the selected target language for decoding on all the 7 languages of MuST-Cinema']] \n",
      "\n",
      "[[[(('parameter', 'm'), 74, 75), (('v number', '1'), 38, 39)], 'We use weights w_m^ = 1r_m and w_m^ = 1 , where r_m was the proportion of m th class label in training samples, to balance the class weights of two losses, respectively'], [[(('parameter', 'layers'), 20, 26), (('parameter', 'layer'), 62, 67)], 'Batch normalization layers are added after each convolutional layer except the segmentation head to accelerate the loss convergence'], [[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 105, 109), (('parameter', 'weight decay'), 125, 137), (('v number', '1e-09'), 144, 149)], \"The model's parameters are initialed by a uniform distribution following the scaling of and optimized by Adam algorithm with weight decay value 1e-09 \"], [[(('parameter', 'learning rate'), 24, 37), (('artifact', 'model'), 45, 50), (('parameter', 'epochs'), 117, 123), (('v number', '0.0001'), 67, 73), (('v number', '0.1'), 103, 106), (('v number', '100'), 113, 116)], 'In every iteration, the learning rate of the model is started with 0.0001 and decreased by a drop rate 0.1 every 100 epochs'], [[(('parameter', 'epochs'), 32, 38), (('parameter', 'epochs'), 55, 61), (('v number', '1000'), 27, 31), (('v number', '200'), 51, 54)], 'Consequently, we train for 1000 epochs where every 200 epochs we update the superpixel labels and boundaries.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 18, 25), (('parameter', 'layers'), 50, 56), (('v number', '20'), 31, 33), (('v number', '4'), 74, 75), (('v number', '200'), 92, 95)], 'We use a CNN with dropout rate 20%, convolutional layers with kernel size 4 and filter size 200'], [[(('artifact', 'BiLSTM'), 24, 30), (('v number', '100'), 47, 50), (('v number', '100'), 52, 55), (('v number', '120'), 59, 62)], 'The temporal cell LSTM, BiLSTM or TCN contains 100, 100 or 120 neurons, respectively'], [[(('parameter', 'steps'), 42, 47), (('artifact', 'model'), 63, 68), (('parameter', 'epochs'), 79, 85), (('parameter', 'batch size'), 112, 122), (('v number', '64'), 34, 36), (('v number', '2'), 73, 74), (('v number', '000'), 75, 78), (('v number', '50'), 126, 128)], 'We interpolate the time-series to 64 time steps, and train the model for 2,000 epochs with early stopping and a batch size of 50.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 169, 176), (('parameter', 'layer'), 177, 182), (('parameter', 'layer'), 239, 244), (('v number', '0.4'), 200, 203)], 'Image classification: We conduct experiments across two different backbones Efficientnetv2S and Mobilenetv2 initialized using imagenet pre-trained weights followed by a dropout layer with probability 0.4 and a final sigmoid classification layer'], [[(('parameter', 'learning rate'), 156, 169), (('v number', '448'), 93, 96), (('v number', '448'), 99, 102)], 'It is important to note that state-of-the-art approaches use larger image sizes for training 448 x 448, autoaugment and cutout for augmentations, one-cycle learning rate scheduler among other tricks']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 37, 42), (('parameter', 'batch size'), 73, 83), (('v number', '64'), 46, 48), (('v number', '4'), 87, 88), (('v number', '096'), 89, 92)], 'We pre-train Bridge-Tower_} for 100k steps on 64 NVIDIA A100 GPUs with a batch size of 4,096 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 44, 49), (('parameter', 'epochs'), 58, 64), (('parameter', 'epochs'), 87, 93), (('v number', '300'), 54, 57), (('v number', '1'), 69, 70), (('v number', '73.0'), 72, 76), (('v number', '1'), 81, 82), (('v number', '000'), 83, 86), (('v number', '1'), 98, 99), (('v number', '74.3'), 101, 105)], 'For the baseline, we follow to train a BYOL model for 300 epochs top-1: 73.0 and 1,000 epochs top-1: 74.3'], [[(('parameter', 'K'), 67, 68), (('v number', '2'), 64, 65), (('v number', '8'), 69, 70), (('v number', '0.9'), 76, 79), (('v number', '0.6'), 85, 88)], 'For our framework, if not explicitly stated otherwise, we use V=2, K=8, s_}=0.9, s_}=0.6 '], [[(('parameter', 'learning rate'), 42, 55), (('parameter', 'epochs'), 72, 78), (('parameter', 'epochs'), 101, 107), (('parameter', 'learning rate'), 123, 136), (('parameter', 'epochs'), 171, 177), (('v number', '0.3'), 56, 59), (('v number', '256'), 60, 63), (('v number', '300'), 68, 71), (('v number', '0.2'), 83, 86), (('v number', '256'), 87, 90), (('v number', '1'), 95, 96), (('v number', '000'), 97, 100), (('v number', '10'), 168, 170)], 'We use LARS optimizer with a base initial learning rate 0.3 256 for 300 epochs and 0.2 256 for 1,000 epochs, with a cosine learning rate decay, and a warm up period of 10 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 42, 47), (('parameter', 'layer'), 62, 67), (('parameter', 'dropout'), 164, 171), (('v number', '12'), 39, 41), (('v number', '12'), 59, 61), (('v number', '768'), 77, 80), (('v number', '12'), 99, 101), (('v number', '2048'), 109, 113), (('v number', '0.1'), 180, 183)], 'We use the same T5 architecture with a 12-layer encoder, a 12-layer decoder, 768 hidden units d_ , 12 heads, 2048 feedforward linear units d_ , GELU activations, a dropout rate as 0.1, and no embedding tying']] \n",
      "\n",
      "[[[(('artifact', 'model'), 105, 110), (('v number', '540'), 116, 119)], 'We follow previous studies , and conduct our experiments on an autoregressive Transformer-based language model with 540 billion parameters'], [[(('parameter', 'm'), 12, 13), (('v number', '32'), 14, 16)], 'We generate m=32 reasoning paths for each question in a training set'], [[(('artifact', 'model'), 35, 40), (('parameter', 'steps'), 49, 54), (('parameter', 'learning rate'), 62, 75), (('parameter', 'batch size'), 90, 100), (('v number', '5e-5'), 79, 83), (('v number', '32'), 104, 106)], 'For each dataset, we fine-tune the model for 10k steps with a learning rate of 5e-5 and a batch size of 32'], [[(('parameter', 'T'), 61, 62), (('artifact', 'model'), 88, 93), (('v number', '0.7'), 63, 66)], 'For multiple path decoding, we use a sampling temperature of T=0.7 with the pre-trained model as suggested by '], [[(('parameter', 'T'), 7, 8), (('artifact', 'model'), 30, 35), (('v number', '1.2'), 9, 12)], 'We use T=1.2 for the language model after self-improvement LMSI'], [[(('parameter', 'steps'), 37, 42), (('v number', '256'), 46, 49)], 'We set the maximum number of decoded steps to 256 for all experiments.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 28, 41), (('parameter', 'weight decay'), 52, 64), (('artifact', 'Adam'), 83, 87), (('v number', '1e-4'), 45, 49), (('v number', '5e-4'), 68, 72)], 'We train all models using a learning rate of 1e-4 , weight decay of 5e-4 , and the Adam optimizer if otherwise stated'], [[(('parameter', 'learning rate'), 14, 27), (('parameter', 'learning rate'), 84, 97), (('parameter', 'epochs'), 167, 173), (('v number', '0.1'), 113, 116)], 'Regarding the learning rate schedule, we apply a plateau scheduler that reduces the learning rate by a factor of 0.1 if the validation results do not improve for four epochs'], [[(('artifact', 'gradient clipping'), 45, 62), (('v number', '64'), 16, 18)], 'Batches of size 64 are used per default, and gradient clipping is applied after computing the gradients based on the weighted cross-entropy loss function ']] \n",
      "\n",
      "[[[(('parameter', 'K'), 74, 75), (('v number', '1'), 53, 54)], 'We pretrain BERT-Base on the English Wikipedia Phase 1 only by NVLAMB and K-FAC'], [[(('parameter', 'batch size'), 24, 34), (('parameter', 'weight decay'), 67, 79), (('parameter', 'learning rate'), 91, 104), (('parameter', 'steps'), 130, 135), (('parameter', 'learning rate'), 154, 167), (('parameter', 'steps'), 179, 184), (('v number', '8'), 35, 36), (('v number', '192'), 37, 40), (('v number', '128'), 62, 65), (('v number', '0.01'), 80, 84), (('v number', '6'), 105, 106), (('v number', '0.001'), 107, 112), (('v number', '7'), 136, 137), (('v number', '038'), 138, 141), (('v number', '2'), 185, 186), (('v number', '000'), 187, 190)], 'For NVLAMB, we set mini-batch size 8,192, max sequence length 128, weight decay 0.01, base learning rate 6 0.001 , total training steps 7,038, and linear learning rate warming up steps 2,000'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'T'), 25, 26), (('parameter', 'T'), 95, 96), (('v number', '1'), 93, 94)], 'The learning rate at the t -th step after warm-up is determined by the polynomial decay: _t= 1-t ^ '], [[(('parameter', 'K'), 4, 5), (('parameter', 'learning rate'), 71, 84), (('parameter', 'steps'), 96, 101), (('parameter', 'learning rates'), 141, 155), (('v number', '600'), 116, 119), (('v number', '2'), 178, 179)], 'For K-FAC, the same hyperparameters are used except that the number of learning rate warming up steps is reduced to 600, resulting in larger learning rates than NVLAMB until the 2,000th step'], [[(('parameter', 'steps'), 39, 44), (('v number', '5'), 64, 65)], 'The pretraining loss versus the number steps is shown in Figure 5 left']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 63, 69), (('parameter', 'layer'), 86, 91), (('v number', '6'), 61, 62), (('v number', '8'), 74, 75)], 'Both the standard Transformer and the TP-Transformer TPT use 6 layers and 8 heads per layer'], [[(('parameter', 'batch size'), 18, 28), (('parameter', 'learning rate'), 93, 106), (('v number', '80'), 32, 34)], 'We set a training batch size of 80 per GPU and used the Adafactor optimizer with square root learning rate decay'], [[(('artifact', 'model'), 15, 20), (('parameter', 'dropout'), 46, 53), (('v number', '1'), 63, 64)], 'Throughout the model, we used a commonly used dropout rate of .1.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 40, 46), (('v number', '12'), 37, 39), (('v number', '768'), 48, 51), (('v number', '12'), 66, 68)], 'For the generators, a structure with 12 layers, 768 hidden units, 12 heads was employed'], [[(('parameter', 'layers'), 43, 49), (('v number', '24'), 40, 42), (('v number', '1'), 51, 52), (('v number', '024'), 53, 56), (('v number', '16'), 71, 73)], 'For the discriminator, a structure with 24 layers, 1,024 hidden units, 16 heads was put to use'], [[(('artifact', 'Adam'), 12, 16), (('parameter', 'm'), 42, 43), (('parameter', 'learning rate'), 55, 68), (('artifact', 'linear decay'), 90, 102), (('parameter', 'steps'), 120, 125), (('parameter', 'learning rate'), 140, 153), (('v number', '1e-4'), 158, 162)], 'We used the Adam optimizer to train ERNIE-M Extra; the learning rate was scheduled with a linear decay with 10K warm-up steps, and the peak learning rate was 1e-4 '], [[(('parameter', 'batch size'), 115, 125), (('v number', '64'), 79, 81), (('v number', '2'), 109, 110), (('v number', '048'), 111, 114), (('v number', '512'), 130, 133)], 'During intra-cluster training, we conducted the pre-training experiments using 64 NVIDIA A100-40GB GPUs with 2,048 batch size and 512 max length']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 19, 32), (('parameter', 'batch size'), 41, 51), (('artifact', 'model'), 75, 80), (('parameter', 'epochs'), 88, 94), (('v number', '0.1'), 36, 39), (('v number', '128'), 55, 58), (('v number', '80'), 85, 87)], 'We set the initial learning rate to 0.1, batch size to 128, and train each model for 80 epochs with the CosineAnnealing LR scheduler'], [[(('parameter', 'weight decay'), 47, 59), (('v number', '0.9'), 42, 45), (('v number', '5'), 63, 64), (('v number', '0.0001'), 65, 71)], 'For the optimizer, we set the momentum to 0.9, weight decay to 5 0.0001 , and use the Nesterov momentum']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('v number', '64'), 18, 20)], 'The batch size is 64'], [[(('artifact', 'Adam'), 0, 4), (('v number', '0.9'), 32, 35), (('v number', '0.999'), 41, 46), (('v number', '1e-8'), 52, 56)], 'Adam optimizer is used with _1 =0.9, _2 =0.999 and =1e-8']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 104, 114), (('v number', '120'), 27, 30), (('v number', '128'), 39, 42), (('v number', '2'), 118, 119), (('v number', '1'), 144, 145)], 'We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens total of 1 trillion token updates'], [[(('artifact', 'Adam'), 8, 12), (('artifact', 'linear decay'), 40, 52)], 'We used Adam optimizer with lr=1e^ with linear decay to lr=5e^ over 500k updates'], [[(('parameter', 'weight decay'), 8, 20), (('v number', '0.1'), 24, 27)], 'We used weight decay of 0.1 on all parameters except biases and layernorms']] \n",
      "\n",
      "[[[(('parameter', 'T'), 56, 57), (('v number', '25'), 58, 60), (('v number', '1'), 61, 62)], 'We create video inputs by randomly sampling a window of T=25 1 sec'], [[(('parameter', 'T'), 56, 57), (('parameter', 'steps'), 72, 77), (('v number', '80'), 39, 41), (('v number', '100'), 59, 62), (('v number', '1'), 78, 79)], 'We finally obtain melspectrograms with 80 mel-bands and T^=100 mel time-steps 1 sec.'], [[(('parameter', 'batch size'), 9, 19), (('parameter', 'learning rate'), 64, 77), (('artifact', 'model'), 172, 177), (('v number', '32'), 23, 25), (('v number', '0.00005'), 81, 88)], 'We use a batch size of 32 and RMSProp optimizer with an initial learning rate of 0.00005 for both the generator and the discriminator, which is advised for training a WGAN model '], [[(('parameter', 'epochs'), 80, 86), (('v number', '10'), 77, 79)], 'Hence, we stop the training once the discriminator loss does not improve for 10 epochs'], [[(('artifact', 'model'), 10, 15), (('parameter', 'steps'), 51, 56)], 'Since our model can take a variable number of time steps as input, it can directly generate for any length of video without any further changes.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 52, 57), (('parameter', 'layers'), 80, 86), (('v number', '3'), 61, 62), (('v number', '3'), 72, 73)], 'The neural architectures used are Simple CNN custom model of 3 conv and 3 dense layers, VGG , and InceptionResNet '], [[(('artifact', 'model'), 56, 61), (('artifact', 'gradient clipping'), 116, 133)], 'The SADT variants are compared against related works in model generalization: Gradient Centralization GC , Adaptive Gradient Clipping AGC , and Sharpness-Aware Minimization SAM '], [[(('artifact', 'Adam'), 171, 175), (('parameter', 'learning rate'), 177, 190), (('parameter', 'batch size'), 243, 253), (('parameter', 'epoch'), 271, 276), (('v number', '0.0001'), 235, 241), (('v number', '512'), 257, 260), (('v number', '2048'), 265, 269), (('v number', '200'), 283, 286), (('v number', '512'), 294, 297), (('v number', '370'), 302, 305), (('v number', '2048'), 313, 317)], 'In order to make the training landscape uniform for all methods considered, the training always starts from the same initial point, and all methods use the same optimizer Adam, learning rate scheduler cosine-decay with initial rate of 0.0001, batch size BS 512 and 2048, epoch count 200 for BS 512 and 370 for BS 2048, and data augmentation scheme CutMix .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 31, 37), (('parameter', 'activation'), 75, 85), (('v number', '300'), 27, 30)], 'All models are trained for 300 epochs from scratch with binary weights and activation'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 62, 68), (('v number', '2e-4'), 36, 40), (('v number', '200'), 58, 61)], 'The initial learning rate is set to 2e-4 and halved every 200 epochs'], [[(('parameter', 'batch size'), 9, 19), (('artifact', 'Adam'), 41, 45), (('v number', '16'), 30, 32)], 'The mini-batch size is set to 16 and the ADAM optimizer is adapted.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 71, 76), (('parameter', 'layer'), 109, 114), (('v number', '30'), 85, 87), (('v number', '30'), 123, 125)], 'The policy network for the experiment in Section has a fully connected layer of size 30, followed by an LSTM layer of size 30'], [[(('parameter', 'layer'), 42, 47), (('parameter', 'layer'), 99, 104), (('v number', '30'), 56, 58), (('v number', '10'), 113, 115)], 'The actor-network has one fully connected layer of size 30, the critic-network one fully connected layer of size 10'], [[(('artifact', 'model'), 49, 54), (('parameter', 'steps'), 86, 91), (('v number', '10'), 38, 40), (('v number', '5'), 67, 68)], 'The length of each episode was set to 10 and the model trained for 5 million training steps']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 18, 22), (('parameter', 'learning rate'), 42, 55), (('v number', '4'), 7, 8), (('v number', '1e-4'), 59, 63)], 'We use 4 GPUs and Adam optimizer with the learning rate of 1e-4'], [[(('parameter', 'steps'), 15, 20), (('v number', '1'), 25, 26), (('v number', '4'), 39, 40), (('v number', '16'), 74, 76), (('v number', '4'), 88, 89)], 'Total training steps are 1 million and 4 frames per video so the total of 16 frames for 4 videos are taken for a single training step.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 43, 56), (('v number', '0.0001'), 60, 66)], 'We use Adam as our training optimizer with learning rate of 0.0001 ']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 104, 109), (('artifact', 'model'), 118, 123)], 'During training, the [CLS] token hidden representation of the input sentence pairs is fed into a linear Layer and the model is optimized using binary cross entropy loss'], [[(('parameter', 'layer'), 45, 50), (('v number', '0.0'), 93, 96), (('v number', '1.0'), 101, 104)], 'However, at inference time, we add a sigmoid layer to the output to predict a number between 0.0 and 1.0 indicating the likelihood of the bitexts being translations of each other']] \n",
      "\n",
      "[[[(('parameter', 'K'), 72, 73), (('v number', '1'), 81, 82)], 'REF can be simplified into the following formula with the sole neighbor k : ^_ = 1- ^_ + ^.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 20, 30), (('v number', '128'), 34, 37)], 'We set the training batch size to 128'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 29, 35), (('artifact', 'Adam'), 42, 46), (('v number', '60'), 26, 28)], 'The model was trained for 60 epochs using adam optimizer'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 67, 73), (('v number', '0.001'), 29, 34), (('v number', '0.0001'), 51, 57), (('v number', '30'), 64, 66)], 'The initial learning rate is 0.001, and changed to 0.0001 after 30 epochs'], [[(('parameter', 'dropout'), 15, 22), (('parameter', 'dropout'), 60, 67), (('parameter', 'layers'), 68, 74), (('v number', '0.2'), 48, 51)], 'Meanwhile, the dropout probability p_ is set to 0.2 for all dropout layers']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 45, 58), (('v number', '5'), 62, 63), (('v number', '1e-05'), 64, 69)], 'We use Adam as the optimizer, with a maximum learning rate of 5 1e-05 '], [[(('artifact', 'model'), 26, 31), (('v number', '8'), 49, 50)], 'The optimizer updates the model parameters every 8 batches'], [[(('parameter', 'steps'), 37, 42), (('v number', '500'), 46, 49), (('v number', '700'), 51, 54), (('v number', '2400'), 56, 60), (('v number', '5000'), 67, 71)], 'We set the maximum numbers of update steps to 500, 700, 2400 , and 5000 respectively for QSGen-Hier, QSGen-ChildQ, WikiBioSum, and GovReport']] \n",
      "\n",
      "[[[(('artifact', 'model'), 125, 130), (('v number', '64'), 38, 40)], 'Therefore, we set the input length to 64, which allows us to use very large batch sizes: between 4K and 8K, depending on the model size'], [[(('parameter', 'epochs'), 44, 50), (('v number', '200'), 40, 43)], 'Hence, we let KP training run for up to 200 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 64, 77), (('parameter', 'weight decay'), 112, 124), (('v number', '2.5e-4'), 78, 84), (('v number', '0.9'), 104, 107), (('v number', '5e-4'), 125, 129)], 'We trained our models with stochastic gradient descent SGD with learning rate 2.5e-4, Nesterov momentum 0.9 and weight decay 5e-4'], [[(('artifact', 'model'), 15, 20), (('parameter', 'steps'), 34, 39), (('v number', '100'), 26, 29), (('v number', '000'), 30, 33), (('v number', '16'), 70, 72), (('v number', '32'), 113, 115)], 'We trained the model with 100,000 steps each including a minibatch of 16 samples from the Mapillary data set and 32 samples from the FGI autonomous steering data set'], [[(('artifact', 'Adam'), 36, 40), (('parameter', 'learning rate'), 58, 71), (('v number', '1e-4'), 72, 76), (('v number', '0.9'), 95, 98), (('v number', '0.99'), 104, 108)], 'The discriminators are trained with Adam optimizers using learning rate 1e-4 and parameters _1=0.9 , _2=0.99 .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 81, 86), (('artifact', 'model'), 116, 121), (('artifact', 'model'), 165, 170), (('parameter', 'epochs'), 198, 204), (('v number', '10'), 186, 188)], 'In traditional setting, the train and dev sets are independent, and we train the model only train data and evaluate model on dev data, preserve the highest-accuracy model within at most 10 training epochs'], [[(('artifact', 'cross-validation'), 3, 19), (('parameter', 'fold'), 75, 79), (('artifact', 'cross-validation'), 80, 96), (('v number', '9'), 73, 74)], 'In cross-validation setting, we mix train and dev set into one set, then 9-fold cross-validation is utilized'], [[(('parameter', 'epoch'), 14, 19), (('artifact', 'model'), 69, 74), (('v number', '0.5'), 40, 43), (('v number', '5'), 87, 88)], 'Each training epoch takes approximately 0.5 hour, and evaluating the model costs about 5 minutes.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('artifact', 'AdamW'), 39, 44), (('parameter', 'learning rate'), 64, 77)], 'We train the model with CTC loss using AdamW optimizer and peak learning rate of 1e^ '], [[(('parameter', 'dropout'), 46, 53), (('parameter', 'weight decay'), 66, 78), (('v number', '0.1'), 57, 60)], 'During the training, we exploit SpecAugment , dropout of 0.1, and weight decay of 1e^ for the regularization'], [[(('parameter', 'batch size'), 4, 14), (('v number', '16'), 25, 27)], 'The batch size is set to 16 for each GPU and 4x RTX Titan24GB GPUs are used'], [[(('parameter', 'batch size'), 65, 75), (('v number', '4'), 30, 31), (('v number', '256'), 79, 82)], 'Gradients are accumulated for 4 batches, expanding the effective batch size to 256']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 8, 12), (('parameter', 'batch size'), 18, 28), (('v number', '32'), 29, 31)], 'We used Adam with batch size 32, and gradient accumulation was applied'], [[(('parameter', 'learning rates'), 12, 26), (('parameter', 'dropout'), 49, 56), (('v number', '2e-5'), 39, 43), (('v number', '0.2'), 73, 76)], 'The initial learning rates were set at 2e-5, and dropout rate was set to 0.2']] \n",
      "\n",
      "[[[(('artifact', 'model'), 35, 40), (('parameter', 'epochs'), 50, 56), (('parameter', 'learning rate'), 95, 108), (('v number', '50'), 60, 62), (('v number', '128'), 82, 85), (('v number', '0.0025'), 112, 118)], 'We have only set the StyleGAN2-ADA model training epochs to 50, the batch-size to 128, and the learning rate to 0.0025 .']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 44, 50), (('v number', '12'), 41, 43), (('v number', '768'), 95, 98), (('v number', '12'), 116, 118)], 'Our transformer architecture consists of 12 layers of transformer blocks, where each block has 768 hidden units and 12 self-attention heads'], [[(('artifact', 'model'), 18, 23), (('parameter', 'epochs'), 52, 58), (('parameter', 'batch size'), 108, 118), (('v number', '20'), 49, 51), (('v number', '480'), 122, 125)], 'We initialize the model from _ and pre-train for 20 epochs on their respective pre-training datasets with a batch size of 480'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'steps'), 74, 79), (('parameter', 'learning rate'), 98, 111), (('v number', '10'), 58, 60), (('v number', '6e-5'), 115, 119)], 'We use Adam optimizer with a linear warm-up for the first 10% of training steps, and set the peak learning rate as 6e-5'], [[(('parameter', 'learning rate'), 76, 89), (('parameter', 'steps'), 115, 120)], 'After warm up, a linear-decayed learning-rate scheduler gradually drops the learning rate for the rest of training steps']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 41, 51), (('parameter', 'epochs'), 120, 126), (('v number', '32'), 26, 28), (('v number', '87.5'), 55, 59), (('v number', '169'), 116, 119)], 'The models are trained on 32 GPUs with a batch size of 87.5 seconds per GPU for 250k updates, this is equivalent to 169 epochs'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '8'), 50, 51), (('v number', '0'), 97, 98)], 'The learning rate ramps up linearly for the first 8% of updates and then decays linearly down to 0'], [[(('parameter', 'learning rate'), 9, 22), (('v number', '5e-4'), 26, 30)], 'The peak learning rate is 5e-4.']] \n",
      "\n",
      "[[[(('parameter', 'T'), 201, 202), (('v number', '0.9'), 142, 145)], 'where is modulation factor meant to control the time scale of the evolution of the error filters and generally set to be less than one, e.g., 0.9 - note that this update rule is discarded if ^ _ = ^ _^T as we do in this work']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 82, 87), (('v number', '0'), 152, 153)], 'To reduce the difficulty of training for the large datasets, we pretrain for some steps before the main KD training by setting the coefficient of _} to 0 and only optimizing the remaining losses in Eq']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 0, 12), (('v number', '5'), 26, 27), (('v number', '0.0001'), 28, 34)], 'Weight decay is set to be 5 0.0001 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 25, 30), (('v number', '2'), 56, 57)], 'The original pre-trained model has respectively trained 2 million iterations for edge inference and content inpainting, where content inpainting includes training taking ground truth as conditions and training taking generated edge maps as conditions'], [[(('artifact', 'model'), 44, 49), (('artifact', 'model'), 97, 102), (('artifact', 'model'), 198, 203), (('v number', '75'), 55, 57), (('v number', '000'), 58, 61), (('v number', '75'), 152, 154), (('v number', '000'), 155, 158), (('v number', '75'), 250, 252), (('v number', '000'), 253, 256)], 'Additionally, we trained our edge inference model with 75,000 iterations, the content inpainting model taking ground truth edge maps as conditions with 75,000 iterations, and the content inpainting model taking generated edge maps as conditions with 75,000 iterations']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 80, 85), (('v number', '128'), 106, 109), (('v number', '256'), 146, 149)], \"To perform document summarization using GAE-ISumm, we set the first convolution layer's embedding size as 128 for distributed word embeddings and 256 for remaining representations\"]] \n",
      "\n",
      "[[[(('parameter', 'layer'), 39, 44), (('parameter', 'layers'), 101, 107), (('v number', '6'), 124, 125)], 'We used Resnet50 with frozen batchnorm layer as Backbone for MGTR, the number of Encoder and Decoder layers are both set to 6, the same as in '], [[(('parameter', 'batch size'), 37, 47), (('artifact', 'AdamW'), 68, 73), (('parameter', 'learning rate'), 107, 120), (('artifact', 'model'), 183, 188), (('v number', '8'), 54, 55), (('v number', '1e-4'), 124, 128), (('v number', '1e-5'), 152, 156)], 'In the training phase, we choose the batch size to be 8, and we use AdamW as an optimizer, with a constant learning rate of 1e-4 in Encoder-Decoder and 1e-5 in Backbone, we train the model until the performance on the test set no longer improves.']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 84, 89), (('parameter', 'epochs'), 138, 144), (('v number', '1'), 76, 77), (('v number', '2'), 130, 131)], 'Hyperparameters.For all datasets and all methods, we set silos to train for 1 local epoch in every round except Ditto which takes 2 local epochs'], [[(('parameter', 'batch size'), 96, 106), (('v number', '64'), 143, 145), (('v number', '64'), 147, 149), (('v number', '32'), 151, 153), (('v number', '100'), 155, 158), (('v number', '64'), 160, 162), (('v number', '6'), 242, 243), (('v number', '6'), 245, 246), (('v number', '1'), 248, 249), (('v number', '1'), 251, 252), (('v number', '0.5'), 254, 257)], 'For Vehicle, GLEAM, School, Rotated & Masked MNIST, and subsampled ADNI respectively, the local batch size across all silos are fixed with B = 64, 64, 32, 100, 64 , and the clipping norm for per-example gradients are heuristically set to c = 6, 6, 1, 1, 0.5 '], [[(('parameter', 'T'), 13, 14), (('parameter', 'T'), 88, 89), (('parameter', 'T'), 123, 124), (('parameter', 'T'), 165, 166), (('parameter', 'T'), 187, 188), (('v number', '400'), 15, 18), (('v number', '200'), 90, 93), (('v number', '200'), 125, 128), (('v number', '200'), 167, 170), (('v number', '500'), 189, 192)], 'Vehicle uses T=400 rounds for most experiments except fig:finetune-gap which trains for T=200 rounds, School and GLEAM use T=200 rounds, Rotated & Masked MNIST uses T=200 , and ADNI uses T=500 .']] \n",
      "\n",
      "[[[(('artifact', 'GNN'), 46, 49), (('parameter', 'layer'), 72, 77), (('parameter', 'layer'), 110, 115), (('parameter', 'layer'), 157, 162), (('v number', '2'), 70, 71), (('v number', '16'), 87, 89), (('v number', '2'), 108, 109), (('v number', '16'), 131, 133), (('v number', '2'), 155, 156), (('v number', '8'), 172, 173), (('v number', '8'), 195, 196)], 'We perform experiments over different popular GNN models, including a 2-layer GCN with 16 hidden neurons, a 2-layer GraphSAGE with 16 hidden neurons and a 2-layer GAT with 8 attention heads with 8 hidden neurons each'], [[(('artifact', 'model'), 14, 19), (('artifact', 'Adam'), 31, 35), (('parameter', 'learning rate'), 59, 72), (('parameter', 'weight decay'), 87, 99), (('v number', '1'), 76, 77), (('v number', '0.01'), 78, 82), (('v number', '5'), 103, 104), (('v number', '0.0001'), 105, 111)], 'To train each model, we use an Adam optimizer with initial learning rate of 1 0.01 and weight decay of 5 0.0001 '], [[(('artifact', 'GNN'), 118, 121), (('artifact', 'model'), 122, 127), (('parameter', 'epochs'), 143, 149), (('v number', '300'), 139, 142)], 'As in the active learning setup, there should not enough labeled samples to be used as a validation set, we train the GNN model with fixed 300 epochs in all the experiments and evaluate over the full graph.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 8, 13), (('v number', '12'), 84, 86), (('v number', '32'), 103, 105)], 'The FNO model for both Darcy Flow and the Navier-Stokes equations was trained using 12 modes and width 32'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 66, 72), (('v number', '1e-3'), 37, 41), (('v number', '100'), 62, 65)], 'The initial learning rate was set to 1e-3 and is halved every 100 epochs'], [[(('parameter', 'weight decay'), 0, 12), (('parameter', 'epochs'), 60, 66), (('v number', '1e-8'), 23, 27), (('v number', '500'), 56, 59)], 'Weight decay is set to 1e-8 and training finishes after 500 epochs'], [[(('parameter', 'epoch'), 44, 49), (('parameter', 'epochs'), 172, 178)], 'It is well-known that the training time per epoch for FNO can be significantly higher than that of DeepONet, however, FNO trains much faster i.e., with significantly fewer epochs , .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 7, 17), (('parameter', 'batch size'), 112, 122), (('v number', '8'), 21, 22), (('v number', '32'), 48, 50), (('v number', '8'), 126, 127)], 'We set batch size to 8 for few-shot setting and 32 for full-data setting.For FewRel and TACRED datasets, we set batch size to 8 bacause of memory limitation for full-data setting'], [[(('parameter', 'learning rate'), 11, 24), (('v number', '5e-5'), 25, 29), (('v number', '2e-5'), 40, 44)], 'We use the learning rate 5e-5 for _ and 2e-5 for _ '], [[(('artifact', 'AdamW'), 13, 18), (('parameter', 'learning rate'), 42, 55)], 'We adopt the AdamW optimizer and constant learning rate scheduler.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 14, 19), (('artifact', 'GNN'), 33, 36), (('parameter', 'layers'), 51, 57), (('parameter', 'layer'), 102, 107), (('v number', '3'), 100, 101)], 'Each baseline model contains two GNN convolutional layers followed by a readout function and then a 3-layer MLP to produce predictions'], [[(('parameter', 'batch size'), 10, 20), (('v number', '32'), 24, 26), (('v number', '500'), 49, 52), (('v number', '512'), 57, 60), (('v number', '10'), 79, 81), (('v number', '000'), 82, 85)], 'We used a batch size of 32 for the small dataset 500 and 512 for the large one 10,000'], [[(('parameter', 'learning rate'), 28, 41), (('parameter', 'batch size'), 43, 53)], 'We conducted grid search on learning rate, batch size and hidden dimension in GNNs'], [[(('parameter', 'learning rate'), 51, 64), (('v number', '1'), 49, 50), (('v number', '0.1'), 65, 68), (('v number', '0.01'), 70, 74), (('v number', '0.001'), 76, 81), (('v number', '2'), 84, 85), (('v number', '32'), 103, 105), (('v number', '64'), 107, 109), (('v number', '3'), 112, 113), (('v number', '4'), 135, 136)], 'The hyperparameters were tuned as the following: 1 learning rate 0.1, 0.01, 0.001 ; 2 hidden dimension 32, 64 ; 3 readout function , ; 4 edge weight normalization , .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 239, 249), (('v number', '16'), 154, 156), (('v number', '256'), 173, 176), (('v number', '512'), 196, 199), (('v number', '2048'), 234, 238)], 'We experiment with two standard pretraining settings, base and base++: Base is the BERT_ training configuration : Pretraining on Wikipedia and BookCorpus 16 GB of texts for 256 million samples on 512 token sequences 125K batches with 2048 batch size']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 13, 23), (('artifact', 'model'), 84, 89), (('parameter', 'steps'), 118, 123), (('v number', '6'), 27, 28), (('v number', '16'), 33, 35), (('v number', '4'), 127, 128)], 'The training batch size is 6 and 16 respectively for the QA4QG-base and QA4QG-large model, with gradient accumulation steps of 4'], [[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 34, 40), (('v number', '5'), 32, 33)], 'We train all model with maximum 5 epochs'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '3e-5'), 21, 25)], 'The learning rate is 3e-5']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 167, 177), (('parameter', 'batch size'), 254, 264), (('parameter', 'epochs'), 310, 316), (('v number', '2'), 150, 151), (('v number', '8'), 181, 182), (('v number', '3060'), 204, 208), (('v number', '32'), 268, 270), (('v number', '30'), 307, 309)], 'The two versions differ also in the training modality in fact the first was trained keeping fixed the whole convolutional backbone excluding the last 2 blocks, with a batch size of 8 and on an NVIDIA RTX 3060, while MINTIME-XC was trained End2End with a batch size of 32 in parallel on four NVIDIA A100 for 30 epochs'], [[(('parameter', 'learning rate'), 33, 46), (('v number', '0.01'), 50, 54), (('v number', '0.0001'), 72, 78)], 'The optimizer used is SGD with a learning rate of 0.01, which decays to 0.0001 using a cosine scheduler'], [[(('parameter', 'weight decay'), 4, 16), (('v number', '0.0001'), 28, 34)], 'The weight decay was set to 0.0001 as in '], [[(('artifact', 'model'), 19, 24), (('artifact', 'model'), 89, 94), (('v number', '400'), 118, 121)], 'Since the SlowFast model trained in was not available, we retrained it starting from the model pretrained on Kinetics 400 in order to see how it behaved in certain contexts not reported in the original paper']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 36, 40), (('v number', '4'), 64, 65)], 'To pretrain the backbone, we use an Adam optimizer for the Conv-4 network and the stochastic gradient descent SGD optimizer for other backbone networks'], [[(('parameter', 'epochs'), 43, 49), (('parameter', 'batch size'), 57, 67), (('v number', '500'), 39, 42), (('v number', '128'), 71, 74)], 'The pretraining lasts for a maximum of 500 epochs with a batch size of 128'], [[(('parameter', 'learning rate'), 16, 29), (('v number', '0.001'), 40, 45), (('v number', '5'), 67, 68), (('v number', '0.0001'), 69, 75)], 'And the initial learning rate is set to 0.001 with a L2 penalty of 5 0.0001 .']] \n",
      "\n",
      "[[[(('artifact', 'L'), 110, 111), (('artifact', 'L'), 289, 290), (('artifact', 'L'), 291, 292), (('v number', '0'), 112, 113)], '10pt [h] 0pt X, Pt *[h]Context, Prompt tokens Y= r_:[[s_^, e_^], [s_^, e_^]] , r_:[[s_^, e_^]] H_, H X Pt, H_ L 0 *[h]Initialize datum loss role in Y .keys Set _ to empty list _ in .get_nextrole _ ^ W^ ^ W^ ^ ^ H *[h]cos-sim to H ^ ^ H *[h]cos-sim to H _ .insert _, i<j} ^i + ^j Y_,_ Y_,_ L L + CrossEntropyY_,_']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 136, 142), (('v number', '4'), 25, 26), (('v number', '64'), 81, 83), (('v number', '128'), 85, 88), (('v number', '256'), 90, 93), (('v number', '512'), 98, 101), (('v number', '2'), 120, 121), (('v number', '2'), 122, 123)], 'Its encoder consisted of 4 convolutional blocks with increasing output depths of 64, 128, 256 and 512, each followed by 2 2 max-pooling layers for downsampling'], [[(('parameter', 'layers'), 53, 59), (('parameter', 'layer'), 117, 122), (('parameter', 'activation'), 134, 144), (('v number', '3'), 65, 66), (('v number', '3'), 67, 68)], 'These blocks comprised two consecutive convolutional layers with 3 3 filters, each followed by a batch normalization layer and a ReLU activation'], [[(('parameter', 'layers'), 90, 96), (('v number', '1024'), 25, 29)], 'An additional block with 1024 filters was used at the bottleneck, followed by the decoder layers'], [[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 34, 47), (('parameter', 'epochs'), 73, 79), (('v number', '0.01'), 51, 55), (('v number', '200'), 69, 72)], 'Adam optimization with an initial learning rate of 0.01 was used for 200 epochs to minimize the cross-entropy loss'], [[(('parameter', 'epochs'), 43, 49), (('v number', '400'), 39, 42)], 'When training using only DRISHTI data, 400 epochs were used instead'], [[(('parameter', 'learning rate'), 20, 33), (('parameter', 'epochs'), 109, 115), (('v number', '0.2'), 49, 52), (('v number', '2'), 107, 108)], 'We also reduced the learning rate by a factor of 0.2 every time the performance plateaued for a maximum of 2 epochs'], [[(('artifact', 'model'), 9, 14), (('parameter', 'epochs'), 23, 29)], 'The best model through epochs according to the validation set was preserved and finally used for evaluation.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 34, 39), (('v number', '1'), 9, 10)], 'In Stage 1, we first train an ERM model with a cross-entropy loss'], [[(('parameter', 'layers'), 105, 111), (('v number', '2'), 27, 28)], 'For consistency with Stage 2, we depict the output as a composition of the encoder and linear classifier layers'], [[(('artifact', 'model'), 32, 37), (('artifact', 'model'), 120, 125), (('parameter', 'layers'), 259, 265), (('v number', '2'), 14, 15), (('v number', '1'), 114, 115)], 'Then in Stage 2, we train a new model with the same architecture using contrastive batches sampled with the Stage 1 ERM model and a supervised contrastive loss REF which we compute after the depicted representations are first normalized to update the encoder layers'], [[(('parameter', 'layer'), 200, 205), (('parameter', 'layers'), 272, 278)], 'Note that unlike prior work in contrastive learning , , as we have the class labels of the anchors, positives, and negatives, we also continue forward-passing the unnormalized representations encoder layer outputs and compute a cross-entropy loss to update the classifier layers while jointly training the encoder.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 56, 61), (('v number', '1'), 34, 35), (('v number', '4'), 54, 55)], 'On F-EMNIST, we experiment with a 1 million parameter 4 layer Convolutional Neural Network CNN used by '], [[(('parameter', 'layer'), 53, 58), (('artifact', 'model'), 87, 92), (('v number', '4'), 31, 32), (('v number', '4'), 51, 52)], 'On SONWP, we experiment with a 4 million parameter 4 layer long-short term memory LSTM model, which is the same as prior work , .']] \n",
      "\n",
      "[[[(('parameter', 'fold'), 155, 159), (('v number', '10'), 152, 154)], 'Inverse regularization strength C is selected independently for each training sample such that F1 score is maximized within the development splits of a 10-fold cross validation run.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 59, 69), (('v number', '768'), 33, 36), (('v number', '768'), 37, 40), (('v number', '12'), 74, 76), (('v number', '3090'), 102, 106)], 'The input size of the TLPNet was 768 768, and the training batch size was 12 on an Nvidia GeForce RTX 3090 GPU'], [[(('artifact', 'Adam'), 8, 12), (('parameter', 'learning rate'), 62, 75), (('parameter', 'epochs'), 136, 142), (('v number', '0.5'), 44, 47), (('v number', '0.9'), 49, 52), (('v number', '0.0002'), 87, 93), (('v number', '20'), 133, 135)], 'We used Adam to optimize the network with = 0.5, 0.9, and the learning rate started at 0.0002 and decayed to nine-tenths after every 20 epochs in the training phase'], [[(('parameter', 'batch size'), 55, 65), (('v number', '768'), 29, 32), (('v number', '768'), 33, 36), (('v number', '20'), 70, 72), (('v number', '10'), 77, 79), (('v number', '3090'), 141, 145)], 'The input size of TAANet was 768 768, and the training batch size was 20 and 10 in GTM and CHM, respectively, on a single Nvidia GeForce RTX 3090 GPU'], [[(('parameter', 'learning rate'), 53, 66), (('v number', '0.0004'), 101, 107)], 'The optimizer is the same as that in TLPNet, and the learning rate of the discriminators starts from 0.0004, which has the same decay rate as that in TLPNet.']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 9, 14), (('parameter', 'learning rate'), 48, 61), (('parameter', 'weight decay'), 76, 88), (('v number', '5e-4'), 62, 66), (('v number', '5e-4'), 104, 108), (('v number', '5e-5'), 113, 117)], 'We adopt AdamW as the optimizer with an initial learning rate 5e-4 with L_2 weight decay ranges between 5e-4 and 5e-5'], [[(('parameter', 'dropout'), 0, 7), (('v number', '0.4'), 36, 39), (('v number', '0.5'), 44, 47)], 'Dropout is used with a rate between 0.4 and 0.5'], [[(('parameter', 'layers'), 22, 28), (('v number', '1'), 118, 119), (('v number', '5'), 124, 125)], \"The number of encoder layers in each modality's encoder i.e., N_T, N_A, N_V is tuned using a greedy scheme and set to 1 and 5 for MELD and IEMOCAP validation datasets, respectively\"], [[(('parameter', 'layers'), 42, 48), (('parameter', 'm'), 69, 70), (('v number', '5'), 59, 60)], 'The number of multi-head attention fusion layers is set to 5 i.e., = m for both dataset'], [[(('parameter', 'm'), 123, 124), (('parameter', 'K'), 143, 144)], 'The proposed M2FNet framework is trained using the categorical cross-entropy on each utterances softmax output for each of M dialogs and their k utterances'], [[(('parameter', 'm'), 9, 10), (('parameter', 'K'), 13, 14)], 'Loss=- _^M _^k _^C y^ y_^.']] \n",
      "\n",
      "[[[(('parameter', 'fold'), 59, 63), (('artifact', 'cross-validation'), 64, 80)], 'Following , , we further divide the labeled set with three-fold cross-validation'], [[(('parameter', 'fold'), 45, 49), (('artifact', 'cross-validation'), 50, 66), (('artifact', 'model'), 96, 101), (('v number', '5'), 17, 18), (('v number', '5'), 87, 88), (('v number', '75'), 151, 153)], 'In total, we use 5 random data splits, three-fold cross-validation for each split, and 5 random model initializations per data partition, resulting in 75 total runs for each experiment.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'batch size'), 59, 69), (('v number', '1000'), 73, 77)], 'We use ADAM to optimize the evidence lower bound with mini-batch size of 1000.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 17, 22), (('parameter', 'learning rates'), 71, 85)], 'Furthermore, the model is optimized minimizing only a CTC loss and the learning rates LR are updated using the New Bob scheduler'], [[(('parameter', 'batch size'), 33, 43), (('parameter', 'layers'), 90, 96), (('v number', '3'), 47, 48), (('v number', '1.0'), 113, 116), (('v number', '1e-05'), 121, 126)], 'The ASR has been trained using a batch size of 3, setting the starting LRs for the linear layers and Wav2Vec2 of 1.0 and 1e-05 , respectively.']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 32, 37), (('parameter', 'epochs'), 79, 85), (('parameter', 'batch size'), 91, 101), (('v number', '600'), 75, 78), (('v number', '8'), 102, 103)], 'We train all networks using the AdamW optimizer and a one-cycle policy for 600 epochs with batch size 8'], [[(('parameter', 'learning rate'), 43, 56), (('parameter', 'weight decay'), 76, 88), (('parameter', 'learning rate'), 134, 147), (('parameter', 'weight decay'), 167, 179), (('v number', '2'), 64, 65), (('v number', '0.01'), 71, 75), (('v number', '3'), 155, 156), (('v number', '0.05'), 162, 166)], 'For the MinkowskiEngine baseline, we use a learning rate of 1e -2 with 0.01 weight decay, and for the GHA-augmented network, we use a learning rate of 1e -3 with 0.05 weight decay, which performed better in the respective settings']] \n",
      "\n",
      "[[[(('artifact', 'model'), 98, 103), (('v number', '1'), 69, 70), (('v number', '2'), 105, 106), (('v number', '3'), 146, 147)], 'As we summarize in Algorithm REF , there are three phases, including 1 training the velocity flow model, 2 improving straightness via reflow, and 3 flow distillation'], [[(('parameter', 'batch size'), 59, 69), (('parameter', 'learning rate'), 83, 96), (('v number', '1'), 9, 10), (('v number', '256'), 73, 76)], 'For step 1, we mainly follow the setting in DDPM and use a batch size of 256 and a learning rate of 2e^ '], [[(('artifact', 'model'), 13, 18), (('parameter', 'steps'), 28, 33), (('artifact', 'EMA'), 74, 77), (('v number', '0.9999'), 91, 97)], 'We train the model for 200k steps and apply an exponential moving average EMA at a rate of 0.9999'], [[(('parameter', 'steps'), 114, 119), (('v number', '2'), 9, 10)], 'For step 2, we randomly sample 50k data pairs X^_0, X^_1 using the pretrained network v_ and fine-tune it for 10k steps by minimizing Eqn'], [[(('parameter', 'steps'), 94, 99), (('parameter', 'learning rate'), 105, 118), (('v number', '3'), 9, 10), (('v number', '2'), 60, 61)], 'For step 3, we use the samples X^_0, X^_1 generated in step 2 and finetune v_ for another 10k steps with learning rate 2e^ .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 61, 67), (('v number', '3'), 59, 60), (('v number', '2'), 138, 139)], 'The early stopping technique is used: Stopping criteria is 3 epochs without improving the loss function on the dev set, which is a random 2% sample from the same source as training data and is different for each stage.']] \n",
      "\n",
      "[[[(('artifact', 'method'), 37, 43), (('artifact', 'method'), 81, 87), (('artifact', 'model'), 110, 115)], 'FewGen is a training data generation method and can be used with any fine-tuning method on any classification model']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 19, 24), (('parameter', 'weight decay'), 40, 52), (('v number', '0.0001'), 53, 59)], 'QueryProp utilizes AdamW optimizer with weight decay 0.0001'], [[(('parameter', 'learning rate'), 53, 66), (('v number', '2.5'), 77, 80), (('v number', '1e-05'), 81, 86), (('v number', '10'), 100, 102)], 'The training iteration is set to 90k and the initial learning rate is set to 2.5 1e-05 , divided by 10 at iteration 65k and 80k, respectively'], [[(('parameter', 'm'), 43, 44), (('v number', '10'), 45, 47)], 'For each non-key frame, we randomly select m 10 by default adjacent frames as key frames to form a training batch'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'learning rate'), 92, 105), (('v number', '0.0001'), 36, 42)], 'The initial learning rate is set to 0.0001 and the total training iteration is 16k, and the learning rate is dropped after iteration 8k and 12k']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 32, 38), (('v number', '8'), 30, 31), (('v number', '512'), 50, 53), (('v number', '1024'), 81, 85)], 'The network f_ is composed of 8 layers with width 512 Mission Bay experiments or 1024 all other experiments'], [[(('parameter', 'layers'), 10, 16), (('parameter', 'layers'), 46, 52), (('v number', '3'), 8, 9), (('v number', '128'), 28, 31), (('v number', '4'), 44, 45), (('v number', '128'), 64, 67)], 'f_c has 3 layers with width 128 and f_v has 4 layers with width 128'], [[(('artifact', 'Adam'), 35, 39), (('parameter', 'batch size'), 77, 87), (('v number', '16384'), 91, 96)], 'We train each Block-NeRF using the Adam optimizer for 300K iterations with a batch size of 16384'], [[(('parameter', 'learning rate'), 25, 38), (('v number', '2'), 75, 76), (('v number', '0.001'), 77, 82), (('v number', '2'), 86, 87), (('v number', '1e-05'), 88, 93), (('v number', '1024'), 134, 138)], 'Similar to mip-NeRF, the learning rate is an annealed logarithmically from 2 0.001 to 2 1e-05 , with a warm up phase during the first 1024 iterations']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 36, 41), (('parameter', 'learning rate'), 63, 76)], 'For pre-training details, we use an AdamW optimizer and cosine learning rate decay '], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'weight decay'), 50, 62), (('v number', '0.001'), 36, 41), (('v number', '0.05'), 66, 70)], 'The initial learning rate is set to 0.001, with a weight decay of 0.05'], [[(('artifact', 'model'), 17, 22), (('parameter', 'epochs'), 31, 37), (('parameter', 'batch size'), 46, 56), (('v number', '300'), 27, 30), (('v number', '128'), 60, 63)], 'We pre-train our model for 300 epochs, with a batch size of 128.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 16, 29), (('v number', '0.001'), 40, 45)], 'And the initial learning rate is set to 0.001'], [[(('parameter', 'learning rate'), 13, 26), (('parameter', 'epochs'), 90, 96), (('v number', '3'), 88, 89)], 'Besides, the learning rate is halved if the validation loss increases consecutively for 3 epochs'], [[(('parameter', 'epochs'), 78, 84), (('v number', '6'), 76, 77)], 'The training process stops when validation loss increases consecutively for 6 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 0, 5), (('v number', '1'), 11, 12), (('v number', '0.9'), 15, 18), (('v number', '2'), 21, 22), (('v number', '0.999'), 25, 30)], 'AdamW with 1 = 0.9 , 2 = 0.999 is employed for optimization.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 44, 49), (('parameter', 'batch size'), 55, 65), (('v number', '1'), 66, 67), (('v number', '16'), 174, 176)], 'For ArcaneQA, we are only able to train our model with batch size 1 due to the memory consumption, so we choose a workaround to set the number of gradient accumulation to be 16'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 37, 50), (('v number', '0.001'), 54, 59)], 'We use Adam optimizer with an intial learning rate of 0.001 to update our own parameters in BERT-based models'], [[(('parameter', 'learning rate'), 48, 61), (('v number', '2e-5'), 65, 69)], \"For BERT's parameters, we fine-tune them with a learning rate of 2e-5\"], [[(('parameter', 'batch size'), 39, 49), (('parameter', 'learning rate'), 68, 81), (('artifact', 'Adam'), 97, 101), (('v number', '32'), 50, 52), (('v number', '0.001'), 85, 90)], 'For ArcaneQA wo BERT, we train it with batch size 32 and an initial learning rate of 0.001 using Adam optimizer'], [[(('parameter', 'dropout'), 89, 96), (('v number', '768'), 76, 79), (('v number', '0.5'), 112, 115)], 'For both models, the hidden sizes of both encoder and decoder are set to be 768, and the dropout rate is set to 0.5'], [[(('parameter', 'learning rate'), 116, 129), (('parameter', 'steps'), 151, 156), (('parameter', 'dropout'), 186, 193)], 'specifically, we do manual hyper-parameter search from [1e-5, 2e-5, 3e-5], [8, 16, 32], [0.0, 0.2, 0.5] to tune the learning rate of fine-tuning BERT, steps of gradient accumulation and dropout rate respectively.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 68, 81), (('parameter', 'batch size'), 92, 102), (('v number', '0.001'), 82, 87), (('v number', '1'), 103, 104)], 'BMNet used in GAM is trained using an SGD optimizer with an initial learning rate 0.001 and batch size 1'], [[(('parameter', 'epochs'), 23, 29), (('v number', '140'), 19, 22)], 'It converges after 140 epochs of training on one GTX1080Ti GPU'], [[(('parameter', 'K'), 68, 69), (('v number', '3'), 70, 71), (('v number', '0.7'), 82, 85)], 'The following experiments, if not otherwise specified, use GAM with k=3 and ratio=0.7 as the default configuration.']] \n",
      "\n",
      "[[[(('artifact', 'GNN'), 8, 11), (('artifact', 'Adam'), 45, 49), (('parameter', 'learning rate'), 69, 82), (('parameter', 'epochs'), 188, 194), (('parameter', 'learning rate'), 207, 220), (('v number', '0.001'), 101, 106)], 'For the GNN training process, we adopted the Adam optimizer with the learning rate initialized to be 0.001 and a cosine annealing scheduler with the maximum iteration set to the number of epochs to tune the learning rate for minimizing the loss'], [[(('artifact', 'GNN'), 40, 43), (('parameter', 'layer'), 44, 49), (('parameter', 'activation'), 136, 146), (('v number', '0.2'), 117, 120)], 'After obtaining the embedding from each GNN layer, we apply the Leaky Rectified Linear Unit with a negative slope of 0.2 for non-linear activation.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 75, 88), (('v number', '1e-3'), 92, 96)], 'For fine-tuning, we use the Adafactor optimizer , with a linearly decaying learning rate of 1e-3'], [[(('parameter', 'batch size'), 135, 145), (('v number', '32'), 180, 182), (('v number', '256'), 186, 189), (('v number', '2'), 220, 221)], 'Since training with smaller batches is known to be more effective for extremely low-resource language training , we tuned the training batch size for every language - varying from 32 to 256 with gradient accumulation as 2 though we did not see very significant variation in the performance on the basis of this tuning'], [[(('parameter', 'epochs'), 59, 65), (('v number', '60'), 56, 58)], 'For our stopping criteria: we fine-tuned all models for 60 epochs which concluded with considerably overfit models and then selected models by we picking the checkpoint which had the best validation performance on BLEU with only the 13a tokenizer which mimics the mteval-v13a script from Moses .']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 124, 137), (('parameter', 'weight decay'), 182, 194), (('v number', '64'), 63, 65), (('v number', '1e-1'), 141, 145), (('v number', '0.9'), 173, 176), (('v number', '5e-4'), 208, 212)], 'We follow the common settings and set the scale parameter s to 64, Stochastic Gradient DescentSGD optimizer with an initial learning rate of 1e-1, set momentum parameter to 0.9, and weight decay parameter to 5e-4'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '10'), 32, 34)], 'The learning rate is divided by 10 at 80k, 140k, 210k, and 280k training iterations'], [[(('artifact', 'model'), 39, 44), (('v number', '512'), 21, 24), (('v number', '7.9'), 82, 85), (('v number', '2009'), 86, 90), (('v number', '4216'), 116, 120), (('v number', '2'), 127, 128), (('v number', '2'), 147, 148), (('v number', '6000'), 168, 172)], 'We set batch-size to 512 and train our model on Linux device CentOS Linux release 7.9.2009 with IntelR XeonR Silver 4216 CPU @ 2.10GHz, 128RAM and 2 Nvidia GeForce RTX 6000 GPUs']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 19, 24), (('parameter', 'layer'), 41, 46), (('v number', '6'), 17, 18), (('v number', '6'), 39, 40)], 'It consists of a 6-layer encoder and a 6-layer decoder'], [[(('artifact', 'model'), 4, 9), (('v number', '512'), 30, 33), (('v number', '2048'), 65, 69), (('v number', '8'), 107, 108)], 'The model dimension is set to 512, the intermediate dimension is 2048 and the number of attention heads is 8'], [[(('artifact', 'Adam'), 17, 21), (('v number', '0.9'), 40, 43), (('v number', '0.98'), 45, 49)], 'The optimizer is Adam with betas set to 0.9, 0.98'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 45, 50), (('v number', '0.00015'), 21, 28), (('v number', '2000'), 55, 59)], 'The learning rate is 0.00015 and the warm-up steps are 2000'], [[(('parameter', 'learning rate'), 19, 32), (('v number', '4'), 64, 65)], 'We also reduce the learning rate on plateaus with a patience of 4'], [[(('parameter', 'weight decay'), 0, 12), (('v number', '0.000001'), 16, 24)], 'Weight decay is 0.000001'], [[(('parameter', 'dropout'), 27, 34), (('v number', '0.1'), 19, 22), (('v number', '0.3'), 43, 46)], 'Label smoothing is 0.1 and dropout rate is 0.3.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 87, 92), (('parameter', 'batch size'), 107, 117), (('v number', '3090'), 168, 172), (('v number', '5'), 181, 182), (('v number', '000'), 183, 186)], 'For videos, we adopt the weights of SSIS-Track trained on images to initialize the the model, set the mini-batch size as four, and optimize SSIS-Track on an NVIDIA RTX 3090 GPU for 5,000 iterations'], [[(('parameter', 'learning rate'), 19, 32), (('v number', '1e-5'), 36, 40), (('v number', '1e-6'), 58, 62), (('v number', '4'), 66, 67), (('v number', '500'), 68, 71)], 'We set the initial learning rate as 1e-5 and reduce it to 1e-6 at 4,500 iterations']] \n",
      "\n",
      "[[[(('artifact', 'model'), 17, 22), (('v number', '2.0'), 48, 51), (('v number', '4.0'), 124, 127), (('v number', '2.0'), 199, 202)], 'We implement our model based on IFC with Apache 2.0 license and Mask2Former with Creative Commons Attribution-NonCommercial 4.0 International License, using Detectron2 framework under Apache License 2.0'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'learning rate'), 49, 62), (('v number', '110'), 38, 41)], 'The learning rate for the backbone is 110 of the learning rate of other parts of the network']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 12, 17), (('artifact', 'BiLSTM'), 34, 40), (('v number', '1'), 10, 11), (('v number', '256'), 18, 21)], 'We used a 1-layer 256-dimensional BiLSTM to encode words']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 29, 36), (('artifact', 'model'), 119, 124)], 'We train the case-preserving version of BERT_}} with 2M sentences containing funding information to obtain the BERT_}} model'], [[(('parameter', 'batch size'), 76, 86), (('parameter', 'epoch'), 137, 142), (('parameter', 'steps'), 148, 153), (('v number', '12'), 53, 55), (('v number', '2048'), 90, 94), (('v number', '1000'), 143, 147)], 'The training is done on an NVIDIA Tesla K80 GPU with 12 GB of memory with a batch size of 2048 through gradient accumulation and for one epoch 1000 steps'], [[(('parameter', 'epochs'), 38, 44), (('parameter', 'batch size'), 50, 60), (('v number', '3'), 36, 37), (('v number', '8'), 64, 65)], 'The fine-tuning process is done for 3 epochs with batch size of 8'], [[(('artifact', 'model'), 35, 40), (('parameter', 'learning rate'), 81, 94), (('parameter', 'batch size'), 110, 120), (('v number', '2'), 98, 99), (('v number', '1e-05'), 100, 105), (('v number', '16'), 124, 126)], 'For disambiguation, the bi-encoder model is trained on the EDFund dataset with a learning rate of 2 1e-05 and batch size of 16'], [[(('parameter', 'epochs'), 66, 72), (('v number', '4'), 29, 30), (('v number', '2'), 64, 65)], 'The training is performed in 4 rounds, each round consisting of 2 epochs']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 0, 10), (('artifact', 'Adam'), 26, 30), (('parameter', 'weight decay'), 60, 72), (('v number', '32'), 12, 14)], 'Batch size: 32 Optimizer: Adam with lr=5e^ and =1e^ without weight decay'], [[(('parameter', 'epochs'), 3, 9), (('artifact', 'model'), 11, 16), (('v number', '10'), 0, 2)], '10 epochs, model with best F1-score on the dev dataset was selected']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 10, 15), (('v number', '3'), 16, 17), (('v number', '6'), 19, 20), (('v number', '9'), 22, 23), (('v number', '12'), 28, 30), (('v number', '12'), 41, 43)], 'We tested layer 3, 6, 9 and 12 and found 12 to perform the best according to the average benchmark-performance on the development set using three random seeds'], [[(('parameter', 'layer'), 30, 35), (('v number', '12'), 36, 38)], 'We use context integration on layer 12 for all reported results']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 51, 57), (('parameter', 'learning rate'), 65, 78), (('artifact', 'EMA'), 105, 108), (('v number', '24'), 48, 50), (('v number', '2e-4'), 79, 83), (('v number', '3'), 98, 99)], 'We train our models on the nuScenes dataset for 24 epochs, using learning rate 2e-4, depth weight 3, and EMA strategy']] \n",
      "\n",
      "[[[(('parameter', 'm'), 27, 28), (('v number', '4009'), 14, 18), (('v number', '4'), 32, 33), (('v number', '1.8'), 41, 44)], 'For reservoir 4009, we set M to 4 and to 1.8 '], [[(('parameter', 'm'), 26, 27), (('v number', '3'), 28, 29), (('v number', '1.5'), 35, 38)], 'For all other reservoirs, M=3 and =1.5 '], [[(('parameter', 'layers'), 54, 60), (('parameter', 'layers'), 90, 96), (('v number', '4'), 42, 43), (('v number', '6'), 47, 48), (('v number', '5'), 66, 67), (('v number', '6'), 83, 84), (('v number', '4'), 116, 117)], 'For each reservoir, we tested models with 4 or 6 LSTM layers, and 5 reservoirs use 6 LSTM layers while the rest use 4'], [[(('parameter', 'layer'), 20, 25), (('parameter', 'layers'), 75, 81), (('v number', '512'), 36, 39), (('v number', '1024'), 44, 48), (('v number', '1024'), 65, 69), (('v number', '512'), 162, 165)], 'We also tested LSTM layer widths of 512 and 1024 nodes and found 1024 node layers were better suited for the N and C models, while E models performed better with 512 nodes across all reservoirs']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 18, 23), (('parameter', 'batch size'), 34, 44), (('parameter', 'weight decay'), 50, 62), (('parameter', 'epochs'), 65, 71), (('v number', '0.001'), 27, 32), (('v number', '128'), 45, 48)], 'For an optimizer, AdamW lr=0.001, batch size=128, weight decay=, epochs='], [[(('parameter', 'learning rate'), 6, 19), (('parameter', 'learning rate'), 40, 53)], 'For a learning rate shceduler, a cosine learning rate decay is used .']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 54, 58), (('v number', '3070'), 40, 44)], 'All experiments are trained on a NVIDIA 3070 GPU with Adam optimizer'], [[(('parameter', 'learning rate'), 36, 49), (('v number', '0.001'), 52, 57), (('v number', '0.99'), 84, 88)], 'While training the networks, we use learning rate = 0.001 and the discount factor = 0.99 '], [[(('artifact', 'system'), 7, 13), (('v number', '3'), 24, 25)], 'For RL system, we train 3 SAC agents simultaneously']] \n",
      "\n",
      "[[[(('parameter', 'K'), 47, 48), (('parameter', 'K'), 73, 74)], 'The proposed generative classifier consists of K binary classifiers in a K class classification problem'], [[(('artifact', 'model'), 20, 25), (('v number', '0.5'), 81, 84)], 'The ResNet18Thinner model is an ResNet18 architecture with a width multiplier of 0.5, and has 10M parameters']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 94, 100), (('parameter', 'layer'), 127, 132), (('v number', '5'), 92, 93), (('v number', '300'), 105, 108)], 'For the fair comparison with other baselines, We use the Graph Isomorphism Network GIN with 5 layers and 300 hidden units each layer as our backbones for models pre-trained on all those datasets mentioned above except for AP_NFwhose settings are kept the same with GCC , and mean-pooling to get graph-level representations following .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 68, 73), (('artifact', 'model'), 94, 99), (('artifact', 'L'), 104, 105), (('v number', '16'), 111, 113), (('v number', '16'), 114, 116)], 'We utilize two vision Transformer variants as the encoder, the Base model ViT-B and the Large model ViT-L with 16 16 input patch size '], [[(('artifact', 'L'), 95, 96), (('v number', '12'), 30, 32), (('v number', '768'), 76, 79), (('v number', '16'), 117, 119), (('v number', '1024'), 152, 156)], 'The ViT-B encoder consists of 12 Transfomer blocks with embedding dimension 768, while the ViT-L encoder consists of 16 blocks with embedding dimension 1024']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 55, 60), (('parameter', 'layer'), 77, 82), (('v number', '50'), 36, 38), (('v number', '2048'), 83, 87)], 'Following , f_ is a standard ResNet-50 and h_ is a two-layer MLP head hidden layer 2048-d, with ReLU'], [[(('parameter', 'learning rate'), 98, 111), (('parameter', 'weight decay'), 121, 133), (('v number', '0.03'), 115, 119), (('v number', '0.0001'), 134, 140), (('v number', '0.9'), 163, 166)], 'Moreover, when adopting MoCo-v2 in DiRA, f_ , h_ , and g_ are optimized using SGD with an initial learning rate of 0.03, weight decay 0.0001, and the SGD momentum 0.9.']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 89, 94), (('v number', '1'), 87, 88)], 'Specifically, reported TAPT result on NewsQA, Relation, and Medication are obtained by 1 epoch of further pre-training'], [[(('parameter', 'weight decay'), 11, 23), (('parameter', 'learning rate'), 33, 46), (('parameter', 'batch size'), 88, 98), (('parameter', 'learning rate'), 117, 130), (('v number', '0.01'), 27, 31), (('v number', '5e-5'), 50, 54), (('v number', '384'), 83, 86), (('v number', '12'), 102, 104), (('v number', '0.06'), 140, 144)], 'We use the weight decay of 0.01, learning rate of 5e-5, maximum sequence length of 384, batch size of 12, and linear learning rate decay of 0.06 warmup rate, with a half-precision']] \n",
      "\n",
      "[[[(('artifact', 'model'), 56, 61), (('parameter', 'epoch'), 80, 85), (('v number', '10'), 77, 79)], 'We use the original fine-tuned BERT-base as the teacher model and launch the 10-epoch logit distillation flow to train the Rank SuperNet'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 29, 42), (('artifact', 'linear decay'), 57, 69), (('v number', '3e-5'), 46, 50)], 'We use Adam optimizer with a learning rate of 3e-5 and a linear decay schedule']] \n",
      "\n",
      "[[[(('artifact', 'model'), 39, 44), (('v number', '0'), 78, 79), (('v number', '2'), 85, 86), (('v range', '[0,3]'), 92, 97), (('v range', '[0,3]'), 103, 108)], 'To lower the computational cost of the model, we chose some small values: F_H=0, F_W=2, R_H [0,3], R_W [0,3] , and divided samples from the dataset into groups based on the number of terms in order to lower H,W per batch']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 29, 34), (('parameter', 'learning rate'), 44, 57), (('parameter', 'batch size'), 71, 81), (('artifact', 'model'), 130, 135), (('v number', '200'), 21, 24), (('v number', '000'), 25, 28), (('v number', '1e-05'), 61, 66), (('v number', '1024'), 85, 89), (('v number', '16'), 94, 96)], 'Ss, g is trained for 200,000 steps, using a learning rate of 1e-05 , a batch size of 1024 and 16-bit mixed precision used for the model weights and embeddings'], [[(('parameter', 'steps'), 72, 77), (('v number', '20'), 60, 62)], 'During training, models were evaluated both _} and _} every 20 training steps'], [[(('artifact', 'model'), 22, 27), (('v number', '10'), 8, 10)], 'The top-10 performing model checkpoints by F_1 score on _} were stored, along with their F_1 score on _} .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 100, 105), (('v number', '12'), 97, 99), (('v number', '768'), 130, 133), (('v number', '12'), 150, 152)], 'Following previous work , , , we adopt the same base-size network architecture which consists of 12-layer Transformer blocks with 768 hidden size and 12 attention heads'], [[(('artifact', 'model'), 33, 38), (('parameter', 'batch size'), 65, 75), (('parameter', 'steps'), 94, 99), (('parameter', 'epochs'), 110, 116), (('v number', '6'), 79, 80), (('v number', '144'), 81, 84), (('v number', '100'), 106, 109)], 'We mix the data and pretrain the model from scratch with a total batch size of 6,144 for 480k steps i.e., 100 epochs of the image-text pairs'], [[(('artifact', 'model'), 43, 48), (('parameter', 'epochs'), 56, 62), (('v number', '40'), 53, 55)], 'For the ablation experiments, we train the model for 40 epochs'], [[(('artifact', 'Adam'), 0, 4), (('artifact', 'model'), 66, 71), (('v number', '0.9'), 23, 26), (('v number', '0.999'), 32, 37)], 'Adam optimizer with _1=0.9 , _2=0.999 is utilized to optimize the model'], [[(('parameter', 'learning rate'), 9, 22), (('artifact', 'linear warmup'), 37, 50), (('parameter', 'steps'), 73, 78), (('parameter', 'learning rate'), 90, 103), (('v number', '2e-3'), 26, 30), (('v number', '10'), 66, 68), (('v number', '000'), 69, 72)], 'The peak learning rate is 2e-3, with linear warmup over the first 10,000 steps and cosine learning rate decay'], [[(('parameter', 'weight decay'), 4, 16), (('v number', '0.05'), 20, 24)], 'The weight decay is 0.05'], [[(('parameter', 'dropout'), 11, 18), (('v number', '0.1'), 60, 63)], 'We disable dropout, and use stochastic depth with a rate of 0.1.']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 72, 84), (('v number', '0.9'), 64, 67), (('v number', '0.001'), 85, 90)], 'We use mini-batch stochastic gradient descent, with momentum of 0.9 and weight decay 0.001'], [[(('parameter', 'batch size'), 7, 17), (('v number', '8'), 21, 22)], 'We use batch size of 8'], [[(('artifact', 'model'), 60, 65), (('parameter', 'epochs'), 74, 80), (('parameter', 'epochs'), 114, 120), (('v number', '400'), 70, 73), (('v number', '500'), 110, 113)], 'For UCF101 and Kinetics100 in the SSL setting, we train the model for 400 epochs and for HMDB51, we train for 500 epochs'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.1'), 36, 39)], 'The initial learning rate is set to 0.1 and then decayed using cosine annealing policy'], [[(('parameter', 'epochs'), 56, 62), (('parameter', 'epochs'), 88, 94), (('v number', '100'), 52, 55), (('v number', '50'), 85, 87), (('v number', '400'), 107, 110)], 'In the fully supervised setting, we train R2+1D for 100 epochs on UCF101, HMDB51 and 50 epochs on Kinetics-400.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 131, 132), (('v number', '16'), 94, 96)], 'For the hyper-parameters of DACS, the reduced dimension in the auxiliary classifier is set to 16, and we set the number of buckets k in Eq']] \n",
      "\n",
      "[[[(('artifact', 'model'), 9, 14), (('artifact', 'model'), 88, 93), (('v number', '18'), 156, 158), (('v number', '000'), 159, 162)], 'For each model we used the following hyper parameters, initially optimized so that each model has the same number of total parameters that of approximately 18,000 .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 26, 32), (('v number', '90'), 43, 45)], 'The total number training epochs is set to 90'], [[(('artifact', 'model'), 67, 72), (('v number', '0.9'), 41, 44)], 'Stochastic gradient decent with momentum 0.9 is used to update the model parameters'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 68, 74), (('v number', '0.1'), 29, 32), (('v number', '0.1'), 55, 58), (('v number', '30'), 65, 67)], 'The initial learning rate is 0.1 and are multiplied by 0.1 every 30 epochs'], [[(('parameter', 'weight decay'), 4, 16), (('v number', '1e-4'), 20, 24)], 'The weight decay is 1e-4']] \n",
      "\n",
      "[[[(('artifact', 'model'), 29, 34), (('parameter', 'batch size'), 163, 173), (('parameter', 'learning rate'), 185, 198), (('parameter', 'epochs'), 245, 251), (('parameter', 'learning rate'), 266, 279), (('parameter', 'epochs'), 301, 307), (('artifact', 'model'), 341, 346), (('v number', '6'), 109, 110), (('v number', '8'), 177, 178), (('v number', '0.0025'), 202, 208), (('v number', '18'), 242, 244), (('v number', '10'), 295, 297), (('v number', '12'), 308, 310), (('v number', '16'), 315, 317)], 'We fine-tuned a Faster R-CNN model pretrained on COCO using the default training settings from Detectron2 v0.6 with the following modifications: we trained with a batch size of 8 and a learning rate of 0.0025 on two NVIDIA RTX A5000 GPUs for 18 epochs, reducing the learning rate by a factor of 10 at epochs 12 and 16, and selected the best model checkpoint based on validation AP50.']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 17, 22), (('parameter', 'learning rate'), 31, 44), (('v number', '1e-6'), 48, 52)], 'The optimizer is AdamW and the learning rate is 1e-6 as an initial value'], [[(('parameter', 'learning rate'), 4, 17), (('artifact', 'gradient clipping'), 126, 143), (('v number', '10'), 107, 109)], 'The learning rate scheduler used for training is get_linear_schedule_with_warmup, and the maximum value of 10 is used for the gradient clipping']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 52, 57), (('parameter', 'learning rate'), 83, 96), (('parameter', 'learning rate'), 124, 137), (('parameter', 'batch size'), 154, 164), (('v number', '1'), 97, 98), (('v number', '2048'), 168, 172)], 'We pre-train on ImageNet-1K IN-1K training set with AdamW optimizer with the basic learning rate 1.5e^ adjusted by a cosine learning rate scheduler and a batch size of 2048'], [[(('parameter', 'layer'), 77, 82), (('v number', '3'), 58, 59), (('v number', '50'), 70, 72), (('v number', '8'), 90, 91)], 'By default, the learnable mask tokens are placed at stage-3 in ResNet-50 and layer-5layer-8 in ViT-SViT-B, respectively']] \n",
      "\n",
      "[[[(('parameter', 'K'), 11, 12), (('parameter', 'K'), 62, 63), (('parameter', 'epoch'), 88, 93), (('parameter', 'steps'), 136, 141)], 'Let }_ = _^K _X_k,Y_k - be the mean constraint difference for K training samples in one epoch of training or a defined number of update steps'], [[(('parameter', 'epoch'), 87, 92), (('v number', '0'), 112, 113), (('v number', '1'), 114, 115)], 'In our experiments, we find that initializing slightly too large and updating it every epoch with = + }_ & }_ < 0 1 & .}']] \n",
      "\n",
      "[[[(('parameter', 'T'), 26, 27), (('parameter', 'steps'), 47, 52), (('parameter', 'm'), 94, 95), (('parameter', 'steps'), 103, 108), (('v number', '12'), 28, 30), (('v number', '6'), 96, 97)], 'In the experiment, we use T=12 historical time steps to predict the traffic speed of the next M=6 time steps'], [[(('artifact', 'Adam'), 36, 40), (('parameter', 'learning rate'), 56, 69)], 'The ST-GFSL framework is trained by Adam optimizer with learning rate decay in both inner loop and outer loop'], [[(('parameter', 'layer'), 67, 72), (('v number', '2'), 48, 49), (('v number', '1'), 83, 84)], 'In ST-Meta Learner, the number of GAT is set to 2, and the the GRU layer is set to 1'], [[(('artifact', 'model'), 60, 65), (('parameter', 'learning rate'), 134, 147), (('v number', '16'), 124, 126), (('v number', '0.01'), 150, 154), (('v number', '0.001'), 178, 183), (('v number', '5'), 206, 207), (('v number', '1.5'), 254, 257)], 'Totally, there are several important hyperparameters in our model, and we set them as: the dimension of meta knowledge d_ = 16 , task learning rate = 0.01 , meta-training rate = 0.001 , task batch number = 5 , and sum scale factor of two loss function = 1.5 .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 31, 36), (('v number', '10'), 67, 69), (('v number', '6'), 70, 71)], 'Then similar to , we train the model in an unsupervised fashion on 10^6 randomly samples sentences from Wikipedia'], [[(('parameter', 'learning rate'), 25, 38), (('parameter', 'epoch'), 52, 57), (('v number', '3.0'), 42, 45), (('v number', '1'), 50, 51), (('v number', '192'), 75, 78)], 'The LM is trained with a learning rate of 3.0 for 1 epoch at batch-size of 192'], [[(('parameter', 'layers'), 37, 43), (('v number', '4096'), 55, 59)], 'The projector MLP q has three linear layers, each with 4096 output units in conjunction with ReLU and BatchNorm in between'], [[(('parameter', 'dropout'), 51, 58), (('v number', '0.005'), 30, 35), (('v number', '0.013'), 39, 44), (('v number', '5.0'), 73, 76), (('v number', '15.0'), 86, 90)], 'For BERT hyperparameters are =0.005 , =0.013 , and dropout rates are r_A=5.0% and r_B=15.0% '], [[(('parameter', 'dropout'), 55, 62), (('v number', '0.0033'), 33, 39), (('v number', '0.028'), 43, 48), (('v number', '6.5'), 77, 80), (('v number', '24.0'), 90, 94)], 'For RoBERTa hyperparameters are =0.0033 , =0.028 , and dropout rates are r_A=6.5% and r_B=24.0% '], [[(('parameter', 'dropout'), 79, 86), (('v number', '0.1'), 57, 60), (('v number', '10'), 67, 69)], 'First a coarse-grid was put in place with a step-size of 0.1 for , 10% for the dropout rates r_A, r_B ']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 32, 37), (('v number', '20'), 78, 80), (('v number', '300'), 85, 88)], 'As suggested by , the number of steps for the path should be selected between 20 and 300'], [[(('parameter', 'steps'), 14, 19), (('parameter', 'steps'), 54, 59), (('v number', '60'), 24, 26), (('v number', '60'), 51, 53)], 'Hence, we use steps N = 60 , meaning that it takes 60 steps from baseline x^ to the original input sample x , according to x = x^ + } n , n is the current step.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 46, 59), (('parameter', 'weight decay'), 81, 93)], 'We train TISS with SGD optimizer and the same learning rate policy, momentum and weight decay'], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'steps'), 112, 117), (('v number', '0.01'), 36, 40), (('v number', '0.001'), 88, 93)], 'We set the initial learning rate to 0.01 for the first learning step and decrease it to 0.001 for the following steps as done in '], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.9'), 69, 72)], 'The learning rate is decreased with a polynomial strategy with power 0.9 '], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'epochs'), 29, 35), (('v number', '12'), 18, 20), (('v number', '30'), 26, 28), (('v number', '2012'), 63, 67)], 'The batch size is 12 with 30 epochs of training for Pascal-VOC 2012'], [[(('parameter', 'batch size'), 12, 22), (('parameter', 'epoch'), 39, 44), (('v number', '8'), 33, 34), (('v number', '60'), 55, 57)], 'For ADE20K, batch size is set to 8 and epoch is set to 60'], [[(('artifact', 'method'), 29, 35), (('v number', '20'), 109, 111)], 'The hyper-parameters of each method are set refer to the protocol of incremental learning defined in , using 20 % of the training set as validation']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 9, 15), (('v number', '5'), 7, 8), (('v number', '64'), 49, 51)], 'We use 5 layers by default with hidden dimension 64'], [[(('artifact', 'Adam'), 36, 40), (('parameter', 'learning rate'), 64, 77), (('parameter', 'epochs'), 120, 126), (('v number', '0.01'), 78, 82), (('v number', '0.5'), 101, 104), (('v number', '50'), 117, 119)], 'We use weighted cross-entropy loss, Adam optimizer with initial learning rate 0.01 and decay of rate 0.5 after every 50 epochs'], [[(('parameter', 'dropout'), 7, 14), (('parameter', 'layer'), 28, 33), (('parameter', 'epochs'), 55, 61), (('parameter', 'batch size'), 80, 90), (('v number', '300'), 51, 54), (('v number', '32'), 94, 96)], 'We use dropout on the final layer and we train for 300 epochs by default with a batch size of 32.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 16, 22), (('v number', '9'), 14, 15)], 'We choose the 9-layers of ResNet-Generator with encoder-decoder style and the PatchGAN-Discriminator for all of the models'], [[(('parameter', 'T'), 40, 41), (('v number', '19'), 30, 32)], 'Besides, we choose the Resnet-19 as our T network structure'], [[(('artifact', 'model'), 15, 20), (('artifact', 'Adam'), 79, 83), (('parameter', 'learning rate'), 89, 102), (('v number', '4'), 60, 61), (('v number', '2'), 103, 104), (('v number', '0.0001'), 105, 111), (('v range', '[0.5,0.999]'), 117, 128)], 'For all of the model optimization, we set the batch-size to 4 and optimizer to Adam with learning rate 2 0.0001 and =[0.5,0.999] '], [[(('artifact', 'model'), 49, 54), (('artifact', 'model'), 109, 114), (('parameter', 'epoch'), 129, 134), (('v number', '200'), 60, 63)], 'On all of the dataset, to be fair, we train each model with 200 epoches and we report the performance of the model from the last epoch because of no validation is provided.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 34, 40), (('parameter', 'learning rate'), 71, 84), (('parameter', 'weight decay'), 116, 128), (('v number', '150'), 30, 33), (('v number', '128'), 66, 69), (('v number', '0.00001'), 85, 92), (('v number', '0.0004'), 129, 135)], 'As detailed in , we train for 150 epochs with embedding dimension 128, learning rate 0.00001 with no scheduler, and weight decay 0.0004 '], [[(('parameter', 'batch size'), 16, 26), (('artifact', 'Adam'), 44, 48), (('v number', '128'), 30, 33), (('v number', '0'), 163, 164), (('v number', '9'), 165, 166)], 'We train with a batch size of 128, with the Adam optimizer over five seeds inclusive for the balance control datasets, and for CUB200 color experiments; and seeds 0-9 for the manually class imbalanced experiments']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 38, 48), (('parameter', 'learning rate'), 54, 67), (('artifact', 'AdamW'), 84, 89), (('v number', '256'), 49, 52), (('v number', '0.001'), 68, 73)], 'The Tip-Adapter-F is fine-tuned using batch size 256, learning rate 0.001 , and the AdamW optimizer with a cosine scheduler'], [[(('parameter', 'epoch'), 11, 16), (('parameter', 'epoch'), 58, 63), (('v number', '100'), 7, 10), (('v number', '20'), 55, 57), (('v number', '10'), 83, 85)], 'We set 100-epoch training for EuroSAT dataset and only 20-epoch training for other 10 datasets.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 60, 70), (('v number', '256'), 74, 77)], 'An NVIDIA Tesla T4 GPU was used to train the models, with a batch size of 256'], [[(('parameter', 'epochs'), 120, 126), (('v number', '30'), 107, 109)], 'In order to save time and prevent overfitting, we define a callback function that stops the training after 30 unchanged epochs on validation loss.']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 69, 81), (('parameter', 'learning rate'), 115, 128), (('v number', '1'), 85, 86)], 'For the former, we utilise SGD with momentum as the optimiser with a weight decay of 1 , and leverage the OneCycle learning rate policy to implement '], [[(('parameter', 'learning rate'), 49, 62), (('v number', '0'), 90, 91)], 'REF and REF to update the video centres, and its learning rate is defined as =p where p > 0 is a constant number']] \n",
      "\n",
      "[[[(('parameter', 'activation'), 63, 73), (('parameter', 'activation'), 135, 145), (('parameter', 'layers'), 154, 160), (('artifact', 'model'), 168, 173)], 'As observed by , we also found that the Hyperbolic TangentTanh activation function worked better than ReLU, and hence we used the Tanh activation for all layers in our model'], [[(('artifact', 'model'), 4, 9), (('artifact', 'AdamW'), 31, 36), (('parameter', 'learning rate'), 54, 67), (('v number', '0.001'), 71, 76), (('v number', '0.9'), 105, 108), (('v number', '0.99'), 116, 120)], 'The model was trained using an AdamW optimizer with a learning rate of 0.001 and beta values set to _1 = 0.9 , _2 = 0.99 and the loss used was cross entropy loss'], [[(('parameter', 'epochs'), 99, 105), (('v number', '10'), 85, 87)], 'Additionally, early stopping was used if the validation loss does not decrease after 10 successive epochs'], [[(('parameter', 'batch size'), 4, 14), (('artifact', 'model'), 78, 83), (('v number', '64'), 26, 28)], 'The batch size was set to 64 for both Baseline models as well as the proposed model']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('v number', '32'), 18, 20)], 'The batch size of 32 is adopted to conduct the training'], [[(('artifact', 'Adam'), 60, 64), (('parameter', 'learning rate'), 73, 86), (('parameter', 'weight decay'), 121, 133), (('v number', '0.001'), 90, 95), (('v number', '0.0001'), 146, 152)], 'For the experiments without meta-learning, the optimizer is Adam, with a learning rate _1=0.001 and an L2 regularization weight decay strength of 0.0001 '], [[(('artifact', 'model'), 67, 72), (('parameter', 'learning rate'), 101, 114), (('v number', '0.1'), 121, 124)], 'For the experiments with meta-learning, the optimizer for the main model is replaced with SGD with a learning rate of _1=0.1 '], [[(('parameter', 'learning rate'), 4, 17), (('v number', '6'), 36, 37), (('v number', '1e-05'), 38, 43)], 'The learning rate for CMW-Net is _2=6 1e-05 as in the ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 21, 26), (('v number', '1'), 60, 61), (('v number', '2'), 63, 64), (('v number', '3'), 66, 67), (('v number', '4'), 72, 73)], 'In this section, the model parameters are described for MoE-1, 2, 3 and 4']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 30, 43), (('parameter', 'batch size'), 53, 63), (('v number', '5e-5'), 44, 48), (('v number', '32'), 64, 66)], 'We empirically determine that learning rate 5e-5 and batch size 32 are the best choices for our experiments'], [[(('parameter', 'epochs'), 105, 111), (('v number', '0'), 62, 63), (('v number', '0.01'), 88, 92), (('v number', '10'), 102, 104), (('v number', '0.04'), 118, 122)], 'After trial and error, we set the regularization parameter to 0 and then increase it by 0.01 at every 10 epochs up to 0.04']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('parameter', 'steps'), 38, 43), (('v number', '1152'), 48, 52)], 'The batch size and number of training steps are 1152 and 160k'], [[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 24, 28), (('parameter', 'learning rate'), 31, 44), (('v number', '1e-5'), 48, 52), (('v number', '0.03'), 78, 82)], 'The model is trained by Adam w learning rate of 1e-5 and warmup proportion of 0.03'], [[(('parameter', 'dropout'), 19, 26), (('parameter', 'weight decay'), 36, 48), (('v number', '1.0'), 53, 56), (('v number', '0.1'), 58, 61), (('v number', '0.01'), 66, 70)], 'The gradient clip, dropout rate and weight decay are 1.0, 0.1 and 0.01']] \n",
      "\n",
      "[[[(('parameter', 'T'), 158, 159), (('v number', '19'), 114, 116), (('v number', '0.5'), 160, 163)], 'In all the experiments except those in sec:ablation study, we use the loss function defined in eq:finalloss with =19 and the fusion strategy in eq:infer with T=0.5 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 30, 35), (('artifact', 'model'), 82, 87), (('v number', '34'), 24, 26), (('v number', '10'), 27, 29), (('v number', '10'), 50, 52)], \"Firstly, we use the WRN-34-10 model for the CIFAR-10 experiments and the SmallCNN model for the MNIST experiments, as is done in TRADES, and to train our family of TRADES defenses we utilize the default hyperparameters included in the author's GitHub repository\"], [[(('artifact', 'model'), 25, 30), (('v number', '30'), 44, 46), (('v number', '3'), 55, 56), (('v number', '3'), 65, 66)], 'Lastly, in the RT-threat model, we set ^} = 30, _x^} = 3, _y^} = 3 , which is consistent with prior work .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 94, 100), (('v number', '50'), 104, 106)], 'While searching for the optimal hyper-parameters, we temporarily capped the maximum number of epochs to 50']] \n",
      "\n",
      "[[[(('artifact', 'model'), 29, 34), (('parameter', 'weight decay'), 80, 92), (('parameter', 'learning rate'), 122, 135), (('v number', '0.9'), 75, 78), (('v number', '5'), 96, 97), (('v number', '0.0001'), 98, 104), (('v number', '0.01'), 139, 143)], 'Specifically, for the benign model, we use the SGD optimizer with momentum 0.9, weight decay of 5 0.0001 , and an initial learning rate of 0.01'], [[(('parameter', 'learning rate'), 15, 28), (('parameter', 'learning rate'), 63, 76), (('v number', '1e-05'), 80, 85)], 'An exponential learning rate scheduler is adopted with a final learning rate of 1e-05 '], [[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 26, 32), (('parameter', 'batch size'), 40, 50), (('v number', '50'), 23, 25), (('v number', '8'), 54, 55), (('v number', '10'), 132, 134), (('v number', '1'), 229, 230)], 'We train the model for 50 epochs with a batch size of 8 and a backbone of AlexNet-v1 on a single NVIDIA 2080Ti; For BOBA, we sample 10% training samples to generate poisoned samples by adding triggers with a modification rate of 1%'], [[(('artifact', 'model'), 51, 56), (('v number', '10'), 78, 80)], 'Other settings are the same as those of the benign model; For FSBA, we sample 10% of the training data as in BOBA'], [[(('parameter', 'learning rate'), 62, 75), (('v number', '0.25'), 79, 83)], 'When computing the L_ described in Section REF , we decay the learning rate as 0.25 of the original one']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 104, 109), (('parameter', 'learning rate'), 127, 140), (('parameter', 'learning rate'), 159, 172), (('parameter', 'epochs'), 191, 197), (('parameter', 'weight decay'), 207, 219), (('parameter', 'batch size'), 231, 241), (('parameter', 'epochs'), 257, 263), (('v number', '0.2'), 83, 86), (('v number', '5e-4'), 144, 148), (('v number', '10'), 201, 203), (('v number', '1e-4'), 223, 227), (('v number', '32'), 245, 247), (('v number', '100'), 253, 256)], 'For segmentation task in S3DIS, Pix4Point is trained using Cross-Entropy loss with 0.2 label smoothing, AdamW optimizer with a learning rate of 5e-4, a cosine learning rate scheduler, warmup epochs of 10, a weight decay of 1e-4, a batch size of 32, for 100 epochs'], [[(('parameter', 'epochs'), 55, 61), (('parameter', 'batch size'), 69, 79), (('v number', '500'), 51, 54), (('v number', '64'), 83, 85)], 'The difference lies in: the models are trained for 500 epochs with a batch size of 64 and we drop normals instead of colors and do not perform color auto-contrast since there is no color information'], [[(('parameter', 'weight decay'), 2, 14), (('parameter', 'epochs'), 31, 37), (('v number', '0.05'), 18, 22), (('v number', '200'), 27, 30)], 'A weight decay of 0.05 and 200 epochs are used']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 8, 21), (('parameter', 'batch size'), 26, 36), (('v number', '1e-4'), 48, 52), (('v number', '16'), 57, 59)], 'Initial learning rate and batch size are set to 1e-4 and 16, respectively'], [[(('artifact', 'model'), 20, 25), (('parameter', 'epochs'), 33, 39), (('v number', '80'), 30, 32)], 'We train the entire model for 80 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'hidden state'), 32, 44), (('artifact', 'model'), 75, 80), (('v number', '64'), 28, 30), (('v number', '512'), 58, 61)], 'We use embedding dimension =64, hidden state dimension d =512 for base GRU model and RETAIN'], [[(('artifact', 'cross-validation'), 79, 95), (('v number', '32'), 107, 109), (('v number', '64'), 111, 113), (('v number', '128'), 115, 118), (('v number', '256'), 120, 123), (('v number', '512'), 125, 128)], 'Hidden states dimension d^ for each GRU in R-MoE is determined by the internal cross-validation set range: 32, 64, 128, 256, 512'], [[(('artifact', 'cross-validation'), 63, 79), (('v number', '1'), 90, 91), (('v number', '5'), 93, 94), (('v number', '10'), 96, 98), (('v number', '20'), 100, 102), (('v number', '50'), 104, 106), (('v number', '100'), 108, 111)], 'The number of experts for R-MoE is also determined by internal cross-validation set range:1, 5, 10, 20, 50, 100'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.005'), 53, 58), (('v number', '0.0005'), 80, 86)], 'For learning rate of GRU, RETAIN, CNN, and LR we use 0.005 and for R-MoE we use 0.0005 '], [[(('parameter', 'weight decay'), 35, 47), (('artifact', 'cross-validation'), 138, 154)], 'To prevent over-fitting, we use L2 weight decay regularization during the training of all models and weight is determined by the internal cross-validation set'], [[(('parameter', 'K'), 103, 104), (('parameter', 'epochs'), 105, 111), (('parameter', 'K'), 112, 113), (('v number', '5'), 115, 116)], \"That is, we stop the training when the internal validation set's loss does not improve during the last K epochs K =5.\"]] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 22, 32), (('artifact', 'model'), 49, 54), (('parameter', 'learning rates'), 83, 97), (('parameter', 'learning rate'), 102, 115), (('parameter', 'learning rate'), 255, 268), (('v number', '0.1'), 128, 131), (('v number', '0.01'), 158, 162), (('v number', '0.001'), 190, 195), (('v number', '3e-5'), 217, 221)], 'We follow the popular batch size setting of each model , , and use the recommended learning rates and learning rate schedulers: 0.1 for image classification, 0.01 for semantic segmentation, 0.001 for Transformer, and 3e-5 for fine-tuning BERT; step decay learning rate schedule for CV training, inverse square root schedule for Transformer training, and linear schedule for fine-tuning BERT']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('v number', '512'), 53, 56)], 'We train the model with a maximum sequence length of 512 for the entire training time'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'batch size'), 46, 56), (('v number', '8'), 57, 58), (('v number', '192'), 59, 62)], 'We use the ADAM optimizer used in BERT with a batch size 8,192 on 64x Tesla V100 GPUs'], [[(('parameter', 'learning rate'), 11, 24), (('artifact', 'linear warmup'), 40, 53), (('v number', '1e-4'), 28, 32)], 'We set the learning rate to 1e-4 with a linear warmup scheduler'], [[(('parameter', 'steps'), 34, 39), (('parameter', 'steps'), 55, 60)], 'We run the warmup process for 10k steps and train 100k steps in total.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 87, 100), (('parameter', 'weight decay'), 109, 121)], 'In particular, We ran a grid search over the two most influential hyperparameters, the learning rate and the weight decay rate'], [[(('parameter', 'learning rate'), 29, 42), (('parameter', 'learning rate'), 77, 90), (('parameter', 'learning rate'), 133, 146), (('parameter', 'epochs'), 192, 198), (('v number', '60'), 158, 160), (('v number', '80'), 163, 165), (('v number', '93'), 172, 174)], 'We deploy a custom staircase learning rate decay schedule that decreases the learning rate over the training process by dividing the learning rate by four at 60%, 80%, and 93% of the training epochs'], [[(('parameter', 'epochs'), 45, 51), (('parameter', 'learning rate'), 78, 91), (('artifact', 'Adam'), 135, 139), (('parameter', 'layer'), 168, 173)], \"We warm up the training by running the first epochs with 110th of the initial learning rate in order to have the moments' estimates in Adam , Batch-Normalization , and Layer-Normalization modules initialized properly\"], [[(('artifact', 'Adam'), 24, 28), (('artifact', 'AdamW'), 44, 49), (('parameter', 'weight decay'), 72, 84), (('artifact', 'Adam'), 163, 167)], \"We replace the standard Adam optimizer with AdamW , which decouples the weight decay rate from the loss function, thus avoiding biasing the moments' estimators of Adam\"]] \n",
      "\n",
      "[[[(('parameter', 'steps'), 26, 31), (('v number', '3'), 17, 18)], 'We pretrain BEiT-3 for 1M steps'], [[(('artifact', 'AdamW'), 11, 16), (('v number', '0.9'), 35, 38), (('v number', '0.98'), 44, 48), (('v number', '1e-6'), 55, 59)], 'We use the AdamW optimizer with _1=0.9 , _2=0.98 and = 1e-6 for optimization'], [[(('parameter', 'learning rate'), 16, 29), (('parameter', 'learning rate'), 58, 71), (('artifact', 'linear warmup'), 86, 99), (('parameter', 'steps'), 107, 112), (('v number', '1e-3'), 75, 79)], 'We use a cosine learning rate decay scheduler with a peak learning rate of 1e-3 and a linear warmup of 10k steps'], [[(('parameter', 'weight decay'), 4, 16), (('v number', '0.05'), 20, 24)], 'The weight decay is 0.05 '], [[(('artifact', 'L'), 21, 22), (('parameter', 'layer'), 39, 44)], \"Next, we rescale the l -th Transformer layer's output matrices i.e., the last linear projection within each sublayer of self-attention and FFN by } \"]] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 50, 54)], 'The model is trained with gradient descent, using ADAM optimizer'], [[(('parameter', 'dropout'), 33, 40), (('v number', '0.3'), 56, 59)], 'For the RNN components, we use a dropout probability of 0.3 '], [[(('artifact', 'model'), 10, 15), (('v number', '7'), 24, 25), (('v number', '047'), 26, 29), (('v number', '529'), 30, 33)], 'The total model size is 7,047,529 parameters']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 4, 9), (('parameter', 'learning rate'), 27, 40), (('parameter', 'weight decay'), 55, 67), (('parameter', 'batch size'), 91, 101), (('v number', '5e-4'), 44, 48), (('v number', '0.5'), 71, 74), (('v number', '4'), 105, 106), (('v number', '096'), 107, 110)], 'The AdamW optimizer with a learning rate of 5e-4 and a weight decay of 0.5 is used, with a batch size of 4,096'], [[(('artifact', 'EMA'), 35, 38), (('artifact', 'model'), 39, 44), (('v number', '0.996'), 106, 111), (('v number', '1'), 139, 140)], 'For the exponential moving average EMA model which generates the attentive mask, its momentum starts from 0.996 and gradually increases to 1 using a cosine scheduler during training, following '], [[(('artifact', 'EMA'), 81, 84), (('artifact', 'model'), 85, 90), (('artifact', 'method'), 122, 128)], 'For the A-CLIP-eff variant, we use a halved resolution image as the input of the EMA model, with a bi-cubic interpolation method to get the new position encodings for the lower-resolution images.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 19, 24), (('v number', '64'), 52, 54), (('v number', '8'), 83, 84)], 'Within the concept model, LSTM dimension was set to 64 and LSTM sequence length to 8'], [[(('parameter', 'learning rate'), 21, 34), (('artifact', 'model'), 58, 63), (('parameter', 'epochs'), 91, 97), (('v number', '2'), 39, 40), (('v number', '0.0001'), 41, 47), (('v number', '25'), 80, 82), (('v number', '20'), 84, 86), (('v number', '10'), 88, 90)], 'During training, the learning rate was 2 0.0001 , and the model was trained for 25, 20, 10 epochs for PGS, and CVS tasks separately']] \n",
      "\n",
      "[[[(('artifact', 'method'), 21, 27), (('parameter', 'epochs'), 288, 294), (('parameter', 'weight decay'), 347, 359), (('parameter', 'learning rate'), 372, 385), (('parameter', 'epochs'), 409, 415), (('v number', '0.1'), 227, 230), (('v number', '64'), 268, 270), (('v number', '35'), 295, 297), (('v number', '0.9'), 324, 327), (('v number', '0.001'), 341, 346), (('v number', '0.0001'), 365, 371), (('v number', '0.5'), 397, 400), (('v number', '5'), 407, 408)], 'For all our proposed method variants and the re-implemented baselines, we use the same training parameters as the original work using a PyTorch re-implementationhttps:github.comNannepytorch-NetVlad and as listed here: margin g=0.1 , clusters centers vocabulary size V=64 , total training epochs 35, optimized using SGD with 0.9 momentum and 0.001 weight decay, and 0.0001 learning rate decayed by 0.5 every 5 epochs'], [[(('parameter', 'epochs'), 367, 373), (('v number', '1'), 208, 209), (('v number', '10'), 234, 236), (('v number', '1000'), 310, 314)], 'For triplet set mining, we have used the same methodology of NetVLAD: training and selection of the query-positive-negative triplets are carried using weakly supervised GPS data; for a single query image q , 1 positive within 10m and 10 negatives far away than 25m are selected from a pool of randomly sampled 1000 negatives; finally, hard negatives are tracked over epochs and used along with new hard negatives for training stability.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 50, 56), (('v number', '40'), 47, 49)], 'On a single NVIDIA TITAN RTX GPU, we train for 40 epochs'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'learning rate'), 51, 64), (('parameter', 'epochs'), 102, 108), (('v number', '0.0001'), 36, 42), (('v number', '50'), 78, 80)], 'The initial learning rate is set to 0.0001 and the learning rate decreases by 50% every five training epochs'], [[(('parameter', 'batch size'), 4, 14), (('v number', '4'), 25, 26)], 'The batch size is set to 4']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 14, 19), (('parameter', 'learning rate'), 43, 56), (('artifact', 'model'), 94, 99)], 'Specifically, AdamW optimizer and the step learning rate schedule are applied to optimize our model'], [[(('parameter', 'learning rate'), 11, 24), (('parameter', 'weight decay'), 41, 53), (('v number', '0.0001'), 28, 34), (('v number', '0.05'), 57, 61)], 'An initial learning rate of 0.0001 and a weight decay of 0.05 are utilized for all backbones'], [[(('parameter', 'learning rate'), 9, 22), (('parameter', 'learning rate'), 74, 87), (('parameter', 'steps'), 146, 151), (('v number', '0.1'), 53, 56), (('v number', '0.9'), 91, 94), (('v number', '0.95'), 99, 103), (('v number', '10'), 167, 169)], 'We set a learning rate multiplier of the backbone to 0.1 and we decay the learning rate at 0.9 and 0.95 fractions of the total number of training steps by a factor of 10'], [[(('parameter', 'batch size'), 67, 77), (('parameter', 'batch size'), 164, 174), (('v number', '38'), 41, 43), (('v number', '10'), 44, 46), (('v number', '4'), 47, 48), (('v number', '16'), 81, 83), (('v number', '12'), 131, 133), (('v number', '10'), 134, 136), (('v number', '4'), 137, 138)], 'On COCO dataset, we train our models for 38 10^4 iterations with a batch size of 16, while on UVO dataset, we train our models for 12 10^4 iterations with the same batch size.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 74, 80), (('v number', '512'), 7, 10), (('v number', '2'), 33, 34), (('v number', '048'), 35, 38)], 'We use 512 as embedding size and 2,048 hidden neurons in the feed-forward layers both in the encoder and in the decoder'], [[(('parameter', 'dropout'), 7, 14), (('parameter', 'layers'), 67, 73), (('v number', '0.1'), 18, 21)], 'We set dropout at 0.1 for feed-forward, attention, and convolution layers'], [[(('parameter', 'layer'), 25, 30), (('v number', '31'), 39, 41)], 'Also, in the convolution layer, we set 31 as kernel size for the point- and depth-wise convolutions'], [[(('artifact', 'Adam'), 17, 21), (('v number', '0.1'), 74, 77)], 'We optimize with Adam by using the label-smoothed cross-entropy loss with 0.1 as smoothing factor '], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 68, 73), (('v number', '5'), 28, 29), (('v number', '0.001'), 30, 35)], 'The learning rate is set to 5 0.001 with Noam scheduler and warm-up steps of 20k'], [[(('parameter', 'epochs'), 30, 36), (('v number', '15'), 27, 29), (('v number', '7'), 86, 87)], 'We stop the training after 15 epochs without loss decrease on the dev set and average 7 checkpoints around the best best, three preceding, and three succeeding']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 23, 28), (('parameter', 'weight decay'), 75, 87), (('v number', '101'), 14, 17), (('v number', '0.9'), 56, 59), (('v number', '0.982'), 67, 72), (('v number', '5'), 96, 97), (('v number', '0.0001'), 98, 104), (('v number', '1e-09'), 111, 116)], 'For NAS-Bench-101, the AdamW optimizer is set with _1 = 0.9 , _2 = 0.982 , weight decay term is 5 0.0001 and = 1e-09 '], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'epochs'), 59, 65), (('v number', '256'), 25, 28), (('v number', '35'), 56, 58), (('v number', '50'), 71, 73)], 'The batch size is set to 256 and the NAR is trained for 35 epochs with 50 iterations as warm-up'], [[(('artifact', 'AdamW'), 23, 28), (('parameter', 'weight decay'), 74, 86), (('v number', '201'), 14, 17), (('v number', '0.9'), 56, 59), (('v number', '0.99'), 67, 71), (('v number', '1'), 95, 96), (('v number', '0.01'), 97, 101), (('v number', '1e-09'), 108, 113)], 'For NAS-Bench-201, the AdamW optimizer is set with _1 = 0.9 , _2 = 0.99 , weight decay term is 1 0.01 and = 1e-09 '], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'epochs'), 59, 65), (('v number', '128'), 25, 28), (('v number', '55'), 56, 58), (('v number', '30'), 71, 73)], 'The batch size is set to 128 and the NAR is trained for 55 epochs with 30 iterations as warm-up.']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 30, 43), (('v number', '3072'), 53, 57), (('v number', '1536'), 62, 66)], 'For _ , the regressor has two hidden layers of sizes 3072 and 1536, similar to comet '], [[(('parameter', 'learning rate'), 9, 22), (('parameter', 'learning rate'), 42, 55), (('v number', '5e-5'), 26, 30)], 'We use a learning rate of 5e-5 and employ learning rate annealing with a linear schedule'], [[(('artifact', 'model'), 84, 89), (('artifact', 'model'), 111, 116)], 'When training comet, we follow the official implementation and fine-tune the entire model from the xlm-r-large model checkpoint '], [[(('parameter', 'epochs'), 43, 49), (('v number', '20'), 40, 42)], 'For both _ and comet, we train them for 20 epochs']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 19, 23), (('parameter', 'learning rate'), 41, 54), (('v number', '0.0001'), 58, 64)], 'We use the vanilla Adam optimizer with a learning rate of 0.0001 .']] \n",
      "\n",
      "[[[(('parameter', 'learning rates'), 40, 54), (('parameter', 'learning rate'), 69, 82), (('v number', '0.1'), 58, 61), (('v number', '0.6'), 63, 66), (('v number', '0.9'), 112, 115), (('v number', '1.0'), 117, 120), (('v range', '[4, 16]'), 30, 37)], 'The fixed batch sizes lied in [4, 16] , learning rates in 0.1, 0.6 , learning rate exponential decay factors in 0.9, 1.0 ']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 180, 186), (('parameter', 'hidden layers'), 194, 207), (('parameter', 'activation'), 260, 270), (('v number', '37'), 127, 129), (('v number', '000'), 130, 133), (('v number', '2'), 192, 193), (('v number', '128'), 215, 218), (('v number', '256'), 223, 226)], 'The summary of the DL framework within the RL architectures are as follows: a Deep Neural Network DNN consisting of a total of 37,000 training parameters and fully-connected dense layers with 2 hidden layers having 128 and 256 neurons, respectively, with ReLU activation'], [[(('parameter', 'layers'), 107, 113), (('parameter', 'hidden layers'), 121, 134), (('v number', '468'), 54, 57), (('v number', '000'), 58, 61), (('v number', '2'), 119, 120), (('v number', '128'), 142, 145), (('v number', '256'), 150, 153)], 'b Recurrent Neural Network RNN consists of a total of 468,000 training parameters and fully connected LSTM layers with 2 hidden layers having 128 and 256 neurons, respectively'], [[(('parameter', 'layer'), 11, 16), (('parameter', 'activation'), 104, 114)], 'The output layer consists of the number of actions the agent can decide for decision-making with linear activation'], [[(('parameter', 'learning rate'), 70, 83), (('v number', '0.95'), 64, 68), (('v number', '1e-4'), 86, 90), (('v number', '0.99'), 121, 125), (('v number', '0.5'), 165, 168)], 'The parameters of the DRL agent are as follows: discount rate = 0.95, learning rate = 1e-4, and the epsilon decay rate = 0.99 is selected with the initial epsilon = 0.5.']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 66, 71), (('parameter', 'weight decay'), 82, 94), (('parameter', 'learning rate'), 112, 125), (('artifact', 'linear decay'), 133, 145), (('parameter', 'steps'), 180, 185), (('v number', '1e-2'), 77, 81), (('v number', '2e-5'), 157, 161), (('v number', '10'), 168, 170)], 'During the training procedure, we optimize the student network by AdamW with 1e-2 weight decay and schedule the learning rate with a linear decay peaking at 2e-5 after 10% warm-up steps'], [[(('parameter', 'batch size'), 67, 77), (('v number', '128'), 7, 10), (('v number', '256'), 81, 84)], 'We set 128 tokens as the maximum length of each sequence and use a batch size of 256'], [[(('parameter', 'epochs'), 43, 49), (('v number', '3'), 29, 30), (('v number', '15'), 40, 42), (('v number', '8'), 53, 54)], 'The training procedure takes 3 days for 15 epochs on 8 40GB Nvidia A100 GPUs'], [[(('parameter', 'epochs'), 71, 77), (('parameter', 'batch size'), 85, 95), (('v number', '32'), 99, 101)], 'For the evaluation procedure, we fine-tune the student encoder for few epochs with a batch size of 32 on English training data, and evaluate on target languages.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 48, 54), (('v number', '4500'), 43, 47)], 'GRAF : On each dataset, we train more than 4500 epochs until convergence']] \n",
      "\n",
      "[[[(('parameter', 'm'), 35, 36), (('artifact', 'model'), 97, 102), (('v number', '8000'), 23, 27), (('v number', '8000'), 30, 34), (('v number', '1600'), 47, 51), (('v number', '50'), 94, 96)], 'We further divide this 8000 x 8000 m area into 1600 small patches, on which we ran our ResNet-50 model'], [[(('artifact', 'model'), 14, 19), (('v number', '50'), 11, 13)], 'The ResNet-50 model is pretrained on ImageNet'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 28, 34), (('parameter', 'learning rate'), 42, 55), (('v number', '20'), 25, 27), (('v number', '0.0001'), 59, 65)], 'The model is trained for 20 epochs with a learning rate of 0.0001'], [[(('parameter', 'epoch'), 12, 17), (('parameter', 'learning rate'), 23, 36), (('v number', '0.1'), 63, 66)], 'After every epoch, the learning rate is reduced by a factor of 0.1'], [[(('artifact', 'model'), 64, 69), (('parameter', 'epochs'), 77, 83), (('parameter', 'learning rate'), 91, 104), (('v number', '20'), 74, 76), (('v number', '0.00001'), 108, 115)], 'To incorporate the multi-temporal approach, we trained our LSTM model for 20 epochs with a learning rate of 0.00001 using KL Divergence loss.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 101, 107), (('v number', '768'), 63, 66), (('v number', '3072'), 89, 93), (('v number', '6'), 99, 100)], 'We use the Transformer architecture with a hidden dimension of 768, feed-forward size of 3072, and 6 layers for both the encoder and decoder'], [[(('parameter', 'batch size'), 51, 61), (('v number', '2048'), 37, 41), (('v number', '8'), 93, 94)], 'We set the maximum sequence length = 2048, using a batch size of 65K tokens distributed over 8 A100 GPUs.']] \n",
      "\n",
      "[[[(('artifact', 'system'), 25, 31), (('artifact', 'Adam'), 52, 56), (('v number', '0.9'), 75, 78), (('v number', '0.98'), 83, 87), (('v number', '1e-16'), 91, 96)], 'The proposed compact SLU system is trained with the Adam optimizer with _1=0.9, _2=0.98 , =1e-16 and warmup '], [[(('artifact', 'model'), 10, 15), (('artifact', 'model'), 48, 53), (('parameter', 'steps'), 89, 94), (('v number', '10'), 77, 79)], 'The final model is constructed by averaging the model parameters of the last 10 training steps'], [[(('parameter', 'dropout'), 40, 47), (('parameter', 'layer'), 78, 83)], 'To regularize during training, we apply dropout with a rate of P_ to each sub-layer, including the content and position embeddings.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 42, 55)], 'We use the Adam optimizer with an initial learning rate at 5e^ which decays exponentially to 5e^ during optimization'], [[(('parameter', 'batch size'), 9, 19), (('artifact', 'model'), 47, 52), (('v number', '1024'), 23, 27)], 'We use a batch size of 1024 rays and train our model on a single RTX2080Ti GPU.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 15, 20), (('parameter', 'epochs'), 43, 49), (('artifact', 'Adam'), 92, 96), (('parameter', 'learning rate'), 157, 170), (('v number', '5000'), 38, 42), (('v number', '128'), 76, 79), (('v number', '0.9'), 114, 117), (('v number', '0.999'), 122, 127)], 'We train every model for a maximum of 5000 epochs with a mini-batch of size 128 and use the Adam optimizer with _=0.9 , _=0.999 in combination with a cosine learning rate scheduler']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 48, 54), (('parameter', 'learning rate'), 77, 90), (('parameter', 'epoch'), 121, 126), (('v number', '16'), 8, 10), (('v number', '240'), 58, 61), (('v number', '150'), 94, 97), (('v number', '180'), 102, 105), (('v number', '210'), 114, 117)], 'For VGG-16+, we increase the number of training epochs to 240, and decay the learning rate at 150-th, 180-th, and 210-th epoch'], [[(('parameter', 'epochs'), 23, 29), (('parameter', 'learning rate'), 58, 71), (('parameter', 'epoch'), 113, 118), (('v number', '240'), 46, 49), (('v number', '150'), 86, 89), (('v number', '180'), 94, 97), (('v number', '210'), 106, 109)], 'The number of training epochs is increased to 240 and the learning rate is decayed at 150-th, 180-th, and 210-th epoch'], [[(('parameter', 'layer'), 45, 50), (('v number', '50'), 7, 9), (('v number', '3'), 72, 73), (('v number', '1'), 88, 89), (('v number', '1'), 103, 104)], 'For RN-50+, we replace the first convolution layer to be of kernel size 3, padding size 1, and strides 1.']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 4, 9), (('parameter', 'learning rate'), 27, 40), (('parameter', 'weight decay'), 52, 64), (('v number', '1'), 44, 45), (('v number', '1'), 68, 69)], 'The AdamW optimizer with a learning rate of 1 and a weight decay of 1 was used for updating the trainable parameters'], [[(('parameter', 'learning rate'), 59, 72), (('parameter', 'epoch'), 167, 172), (('v number', '5'), 46, 47), (('v number', '0.95'), 88, 92)], 'Early stopping was applied with a patience of 5, while the learning rate was decayed by 0.95 when the validation loss was worse than the best validation loss at every epoch'], [[(('parameter', 'epoch'), 12, 17), (('v number', '20'), 29, 31)], 'The maximum epoch was set to 20']] \n",
      "\n",
      "[[[(('artifact', 'model'), 24, 29), (('artifact', 'AdamW'), 47, 52), (('parameter', 'batch size'), 72, 82), (('parameter', 'learning rate'), 104, 117), (('v number', '512'), 86, 89), (('v number', '8'), 93, 94), (('v number', '0.001'), 121, 126)], 'During pretraining, the model is trained by an AdamW optimizer with the batch size of 512 on 8 GPUs and learning rate of 0.001']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 21, 27), (('parameter', 'learning rate'), 45, 58), (('parameter', 'weight decay'), 73, 85), (('v number', '50'), 18, 20), (('v number', '0.001'), 62, 67), (('v number', '1e-05'), 86, 91)], 'We then train for 50 epochs using a starting learning rate of 0.001 with weight decay 1e-05 , optimizing using stochastic gradient descent'], [[(('parameter', 'epochs'), 38, 44), (('parameter', 'learning rate'), 58, 71), (('v number', '10'), 35, 37), (('v number', '10'), 87, 89)], 'When validation loss saturates for 10 epochs, we decrease learning rate by a factor of 10']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 17, 30), (('v number', '2.0'), 88, 91)], 'We use different learning rate and schedule for the encoder and the decoder, as Wav2vec 2.0 paper did, because the encoder is pre-trained']] \n",
      "\n",
      "[[[(('artifact', 'model'), 67, 72), (('artifact', 'model'), 103, 108)], 'We apply an Imagen-like training strategy, i.e., training the base model and then the super-resolution model twice'], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 40, 53), (('v number', '1e-4'), 57, 61)], 'The Adam optimiser is adopted, having a learning rate of 1e-4'], [[(('parameter', 'steps'), 30, 35), (('parameter', 'batch size'), 43, 53), (('parameter', 'epochs'), 72, 78), (('v number', '10'), 8, 10), (('v number', '000'), 11, 14), (('v number', '8'), 57, 58), (('v number', '1'), 82, 83), (('v number', '000'), 84, 87)], 'We give 10,000 linear warm-up steps with a batch size of 8 and training epochs of 1,000'], [[(('parameter', 'K'), 6, 7), (('v number', '2'), 32, 33)], 'MSEI, K = _^ _^ [Ii, j - Ki, j]^2 ,']] \n",
      "\n",
      "[[[(('artifact', 'model'), 16, 21), (('parameter', 'layers'), 29, 35), (('v number', '12'), 26, 28), (('v number', '256'), 79, 82)], 'The pre-trained model has 12 layers, and the hidden dimension of each block is 256'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 33, 39), (('artifact', 'Adam'), 46, 50), (('parameter', 'learning rate'), 74, 87), (('parameter', 'batch size'), 99, 109), (('v number', '100'), 29, 32), (('v number', '2'), 88, 89), (('v number', '0.0001'), 90, 96), (('v number', '128'), 110, 113)], 'The model is pre-trained for 100 epochs using Adam optimizer with initial learning rate 2 0.0001 , batch size 128 and is trained on four P40 GPUs.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '4'), 28, 29)], 'The model is trained on the 4-label classification task on CANARD GECOR'], [[(('artifact', 'model'), 4, 9), (('v number', '4'), 28, 29)], 'The model is trained on the 4-label classification task on the training set of ConvQuestions'], [[(('artifact', 'model'), 4, 9), (('v number', '4'), 28, 29)], 'The model is trained on the 4-label classification task on CANARDGECOR, but labeled instances of CANARD are added via active learning'], [[(('artifact', 'model'), 136, 141), (('v number', '2'), 160, 161)], 'They are respectively identical to baseline, to baseline + all AL, and to baseline + all AL + fine tuning, with the difference that the model is trained on the 2-labels classification task.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 90, 94), (('v number', '45100'), 11, 16), (('v number', '10'), 34, 36), (('v number', '70100'), 49, 54), (('v number', '0.9'), 114, 117), (('v number', '0.999'), 126, 131)], 'We trained 45100 iterations for C-10 and TIN and 70100 iterations for ImaneNet, using the Adam optimizer with _ = 0.9 and _ = 0.999 '], [[(('parameter', 'batch size'), 4, 14), (('v number', '256'), 18, 21), (('v number', '10'), 28, 30), (('v number', '16'), 32, 34), (('v number', '4'), 48, 49)], 'The batch size is 256 for C-10, 16 for TIN, and 4 for ILSVRC2012'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '5100'), 32, 36), (('v number', '1e-4'), 58, 62), (('v number', '5e-5'), 68, 72)], 'The learning rate for the first 5100 iterations is set to 1e-4, and 5e-5 for the rest']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 18, 22), (('parameter', 'learning rate'), 38, 51), (('v number', '7e-4'), 52, 56), (('v number', '0.5'), 76, 79), (('v number', '0.1'), 97, 100), (('v number', '0.5'), 125, 128), (('v number', '0.01'), 154, 158)], 'We train PPO with Adam optimizer with learning rate 7e-4, max gradient norm 0.5, clip parameters 0.1, value loss coefficient 0.5, and entropy coefficient 0.01'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rates'), 42, 56)], 'We use two Adam optimizers with different learning rates'], [[(('parameter', 'learning rate'), 25, 38), (('parameter', 'learning rate'), 80, 93), (('v number', '1e-3'), 42, 46), (('v number', '7e-4'), 97, 101)], 'The VAE optimizer uses a learning rate of 1e-3, and the policy optimizer uses a learning rate of 7e-4']] \n",
      "\n",
      "[[[(('artifact', 'model'), 95, 100), (('v number', '32'), 50, 52), (('v number', '40'), 64, 66)], 'We also trained our network on cross-views, i.e., 32 out of the 40 classes, to secure that the model is used to generalize different actions']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 39, 52), (('v number', '0.001'), 60, 65)], 'We use the adam optimizer with initial learning rate set as 0.001 for training all experiments']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 215, 220), (('artifact', 'model'), 245, 250)], 'For ESBN, Transformer, RelationNet and PrediNet, we follow the same settings as , where all given images including examples and answer candidates are treated as a sequence and passed through a context normalization layer before being fed to the model'], [[(('parameter', 'layer'), 158, 163), (('parameter', 'layer'), 335, 340), (('parameter', 'layer'), 405, 410)], 'For HyperNetwork, we also use the NICE backbone for fair comparisons and maintain the key memories but not the value memories to compute the weights; at each layer of the backbone, the attention weights are computed as the output of an LSTM cell, where the input for LSTM is the concatenation of the input and pseudo-output of current layer, and the hidden states are taken from the LSTM cell of previous layer'], [[(('parameter', 'layers'), 43, 49), (('parameter', 'layer'), 64, 69), (('v number', '4'), 36, 37), (('v number', '2'), 62, 63)], 'For FINE with NICE backbone, we use 4 NICE layers while using 2-layer MLP for FINE with MLP backbone']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 92, 98), (('parameter', 'layers'), 129, 135), (('parameter', 'layer'), 171, 176), (('v number', '128'), 70, 73), (('v number', '8'), 103, 104)], 'When training the general VRP agent, we used and encoding dimension d=128 and three encoder layers and 8 attention heads for all layers except for the final policy output layer'], [[(('parameter', 'layers'), 16, 22), (('parameter', 'dimensional hidden layers'), 34, 59), (('v number', '64'), 31, 33)], 'For feedforward layers we used 64 dimensional hidden layers']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 44, 57), (('parameter', 'batch size'), 74, 84), (('artifact', 'Adam'), 91, 95), (('v number', '3'), 61, 62), (('v number', '0.0001'), 63, 69), (('v number', '5'), 88, 89)], 'These parameters have been the following: a learning rate of 3 0.0001 , a batch size of 5, Adam has been used as the optimizer, and the MSE has been used as the loss function']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 35, 48), (('parameter', 'weight decay'), 66, 78), (('v number', '0.0005'), 52, 58)], 'The Adam optimizer was used with a learning rate of 0.0005 and no weight decay'], [[(('parameter', 'layer'), 31, 36), (('v number', '0.9'), 101, 104)], 'The weights and biases of each layer were initialized with Xavier normal initializer using a gain of 0.9'], [[(('parameter', 'layer'), 65, 70), (('v number', '128'), 25, 28)], 'A convolutional width of 128 was used for each GraphSAGE and GCN layer']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 40, 46), (('v number', '3'), 30, 31), (('v number', '16'), 66, 68), (('v number', '128'), 101, 104)], 'By default, PatchTST contains 3 encoder layers with head number H=16 and dimension of latent space D=128 '], [[(('parameter', 'layers'), 75, 81), (('parameter', 'activation'), 92, 102), (('parameter', 'layer'), 199, 204), (('v number', '2'), 66, 67), (('v number', '128'), 156, 159), (('v number', '256'), 181, 184), (('v number', '128'), 231, 234)], 'The feed forward network in Transformer encoder block consists of 2 linear layers with GELU activation function: one projecting the hidden representation D=128 to a new dimension F=256 , and another layer that project it back to D=128 '], [[(('parameter', 'dropout'), 0, 7), (('v number', '0.2'), 25, 28)], 'Dropout with probability 0.2 is applied in the encoders for all experiments']] \n",
      "\n",
      "[[[(('parameter', 'fold'), 28, 32), (('artifact', 'cross-validation'), 33, 49), (('parameter', 'fold'), 99, 103), (('v number', '5'), 26, 27)], 'For this purpose, we used 5-fold cross-validation and trained one base deep neural network on each fold dataset'], [[(('parameter', 'fold'), 24, 28), (('v number', '90'), 63, 65)], 'We further divided each fold dataset into the training dataset 90% and the validation dataset10%'], [[(('parameter', 'learning rates'), 54, 68), (('parameter', 'learning rate'), 134, 147), (('v number', '0.001'), 151, 156)], 'After experimenting with different loss functions and learning rates, we found that L1 loss performed better than other losses with a learning rate of 0.001'], [[(('artifact', 'Adam'), 8, 12), (('artifact', 'model'), 67, 72)], 'We used Adam optimizer with Xavier initialization for training the model']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 7, 19), (('v number', '0'), 23, 24)], 'We set weight decay as 0 since we observed no over-fitting during experiments'], [[(('parameter', 'learning rate'), 20, 33), (('parameter', 'learning rate'), 98, 111)], 'We find the initial learning rate significantly affects the accuracy, so we extensively tuned the learning rate for each run to report the best accuracy'], [[(('parameter', 'learning rate'), 15, 28), (('parameter', 'epoch'), 63, 68), (('parameter', 'epochs'), 82, 88), (('v number', '1'), 61, 62), (('v number', '5'), 80, 81)], 'We used cosine learning rate decay and performed warm-up for 1 epoch on VWW and 5 epochs on other datasets']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 47, 53), (('v number', '20'), 44, 46)], 'The initial learning rate is set to 5e^ and 20 epochs warmup'], [[(('parameter', 'epochs'), 44, 50), (('v number', '1'), 23, 24), (('v number', '100'), 40, 43)], 'We report the best top-1 accuracy after 100 epochs'], [[(('parameter', 'batch size'), 4, 14), (('v number', '128'), 18, 21)], 'The batch size is 128 per GPU and four A100 GPUs are used in total']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 43, 56), (('v number', '0.001'), 60, 65), (('v number', '0.9'), 90, 93), (('v number', '0.999'), 98, 103)], 'We use the Adam optimizer with the default learning rate of 0.001, and beta parameters of 0.9 and 0.999'], [[(('parameter', 'epochs'), 27, 33), (('parameter', 'epoch'), 54, 59), (('v number', '20'), 24, 26)], 'We train the models for 20 epochs and select the best epoch based on the highest CSI performance on the validation set']] \n",
      "\n",
      "[[[(('parameter', 'T'), 24, 25), (('v number', '4'), 30, 31), (('v number', '3'), 79, 80), (('v number', '3'), 81, 82), (('v number', '2'), 129, 130)], 'The synthesizer network T has 4 convolutional blocks, each block consisting of 3 3 convolutional kernel with the channel size of 2, Leaky-ReLU and AdaIN'], [[(('artifact', 'model'), 12, 17), (('parameter', 'layer'), 83, 88)], 'For the MIM model, we use the encoder part of the generator with a fully connected layer as the feature extractor F from '], [[(('artifact', 'Adam'), 10, 14), (('parameter', 'learning rate'), 60, 73), (('v number', '3'), 74, 75), (('v number', '0.0001'), 76, 82), (('v range', '[0.5,0.999]'), 88, 99)], 'We choose Adam optimizer for all of the models with initial learning rate 3 0.0001 and =[0.5,0.999] '], [[(('parameter', 'epochs'), 19, 25), (('v number', '2'), 30, 31), (('v number', '000'), 32, 35)], 'The total training epochs are 2,000']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 68, 73), (('parameter', 'weight decay'), 82, 94), (('v number', '600'), 64, 67), (('v number', '0.001'), 98, 103)], 'For the ConvNet baseline, we use the SGD optimizer to train for 600 epoch, with a weight decay of 0.001 '], [[(('parameter', 'learning rate'), 19, 32), (('v number', '0.01'), 36, 40), (('v number', '0.98'), 63, 67), (('v number', '0.1'), 89, 92)], 'We set the initial learning rate to 0.01 and use a momentum of 0.98 with a decay rate of 0.1^ ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 38, 43), (('v number', '5'), 106, 107), (('v number', '0'), 122, 123), (('v number', '25'), 138, 140)], 'Unless stated otherwise, we train our model with audio-visual input corresponding to a sequence length of 5 visual frames 0.2s sampled at 25 fps']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 36, 49), (('v number', '3'), 34, 35), (('v number', '12'), 58, 60), (('v number', '9'), 62, 63), (('v number', '9'), 69, 70)], 'The neural network is composed of 3 hidden layers of size 12, 9, and 9']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 65, 69), (('parameter', 'learning rate'), 87, 100), (('v number', '300'), 36, 39), (('v number', '000'), 40, 43), (('v number', '0.0003'), 104, 110), (('v number', '0.9'), 143, 146), (('v number', '0.99'), 151, 155)], 'We trained the defocus renderer for 300,000 iterations using the Adam optimizer with a learning rate of 0.0003 and momentum terms _1 and _2 of 0.9 and 0.99, respectively'], [[(('parameter', 'batch size'), 4, 14), (('v number', '4'), 26, 27)], 'The batch size was set to 4'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'learning rate'), 99, 112), (('v number', '30'), 73, 75)], 'The learning rate was kept constant during training, except for the last 30% iterations, where the learning rate was smoothly ramped down to zero.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 95, 105), (('parameter', 'learning rate'), 115, 128), (('parameter', 'epochs'), 232, 238), (('parameter', 'epochs'), 281, 287), (('parameter', 'learning rate'), 301, 314), (('v number', '0.9'), 90, 93), (('v number', '1028'), 109, 113), (('v number', '0.0001'), 141, 147), (('v number', '0.1'), 219, 222), (('v number', '50'), 229, 231), (('v number', '500'), 277, 280), (('v number', '1e-06'), 320, 325)], 'For the experiments with ConvActuallys of varying depth, we train using SGD with momentum 0.9, batch size of 1028, learning rate starting at 0.0001 and following a “reduce on plateau” schedule with drops by a factor of 0.1 after 50 epochs with no improvement, for a maximum of 500 epochs or until the learning rate hits 1e-06 '], [[(('parameter', 'weight decay'), 7, 19), (('v number', '1e-09'), 23, 28)], 'We use weight decay of 1e-09 i.e., basically no decay'], [[(('parameter', 'batch size'), 93, 103), (('parameter', 'learning rate'), 117, 130), (('parameter', 'epochs'), 176, 182), (('parameter', 'epochs'), 225, 231), (('parameter', 'learning rate'), 245, 258), (('v number', '0.9'), 88, 91), (('v number', '32'), 104, 106), (('v number', '32512'), 131, 136), (('v number', '0.1'), 163, 166), (('v number', '10'), 173, 175), (('v number', '500'), 221, 224), (('v number', '1e-06'), 264, 269)], 'For the experiments training ResNets of varying depth, we train using SGD with momentum 0.9, batch size 32, initieal learning rate 32512 with drops by a factor of 0.1 after 10 epochs with no improvement, for a maximum of 500 epochs or until the learning rate hits 1e-06']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 75, 80), (('parameter', 'activation'), 101, 111)], 'In node-subgraph and subgraph-subgraph contrasts, both GCN models have one layer and use ReLU as the activation function'], [[(('parameter', 'epochs'), 26, 32), (('artifact', 'model'), 36, 41), (('v number', '400'), 22, 25), (('v number', '256'), 55, 58)], 'Besides, we implement 400 epochs of model training and 256 rounds of anomaly score calculation.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 82, 87), (('v number', '1024'), 99, 103), (('v number', '3'), 129, 130)], 'For CLS training, we use a shallow neural network with one hidden fully-connected layer, which has 1024 neurons and an output of 3 classes'], [[(('parameter', 'learning rate'), 36, 49), (('v number', '0.001'), 50, 55)], 'For optimizser, we use Adagrad with learning rate 0.001'], [[(('artifact', 'Adam'), 28, 32), (('parameter', 'learning rate'), 38, 51), (('v number', '0.0001'), 52, 58), (('v number', '0'), 68, 69), (('v number', '0.999'), 76, 81)], 'For comparison, we also run Adam with learning rate 0.0001 and _1 = 0, _2 = 0.999 '], [[(('parameter', 'batch size'), 2, 12), (('v number', '3'), 16, 17)], 'A batch size of 3 is used in this task.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 66, 76), (('parameter', 'learning rate'), 99, 112), (('v number', '128'), 80, 83), (('v number', '0.001'), 116, 121), (('v number', '0.9'), 144, 147)], 'The database was trained using Stochastic Gradient Descent with a batch size of 128 and an initial learning rate of 0.001 and a momentum set to 0.9'], [[(('artifact', 'model'), 4, 9), (('v number', '28'), 29, 31)], 'The model converges in about 28 hours of training on a NVIDIA Titan V GPU with 12GB memory'], [[(('parameter', 'm'), 11, 12), (('v number', '1'), 56, 57)], 'The margin m of equation REF is experimentally fixed to 1']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 120, 126), (('parameter', 'batch size'), 134, 144), (('v number', '200'), 116, 119), (('v number', '64'), 148, 150)], 'In the first set of experiments where we showed the superiority of MetaMax over OpenMax, we trained DenseNet121 for 200 epochs with a batch size of 64 for all three datasets'], [[(('parameter', 'layers'), 59, 65), (('v number', '6'), 27, 28), (('v number', '960'), 29, 32), (('v number', '006'), 33, 36), (('v number', '121'), 55, 58)], 'DenseNet121 has a total of 6,960,006 parameters in its 121 layers'], [[(('artifact', 'Adam'), 33, 37), (('artifact', 'method'), 51, 57)], 'Training was performed using the Adam optimization method'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epoch'), 72, 77)], 'The learning rate decayed by a multiplicative factor at the end of each epoch']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 20, 26), (('parameter', 'batch size'), 34, 44), (('v number', '10'), 17, 19), (('v number', '4'), 48, 49)], 'We trained h for 10 epochs with a batch size of 4'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 42, 55), (('parameter', 'steps'), 118, 123), (('v number', '1e-4'), 59, 63), (('v number', '1e-6'), 76, 80)], 'We used an Adam optimizer with an initial learning rate of 1e-4, decayed to 1e-6 following a cosine schedule over all steps']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 35, 40), (('v number', '5'), 33, 34), (('v number', '256'), 62, 65)], 'The sinusoidal network used is a 5-layer MLP with hidden size 256, following the proposed initialization scheme above'], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 34, 47), (('parameter', 'steps'), 80, 85), (('parameter', 'steps'), 140, 145), (('v number', '3'), 51, 52), (('v number', '0.001'), 53, 58), (('v number', '10'), 73, 75), (('v number', '000'), 76, 79), (('v number', '20'), 133, 135), (('v number', '000'), 136, 139)], 'The Adam optimizer is used with a learning rate of 3 0.001 , trained for 10,000 steps in the short duration training results and for 20,000 steps in the long duration training results.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 19, 24), (('v number', '2'), 17, 18), (('v number', '512'), 54, 57), (('v number', '8'), 62, 63)], 'The encoder is a 2-layer transformer with hidden size 512 and 8 attentions heads as in '], [[(('artifact', 'model'), 12, 17), (('v number', '512'), 26, 29), (('v number', '1024'), 99, 103)], 'In the base model, We use 512-dimensional embeddings for w_ , p_ , and thus the dimension of x_ is 1024']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 13, 26), (('parameter', 'steps'), 77, 82), (('v number', '3e-4'), 30, 34), (('v number', '4000'), 72, 76)], 'We utilize a learning rate of 3e-4 and set the warming-up schedule with 4000 steps for training'], [[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 34, 40), (('v number', '100'), 30, 33)], 'We train our model for around 100 epochs']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 41, 45), (('parameter', 'learning rate'), 63, 76), (('v number', '0.001'), 80, 85)], 'We follow for the pre-training setup; an Adam optimizer with a learning rate of 0.001 is used'], [[(('parameter', 'epochs'), 45, 51), (('parameter', 'batch size'), 59, 69), (('parameter', 'epochs'), 104, 110), (('parameter', 'batch size'), 118, 128), (('v number', '400'), 41, 44), (('v number', '16'), 73, 75), (('v number', '90'), 101, 103), (('v number', '256'), 132, 135)], 'For miniImageNet, models are trained for 400 epochs with a batch size of 16, and for tieredImageNet, 90 epochs with a batch size of 256'], [[(('parameter', 'learning rate'), 68, 81), (('parameter', 'weight decay'), 112, 124), (('v number', '0.01'), 85, 89), (('v number', '0.9'), 103, 106), (('v number', '0.0001'), 128, 134)], 'We also follow for the fine-tuning setting; an SGD optimizer with a learning rate of 0.01, momentum of 0.9, and weight decay of 0.0001 is used'], [[(('artifact', 'model'), 16, 21), (('parameter', 'epochs'), 41, 47), (('parameter', 'batch size'), 55, 65), (('v number', '100'), 37, 40), (('v number', '4'), 69, 70)], 'The pre-trained model is trained for 100 epochs with a batch size of 4.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 20, 30), (('v number', '2'), 42, 43)], 'For all models, the batch size is set for 2 images for each GPUs'], [[(('parameter', 'epochs'), 32, 38), (('v number', '100'), 28, 31)], 'All experiments are run for 100 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 80, 85), (('artifact', 'model'), 105, 110), (('v number', '2'), 41, 42), (('v number', '048'), 43, 46)], 'Following , each point cloud consists of 2,048 points by random sampling on the model surface from every model in ModelNet40']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 69, 74), (('v number', '3'), 67, 68), (('v number', '1024'), 84, 88)], 'The discriminator and the relational networks are implemented as a 3-layer MLP with 1024 units each'], [[(('parameter', 'batch size'), 16, 26), (('v number', '1024'), 30, 34), (('v number', '128'), 64, 67)], \"All tasks use a batch size of 1024 for the AbsAE's training and 128 for the ReL's training\"], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 31, 44), (('v number', '0.0001'), 48, 54), (('v number', '1e-05'), 77, 82)], 'We use the Adam optimizer with learning rate of 0.0001 for HWF and dSprites, 1e-05 for Shapes3D.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 165, 170), (('v number', '1'), 142, 143), (('v number', '60'), 148, 150), (('v number', '0.01'), 172, 176), (('v number', '0.05'), 178, 182), (('v number', '0.1'), 184, 187), (('v number', '0.2'), 189, 192), (('v number', '0.4'), 194, 197), (('v number', '0.6'), 199, 202), (('v number', '15'), 242, 244)], 'To analyze the effect of training data availability on prediction stability, we vary the number of node labels available for training between 1 and 60% of all nodes steps: 0.01, 0.05, 0.1, 0.2, 0.4, 0.6 and use a fixed-size validation set of 15% of the data']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 31, 36), (('v number', '3'), 29, 30)], 'Following MoCo v3 , we use a 3-layer projection head on top of the backbone network for pre-training and discard it when transferring to downstream tasks'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 54, 60), (('parameter', 'learning rate'), 73, 86), (('v number', '5e-4'), 28, 32), (('v number', '10'), 51, 53)], 'The learning rate is set to 5e-4, with a warmup of 10 epochs, and cosine learning rate decay'], [[(('parameter', 'epochs'), 31, 37), (('parameter', 'epochs'), 82, 88), (('v number', '800'), 27, 30), (('v number', '300'), 78, 81)], 'ViT-B16 is pre-trained for 800 epochs in total and ViT-S16 is pre-trained for 300 epochs if not specified']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 29, 42), (('parameter', 'epochs'), 90, 96), (('v number', '0.1'), 46, 49), (('v number', '0.2'), 76, 79), (('v number', '150'), 86, 89)], 'For all networks, an initial learning rate of 0.1 is decayed by a factor of 0.2 every 150 epochs'], [[(('parameter', 'weight decay'), 52, 64), (('v number', '5'), 65, 66), (('v number', '4'), 68, 69)], 'To avoid overfitting, all networks are trained with weight decay 5 -4 .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 69, 75), (('parameter', 'batch size'), 94, 104), (('artifact', 'system'), 172, 178), (('v number', '5'), 67, 68), (('v number', '8'), 108, 109), (('v number', '16'), 134, 136)], 'Leveraging data parallelisation, the models were trained for up to 5 epochs with a per-device batch size of 8 on a node consisting of 16 NVIDIA Tesla K80 GPUs with 16GB of system memory per GPU, running on average for approximately eight hours per training'], [[(('parameter', 'batch size'), 10, 20), (('artifact', 'model'), 54, 59), (('parameter', 'batch size'), 161, 171)], 'The small batch size was conditioned on the fact that model parallelisation was not possible within our infrastructure, leading to out-of-memory OOM if a larger batch size was chosen'], [[(('artifact', 'system'), 130, 136), (('parameter', 'batch size'), 169, 179), (('v number', '8'), 92, 93), (('v number', '32'), 183, 185)], 'All post-deployment performance evaluations were carried out on a single node consisting of 8 NVIDIA Tesla V100 GPUs with 32GB of system memory per GPU and a per-device batch size of 32, taking on average approximately four hours per prediction task']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 73, 79), (('parameter', 'weight decay'), 131, 143), (('v number', '128'), 61, 64), (('v number', '400'), 69, 72), (('v number', '0.9'), 121, 124), (('v number', '0.0001'), 147, 153)], 'Unless otherwise stated, we train the models with batch-size 128 for 400 epochs with an SGD-optimizer, a momentum set to 0.9 and a weight decay of 0.0001 '], [[(('parameter', 'activation'), 70, 80), (('parameter', 'layer'), 116, 121)], 'In accordance to ResNet we use batch normalization followed by a ReLU activation function after every convolutional layer'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'learning rate'), 71, 84), (('parameter', 'learning rate'), 108, 121), (('v number', '0.05'), 36, 40)], 'The initial learning rate is set to 0.05 and we use a cosine-annealing learning rate scheduler to adapt the learning rate during training.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 71, 76), (('parameter', 'batch size'), 84, 94), (('v number', '2560'), 98, 102)], 'We continuously pretrain both large and base versions of BART for 120k steps with a batch size of 2560'], [[(('parameter', 'learning rate'), 9, 22), (('artifact', 'linear decay'), 59, 71), (('v number', '0.02'), 36, 40)], 'We use a learning rate scheduler of 0.02 warm-up ratio and linear decay'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '1e-4'), 28, 32)], 'The learning rate is set to 1e-4'], [[(('parameter', 'Version'), 18, 25), (('parameter', 'Version'), 103, 110), (('v number', '2'), 40, 41), (('v number', '16'), 51, 53), (('v number', '100'), 79, 82), (('v number', '168'), 146, 149)], 'We train the base version of BioBART on 2 DGX with 16 40GB A100 GPUs for about 100 hours and the large version of BioBART on the same devices for 168 hours with the help of the open-resource framework DeepSpeed .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 56, 61), (('parameter', 'm'), 66, 67), (('parameter', 'hidden state'), 117, 129), (('v number', '12'), 68, 70), (('v number', '768'), 133, 136)], 'We follow the U-VB architecture, where each Transformer layer has M=12 attention heads and the dimensionality of the hidden state is 768'], [[(('parameter', 'layers'), 100, 106), (('v number', '1024'), 63, 67), (('v number', '1536'), 69, 73), (('v number', '3072'), 75, 79), (('v number', '1'), 114, 115), (('v number', '2'), 117, 118), (('v number', '3'), 120, 121)], 'The size of the pre-learned visual dictionary C is chosen from 1024, 1536, 3072 ; the number of WFH layers J from 1, 2, 3 ; in Eq']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 36, 42), (('parameter', 'learning rate'), 50, 63), (('parameter', 'batch size'), 77, 87), (('v number', '40'), 33, 35), (('v number', '256'), 91, 94)], 'Pre-training was conducted using 40 epochs with a learning rate of 1e^ and a batch size of 256']] \n",
      "\n",
      "[[[(('artifact', 'method'), 114, 120), (('v number', '0.5'), 28, 31), (('v number', '3'), 45, 46)], \"We use a resizing factor of 0.5 and generate 3 `clicks' from the relevancy maps for the single image segmentation method\"]] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('parameter', 'weight decay'), 40, 52), (('v number', '0.0003'), 29, 35), (('v number', '0.01'), 56, 60)], 'The initial learning rate is 0.0003 and weight decay is 0.01'], [[(('parameter', 'Version'), 20, 27), (('v number', '1024'), 105, 109), (('v number', '1024'), 110, 114)], 'For full-resolution version, the training images are randomly scaling and then cropping to fixed size of 1024 1024 '], [[(('parameter', 'Version'), 27, 34), (('v number', '1024'), 71, 75), (('v number', '512'), 76, 79), (('v number', '1024'), 119, 123), (('v number', '512'), 124, 127)], 'As for the half-resolution version, the training images are resized to 1024 512 and randomly scaling, the crop size is 1024 512 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 22, 27), (('parameter', 'epochs'), 68, 74), (('parameter', 'batch size'), 97, 107), (('parameter', 'learning rate'), 130, 143), (('v number', '0.0001'), 53, 59), (('v number', '200'), 64, 67), (('v number', '128'), 111, 114), (('v number', '0.1'), 144, 147), (('v number', '0.9'), 161, 164)], 'We train the baseline model with _ regularization of 0.0001 for 200 epochs on CIFAR10100, with a batch size of 128 using SGD with learning rate 0.1 and momentum 0.9'], [[(('parameter', 'epochs'), 40, 46), (('parameter', 'epoch'), 66, 71), (('parameter', 'batch size'), 94, 104), (('parameter', 'learning rate'), 125, 138), (('parameter', 'steps'), 197, 202), (('v number', '90'), 37, 39), (('v number', '5'), 64, 65), (('v number', '256'), 108, 111), (('v number', '0.1'), 142, 145), (('v number', '10'), 151, 153), (('v number', '30'), 163, 165), (('v number', '60'), 168, 170), (('v number', '90'), 177, 179)], 'In ImageNet-ILSVRC2012, we train for 90 epochs in addition to a 5-epoch warm-up phase using a batch size of 256 with initial learning rate of 0.1 with 10 decay at 30%, 60%, and 90% of the training steps']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 10, 23), (('v number', '2.5'), 27, 30), (('v number', '0.0001'), 31, 37), (('v number', '0'), 55, 56)], 'We used a learning rate of 2.5 0.0001 which decayed to 0 at the end of training with a cosine schedule'], [[(('artifact', 'model'), 32, 37), (('parameter', 'steps'), 48, 53), (('parameter', 'batch size'), 62, 72), (('v number', '103'), 12, 15), (('v number', '64'), 76, 78)], 'On Wikitext-103, we trained the model with 250K steps using a batch size of 64'], [[(('artifact', 'model'), 36, 41), (('parameter', 'steps'), 88, 93), (('artifact', 'model'), 150, 155)], 'On enwik8 and text8, we trained the model with 100KWe used a smaller number of training steps compared to , since it would take too long to train one model'], [[(('parameter', 'steps'), 0, 5), (('parameter', 'batch size'), 14, 24), (('v number', '40'), 28, 30)], 'steps using a batch size of 40']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 122, 135), (('artifact', 'Adam'), 145, 149), (('v number', '2'), 73, 74), (('v number', '512'), 116, 119), (('v number', '0.002'), 137, 142)], 'For other hyperparameters, we followed the default protocols of StyleGAN-2, including latent space dimensionality D=512 , learning rate =0.002 , Adam optimizer, and exponential moving average of G_ .']] \n",
      "\n",
      "[[[(('artifact', 'method'), 24, 30), (('v number', '3'), 51, 52)], 'First, the augmentation method is used to generate 3 candidate utterances at a time, to allow for the different methods to cover their own hyper-parameters'], [[(('artifact', 'method'), 60, 66), (('v number', '10'), 3, 5)], 'If 10 retries are accumulated for a given data augmentation method, the generation process terminates']] \n",
      "\n",
      "[[[(('parameter', 'K'), 0, 1), (('parameter', 'layers'), 21, 27), (('v number', '4'), 2, 3), (('v number', '32'), 63, 65), (('v number', '16'), 72, 74), (('v number', '16'), 85, 87)], 'k=4 spatial-temporal layers were used for the encoder, and d_c=32 , n_c=16 , and d_e=16 were set for the quantization branch in the decoder'], [[(('parameter', 'T'), 37, 38), (('parameter', 'steps'), 42, 47), (('v number', '12'), 39, 41), (('v number', '5'), 60, 61), (('v number', '12'), 98, 100), (('v number', '12'), 162, 164), (('v number', '12'), 165, 167), (('v number', '12'), 172, 174), (('v number', '1'), 175, 176)], 'Since the baselines predict the next T=12 steps in units of 5 minutes and each step has errors of 12 horizons, the input residuals and predictions have a size of 12 12 and 12 1 , respectively'], [[(('parameter', 'batch size'), 61, 71), (('v number', '256'), 75, 78)], 'Our ResCAL is trained with the mean absolute error MAE and a batch size of 256'], [[(('artifact', 'Adam'), 3, 7), (('parameter', 'learning rate'), 25, 38), (('v number', '0.001'), 42, 47), (('v number', '0.9'), 52, 55), (('v number', '0.999'), 63, 68)], 'An Adam optimizer with a learning rate of 0.001, _ =0.9 and _ =0.999 is also used'], [[(('artifact', 'model'), 102, 107), (('v number', '7'), 88, 89), (('v number', '1'), 90, 91), (('v number', '2'), 92, 93)], 'Each dataset is split into a training set, validation set, and test set with a ratio of 7:1:2 and the model with the best validation score is selected in all experimental evaluations.']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 26, 31), (('artifact', 'model'), 77, 82), (('artifact', 'model'), 157, 162), (('parameter', 'epoch'), 177, 182), (('v number', '5'), 24, 25), (('v number', '1'), 175, 176)], 'More precisely, a local 5-epoch training is performed on the received global model; the local personalization phase again considers the same received global model, and we use 1 epoch in our cases'], [[(('parameter', 'epochs'), 72, 78), (('parameter', 'epoch'), 127, 132), (('v number', '1'), 109, 110)], 'We also investigate the impact of different local personalized training epochs in Appendix REF and find that 1 personalization epoch is a good trade-off point for time complexity and performance.']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 33, 46), (('v number', '7'), 31, 32), (('v number', '200'), 50, 53)], 'Lastly, the DNN is an MLP with 7 hidden layers of 200 neurons each'], [[(('parameter', 'fold'), 63, 67), (('artifact', 'cross-validation'), 68, 84), (('v number', '5'), 61, 62)], 'The hyperparameters of all models were each determined using 5-fold Cross-Validation to maximize AUROCs while ensuring good calibration and using heuristic methods to minimize overfitting; all parameters not mentioned are common defaults'], [[(('parameter', 'fold'), 9, 13), (('v number', '5'), 7, 8)], 'We use 5-fold CV for all models to make computations feasible.']] \n",
      "\n",
      "[[[(('parameter', 'm'), 173, 174), (('v number', '180'), 97, 100), (('v number', '5000'), 133, 137), (('v number', '5000'), 138, 142), (('v number', '0.3'), 169, 172)], 'We leverage image-label pairs from the existing Inria building labeling dataset , which provides 180 labeled RS images, each size of 5000 5000 and spatial resolution of 0.3 m']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 93, 106), (('v number', '1e-4'), 110, 114), (('v number', '1e-3'), 126, 130)], 'We use the exact same hyperparameters as in , following , with one major exception: we use a learning rate of 1e-4 instead of 1e-3, which we found to converge too quickly.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 50, 60), (('parameter', 'epochs'), 69, 75), (('artifact', 'Adam'), 81, 85), (('parameter', 'learning rate'), 105, 118), (('parameter', 'learning rate'), 138, 151), (('parameter', 'epochs'), 212, 218), (('parameter', 'learning rate'), 245, 258), (('v number', '16'), 61, 63), (('v number', '300'), 65, 68), (('v number', '0.001'), 152, 157), (('v number', '10'), 209, 211), (('v number', '1e-06'), 259, 264)], 'Our baseline training configuration is set to use batch size 16, 300 epochs, the Adam optimizer , cosine learning rate decay with initial learning rate 0.001 , weak augmentation by random horizontal flip, and 10 epochs for warm up training from learning rate 1e-06 '], [[(('artifact', 'model'), 23, 28), (('v number', '512'), 43, 46), (('v number', '512'), 47, 50)], 'The input image to the model is resized to 512 512 pixels while the ratio of original image height and width are kept the same']] \n",
      "\n",
      "[[[(('artifact', 'model'), 5, 10), (('v number', '4'), 47, 48), (('v number', '2080'), 61, 65)], 'Each model was trained and evaluated on one of 4 GeForce RTX 2080 GPU of our internal cluster']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 91, 101), (('parameter', 'epochs'), 114, 120), (('parameter', 'learning rate'), 132, 145), (('parameter', 'weight decay'), 168, 180), (('parameter', 'epochs'), 214, 220), (('v number', '416'), 81, 84), (('v number', '416'), 85, 88), (('v number', '16'), 105, 107), (('v number', '1000'), 109, 113), (('v number', '0.01'), 146, 150), (('v number', '0.937'), 161, 166), (('v number', '0.0005'), 181, 187), (('v number', '3'), 212, 213), (('v number', '0.8'), 247, 250)], 'The specific hyper parameters used for training are as follows: image input size 416 416 , batch size of 16, 1000 epochs in length, learning rate 0.01, momentum 0.937, weight decay 0.0005, with a warm-up time of 3 epochs and a warm-up momentum of 0.8']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 9, 14), (('artifact', 'model'), 55, 60)], 'We apply AdamW as the optimization algorithm to update model parameters and to allow for better generalisation'], [[(('parameter', 'batch size'), 27, 37), (('v number', '32'), 41, 43)], 'A data loader class with a batch size of 32 is used, and models are trained until convergence on the validation set.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('parameter', 'epoch'), 53, 58), (('v number', '64'), 74, 76)], 'Our model is trained over this large dataset for one epoch, in batches of 64 pairs, using the InfoNCE loss ']] \n",
      "\n",
      "[[[(('artifact', 'system'), 27, 33), (('parameter', 'T'), 88, 89)], 'The phase portrait of this system representing the true solution trajectories, showing }t} against , is shown in Fig'], [[(('parameter', 'T'), 73, 74), (('v number', '0.1'), 77, 80)], 'An Euler-Net and an RK4-Net are trained on trajectories, spaced apart by t = 0.1 , starting at certain initial conditions shown by the black lines in Fig']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 52, 56), (('v number', '0.5'), 65, 68), (('v number', '0.5'), 71, 74), (('v number', '0.999'), 84, 89)], 'The optimizer of the generator the discriminator is Adam with _1=0.5 = 0.5 and _2 = 0.999 '], [[(('parameter', 'learning rate'), 4, 17), (('v number', '2e-4'), 31, 35), (('v number', '1e-5'), 53, 57)], 'The learning rate is initially 2e-4 and decreases to 1e-5 by cosine annealing decay'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 29, 35), (('parameter', 'batch size'), 41, 51), (('v number', '50'), 26, 28), (('v number', '1'), 52, 53)], 'The model are trained for 50 epochs with batch size 1'], [[(('parameter', 'layer'), 34, 39), (('v number', '16'), 58, 60)], 'We extract features from relu_4_1 layer of pretrained VGG-16 to calculate the perceptual loss']] \n",
      "\n",
      "[[[(('artifact', 'model'), 20, 25), (('parameter', 'batch size'), 35, 45), (('parameter', 'learning rate'), 50, 63), (('v number', '16'), 46, 48), (('v number', '1e-4'), 64, 68), (('v number', '38'), 73, 75), (('v number', '8'), 112, 113), (('v number', '5'), 156, 157)], 'We train the entire model with the batch size 16, learning rate 1e-4 for 38.4k iterations on a single node with 8 NVIDIA V100 32GB GPUs, which takes around 5 hours'], [[(('artifact', 'AdamW'), 15, 20), (('parameter', 'weight decay'), 85, 97), (('v number', '0.9'), 51, 54), (('v number', '0.999'), 56, 61), (('v number', '0.01'), 63, 67)], 'We utilize the AdamW optimizer with _1 , _2 , wd = 0.9, 0.999, 0.01, where wd is the weight decay'], [[(('parameter', 'learning rate'), 11, 24), (('v number', '30'), 67, 69)], 'The linear learning rate warm-up strategy is applied for the first 30% iterations'], [[(('parameter', 'learning rate'), 21, 34), (('parameter', 'learning rate'), 63, 76)], 'The cosine annealing learning rate strategy is adopted for the learning rate decay']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 27, 40), (('parameter', 'epochs'), 127, 133), (('v number', '0.0001'), 44, 50), (('v number', '10'), 124, 126)], 'We use Adam optimizer with learning rate of 0.0001, and adopt early stopping if the validation loss does not decrease after 10 epochs'], [[(('artifact', 'model'), 211, 216), (('v range', '[0.01, 100]'), 101, 112), (('v range', '[0.0001, 10]'), 119, 131)], 'For the hyper-parameters v, r of the kernel function in eq:gp-ours, we try a range of values where v [0.01, 100] and r [0.0001, 10] , and do grid search cross validation on the validation set to select the best model']] \n",
      "\n",
      "[[[(('artifact', 'model'), 9, 14), (('v number', '6'), 31, 32), (('v number', '708'), 33, 36), (('v number', '450'), 37, 40)], 'The cnn_ model uses a total of 6,708,450 trainable parameters'], [[(('parameter', 'epochs'), 47, 53), (('parameter', 'batch size'), 75, 85), (('artifact', 'model'), 115, 120), (('artifact', 'Adam'), 131, 135), (('v number', '20'), 35, 37), (('v number', '128'), 89, 92)], 'We have conducted experiments with 20 training epochs per iteration with a batch size of 128, and have trained the model using the Adam optimizer'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.001'), 47, 52)], 'The learning rate to train the cnn_ is kept at 0.001'], [[(('parameter', 'activation'), 4, 14), (('parameter', 'layers'), 47, 53), (('parameter', 'layer'), 93, 98)], 'The activation function considered for all the layers is ReLu with the Softmax at the output layer'], [[(('parameter', 'epochs'), 118, 124), (('v number', '5'), 116, 117)], 'An early stopping mechanism is used to avoid overfitting and accelerate training, if accuracy does not increases in 5 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 38, 48), (('artifact', 'Adam'), 75, 79), (('parameter', 'learning rate'), 97, 110), (('v number', '8'), 15, 16), (('v number', '64'), 56, 58), (('v number', '5e6'), 114, 117)], 'For PR, we use 8 Nvidia-A100 GPU with batch size set at 64 per GPU and the Adam optimizer with a learning rate of 5e6 for training'], [[(('parameter', 'epochs'), 49, 55), (('v number', '512'), 27, 30), (('v number', '3'), 59, 60)], 'The max sequence length is 512 and the number of epochs is 3'], [[(('parameter', 'epoch'), 54, 59), (('v number', '10'), 63, 65)], 'In the multi-task training, we increase the number of epoch to 10'], [[(('parameter', 'epochs'), 75, 81), (('v number', '20'), 72, 74)], 'For EASI, we also use the same training configuration as in PR, but use 20 epochs in the training']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 47, 57), (('parameter', 'steps'), 148, 153), (('parameter', 'batch size'), 170, 180), (('v number', '4'), 12, 13), (('v number', '8'), 61, 62), (('v number', '128'), 107, 110), (('v number', '64'), 123, 125), (('v number', '64'), 184, 186), (('v number', '8'), 187, 188), (('v number', '4'), 189, 190), (('v number', '2048'), 191, 195)], 'We train on 4 Nvidea V100 GPUs, with a per-gpu batch size of 8 sentences with a maximum sequence length of 128 tokens, and 64 gradient accumulation steps, for an overall batch size of 64 8 4=2048 sentences'], [[(('parameter', 'learning rate'), 64, 77), (('artifact', 'Adam'), 95, 99), (('v number', '15'), 54, 56), (('v number', '1e-4'), 81, 85), (('v number', '16'), 121, 123)], 'We use a masked language modeling mask probability of 15% and a learning rate of 1e-4 with the Adam optimizer , and used 16-bit floating point operations']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('v number', '2'), 35, 36), (('v number', '1e-05'), 37, 42)], 'The learning rate for generator is 2 1e-05 '], [[(('parameter', 'learning rate'), 4, 17), (('v number', '2'), 39, 40), (('v number', '0.0001'), 41, 47)], 'The learning rate for discriminator is 2 0.0001 ']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 18, 31), (('parameter', 'epochs'), 83, 89), (('parameter', 'weight decay'), 95, 107), (('v number', '0.001'), 42, 47), (('v number', '0.1'), 69, 72), (('v number', '0.00001'), 125, 132), (('v number', '0.9'), 137, 140)], 'REF , the initial learning rate is set to 0.001 and is multiplied by 0.1 every ten epochs, and weight decay and momentum are 0.00001 and 0.9, respectively'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 28, 34), (('v number', '20'), 25, 27)], 'The model is trained for 20 epochs'], [[(('parameter', 'layer'), 90, 95), (('v number', '25'), 51, 53), (('v number', '088'), 54, 57), (('v number', '512'), 104, 107)], 'Feature maps are reshaped to form a vector of size 25,088 and passed to a fully-connected layer of size 512 to construct the final representation'], [[(('parameter', 'hidden layers'), 42, 55), (('parameter', 'activation'), 121, 131), (('v number', '256'), 64, 67)], 'The view discriminator is an MLP with two hidden layers of size 256, each followed by batch normalization and leaky relu activation function'], [[(('parameter', 'hidden layers'), 20, 33), (('parameter', 'activation'), 75, 85)], 'At the top of these hidden layers, there is a single neuron with a sigmoid activation function'], [[(('artifact', 'model'), 4, 9), (('parameter', 'batch size'), 71, 81), (('v number', '32'), 85, 87)], 'The model is trained using Stochastic Gradient Descent SGD with a mini-batch size of 32 on an NVIDIA TITAN X GPU using Pytorch .']] \n",
      "\n",
      "[[[(('parameter', 'activation'), 33, 43), (('v number', '3'), 68, 69)], 'Further, we choose the number of activation map channels to be n_ = 3 and discuss the reasons for that in Section ']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 43, 49), (('parameter', 'layers'), 68, 74), (('v number', '5'), 27, 28), (('v number', '5'), 54, 55), (('v number', '4'), 122, 123), (('v number', '4'), 125, 126), (('v number', '64'), 128, 130)], 'The encoder is composed by 5 convolutional layers and 5 max-pooling layers for downsampling, that return a tensor of size 4, 4, 64 .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 73, 78), (('parameter', 'layer'), 150, 155), (('v number', '18'), 18, 20), (('v number', '512'), 61, 64), (('v number', '1024'), 174, 178), (('v number', '64'), 183, 185)], 'We use the ResNet-18 as the encoder dimension of encoding is 512., a two layer MLP with ReLU and BN appended as the projector dimension of the hidden layer and embedding are 1024 and 64 respectively'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 41, 47), (('parameter', 'batch size'), 55, 65), (('artifact', 'Adam'), 80, 84), (('parameter', 'learning rate'), 102, 115), (('parameter', 'learning rate'), 133, 146), (('parameter', 'learning rate'), 194, 207), (('parameter', 'epochs'), 235, 241), (('v number', '10'), 30, 32), (('v number', '200'), 37, 40), (('v number', '256'), 69, 72), (('v number', '3'), 119, 120), (('v number', '0.001'), 121, 126), (('v number', '500'), 169, 172), (('v number', '0.2'), 190, 193), (('v number', '50'), 225, 227), (('v number', '25'), 232, 234)], 'The model is trained on CIFAR-10 for 200 epochs with a batch size of 256, using Adam optimizer with a learning rate of 3 0.001 , and learning rate warm-up for the first 500 iterations and a 0.2 learning rate drop at the last 50 and 25 epochs'], [[(('parameter', 'weight decay'), 4, 16), (('v number', '1e-06'), 27, 32)], 'The weight decay is set as 1e-06 ']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 95, 100), (('v number', '128'), 68, 71), (('v number', '128'), 72, 75), (('v number', '128'), 76, 79), (('v number', '30'), 87, 89)], 'Vortex is a simulation of vortex structures with spatial resolution 128 128 128 across 30 time steps'], [[(('parameter', 'steps'), 49, 54), (('v number', '1000'), 20, 24), (('v number', '5'), 42, 43)], 'We randomly sampled 1000 data blocks from 5 time steps as the training data'], [[(('parameter', 'steps'), 69, 74), (('v number', '512'), 45, 48), (('v number', '512'), 49, 52), (('v number', '96'), 53, 55), (('v number', '48'), 61, 63)], 'In our experiment, the resolution of data is 512 512 96 with 48 time steps'], [[(('parameter', 'steps'), 51, 56), (('v number', '5500'), 22, 26), (('v number', '5'), 44, 45)], 'Training data contain 5500 data blocks from 5 time steps.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 27, 40), (('artifact', 'linear decay'), 69, 81), (('parameter', 'learning rate'), 82, 95), (('v number', '0.0005'), 44, 50), (('v number', '8'), 109, 110)], 'For all models, we fix the learning rate at 0.0005 with a polynomial linear decay learning rate schedule and 8% warmup, which we found to be optimal for most settings after a large grid search'], [[(('parameter', 'batch size'), 30, 40), (('parameter', 'steps'), 94, 99), (('v number', '16'), 44, 46), (('v number', '32'), 91, 93)], 'For all experiments, we use a batch size of 16 for each GPU, with gradient accumulation of 32 steps, and train with fp16']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 29, 39), (('parameter', 'learning rate'), 51, 64), (('parameter', 'steps'), 91, 96), (('parameter', 'steps'), 120, 125), (('v number', '32'), 43, 45), (('v number', '7.0'), 68, 71), (('v number', '1e-06'), 72, 77), (('v number', '100'), 100, 103), (('v number', '1'), 129, 130), (('v number', '000'), 132, 135), (('v number', '50'), 167, 169), (('v number', '000'), 171, 174)], 'To train Blender, we set the batch size to 32, the learning rate to 7.0 1e-06 , the warmup steps to 100, the evaluation steps to 1,!000 , and the number of updates to 50,!000 ']] \n",
      "\n",
      "[[[(('artifact', 'L'), 41, 42), (('v number', '600'), 22, 25), (('v number', '3000'), 44, 48), (('v number', '1445'), 72, 76)], 'In summary, there are 600 images for TUD-L, 3000 images for LINEMOD and 1445 images for Occluded-LINEMOD.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 35, 40), (('parameter', 'K'), 69, 70), (('parameter', 'K'), 134, 135), (('parameter', 'K'), 144, 145), (('v number', '1'), 72, 73), (('v number', '1'), 147, 148)], 'RAMP using TD3 n_} Number training steps _} Training environments ^, k= 1,| _}| Initialize critic, actor, and replay buffer Add ^ to ^k for all k= 1,| _}| i <n_}']] \n",
      "\n",
      "[[[(('artifact', 'model'), 31, 36), (('parameter', 'Version'), 48, 55)], 'We first convert the BART-base model to its LSG version by replacing the full attention in the encoder part and adding global tokens'], [[(('artifact', 'model'), 4, 9), (('v number', '4'), 32, 33), (('v number', '096'), 34, 37)], 'The model is then fine-tuned on 4,096-length inputs and evaluated'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epoch'), 45, 50)], 'The model is then fine-tuned during a single epoch if necessary using the same training parameters'], [[(('artifact', 'model'), 22, 27), (('v number', '4'), 57, 58), (('v number', '096'), 59, 62)], 'First we evaluate the model with pure extrapolation from 4,096-length no additional training'], [[(('artifact', 'model'), 85, 90), (('v number', '64'), 42, 44)], 'In the last setup, we extrapolate, we add 64 global tokens and we fine-tune the full model']] \n",
      "\n",
      "[[[(('artifact', 'model'), 20, 25), (('parameter', 'epochs'), 138, 144), (('parameter', 'epoch'), 181, 186), (('artifact', 'model'), 239, 244), (('parameter', 'T'), 272, 273), (('parameter', 'epochs'), 301, 307), (('v number', '3000'), 133, 137), (('v number', '500'), 187, 190), (('v number', '50'), 298, 300)], \"The training of the model was performed in a self-supervised fashion, using only the normal images of the dataset, over a maximum of 3000 epochs with an early stop introduced after epoch 500 which would trigger when the performance of the model on the validation set didn't improve in the previous 50 epochs\"]] \n",
      "\n",
      "[[[(('parameter', 'layer'), 84, 89), (('v number', '6'), 23, 24), (('v number', '6'), 63, 64), (('v number', '2'), 103, 104)], 'The resulting encoding 6 dimensions is passed through , with a 6 dimensional hidden layer, obtaining a 2 dimensional encoding S_x, S_y in the embedding space S that is used for training and subsequent experiments']] \n",
      "\n",
      "[[[(('artifact', 'model'), 21, 26), (('parameter', 'epochs'), 34, 40), (('parameter', 'epochs'), 82, 88), (('v number', '12'), 31, 33)], 'We trained the NSCIL model for 12 epochs in total, as we found that training more epochs for NSCIL resulted in performance degradation'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 119, 125), (('v number', '0.001'), 29, 34), (('v number', '0.0001'), 58, 64), (('v number', '2'), 103, 104), (('v number', '4'), 111, 112), (('v number', '8'), 117, 118)], 'The initial learning rate is 0.001 for the first task and 0.0001 for all other tasks and is divided by 2 after 4 and 8 epochs for the EASY evaluation'], [[(('parameter', 'layers'), 24, 30), (('parameter', 'learning rate'), 36, 49), (('v number', '5'), 62, 63), (('v number', '1e-05'), 64, 69)], 'For batch normalization layers, the learning rate starts from 5 1e-05 ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('parameter', 'learning rate'), 59, 72), (('artifact', 'Adam'), 130, 134), (('artifact', 'model'), 161, 166), (('parameter', 'weight decay'), 174, 186), (('v number', '16'), 18, 20), (('v number', '224'), 46, 49), (('v number', '224'), 50, 53), (('v number', '1e-4'), 91, 95), (('v number', '4e-4'), 120, 124), (('v number', '1e-6'), 190, 194)], 'The batch size is 16, the input image size is 224 224, the learning rate of the encoder is 1e-4, that of the decoder is 4e-4, the Adam optimizer is used in this model with a weight decay of 1e-6'], [[(('artifact', 'model'), 17, 22), (('parameter', 'K'), 28, 29), (('parameter', 'fold'), 31, 35), (('parameter', 'K'), 58, 59), (('v number', '4'), 60, 61)], 'In addition, the model uses k -fold cross validation with k=4 ']] \n",
      "\n",
      "[[[(('artifact', 'method'), 107, 113), (('v number', '16'), 68, 70)], 'We apply the vertical flip and the pad-and-crop augmentations and a 16 pixel cutout after any augmentation method']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 107, 117), (('parameter', 'epochs'), 130, 136), (('v number', '448'), 79, 82), (('v number', '448'), 83, 86), (('v number', '3'), 87, 88), (('v number', '6'), 121, 122), (('v number', '80'), 127, 129)], 'Training for both foggy and low-lighting setting is done by resizing images to 448 448 3 pixels and with a batch size of 6 for 80 epochs'], [[(('parameter', 'learning rate'), 16, 29), (('parameter', 'learning rates'), 45, 59), (('parameter', 'weight decay'), 121, 133), (('v number', '1'), 73, 74), (('v number', '1e-06'), 75, 80), (('v number', '1'), 84, 85), (('v number', '0.0001'), 86, 92), (('v number', '5'), 137, 138), (('v number', '0.0001'), 139, 145)], 'We use a cosine learning rate scheduler with learning rates ranging from 1 1e-06 to 1 0.0001 and an SGD optimizer with a weight decay of 5 0.0001 .']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 126, 132), (('v number', '224'), 40, 43), (('v number', '224'), 44, 47), (('v number', '1'), 124, 125), (('v number', '56'), 165, 167), (('v number', '56'), 168, 170)], 'For example, ImageNet classification at 224×224 is downsampled to a quarter of the original size initially, therefore Level 1 layers take feature maps of resolution 56×56 as input'], [[(('parameter', 'layers'), 50, 56), (('parameter', 'layers'), 93, 99), (('parameter', 'layers'), 108, 114)], 'Note that we only change dilation values for DiNA layers, since we found that fine-tuning NA layers to DiNA layers may result in a slight decrease in initial performance see subsec:miscexps, tab:testtimedilationchange.']] \n",
      "\n",
      "[[[(('artifact', 'method'), 4, 10), (('v number', '21'), 47, 49), (('v number', '2'), 76, 77)], 'The method described above gives us a total of 21.6M training examples, and 2.4M testing examples.']] \n",
      "\n",
      "[[[(('artifact', 'AdamW'), 24, 29), (('parameter', 'learning rate'), 53, 66), (('parameter', 'batch size'), 105, 115), (('v number', '0.001'), 70, 75), (('v number', '128'), 119, 122)], 'Training was done using AdamW optimizer with initial learning rate of 0.001 with cosine annealing with a batch size of 128'], [[(('parameter', 'epochs'), 34, 40), (('parameter', 'epochs'), 87, 93), (('v number', '100'), 30, 33), (('v number', '300'), 83, 86)], 'SeRP-PointNet was trained for 100 epochs, whereas SeRP-Transformer was trained for 300 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 28, 33), (('v number', '12'), 39, 41), (('v number', '768'), 76, 79)], 'We investigate the ViT-base model with 12 blocks and a channel dimension of 768'], [[(('parameter', 'epochs'), 67, 73), (('parameter', 'learning rate'), 86, 99), (('v number', '300'), 63, 66), (('v number', '1.5'), 114, 117), (('v number', '0.0001'), 118, 124)], 'Unless otherwise specified, the pre-training procedure elapses 300 epochs with a base learning rate starting with 1.5 0.0001 and decays following the cosine annealing schedule']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 55, 68), (('v number', '0.01'), 79, 83), (('v number', '0.05'), 85, 89), (('v number', '0.01'), 95, 99), (('v number', '0.98'), 138, 142), (('v number', '10'), 149, 151)], 'For FEMNIST, OpenImage, and Google Speech, the initial learning rate is set to 0.01, 0.05, and 0.01, respectively, with a decay factor of 0.98 every 10 rounds']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 45, 49), (('parameter', 'learning rate'), 91, 104), (('v number', '0.9'), 69, 72), (('v number', '0.999'), 78, 83), (('v number', '1'), 108, 109), (('v number', '0.0001'), 110, 116)], 'All experiments conducted in this paper used Adam optimizer with _1 =0.9, _2 =0.999, and a learning rate of 1 0.0001 '], [[(('artifact', 'model'), 43, 48), (('parameter', 'epochs'), 98, 104), (('v number', '64'), 74, 76), (('v number', '500'), 94, 97)], 'In each round of active learning, the main model is trained on a batch of 64 examples and for 500 epochs over the training set'], [[(('artifact', 'model'), 45, 50), (('v number', '0.999'), 88, 93)], 'The exponentially moving average of the main model is computed using a decay parameter =0.999']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 67, 73), (('v number', '64'), 24, 26)], 'Individual networks are 64 cells GRU enclosed with fully connected layers see Fig REF ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '0'), 111, 112)], 'The model file is first read from disk to a CPU memory buffer [baseline=char.base] shape=circle,fill,inner sep=0.5pt] char white1;'], [[(('parameter', 'layers'), 69, 75), (('v number', '0'), 150, 151)], 'After the copy operation, GPU kernels can access and execute the DNN layers in the GPU memory buffer [baseline=char.base] shape=circle,fill,inner sep=0.5pt] char white3;']] \n",
      "\n",
      "[[[(('parameter', 'K'), 133, 134), (('v number', '2'), 135, 136), (('v number', '100'), 139, 142), (('v number', '40'), 145, 147), (('v number', '2'), 148, 149), (('v number', '29'), 173, 175), (('v number', '32'), 176, 178), (('v number', '4'), 179, 180)], 'In the following experiments, we choose the same network architectures as AugMix , including All Convolutional Network , DenseNet-BC k=2,d=100 , 40-2 WideResNet and ResNeXt-29 32 4 '], [[(('parameter', 'learning rate'), 51, 64), (('v number', '0.1'), 68, 71)], 'We use stochastic gradient descent with an initial learning rate of 0.1 and ReduceOnPlateau scheduler'], [[(('parameter', 'epochs'), 36, 42), (('v number', '150'), 32, 35)], 'We train all architectures over 150 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 161, 167), (('v number', '10'), 40, 42)], 'Considering that the anomalies in CIFAR-10 are semantically different objects not structural damages or texture perturbations in MVTec-AD , the features in deep layers containing more semantic information must be helpful'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 30, 36), (('parameter', 'batch size'), 75, 85), (('artifact', 'AdamW'), 93, 98), (('parameter', 'weight decay'), 114, 126), (('v number', '1000'), 25, 29), (('v number', '8'), 40, 41), (('v number', '128'), 86, 89), (('v number', '1'), 127, 128), (('v number', '0.0001'), 129, 135)], 'Our model is trained for 1000 epochs on 8 GPUs NVIDIA Tesla V100 16GB with batch size 128 by AdamW optimizer with weight decay 1 0.0001 '], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 70, 76), (('v number', '1'), 21, 22), (('v number', '0.0001'), 23, 29), (('v number', '0.1'), 56, 59), (('v number', '800'), 66, 69)], 'The learning rate is 1 0.0001 initially, and dropped by 0.1 after 800 epochs'], [[(('parameter', 'layer'), 4, 9), (('v number', '4'), 50, 51)], 'The layer numbers of both encoder and decoder are 4']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 77, 90), (('parameter', 'batch size'), 116, 126), (('v number', '50'), 12, 14), (('v number', '0.001'), 91, 96), (('v number', '0.0001'), 97, 103), (('v number', '0.00001'), 104, 111), (('v number', '8'), 127, 128), (('v number', '16'), 129, 131)], 'With ResNet-50, two runs with different random seeds for each combination of learning rate 0.001,0.0001,0.00001 and batch size 8,16 are used for hyperparameter tuning']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 23, 33), (('parameter', 'learning rate'), 87, 100), (('v number', '416'), 37, 40), (('v number', '128'), 55, 58), (('v number', '1e-4'), 104, 108)], 'Optimization: We use a batch size of 416 base-level or 128 large-level with an initial learning rate of 1e-4'], [[(('artifact', 'linear warmup'), 13, 26), (('parameter', 'steps'), 55, 60)], 'We perform a linear warmup schedule with the first 10K steps'], [[(('artifact', 'Adam'), 7, 11), (('parameter', 'weight decay'), 17, 29), (('v number', '0.1'), 37, 40), (('v number', '0.9'), 68, 71), (('v number', '0.999'), 73, 78), (('v number', '1e-6'), 100, 104)], 'We use Adam with weight decay rate = 0.1 optimizer with beta values 0.9, 0.999 and an epsilon value 1e-6']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 86, 99), (('parameter', 'steps'), 135, 140), (('parameter', 'batch size'), 177, 187), (('v number', '0.9'), 36, 39), (('v number', '0.98'), 47, 51), (('v number', '1e-09'), 58, 63)], 'We use the Adam optimizer with _1 = 0.9 , _2 = 0.98 and = 1e-09 ; and ii increase the learning rate linearly for the first 4K training steps and decrease it thereafter; iii use batch size of 32K source tokens and 32K target tokens']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 70, 76), (('v number', '6'), 46, 47), (('v number', '6'), 60, 61)], 'We employed Transformer big architecture with 6 encoder and 6 decoder layers']] \n",
      "\n",
      "[[[(('artifact', 'model'), 23, 28), (('artifact', 'L'), 41, 42), (('parameter', 'layers'), 59, 65), (('parameter', 'layers'), 134, 140), (('v number', '12'), 44, 46), (('v number', '6'), 132, 133)], 'Specifically, the Base model consists of L =12 Transformer layers where both the Speech Transformer and the Shared Transformer have 6 layers'], [[(('artifact', 'model'), 10, 15), (('parameter', 'layers'), 50, 56)], 'The Large model doubles the number of Transformer layers'], [[(('parameter', 'layer'), 8, 13), (('parameter', 'layer'), 53, 58), (('parameter', 'layer'), 80, 85), (('parameter', 'steps'), 185, 190), (('v number', '1'), 35, 36), (('v number', '32'), 168, 170)], 'The CTC layer consists of a single 1-D convolutional layer followed by a linear layer, which outputs the probabilities of text characters All models are pre-trained on 32 GPUs for 400K steps'], [[(('parameter', 'batch size'), 4, 14), (('artifact', 'model'), 28, 33), (('artifact', 'model'), 121, 126), (('v number', '4375'), 37, 41), (('v number', '2800'), 140, 144)], 'The batch size for the Base model is 4375 tokens after downup-sampling for both speech and text input, and for the Large model it is set to 2800']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 4, 11), (('parameter', 'batch size'), 49, 59), (('artifact', 'AdamW'), 74, 79), (('v number', '0.2'), 27, 30), (('v number', '32'), 63, 65)], 'The dropout rate is set to 0.2 , we train with a batch size of 32, we use ADAMW , , '], [[(('parameter', 'weight decay'), 25, 37), (('parameter', 'learning rate'), 83, 96), (('v number', '0.01'), 41, 45), (('v number', '0.06'), 68, 72), (('v number', '1e-05'), 100, 105)], 'Additionally, we set the weight decay to 0.01 , the warmup ratio to 0.06 , and the learning rate to 1e-05 ']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 7, 12), (('v number', '100'), 50, 53)], 'Hidden layer size of the policy network is set as 100'], [[(('artifact', 'Adam'), 0, 4), (('artifact', 'method'), 18, 24), (('parameter', 'learning rate'), 38, 51), (('v number', '1e-3'), 55, 59)], 'Adam optimization method with initial learning rate of 1e-3 is used for mini-batch training'], [[(('parameter', 'dropout'), 0, 7), (('artifact', 'model'), 62, 67), (('v number', '0.5'), 16, 19)], 'Dropout rate of 0.5 is applied during training to prevent the model from over-fitting.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 8, 13), (('parameter', 'learning rate'), 48, 61), (('parameter', 'layers'), 77, 83), (('v number', '10'), 32, 34)], 'The new layer is optimized with 10 times larger learning rate than the other layers'], [[(('parameter', 'learning rate'), 16, 29), (('v number', '0.0001'), 33, 39), (('v number', '200'), 76, 79), (('v number', '2011'), 80, 84), (('v number', '1e-05'), 126, 131)], 'We fix the base learning rate to 0.0001 for all datasets except for the CUB-200-2011 dataset, for which we use a smaller rate 1e-05 as it has fewer images and is more likely to meet the overfitting problem'], [[(('parameter', 'batch size'), 53, 63), (('v number', '128'), 44, 47)], 'We use SGD with 20k training iterations and 128 mini-batch size'], [[(('artifact', 'method'), 16, 22), (('artifact', 'Triplet Loss'), 84, 96)], 'Notice that our method incurs negligible computational cost compared to traditional triplet loss']] \n",
      "\n",
      "[[[(('artifact', 'cross-validation'), 106, 122), (('parameter', 'learning rate'), 173, 186), (('parameter', 'dropout'), 195, 202), (('parameter', 'layers'), 244, 250), (('parameter', 'K'), 251, 252), (('v number', '300'), 168, 171), (('v number', '0.0001'), 187, 193), (('v number', '0.5'), 215, 218), (('v number', '2'), 253, 254)], 'We set the network parameters based on several experiments performed on the development set of one of the cross-validation folds see Section REF .We use embedding size 300, learning rate 0.0001, dropout probability 0.5, and two fully connected layers k=2 .']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 43, 48), (('parameter', 'layer'), 84, 89), (('v number', '128'), 54, 57), (('v number', '512'), 61, 64), (('v number', '2'), 94, 95), (('v number', '558'), 96, 99)], 'We increase the dimension of the embedding layer from 128 to 512 and adjust Softmax layer for 2,558 identities'], [[(('artifact', 'Adam'), 20, 24), (('parameter', 'batch size'), 37, 47), (('v number', '1024'), 51, 55)], 'SGD is optimized by Adam solver with batch size of 1024 on Nvidia Titan X GPU and we use MatConvNet library with a number of modification'], [[(('parameter', 'weight decay'), 7, 19), (('v number', '0.0005'), 23, 29)], 'We set weight decay to 0.0005 and use batch normalization to avoid overfitting'], [[(('parameter', 'learning rate'), 27, 40), (('parameter', 'epochs'), 89, 95), (('parameter', 'epoch'), 116, 121), (('v number', '0.001'), 44, 49), (('v number', '10'), 65, 67)], 'Training is started with a learning rate of 0.001 and divided by 10 at the 15th and 25th epochs and stopped at 30th epoch.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 158, 163), (('v number', '512'), 53, 56), (('v number', '8'), 94, 95), (('v number', '1'), 121, 122)], 'We use a similar setup to the Toy Copy task, but use 512 RNN and embedding units, train using 8 distributed workers with 1 GPU each, and train for at most 1M steps']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 42, 55), (('parameter', 'batch size'), 87, 97), (('v number', '0.001'), 56, 61), (('v number', '0.9'), 77, 80), (('v number', '32'), 98, 100)], 'We use the Adam optimizer with an initial learning rate 0.001, a momentum of 0.9 and a batch size 32'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 75, 81), (('v number', '1.0'), 107, 110)], 'The learning rate lr is iteratively reduced based on the current number of epochs, according to: lr_ = lr_ 1.0-}}']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 35, 40), (('parameter', 'layer'), 86, 91), (('v number', '3'), 33, 34), (('v number', '256'), 61, 64)], 'Both the encoder and decoder are 3-layer deep LSTM-RNNs with 256 hidden units in each layer']] \n",
      "\n",
      "[[[(('parameter', 'hidden state'), 50, 62), (('v number', '300'), 34, 37), (('v number', '512'), 72, 75)], 'We set the word embedding size to 300 and all GRU hidden state sizes to 512'], [[(('parameter', 'dropout'), 7, 14), (('v number', '0.5'), 36, 39)], 'We use dropout with probability p = 0.5 .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 0, 6), (('v number', '3'), 29, 30), (('v number', '50'), 31, 33)], 'Epochs to train: ranges from 3-50 have been reported, but this choice often depends on the constraints imposed by the size of the training set, the computational complexity resulting from other parameter choices, and available resources.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 40, 45), (('parameter', 'dropout'), 98, 105), (('v number', '512'), 93, 96), (('v number', '0.1'), 114, 117), (('v number', '8'), 141, 142)], 'For the Transformer, following the base model in , we set the dimension of word embedding as 512, dropout rate as 0.1 and the head number as 8'], [[(('parameter', 'layers'), 47, 53), (('v number', '6'), 45, 46)], 'The encoder and decoder both have a stack of 6 layers']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 29, 35), (('v number', '5'), 27, 28), (('v number', '128'), 101, 104)], 'The models are trained for 5 epochs with the AdaGrad algorithm , with batch sizes set to k_1=k_2=k_3=128 for faster convergence.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 18, 24), (('parameter', 'batch size'), 42, 52), (('parameter', 'dropout'), 73, 80), (('parameter', 'layers'), 96, 102), (('v number', '2'), 16, 17), (('v number', '64'), 56, 58), (('v number', '0.2'), 84, 87)], 'The models were 2 layers deep, utilized a batch size of 64, and standard dropout of 0.2 between layers'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '1'), 27, 28), (('v number', '0.5'), 44, 47)], 'The learning rate began at 1 and decayed by 0.5 whenever validation perplexity failed to improve'], [[(('parameter', 'learning rate'), 9, 22), (('v number', '0.03'), 34, 38)], 'When the learning rate fell below 0.03 training was finished'], [[(('parameter', 'batch size'), 30, 40), (('v number', '1'), 44, 45)], 'Models were evaluated using a batch size of 1 to ensure RNN padding did not impact the results.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 19, 29), (('artifact', 'Adam'), 47, 51), (('parameter', 'learning rate'), 85, 98), (('v number', '8'), 33, 34), (('v number', '0.0001'), 102, 108), (('v number', '0.96'), 156, 160)], 'Moreover, we use a batch size of 8, and employ Adam for optimization with an initial learning rate of 0.0001, which is decreased exponentially decay rate = 0.96 with an increasing number of iterations.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 65, 78), (('artifact', 'model'), 106, 111)], 'We use Adam optimizer to allow it taking the role of setting the learning rate automatically based on the model’s weight update history'], [[(('parameter', 'epochs'), 87, 93), (('parameter', 'epochs'), 183, 189), (('v number', '64'), 28, 30), (('v number', '50'), 84, 86), (('v number', '10'), 168, 170)], 'We use mini-batches of size 64 and each training volume is trained for a maximum of 50 epochs or until the reconstruction loss of validation data stop decreasing after 10 consecutive epochs']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('v number', '128'), 26, 29), (('v number', '22'), 34, 36), (('v number', '0.9'), 71, 74)], 'The batch size was set to 128 and 22 for the CNN I and II, momentum to 0.9'], [[(('parameter', 'dropout'), 55, 62), (('parameter', 'layers'), 122, 128), (('v number', '0.003'), 30, 35), (('v number', '0.0001'), 44, 50), (('v number', '0.5'), 89, 92), (('v number', '1'), 149, 150), (('v number', '1'), 151, 152)], 'We used L2-regularization _ = 0.003 and _ = 0.0001 and dropout regularization with ratio 0.5 only applied to the last two layers of the network with 1 1 convolutions'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 174, 180), (('parameter', 'epoch'), 202, 207), (('v number', '0.01'), 39, 43), (('v number', '5'), 78, 79)], 'The learning rate was initially set to 0.01 and then decreased by a factor of 5 if no increase in performance was observed on the evaluation set, over a predefined number of epochs which we refer to as epoch patience E_p '], [[(('parameter', 'learning rate'), 64, 77), (('v number', '20'), 27, 29)], 'We increased this value by 20% after each drop incidence in the learning rate'], [[(('parameter', 'learning rate'), 45, 58), (('parameter', 'learning rates'), 77, 91)], 'This prevented the network from dropping the learning rate too fast at lower learning rates']] \n",
      "\n",
      "[[[(('artifact', 'model'), 21, 26), (('v number', '5'), 42, 43), (('v number', '15'), 82, 84)], 'We apply a skip-gram model of window-size 5 and filter words that occur less than 15 times ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 77, 82), (('artifact', 'model'), 101, 106)], 'We randomly initialize the word embedding matrix and fine-tune it with other model parameters during model training'], [[(('parameter', 'dropout'), 0, 7), (('artifact', 'model'), 68, 73), (('v number', '0.5'), 13, 16)], 'Dropout rate 0.5 is applied to the non-recurrent connections during model training for regularization'], [[(('artifact', 'gradient clipping'), 17, 34), (('v number', '5'), 45, 46)], 'Maximum norm for gradient clipping is set to 5'], [[(('parameter', 'batch size'), 31, 41), (('artifact', 'Adam'), 51, 55), (('parameter', 'learning rate'), 82, 95), (('v number', '16'), 42, 44), (('v number', '1e-3'), 99, 103)], 'We perform mini-batch training batch size 16 using Adam optimization with initial learning rate of 1e-3.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 111, 116), (('artifact', 'L'), 117, 118), (('parameter', 'layer'), 182, 187)], 'In this strategy, a zero-mean Gaussian distribution of standard deviation is used to initialize the weights in layer l , where n_l denotes the number of connections to units in that layer'], [[(('parameter', 'learning rate'), 40, 53), (('parameter', 'epochs'), 109, 115), (('parameter', 'epoch'), 130, 135), (('v number', '0.6'), 20, 23), (('v number', '0.001'), 57, 62), (('v number', '2'), 93, 94), (('v number', '5'), 107, 108), (('v number', '10'), 136, 138)], 'Momentum was set to 0.6 and the initial learning rate to 0.001, being reduced by a factor of 2 after every 5 epochs starting from epoch 10'], [[(('parameter', 'epochs'), 31, 37), (('v number', '30'), 28, 30), (('v number', '20'), 60, 62)], 'The network was trained for 30 epochs, each one composed of 20 subepochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 14, 19), (('v number', '151'), 37, 40), (('v number', '082'), 41, 44)], 'We apply this model to a database of 151,082 papers from the Physical Review journals']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 37, 42), (('v number', '512'), 66, 69), (('v number', '512'), 102, 105)], 'We use a siamese BiRNN with a single layer in each direction with 512-dimensional word embeddings and 512-dimensional recurrent states'], [[(('parameter', 'layer'), 11, 16), (('parameter', 'layers'), 40, 46), (('v number', '256'), 51, 54)], 'The hidden layer of the fully connected layers has 256 hidden units'], [[(('artifact', 'Adam'), 28, 32), (('parameter', 'learning rate'), 50, 63), (('v number', '0.0002'), 67, 73), (('v number', '128'), 93, 96)], 'To train our models, we use Adam optimizer with a learning rate of 0.0002 and a minibatch of 128 examples'], [[(('parameter', 'epochs'), 37, 43), (('v number', '15'), 34, 36)], 'Models are trained for a total of 15 epochs'], [[(('artifact', 'gradient clipping'), 39, 56), (('v number', '5'), 111, 112)], 'To avoid exploding gradients, we apply gradient clipping such that the norm of all gradients is no larger than 5 '], [[(('parameter', 'dropout'), 9, 16), (('v number', '0.2'), 62, 65), (('v number', '0.3'), 70, 73)], 'We apply dropout to prevent overfitting with a probability of 0.2 and 0.3 for the non-recurrent input and output connections respectively']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 28, 32), (('parameter', 'dropout'), 63, 70), (('v number', '80'), 83, 85)], 'During training, we use the Adam optimizer , with L_2 loss and dropout rate set to 80% for training'], [[(('parameter', 'epochs'), 36, 42), (('parameter', 'T'), 197, 198), (('parameter', 'T'), 262, 263), (('v number', '500'), 28, 31), (('v number', '000'), 32, 35), (('v number', '16'), 85, 87), (('v number', '0'), 169, 170)], 'Training is performed up to 500,000 epochs with randomized minibatches consisting of 16 samples, where each sample contains one input image at current relative time t_0=0 , a temporal displacement t and the real target frame at the desired temporal displacement t ']] \n",
      "\n",
      "[[[(('parameter', 'hidden state'), 50, 62), (('v number', '300'), 34, 37), (('v number', '512'), 72, 75)], 'We set the word embedding size to 300 and all GRU hidden state sizes to 512'], [[(('parameter', 'dropout'), 7, 14), (('v number', '0.5'), 36, 39)], 'We use dropout with probability p = 0.5 ']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 23, 28), (('v number', '0.0001'), 107, 113)], 'The glimpse generation layer is initialized from zero-mean Gaussian distributions with standard deviations 0.0001'], [[(('parameter', 'layers'), 18, 24), (('v number', '0.01'), 90, 94), (('v number', '0'), 121, 122)], 'All the recurrent layers are initialized from zero-mean Gaussian with standard deviations 0.01 and the biases are set to 0'], [[(('parameter', 'layer'), 20, 25), (('v number', '32'), 62, 64)], 'The fully connected layer applied to the glimpse vectors have 32 output neurons']] \n",
      "\n",
      "[[[(('artifact', 'model'), 9, 14), (('v number', '54'), 49, 51), (('v number', '3'), 80, 81), (('v number', '0'), 103, 104), (('v number', '10'), 106, 108), (('v number', '20'), 114, 116), (('v number', '18'), 121, 123), (('v number', '20'), 173, 175), (('v range', '[0,340]'), 160, 167)], 'For each model, we render images from a total of 54 viewpoints corresponding to 3 different elevations 0, 10, and 20 and 18 azimuth angles sampled in the range [0,340] with 20-degree increments']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '26'), 37, 39), (('v number', '6'), 112, 113), (('v number', '870'), 114, 117)], \"The model's action space consists of 26 matching pairs of Open and Close actions, one for each nonterminal, and 6,870 Shift actions, one for each preprocessed word type\"]] \n",
      "\n",
      "[[[(('artifact', 'model'), 9, 14), (('artifact', 'Adam'), 36, 40)], 'The HTDN model is trained using the Adam optimizer '], [[(('artifact', 'model'), 18, 23), (('v number', '10'), 29, 31), (('v number', '2'), 106, 107)], 'The random forest model uses 10 estimators, with no maximum depth, and minimum-samples-per-split value of 2'], [[(('artifact', 'model'), 15, 20), (('v number', '1'), 74, 75)], 'The linear SVM model uses an _2 -penalty and a square hinge loss with C = 1 .']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 9, 14), (('parameter', 'batch size'), 23, 33), (('artifact', 'Adam'), 65, 69), (('parameter', 'learning rate'), 71, 84), (('parameter', 'T'), 117, 118), (('v number', '5'), 16, 17), (('v number', '10'), 18, 20), (('v number', '5'), 21, 22), (('v number', '4'), 35, 36), (('v number', '5'), 85, 86), (('v number', '0'), 87, 88), (('v number', '1.5'), 104, 107), (('v number', '10'), 108, 110), (('v number', '5'), 111, 112)], 'Training steps: 5 10^5 Batch Size: 4 Gradient Descent Optimizer: Adam, learning rate 5 0.0001e^ where = 1.5 10^5 and t is the training step'], [[(('parameter', 'T'), 128, 129), (('v number', '1'), 93, 94), (('v number', '2.5'), 115, 118), (('v number', '10'), 119, 121), (('v number', '4'), 122, 123)], 'Rollout frame temporal discount factor by which future frames are weighted less in the loss: 1 - with = e^ where = 2.5 10^4 and t is the training step.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 0, 5), (('parameter', 'layer'), 47, 52), (('parameter', 'layer'), 122, 127), (('v number', '1'), 6, 7), (('v number', '64'), 20, 22), (('v number', '2'), 53, 54), (('v number', '32'), 67, 69)], 'Layer 1 consists of 64 filters of 9x9 kernels, layer 2 consists of 32 fully connected neurons 1x1 filters, and the output layer uses a 5x5 kernel see Figure REF '], [[(('artifact', 'Adam'), 30, 34), (('parameter', 'learning rate'), 55, 68), (('parameter', 'layers'), 97, 103), (('parameter', 'layers'), 127, 133), (('v number', '0.0001'), 72, 78), (('v number', '1e-05'), 108, 113)], 'Each network is trained using Adam optimization with a learning rate of 0.0001 for the first two layers and 1e-05 for the last layers'], [[(('artifact', 'model'), 5, 10), (('parameter', 'batch size'), 49, 59), (('v number', '10'), 27, 29), (('v number', '200'), 63, 66)], 'Each model was trained for 10^ iterations with a batch size of 200']] \n",
      "\n",
      "[[[(('artifact', 'model'), 22, 27), (('parameter', 'layer'), 63, 68), (('parameter', 'layer'), 122, 127), (('v number', '256'), 74, 77), (('v number', '1'), 258, 259), (('v number', '0.25'), 271, 275)], 'For the Seq2Seq-FHVAE model, each LSTM network consists of one layer with 256 hidden units, while each MLP network is one layer with the output dimension equal to the variable whose mean or log variance the MLP parameterizes, and variances ^2__1} = ^2__2} = 1 , ^2__2} = 0.25 '], [[(('parameter', 'batch size'), 70, 80), (('v number', '256'), 84, 87), (('v number', '0.0001'), 199, 205)], 'All models were trained with stochastic gradient descent using a mini-batch size of 256 to minimize the negative discriminative segment variational lower bound plus an L2 -regularization with weight 0.0001 '], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 79, 92), (('v number', '0.95'), 37, 41), (('v number', '0.999'), 49, 54), (('v number', '1e-08'), 59, 64), (('v number', '0.001'), 96, 101)], 'The Adam optimizer is used with _1 = 0.95 , _2 = 0.999 , = 1e-08 , and initial learning rate of 0.001 '], [[(('parameter', 'epochs'), 27, 33), (('parameter', 'epochs'), 124, 130), (('v number', '500'), 23, 26), (('v number', '50'), 121, 123)], 'Training continues for 500 epochs unless the segment variational lower bound on the development set does not improve for 50 epochs']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 9, 19), (('v number', '128'), 23, 26), (('v number', '10'), 77, 79), (('v number', '0.6'), 120, 123)], 'We use a batch size of 128 and decode using beam search with a beam width of 10 and the length normalization penalty of 0.6 described in '], [[(('parameter', 'steps'), 45, 50), (('v number', '2'), 40, 41), (('v number', '4'), 66, 67)], 'Each experiment is run for a maximum of 2.5M steps and replicated 4 times with different initializations'], [[(('artifact', 'model'), 8, 13), (('v number', '30'), 32, 34)], 'We save model checkpoints every 30 minutes and choose the best checkpoint based on the validation set BLEU score']] \n",
      "\n",
      "[[[(('artifact', 'L'), 94, 95), (('v number', '50'), 66, 68), (('v number', '2'), 96, 97), (('v number', '23'), 159, 161)], 'Estimation of the row of matrix corresponding to agent p requires 50 previous frames assuming L=2.5N whereas the neighborhood based estimation reduces this to 23']] \n",
      "\n",
      "[[[(('artifact', 'BiLSTM'), 4, 10), (('parameter', 'layer'), 19, 24)], 'The BiLSTM encoder layer use 300D hidden states, thus 600D as it’s a bidirectional encoder'], [[(('parameter', 'dropout'), 0, 7), (('parameter', 'dropout'), 30, 37), (('artifact', 'model'), 65, 70), (('v number', '0.2'), 46, 49)], 'Dropout is implemented with a dropout rate of 0.2 to prevent the model from overfitting']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 227, 237), (('v number', '64'), 241, 243)], 'To train the feature prediction network, we apply the standard maximum-likelihood training procedure feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing with a batch size of 64 on a single GPU'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 62, 75), (('v number', '0.9'), 34, 37), (('v number', '0.999'), 42, 47), (('v number', '1e-06'), 50, 55), (('v number', '0.001'), 79, 84), (('v number', '1e-05'), 111, 116), (('v number', '50'), 132, 134), (('v number', '000'), 135, 138)], 'We use the Adam optimizer with _1=0.9, _2=0.999, =1e-06 and a learning rate of 0.001 exponentially decaying to 1e-05 starting after 50,000 iterations']] \n",
      "\n",
      "[[[(('parameter', 'T'), 28, 29), (('v number', '1'), 32, 33)], 'In other words, b_o = b and t = 1 ']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 122, 126), (('parameter', 'batch size'), 139, 149), (('v number', '1'), 100, 101), (('v number', '64'), 153, 155), (('v number', '32'), 190, 192)], 'All models are trained on a single Titan X GPU with two supporting CPU threads, using TensorFlow r1.1 and optimized using Adam with a mini-batch size of 64 for LSTM BasicLSTMCell models and 32 for CNN models unless otherwise mentioned'], [[(('parameter', 'learning rate'), 30, 43), (('v number', '0.001'), 47, 52)], 'For the LSTM models, we use a learning rate of 0.001'], [[(('parameter', 'learning rate'), 30, 43), (('v number', '0.0002'), 47, 53)], 'For the CNN models, a smaller learning rate of 0.0002 was preferred'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'T'), 66, 67), (('parameter', 'epochs'), 86, 92), (('v number', '5'), 32, 33)], \"The learning rate is decayed by 5% whenever validation loss doesn't decrease over two epochs\"], [[(('parameter', 'epoch'), 36, 41), (('artifact', 'model'), 51, 56)], 'We report average training time per epoch for each model as both wall-clock hours t_ and CPU-hours t_ .']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 149, 156), (('v number', '10'), 63, 65)], 'Hence in our experiments, once a training image from the CIFAR-10 training set is used to create a backdoor training image, its clean, backdoor-free version will no longer appear in the training set'], [[(('parameter', 'Version'), 47, 54), (('v number', '1'), 120, 121)], 'If for any backdoor training image , its clean version also exists in the training set, we will have p_s; very close to 1 for some s }^ where }^ is the set of source classes']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 28, 32), (('parameter', 'learning rate'), 38, 51), (('parameter', 'learning rate'), 73, 86), (('parameter', 'epochs'), 95, 101), (('v number', '0.001'), 52, 57), (('v number', '8'), 93, 94), (('v number', '10'), 105, 107)], 'For optimization, we employ Adam with learning rate 0.001, and decay the learning rate every 8 epochs by 10'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 28, 34), (('v number', '30'), 25, 27), (('v number', '112'), 61, 64)], 'Our model is trained for 30 epochs with mini-batches of size 112.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 5, 10), (('artifact', 'Adam'), 74, 78), (('parameter', 'learning rate'), 96, 109), (('parameter', 'weight decay'), 124, 136), (('v number', '1e-4'), 113, 117), (('v number', '5e-4'), 140, 144)], 'Each model has been fine-tuned using a weighted cross entropy loss and an Adam optimizer with a learning rate of 1e-4 and a weight decay of 5e-4'], [[(('parameter', 'batch size'), 11, 21), (('v number', '32'), 25, 27), (('v number', '64'), 76, 78)], 'We set the batch size to 32 for the training of the IMAGO classifier and to 64 for the training of the IMAGO-FACES and IMAGO-PEOPLE models.']] \n",
      "\n",
      "[[[(('parameter', 'hidden state'), 9, 21), (('v number', '64'), 26, 28)], 'The LSTM hidden state was 64-dimensions'], [[(('parameter', 'batch size'), 17, 27), (('v number', '32'), 39, 41)], 'During training, batch size was set to 32'], [[(('artifact', 'Adam'), 3, 7), (('parameter', 'learning rate'), 37, 50), (('v number', '0.0025'), 54, 60)], 'An Adam optimizer was applied with a learning rate of 0.0025'], [[(('parameter', 'epochs'), 23, 29), (('v number', '20'), 34, 36)], 'The number of training epochs was 20'], [[(('artifact', 'model'), 25, 30), (('parameter', 'epochs'), 38, 44), (('artifact', 'model'), 69, 74), (('artifact', 'model'), 91, 96), (('v number', '20'), 35, 37)], 'We first trained the CNN model for 20 epochs then fine-tuned the CNN model during temporal model training']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 37, 50), (('v number', '0.001'), 54, 59)], 'We fit joint transformations using a learning rate of 0.001 for 3k iterations'], [[(('parameter', 'learning rate'), 67, 80), (('parameter', 'batch size'), 95, 105), (('v number', '0.0001'), 84, 90), (('v number', '85'), 109, 111)], 'We fit the global skinning weights across the training set using a learning rate of 0.0001 and batch size of 85 for 3k iterations'], [[(('parameter', 'learning rate'), 45, 58), (('parameter', 'learning rate'), 124, 137), (('parameter', 'batch size'), 206, 216), (('v number', '0.001'), 62, 67), (('v number', '0.0001'), 141, 147), (('v number', '85'), 220, 222)], 'Neural skinning weights were trained using a learning rate of 0.001 during the first phase of training the autoencoder, and learning rate of 0.0001 during the second phase of fine-tuning the decoder with a batch size of 85 for 30k iterations']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 66, 79), (('v number', '0'), 137, 138)], 'This bound was not implemented in this approach and therefore the learning rate, the clip range, and the entropy bonus coefficient equal 0 at the end of the training']] \n",
      "\n",
      "[[[(('parameter', 'learning rates'), 73, 87), (('v number', '32'), 43, 45), (('v number', '0.1'), 91, 94), (('v number', '0.009'), 123, 128)], 'For the experiment in Figure REF the depth 32 networks were trained with learning rates of 0.1 for batch normalization and 0.009 for NormReLU'], [[(('parameter', 'learning rates'), 41, 55), (('v number', '110'), 10, 13), (('v number', '0.05'), 59, 63), (('v number', '0.005'), 92, 97)], 'The depth 110 networks were trained with learning rates of 0.05 for batch normalization and 0.005 for NormReLU'], [[(('parameter', 'learning rate'), 86, 99), (('parameter', 'learning rate'), 115, 128), (('v number', '0.1'), 103, 106)], 'We switch on data augmentation as used in and train with fixup initialization using a learning rate of 0.1 and the learning rate schedule as proposed by the authors in '], [[(('parameter', 'learning rate'), 31, 44), (('v number', '0.005'), 48, 53)], 'We train with NormReLU using a learning rate of 0.005 '], [[(('parameter', 'learning rate'), 42, 55), (('v number', '0.001'), 59, 64), (('v number', '32'), 75, 77), (('v number', '0.0005'), 85, 91), (('v number', '64'), 102, 104)], 'For the experiment in Figure REF we use a learning rate of 0.001 for depth 32 and of 0.0005 for depth 64'], [[(('parameter', 'learning rate'), 51, 64), (('v number', '0.01'), 68, 72), (('v number', '32'), 82, 84), (('v number', '0.009'), 92, 97), (('v number', '110'), 107, 110)], 'Finally, for the experiment in Figure REF we use a learning rate of 0.01 at depth 32 and of 0.009 at depth 110 and keep it the same for both the activations.']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 4, 16), (('v number', '1e-5'), 20, 24)], 'The weight decay is 1e-5'], [[(('parameter', 'steps'), 36, 41), (('v number', '2'), 100, 101)], 'To train the FracBNN, the first two steps are the same except that the activations are quantized to 2 bits'], [[(('artifact', 'model'), 85, 90), (('artifact', 'model'), 115, 120)], 'For ImageNet, we calculate the KL divergence between the softmax output of a teacher model and that of the trained model as the loss function, same as ReActNet '], [[(('artifact', 'model'), 31, 36), (('v number', '50'), 60, 62)], 'In our experiments the teacher model is a pretrained ResNet-50.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('artifact', 'linear warmup'), 73, 86), (('v number', '0.0001'), 60, 66), (('v number', '500'), 91, 94)], 'We use the Adam optimizer, with a constant learning-rate of 0.0001 , and linear warmup for 500 iterations'], [[(('artifact', 'model'), 23, 28), (('parameter', 'epochs'), 47, 53), (('parameter', 'epochs'), 97, 103), (('v number', '40'), 44, 46), (('v number', '80'), 94, 96)], 'The element generation model is trained for 40 epochs, while the other models are trained for 80 epochs'], [[(('artifact', 'model'), 56, 61), (('v number', '6'), 23, 24)], 'It takes approximately 6 hours to train for our largest model for constrained generation.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 39, 44), (('parameter', 'epochs'), 51, 57), (('parameter', 'batch size'), 78, 88), (('v number', '10'), 48, 50), (('v number', '64'), 92, 94)], 'In all experiments, we train the U-Net model to 10 epochs with an accumulated batch size of 64'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 42, 55), (('parameter', 'epochs'), 95, 101), (('v number', '0.0001'), 59, 65), (('v number', '5'), 93, 94)], 'We use the Adam optimizer with an initial learning rate of 0.0001 that decays linearly after 5 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('artifact', 'model'), 27, 32), (('parameter', 'layer'), 71, 76)], 'Our model is based on BERT model architecture , which includes a multi-layer bidirectional Transformer'], [[(('parameter', 'hidden layers'), 63, 76), (('v number', '12'), 60, 62), (('v number', '12'), 78, 80), (('v number', '768'), 98, 101)], 'In particular, we use the original BERT BASE configuration: 12 hidden layers, 12 attention heads, 768 hidden sizes']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 31, 37), (('parameter', 'learning rate'), 76, 89), (('parameter', 'batch size'), 109, 119), (('parameter', 'weight decay'), 128, 140), (('v number', '100'), 27, 30), (('v number', '0.1'), 90, 93), (('v number', '0.9'), 104, 107), (('v number', '128'), 120, 123), (('v number', '1e-4'), 141, 145)], 'All models are trained for 100 epochs using the SGD algorithm with starting learning rate 0.1, momentum 0.9, batch size 128 and weight decay 1e-4'], [[(('parameter', 'epoch'), 21, 26), (('parameter', 'learning rate'), 31, 44), (('v number', '10'), 68, 70)], 'In the 80th and 90th epoch the learning rate is decreased by factor 10']] \n",
      "\n",
      "[[[(('parameter', 'T'), 18, 19), (('parameter', 'm'), 25, 26), (('v number', '0.01'), 2, 6), (('v number', '2.0'), 20, 23), (('v number', '0.2'), 71, 74), (('v number', '1.6'), 80, 83), (('v number', '0.2'), 89, 92), (('v number', '1.0'), 107, 110), (('v number', '1.0'), 120, 123)], 'c=0.01 for IGNNS, t=2.0 [m] meters for the boundary derivation, and _s=0.2 , _a=1.6 , _d=0.2 , w= , _p= _q=1.0 , _u= _v=1.0 for the energy minimization'], [[(('parameter', 'm'), 20, 21), (('v number', '1000'), 3, 7), (('v number', '0.2'), 15, 18)], 'N_=1000 and t_=0.2 [m] for the ground detection']] \n",
      "\n",
      "[[[(('artifact', 'L'), 148, 149), (('parameter', 'hidden layers'), 150, 163), (('parameter', 'layer'), 192, 197), (('v number', '1'), 144, 145)], 'It is worth noticing that our proposal can be generalized to multi-parties and the situations that the data holders collaboratively calculate i 1 i L hidden layers instead of the first hidden layer only']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 17, 21), (('parameter', 'learning rate'), 27, 40), (('v number', '2'), 41, 42), (('v number', '0.9'), 48, 51), (('v number', '0.998'), 60, 65)], 'The optimizer is Adam with learning rate 2, _1 =0.9 and _2 =0.998'], [[(('parameter', 'learning rate'), 14, 27), (('parameter', 'steps'), 56, 61), (('v number', '8'), 50, 51), (('v number', '000'), 52, 55)], 'We also apply learning rate warmup over the first 8,000 steps and decay as in '], [[(('artifact', 'gradient clipping'), 0, 17), (('v number', '2.0'), 45, 48)], 'Gradient clipping with maximum gradient norm 2.0 is also utilized during training'], [[(('parameter', 'steps'), 56, 61), (('parameter', 'steps'), 100, 105), (('v number', '4'), 26, 27), (('v number', '500'), 48, 51), (('v number', '000'), 52, 55)], 'All models are trained on 4 GPUs Tesla V100 for 500,000 steps with gradient accumulation every four steps'], [[(('parameter', 'dropout'), 9, 16), (('parameter', 'layers'), 56, 62), (('v number', '0.1'), 34, 37)], 'We apply dropout with probability 0.1 before all linear layers in our models'], [[(('parameter', 'layers'), 35, 41), (('parameter', 'layers'), 58, 64), (('parameter', 'layers'), 84, 90), (('v number', '6'), 102, 103), (('v number', '2'), 105, 106), (('v number', '8'), 111, 112)], 'The number of transformer encoding layers, graph encoding layers and graph decoding layers are set as 6, 2 and 8, respectively']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 56, 66), (('v number', '1'), 70, 71)], 'For models trained in Caffe, we use a single GPU with a batch size of 1, which is the default setting'], [[(('parameter', 'batch size'), 46, 56), (('parameter', 'batch size'), 112, 122), (('v number', '16'), 126, 128)], 'For models trained in Detectron, we utilize a batch size of two images per graphics card, for a total effective batch size of 16, and use synchronized SGD.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 62, 67), (('parameter', 'layers'), 102, 108), (('v number', '64'), 44, 46), (('v number', '128'), 119, 122)], 'The embedding dimensionality, d , is set to 64 and the hidden layer width d_h for all fully connected layers is set to 128'], [[(('parameter', 'layers'), 14, 20), (('artifact', 'L'), 21, 22), (('artifact', 'model'), 49, 54), (('v number', '3'), 67, 68), (('v number', '2'), 108, 109)], 'The number of layers L in the fully-connected TS model is equal to 3 and the number of blocks R is equal to 2'], [[(('parameter', 'weight decay'), 7, 19), (('parameter', 'layers'), 58, 64), (('v number', '1e-5'), 23, 27)], 'We use weight decay of 1e-5 to regularize fully-connected layers'], [[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 31, 35), (('parameter', 'learning rate'), 91, 104), (('parameter', 'epochs'), 121, 127), (('v number', '0.001'), 108, 113), (('v number', '60'), 118, 120)], 'The model is trained using the Adam optimizer with default tensorflow settings and initial learning rate of 0.001 for 60 epochs'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 55, 61), (('parameter', 'epoch'), 74, 79), (('v number', '2'), 45, 46), (('v number', '6'), 53, 54), (('v number', '43'), 80, 82)], 'The learning rate is annealed by a factor of 2 every 6 epochs starting at epoch 43'], [[(('parameter', 'epoch'), 4, 9), (('artifact', 'model'), 52, 57), (('v number', '800'), 22, 25), (('v number', '4'), 42, 43), (('v number', '12'), 79, 81), (('v number', '12'), 102, 104), (('v number', '60'), 112, 114)], 'One epoch consists of 800 batches of size 4 and the model takes the history of 12 points and predicts 12 points 60 min ahead in one shot']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 79, 86), (('parameter', 'batch size'), 109, 119), (('v number', '200'), 67, 70), (('v number', '600'), 74, 77), (('v number', '0.2'), 97, 100), (('v number', '0.8'), 104, 107), (('v number', '64'), 128, 130)], '[leftmargin=*] Gated Recurrent UnitsGRU : we test hidden size from 200 to 600, dropout rate from 0.2 to 0.8, batch size in [32, 64, 128, 256]'], [[(('parameter', 'batch size'), 44, 54), (('parameter', 'dropout'), 78, 85), (('v number', '64'), 63, 65), (('v number', '0.3'), 96, 99), (('v number', '0.7'), 103, 106), (('v number', '100'), 188, 191), (('v number', '600'), 195, 198), (('v number', '2'), 218, 219), (('v number', '3'), 220, 221), (('v number', '3'), 222, 223), (('v number', '4'), 224, 225), (('v number', '5'), 226, 227), (('v number', '5'), 228, 229), (('v number', '6'), 230, 231), (('v number', '7'), 232, 233), (('v number', '7'), 234, 235), (('v number', '8'), 236, 237)], 'TextCNN : we search the best performance in batch size in [32, 64, 128, 256], dropout rate from 0.3 to 0.7, kernel numbers, which is numbers of convolution kernels of each size type, from 100 to 600, kernel size in [1,2,3,3,4,5,5,6,7,7,8,9]'], [[(('parameter', 'learning rate'), 15, 28), (('parameter', 'epochs'), 61, 67), (('parameter', 'batch size'), 91, 101), (('v range', '[16, 32]'), 105, 113)], 'BERT : we test learning rate in [2e-5, 3e-5, 5e-5], training epochs in [2.0, 3.0, 4.0] and batch size in [16, 32].']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 126, 139), (('parameter', 'weight decay'), 145, 157), (('parameter', 'batch size'), 168, 178), (('v number', '6000'), 81, 85), (('v number', '0.9'), 108, 111), (('v number', '32'), 179, 181)], 'Stochastic gradient descent SGD is used as the optimizer in our experiments with 6000 burn-in mini-batches, 0.9 momentum, 1e^ learning rate, 5e^ weight decay, and mini-batch size 32'], [[(('parameter', 'learning rate'), 29, 42), (('parameter', 'learning rate'), 91, 104), (('v number', '4'), 114, 115)], 'During the burn-in time, the learning rate increases gradually until it reaches the target learning rate _ = _ }}^4, _']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 93, 97), (('parameter', 'epochs'), 105, 111), (('parameter', 'learning rate'), 120, 133), (('parameter', 'epochs'), 171, 177), (('v number', '30'), 102, 104), (('v number', '0.0005'), 149, 155)], 'We unroll the Gauss-Newton optimisation and train all the models together from scratch using ADAM for 30 epochs, with a learning rate initialized at 0.0005 and reduced at epochs [5, 10, 20]'], [[(('parameter', 'epochs'), 68, 74), (('v number', '10'), 65, 67)], 'When combining the ICP residual, we do a further fine-tuning for 10 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 115, 116), (('parameter', 'T'), 117, 118), (('v number', '60'), 122, 124)], 'When using Twitter-LDA to project user interactions to a cross-network topical space, we set the number of topics, K^t to 60 using a grid search algorithm'], [[(('parameter', 'K'), 64, 65), (('parameter', 'T'), 66, 67), (('v number', '5'), 68, 69)], 'The final results were not highly sensitive to minor changes in K^t 5 '], [[(('parameter', 'fold'), 42, 46), (('parameter', 'K'), 80, 81), (('v number', '60'), 112, 114), (('v number', '0.5'), 119, 122)], 'We used a grid search algorithm and a two-fold cross validation setup to obtain K and values, which were set to 60 and 0.5, respectively'], [[(('artifact', 'model'), 19, 24), (('v number', '0.8'), 125, 128), (('v number', '0.3'), 133, 136)], 'By configuring the model using the above parameters and using a grid search algorithm, we found optimum values for and to be 0.8 and 0.3, respectively'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.001'), 51, 56)], 'The learning rate was set to a fairly small value, 0.001, to obtain the local minimum.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 35, 39), (('artifact', 'Adam'), 64, 68), (('parameter', 'learning rate'), 92, 105)], 'We used Adaptive Moment Estimation Adam for optimizations since Adam adaptively updates the learning rate during training'], [[(('parameter', 'learning rate'), 19, 32), (('artifact', 'Adam'), 120, 124), (('v number', '0.1'), 36, 39)], 'We set the initial learning rate to 0.1, a fairly large value for faster initial learning before the rate is updated by Adam'], [[(('parameter', 'layer'), 24, 29), (('parameter', 'layer'), 93, 98), (('parameter', 'layer'), 128, 133), (('v number', '2'), 149, 150)], 'We used only one hidden layer for all neural architectures, and given the size of the output layer H_L , the size of the hidden layer was set to H_L 2 to reduce the number of hyper-parameters'], [[(('parameter', 'dropout'), 12, 19), (('parameter', 'dropout'), 53, 60), (('v number', '0.4'), 72, 75)], 'We used the dropout regularization technique and the dropout was set to 0.4 during training to prevent neural networks from overfitting.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 102, 108), (('v number', '32'), 55, 57), (('v number', '32'), 58, 60), (('v number', '200'), 98, 101)], 'Each image in the training dataset was decomposed into 32 32 non-overlapping patches and we train 200 epochs in total'], [[(('parameter', 'layers'), 60, 66), (('v number', '14'), 43, 45), (('v number', '18'), 77, 79), (('v number', '18'), 164, 166)], 'Note that, used here consists of the first 14 convolutional layers in ResNet-18, since the size of the input image is much smaller than that in the original ResNet-18']] \n",
      "\n",
      "[[[(('artifact', 'model'), 41, 46), (('artifact', 'model'), 112, 117), (('artifact', 'L'), 200, 201), (('artifact', 'L'), 202, 203), (('artifact', 'L'), 220, 221), (('artifact', 'L'), 223, 224), (('v number', '4'), 8, 9), (('v number', '1'), 198, 199), (('v number', '1'), 204, 205)], 'Theorem 4 For any given perturbed global model parameters = ^ ^_ , there always exist infinitely many different model parameters W= W^ ^_ and noises R^ _^, R^ satisfying Eq REF , i.e., ^= & R^ W^ , 1 l L-1,& R^ W^ + R^, l =L..']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 15, 20), (('v number', '1'), 21, 22), (('v number', '3'), 27, 28), (('v number', '2'), 51, 52)], 'In this phase, steps 1 and 3 are avoided and, step 2 does a center crop instead of random one.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 21, 25), (('parameter', 'learning rate'), 43, 56), (('v number', '1e-4'), 60, 64), (('v number', '0.9'), 85, 88), (('v number', '0.999'), 98, 103), (('v number', '5'), 134, 135)], 'During training, the Adam optimizer with a learning rate of 1e-4 and parameters _1 = 0.9 and _2 = 0.999 is applied to batches of size 5.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 73, 79), (('parameter', 'activation'), 138, 148), (('parameter', 'layers'), 207, 213), (('parameter', 'activation'), 272, 282), (('v number', '2'), 83, 84), (('v number', '32'), 105, 107), (('v number', '3'), 130, 131), (('v number', '2'), 188, 189), (('v number', '2'), 217, 218), (('v number', '64'), 239, 241), (('v number', '3'), 264, 265), (('v number', '2'), 315, 316)], 'Feature extraction consists of the following sequence of operations: two layers of 2-D convolutions with 32 channels, kernal size 3, RelU activation each, followed by maxpooling by factor 2, followed by two layers of 2-D convolutions with 64 channels, kernel size 3, ReLU activation, and finally another maxpool by 2 operation'], [[(('parameter', 'layer'), 79, 84), (('parameter', 'activation'), 108, 118), (('parameter', 'dropout'), 120, 127), (('parameter', 'layer'), 153, 158), (('parameter', 'activation'), 182, 192), (('parameter', 'layer'), 221, 226), (('v number', '1024'), 93, 97), (('v number', '200'), 98, 101), (('v number', '200'), 167, 170), (('v number', '200'), 171, 174), (('v number', '200'), 235, 238), (('v number', '10'), 239, 241)], 'This is followed by the classification module, consisting of a fully connected layer of size 1024 200, ReLU activation, dropout, another fully connected layer of size 200 200 , ReLU activation and a final fully connected layer of size 200 10 '], [[(('parameter', 'layers'), 67, 73), (('v number', '4'), 29, 30), (('v number', '3'), 49, 50)], 'Effectively this network has 4 convolutional and 3 fully connected layers'], [[(('parameter', 'batch size'), 7, 17), (('v number', '128'), 21, 24)], 'We use batch size of 128 with this configuration.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 419, 429), (('parameter', 'learning rate'), 464, 477), (('parameter', 'steps'), 493, 498), (('v number', '128'), 288, 291), (('v number', '3.5'), 323, 326), (('v number', '3'), 377, 378), (('v number', '10'), 385, 387), (('v number', '1024'), 414, 418), (('v number', '5'), 456, 457), (('v number', '1e-05'), 458, 463)], 'The setup for the Neural Spline Flow was the following refer to for a comprehensive description of the hyperparametersThese hyperparameters were chosen due to the proximity of the nature of the density to the ones explored in the original NSF paper for similar problems and worked well.: 128 hidden features, tail bound of 3.5 i.e., the space transformed within the [-3.5,3.5]^3 cube, 10 composed transformations, 1024 batch size, validation size of 500k, 5 1e-05 learning rate, 100k training steps with cosine annealing scheduler']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('v number', '4'), 26, 27)], 'The batch size was set to 4 on a NVIDIA 1080Ti GPU'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 79, 85), (('v number', '0.002'), 39, 44), (('v number', '10'), 70, 72), (('v number', '40'), 76, 78)], 'The learning rate was initially set to 0.002, and then was divided by 10 at 40 epochs'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 45, 51), (('v number', '50'), 42, 44)], 'Our model has been trained for a total of 50 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 28, 33), (('parameter', 'epochs'), 53, 59), (('parameter', 'learning rate'), 68, 81), (('parameter', 'learning rate'), 106, 119), (('parameter', 'epochs'), 137, 143), (('v number', '100'), 49, 52), (('v number', '0.001'), 85, 90), (('v number', '50'), 134, 136)], 'For pruning the pre-trained model, we pruned for 100 epochs, used a learning rate of 0.001 and reduce the learning rate by half after 50 epochs'], [[(('artifact', 'model'), 16, 21), (('v number', '10'), 44, 46)], 'For pruning the model from scratch on CIFAR-10, we use the normal training schedule without an additional fine-tune process.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 46, 59), (('v number', '0.0001'), 63, 69)], 'Generators G_e and G_c , are trained with the learning rate of 0.0001 until the losses plateau separately'], [[(('parameter', 'learning rate'), 13, 26), (('v number', '1e-05'), 30, 35)], 'We lower the learning rate to 1e-05 and continue to train G_e and G_c until convergence.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 216, 221), (('v number', '1'), 198, 199), (('v number', '1'), 200, 201)], 'In both cases, the non-linear function that produces the task attention mask in the FPM is implemented as two basic residual blocks – that aggressively reduce the number of channels – followed by a 1 1 convolutional layer.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 26, 36), (('artifact', 'Adam'), 52, 56), (('parameter', 'learning rate'), 75, 88), (('v number', '8'), 40, 41), (('v number', '8e-4'), 109, 113)], 'Models are trained with a batch size of 8 using the Adam optimizer and the learning rate is initialized with 8e-4'], [[(('parameter', 'epochs'), 20, 26), (('parameter', 'learning rate'), 68, 81), (('parameter', 'learning rate'), 119, 132), (('v number', '12'), 6, 8), (('v number', '0.25'), 99, 103), (('v number', '6e-5'), 136, 140)], 'After 12 subsequent epochs without validation loss improvement, the learning rate is multiplied by 0.25 till a minimum learning rate of 6e-5 is reached'], [[(('parameter', 'epochs'), 43, 49), (('parameter', 'epochs'), 86, 92), (('v number', '28'), 29, 31), (('v number', '200'), 82, 85)], 'The training is stopped when 28 subsequent epochs without improvement occurred or 200 epochs are reached']] \n",
      "\n",
      "[[[(('parameter', 'm'), 7, 8), (('parameter', 'weight decay'), 32, 44), (('v number', '0.9'), 27, 30), (('v number', '0.00005'), 45, 52)], 'We use M-SGD with momentum=0.9, weight decay=0.00005.']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 143, 148), (('v number', '0'), 106, 107), (('v range', '[0,1]'), 228, 233)], 'Unless otherwise specified, in all cases, when we say “ramp up to a ”, it means increasing the value from 0 to a during the first t_} training steps t_} is a predefined ramp-up length using a sigmoid-shaped function e^} where x [0,1] '], [[(('parameter', 'steps'), 123, 128), (('v number', '0'), 92, 93), (('v range', '[0,1]'), 180, 185)], 'On the other hand, when we say “ramp down from a ”, it means decreasing the value from a to 0 during the last t_} training steps using another sigmoid-shaped function 1e^} where x [0,1] ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 65, 75), (('artifact', 'Adam'), 105, 109), (('parameter', 'weight decay'), 118, 130), (('v number', '48'), 46, 48), (('v number', '64'), 79, 81), (('v number', '1e-05'), 138, 143)], 'All UNIPoint models tested employ an RNN with 48 hidden units, a batch size of 64, and are trained using Adam with L2 weight decay set to 1e-05 ']] \n",
      "\n",
      "[[[(('artifact', 'L'), 57, 58), (('v number', '6'), 59, 60), (('v number', '12'), 61, 63), (('v number', '18'), 64, 66), (('v number', '24'), 67, 69), (('v number', '30'), 70, 72), (('v number', '36'), 73, 75), (('v number', '48'), 76, 78), (('v number', '10'), 141, 143), (('v number', '6'), 144, 145), (('v number', '6'), 150, 151), (('v number', '10'), 152, 154)], 'We trained common self-attention architectures of depths L=6,12,18,24,30,36,48 and varying widths, such that the network sizes range between 10^6 and 6 10^ full details on the widths of the trained architectures are given in the appendix']] \n",
      "\n",
      "[[[(('artifact', 'model'), 52, 57), (('parameter', 'layers'), 74, 80), (('parameter', 'layer'), 252, 257), (('v number', '12'), 63, 65), (('v number', '4'), 82, 83), (('v number', '256'), 105, 108), (('v number', '768'), 167, 170)], 'The generator network G is in our case a small BERT model with 12 encoder layers, 4 attention heads, and 256 hidden sizeIn the generator, the input embeddings of size 768 are first projected into the generator hidden size with the addition of a linear layer.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 0, 13), (('v number', '0.001'), 14, 19)], 'Learning rate 0.001 for all actors and critics'], [[(('parameter', 'epoch'), 106, 111), (('parameter', 'layer'), 120, 125), (('v number', '500'), 41, 44), (('v number', '40'), 84, 86), (('v number', '256'), 170, 173)], 'Replay buffer size was designed to store 500 episodes, similarly as in We performed 40 updates after each epoch on each layer, after the replay buffer contained at least 256 transitions'], [[(('parameter', 'batch size'), 0, 10), (('v number', '1024'), 11, 15)], 'Batch size 1024'], [[(('artifact', 'gradient clipping'), 3, 20), (('v number', '0'), 29, 30), (('v number', '1'), 36, 37)], 'No gradient clipping Rewards 0 and -1 without any normalization'], [[(('parameter', 'layer'), 61, 66), (('v number', '0.05'), 19, 23), (('v number', '0.1'), 28, 31)], 'Exploration noise: 0.05 and 0.1 for the planning and control layer respectively.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 171, 177), (('parameter', 'learning rate'), 184, 197), (('parameter', 'batch size'), 209, 219), (('v number', '1'), 26, 27), (('v number', '1'), 141, 142), (('v number', '2'), 144, 145), (('v number', '3'), 147, 148), (('v number', '5'), 150, 151), (('v number', '8'), 153, 154), (('v number', '11'), 156, 158), (('v number', '15'), 160, 162), (('v number', '500'), 167, 170), (('v number', '0.0001'), 198, 204), (('v number', '128'), 220, 223)], '[leftmargin=*] Experiment 1: synthetic ball throwing trajectories: We train GANs with local discriminators with localization width equals to 1, 2, 3, 5, 8, 11, 15 for 500 epochs, with learning rate 0.0001 and batch size 128'], [[(('parameter', 'epochs'), 89, 95), (('parameter', 'learning rate'), 102, 115), (('parameter', 'batch size'), 126, 136), (('v number', '2'), 11, 12), (('v number', '100'), 85, 88), (('v number', '0.001'), 116, 121), (('v number', '128'), 137, 140)], 'Experiment 2: real Bayes-nets: We train the standard GANs and the Bayes-net GANs for 100 epochs, with learning rate 0.001 and batch size 128']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 19, 31), (('v number', '0.0005'), 57, 63), (('v number', '0.9'), 68, 71)], 'Following , we set weight decay and Nesterov momentum to 0.0005 and 0.9, respectively'], [[(('artifact', 'model'), 18, 23), (('parameter', 'epochs'), 43, 49), (('parameter', 'epoch'), 81, 86), (('v number', '60'), 40, 42), (('v number', '1000'), 55, 59)], 'The meta-learning model was trained for 60 epochs, and 1000 mini-batchs for each epoch'], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'epochs'), 90, 96), (('v number', '0.1'), 36, 39), (('v number', '0.06'), 63, 67), (('v number', '0.012'), 69, 74), (('v number', '0.0024'), 80, 86), (('v number', '20'), 97, 99), (('v number', '40'), 101, 103), (('v number', '50'), 108, 110)], 'We set the initial learning rate to 0.1, then multiplied it by 0.06, 0.012, and 0.0024 at epochs 20, 40 and 50, respectively, as in '], [[(('parameter', 'epoch'), 81, 86), (('artifact', 'model'), 109, 114), (('v number', '60'), 54, 56)], 'The results, which are marked by “+ens” were used the 60 models saved after each epoch to become an ensemble model'], [[(('artifact', 'model'), 23, 28), (('artifact', 'model'), 48, 53), (('parameter', 'epoch'), 61, 66), (('artifact', 'model'), 89, 94)], 'When we only chose one model, we will chose the model at the epoch where we got the best model during training on the training classes set']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 28, 32), (('parameter', 'learning rate'), 56, 69), (('parameter', 'epochs'), 107, 113), (('parameter', 'epochs'), 126, 132), (('v number', '0.001'), 73, 78), (('v number', '0.0001'), 83, 89), (('v number', '20'), 104, 106), (('v number', '20'), 123, 125)], 'For Caltech256 dataset, the Adam optimizer is used with learning rate of 0.001 and 0.0001 for the first 20 epochs and next 20 epochs, respectively.']] \n",
      "\n",
      "[[[(('parameter', 'T'), 14, 15), (('parameter', 'learning rate'), 87, 100), (('v number', '2'), 63, 64), (('v number', '25'), 170, 172), (('v number', '50'), 174, 176), (('v number', '10000'), 178, 183)], 'When applying T-SNE, the number of components was specified as 2, with the perplexity, learning rate, number of iterations, and the embedding initialization specified as 25, 50, 10000, and Principal Components Analysis PCA, respectively']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 192, 205), (('parameter', 'epochs'), 251, 257), (('parameter', 'learning rate'), 274, 287), (('v number', '0.9'), 96, 99), (('v number', '4'), 114, 115), (('v number', '0001'), 220, 224), (('v number', '1'), 319, 320)], 'The spatial CNN parameters are optimized using stochastic gradient descent with momentum set to 0.9 Code and LENs-4 dataset are publicly available at https:github.commcgridlesLENS.The initial learning rate is set to 5x0.0001 and decayed over multiple epochs using a Plateau learning rate scheduler with patience set to 1']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 54, 58), (('parameter', 'learning rate'), 76, 89), (('parameter', 'batch size'), 106, 116), (('v number', '2'), 93, 94), (('v number', '0.0001'), 95, 101), (('v number', '64'), 120, 122)], 'For training the models on all datasets, we adopt the Adam optimizer with a learning rate of 2 0.0001 and batch size of 64, following , '], [[(('parameter', 'learning rate'), 93, 106), (('parameter', 'steps'), 127, 132), (('parameter', 'steps'), 174, 179), (('v number', '10'), 24, 26), (('v number', '100'), 34, 37), (('v number', '10'), 46, 48), (('v number', '5'), 151, 152)], 'Specifically, for CIFAR-10, CIFAR-100 and STL-10, we follow settings in by linearly decaying learning rate over 100K generator steps, each taken every 5 discriminator update steps'], [[(('parameter', 'steps'), 78, 83), (('parameter', 'learning rate'), 105, 118)], 'For ImageNet, we follow by increasing the number of generator updates to 450K steps instead, but with no learning rate decay'], [[(('parameter', 'steps'), 47, 52), (('parameter', 'learning rate'), 107, 120), (('v number', '2'), 71, 72)], 'For CelebA, we follow by taking 100K generator steps, each taken after 2 discriminator updates and with no learning rate decay.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 49, 59), (('v number', '10'), 34, 36), (('v number', '64'), 63, 65), (('v number', '128'), 83, 86)], 'For base models training on CIFAR-10, we set the batch size to 64 for DenseNet and 128 for other architectures'], [[(('parameter', 'weight decay'), 0, 12), (('v number', '1e-4'), 23, 27)], 'Weight decay is set to 1e-4 '], [[(('parameter', 'epochs'), 31, 37), (('parameter', 'learning rate'), 47, 60), (('parameter', 'epoch'), 100, 105), (('v number', '160'), 27, 30), (('v number', '0.1'), 75, 78), (('v number', '10'), 94, 96), (('v number', '80'), 106, 108), (('v number', '120'), 113, 116)], 'The models are trained for 160 epochs with the learning rate starting from 0.1 and divided by 10 at epoch 80 and 120'], [[(('parameter', 'batch size'), 63, 73), (('parameter', 'weight decay'), 93, 105), (('v number', '256'), 84, 87), (('v number', '1e-4'), 109, 113)], 'For all base models training on TinyImageNet and ImageNet, the batch size is set to 256, and weight decay is 1e-4 '], [[(('parameter', 'epochs'), 31, 37), (('v number', '100'), 27, 30)], 'All models are trained for 100 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 63, 69), (('parameter', 'batch size'), 75, 85), (('v number', '10'), 60, 62), (('v number', '32'), 86, 88)], 'For source-training, we finetune the pretrained encoder for 10 epochs with batch size 32'], [[(('parameter', 'epochs'), 139, 145), (('parameter', 'epochs'), 172, 178), (('v number', '50'), 136, 138), (('v number', '10'), 169, 171)], 'For target-adapting to every target language, the few-shot data is a sampled bucket in this language, and we finetune on the bucket for 50 epochs with early-stopping of 10 epochs'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rates'), 70, 84), (('v number', '1e-5'), 99, 103), (('v number', '3e-5'), 105, 109), (('v number', '5e-5'), 111, 115), (('v number', '7e-5'), 117, 121)], 'We use the Adam optimizer with default parameters in both stages with learning rates searched over 1e-5, 3e-5, 5e-5, 7e-5 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 26, 31), (('v number', '2.6'), 44, 47)], 'We trained and tested our model on an Intel 2.6 GHz CPU cluster with NVIDIA TITAN RTX GPUs'], [[(('parameter', 'learning rate'), 4, 17), (('artifact', 'Adam'), 77, 81), (('v number', '2'), 28, 29), (('v number', '0.001'), 30, 35), (('v number', '1'), 65, 66), (('v number', '0.001'), 67, 72)], 'The learning rate is set at 2 0.001 and regularization parameter 1 0.001 for Adam optimizer'], [[(('parameter', 'batch size'), 0, 10), (('v number', '12'), 21, 23)], 'Batch size is set to 12']] \n",
      "\n",
      "[[[(('artifact', 'model'), 16, 21), (('artifact', 'model'), 45, 50)], 'We denoted this model by PG, which is also a model variant proposed in ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 20, 25), (('artifact', 'BiLSTM'), 54, 60)], 'To train the ConCET model, the parameters for CNN and BiLSTM described in Figure REF were chosen based on our experience and previous literature'], [[(('artifact', 'model'), 32, 37), (('parameter', 'learning rate'), 68, 81), (('v number', '0.001'), 85, 90)], 'Finally, we trained the overall model with an Adams optimizer and a learning rate of 0.001'], [[(('parameter', 'layer'), 34, 39), (('v number', '29'), 31, 33)], 'The results are reported for a 29-layer VDCNN, based on the original paper.']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 49, 56), (('parameter', 'dropout'), 127, 134), (('v number', '3'), 57, 58), (('v number', '0.3'), 153, 156)], \"We used the hyper-parameters specified in latest version 3 of Google's Tensor2Tensor implementation, with the exception of the dropout rate, as we found 0.3 to be more robust across all the models trained on Europarl.\"]] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 7, 14), (('parameter', 'layers'), 15, 21), (('v number', '0.3'), 63, 66)], 'We use dropout layers at the end of both stages with a rate as 0.3 for avoid over-fitting'], [[(('artifact', 'Adam'), 26, 30), (('v number', '0.001'), 71, 76), (('v number', '0.9'), 85, 88), (('v number', '0.999'), 97, 102), (('v number', '1e-07'), 116, 121)], 'For the optimizer, we use Adam for both stages, with the learning_rate=0.001, beta_1=0.9, beta_2=0.999, and epsilon=1e-07.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 75, 79), (('parameter', 'learning rate'), 107, 120), (('v number', '0.001'), 136, 141)], 'During the training phase, Blackthe adaptive moment estimator known as the Adam optimizer is used, and the learning rate Blackis set at 0.001 '], [[(('artifact', 'model'), 148, 153), (('v number', '61'), 21, 23), (('v number', '55'), 80, 82)], 'The dataset contains 61 Blackgroups of high-fidelity HOSS simulations, Blackand 55 of them Blackare selected as the training data Blackto build the model'], [[(('parameter', 'epoch'), 29, 34), (('artifact', 'model'), 55, 60)], 'In the training process, one epoch represents that the model was trained once throughout the entire training dataset'], [[(('parameter', 'epoch'), 8, 13), (('artifact', 'model'), 71, 76)], 'At each epoch, six simulations are randomly selected and set aside for model validation'], [[(('parameter', 'epoch'), 53, 58), (('artifact', 'model'), 73, 78)], \"Note that validation is conducted at the end of each epoch to assess the model's performance\"], [[(('artifact', 'model'), 47, 52), (('parameter', 'epoch'), 91, 96)], 'The Blackgoal of validation is to indicate the model performance in data unseen during the epoch to avoid overfitting']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 22, 26), (('parameter', 'learning rate'), 39, 52), (('v number', '0.001'), 64, 69), (('v number', '0.9'), 108, 111), (('v number', '0.999'), 113, 118)], 'The used optimizer is Adam , where the learning rate was set to 0.001 and the parameters _1, _2 were set to 0.9, 0.999 '], [[(('parameter', 'batch size'), 60, 70), (('v number', '3'), 13, 14), (('v number', '000'), 15, 18), (('v number', '32'), 82, 84)], 'We performed 3,000 times mini-batch updates, where the mini-batch size was set to 32.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 38, 44), (('parameter', 'batch size'), 54, 64), (('v number', '200'), 34, 37), (('v number', '256'), 50, 53)], 'Each architecture was trained for 200 epochs with 256 batch size'], [[(('parameter', 'learning rate'), 42, 55), (('parameter', 'learning rate'), 85, 98), (('v number', '0.1'), 59, 62)], 'We used the SGD optimizer with an initial learning rate of 0.1, followed by a cosine learning rate schedule through the training'], [[(('parameter', 'weight decay'), 10, 22), (('v number', '5'), 26, 27), (('v number', '0.0001'), 28, 34), (('v number', '5'), 66, 67)], 'We used a weight decay of 5 0.0001 and a norm gradient clipped at 5'], [[(('parameter', 'learning rate'), 46, 59), (('parameter', 'epochs'), 79, 85), (('v number', '0.025'), 63, 68), (('v number', '300'), 89, 92)], 'For supernet training, we changed the initial learning rate to 0.025 and total epochs to 300'], [[(('parameter', 'batch size'), 4, 14), (('parameter', 'weight decay'), 30, 42), (('v number', '128'), 18, 21), (('v number', '1'), 54, 55), (('v number', '0.0001'), 56, 62)], 'The batch size is 128 and the weight decay was set to 1 0.0001 '], [[(('parameter', 'epochs'), 43, 49), (('v number', '40'), 37, 39), (('v number', '50'), 40, 42)], 'Each sub-supernet approximately took 40-50 epochs to converge after transfer learning']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 37, 43), (('v number', '100'), 89, 92)], 'The difference lies in the number of layers and initial channels for evaluation on CIFAR-100'], [[(('parameter', 'layers'), 15, 21), (('v number', '8'), 13, 14), (('v number', '16'), 26, 28)], 'R-DARTS sets 8 layers and 16 initial channels'], [[(('artifact', 'Adam'), 122, 126), (('parameter', 'learning rate'), 155, 168), (('v number', '0.5'), 130, 133), (('v number', '0.999'), 141, 146), (('v number', '0.001'), 149, 154), (('v number', '768'), 220, 223)], 'For the proxyless searching on ImageNet in search space we named it S5 by FBNet , we use the SGD optimizer for weight and Adam _1=0.5 and _2=0.999 , 0.001 learning rate for architecture parameters with the batch-size of 768'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 62, 68), (('v number', '0.045'), 29, 34), (('v number', '0'), 50, 51), (('v number', '30'), 59, 61)], 'The initial learning rate is 0.045 and decayed to 0 within 30 epochs following the cosine schedule']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 32, 38), (('artifact', 'Adam'), 82, 86), (('parameter', 'learning rate'), 104, 117), (('v number', '0.0001'), 121, 127)], 'All models were trained for ten epochs as we have a large training set, using the Adam optimizer with a learning rate of 0.0001 '], [[(('parameter', 'batch size'), 4, 14), (('artifact', 'model'), 100, 105), (('v number', '12'), 46, 48), (('v number', '32'), 78, 80), (('v number', '16'), 84, 86)], 'The batch size was based on a GPU capacity of 12 GB Tesla K80, and was set to 32 or 16 based on the model'], [[(('artifact', 'model'), 9, 14), (('artifact', 'model'), 53, 58)], 'The best model was saved at each checkpoint when the model resulted in an improved validation ROC AUC'], [[(('artifact', 'model'), 33, 38), (('v number', '13'), 50, 52)], 'Under this setting, the Baseline Model took about 13 hours to converge, and the Sequence Models took about six hours to converge.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 17, 23), (('v number', '600'), 13, 16), (('v number', '1024'), 40, 44), (('v number', '224'), 66, 69), (('v number', '224'), 70, 73), (('v number', '100'), 98, 101), (('v number', '32'), 126, 128), (('v number', '32'), 129, 131)], 'We train for 600 epochs with batches of 1024 images of resolution 224 224 pixels except for CIFAR-100 where the resolution is 32 32 '], [[(('parameter', 'T'), 25, 26), (('parameter', 'T'), 74, 75), (('v number', '4'), 27, 28), (('v number', '8'), 76, 77), (('v number', '100'), 87, 90)], 'For Grafit with _ we use T=4 different data-augmentations on ImageNet and T=8 on CIFAR-100']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 11, 21), (('parameter', 'epochs'), 41, 47), (('v number', '64'), 25, 27), (('v number', '300'), 51, 54)], 'We set the batch size to 64 and training epochs to 300'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 98, 104), (('v number', '0.25'), 38, 42), (('v number', '110'), 75, 78), (('v number', '150'), 82, 85), (('v number', '225'), 92, 95)], 'The learning rate is initially set to 0.25 and is decayed by the factor of 110 at 150^} and 225^} epochs']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 27, 32), (('v number', '12'), 24, 26), (('v number', '12'), 50, 52)], 'Specifically, we used a 12-layer Transformer with 12 attention heads']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 24, 30), (('v number', '1'), 59, 60)], 'For batch normalization layers, weights are initialized to 1.']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 36, 49), (('v number', '3'), 34, 35), (('v number', '32'), 55, 57), (('v number', '128'), 104, 107)], 'We start from a neural network of 3 hidden layers with 32 hidden units for COMPAS and German Credit and 128 for Adult, due to its larger encoded input'], [[(('parameter', 'activation'), 5, 15), (('parameter', 'activation'), 176, 186)], 'This activation function is computationally efficient and mitigates the issue of vanishing gradients since the function never saturates, which makes it one of the most popular activation functions'], [[(('parameter', 'activation'), 32, 42), (('parameter', 'activation'), 91, 101)], 'For the output units, a softmax activation was used to get the classification and a linear activation for COMPAS'], [[(('artifact', 'Adam'), 67, 71), (('parameter', 'learning rate'), 120, 133), (('v number', '0.9'), 90, 93), (('v number', '0.9999'), 98, 104), (('v number', '0.01'), 140, 144), (('v number', '0.1'), 180, 183)], 'The network, including the adversarial reader, is trained with the Adam optimizer with _1=0.9, _2=0.9999 and an initial learning rate l_r = 0.01 , which is adjusted by a factor of 0.1 when reaching a plateau.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 18, 19), (('parameter', 'K'), 111, 112), (('v number', '128'), 114, 117), (('v number', '2'), 197, 198), (('v number', '048'), 199, 202), (('v number', '128'), 203, 206), (('v number', '10'), 207, 209), (('v number', '2'), 237, 238), (('v number', '048'), 239, 242), (('v number', '128'), 283, 286), (('v number', '10'), 333, 335)], 'To generate the N K F array containing neighborhood relationships, we define a search radius R =25m and sample K =128 points from within each spherical region, thus creating an array of dimensions 2,048 128 10 where each of the original 2,048 points within a block is represented by 128 points from its 25m neighborhood, each with a 10-dimensional feature vector']] \n",
      "\n",
      "[[[(('artifact', 'method'), 56, 62), (('v number', '2'), 136, 137)], 'We learn the loss function using iterative optimization method via gradient descent using back propagation , as summarized in Algorithm 2']] \n",
      "\n",
      "[[[(('artifact', 'L'), 9, 10), (('artifact', 'model'), 129, 134), (('v number', '24'), 11, 13), (('v number', '1024'), 17, 21), (('v number', '16'), 25, 27)], 'We use _ L=24, H=1024, A=16, 355M params implementation by Huggingfacehttps:github.comhuggingfacetransformers as the pre-trained model in all our experiments'], [[(('parameter', 'layer'), 20, 25), (('parameter', 'layer'), 63, 68), (('parameter', 'layer'), 112, 117), (('parameter', 'layers'), 229, 235)], 'As for each adapter layer, we denote the number of transformer layer as N , the hidden dimension of transformer layer as H_A , the number of self-attention heads as A_A , the hidden dimension of down-projection and up-projection layers as H_d and H_u '], [[(('parameter', 'layers'), 12, 18), (('parameter', 'layers'), 33, 39), (('parameter', 'layers'), 76, 82)], 'The RoBERTa layers where adapter layers plug in are , and different adapter layers do not share parameters']] \n",
      "\n",
      "[[[(('artifact', 'model'), 46, 51), (('parameter', 'T'), 75, 76)], 'We first use SGD to analyze change in norm of model parameters, denoted as t _^ '], [[(('parameter', 'T'), 0, 1), (('parameter', 'T'), 8, 9), (('parameter', 'T'), 12, 13), (('parameter', 'T'), 23, 24), (('parameter', 'T'), 30, 31), (('parameter', 'T'), 42, 43), (('parameter', 'T'), 45, 46), (('parameter', 'T'), 55, 56), (('parameter', 'T'), 66, 67), (('parameter', 'T'), 80, 81), (('parameter', 'T'), 83, 84), (('parameter', 'T'), 100, 101), (('parameter', 'T'), 103, 104), (('v number', '2'), 38, 39), (('v number', '2'), 76, 77), (('v number', '2'), 96, 97)], 't _^ &= t - t; X_ _^ - t _^&= t _^} - 2 , t^ t; X_ + ^ t; X_ _^ - t _^}&= - 2 , t^ t; X_ + ^& - 2 , t^ t; X_,']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 46, 59), (('parameter', 'weight decay'), 109, 121), (('v number', '0.1'), 63, 66), (('v number', '0.0005'), 125, 131)], 'We use SGD with Nesterov momentum, an initial learning rate of 0.1 decayed following a cosine schedule , and weight decay of 0.0005'], [[(('parameter', 'epochs'), 27, 33), (('parameter', 'epochs'), 63, 69), (('v number', '250'), 23, 26), (('v number', '200'), 59, 62)], 'We train ensembles for 250 epochs, and otherwise train for 200 epochs, following '], [[(('parameter', 'batch size'), 9, 19), (('v number', '128'), 23, 26), (('v number', '4'), 37, 38), (('v number', '128'), 116, 119)], 'We use a batch size of 128, which is 4 larger when using BatchEnsemble each ensemble member sees a copy of the same 128 images.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 18, 28), (('v number', '1024'), 33, 37), (('v number', '512'), 42, 45)], 'The train and dev batch size are 1024 and 512'], [[(('parameter', 'learning rate'), 38, 51), (('v number', '0.002'), 55, 60)], 'Nadam is used as our optimizer with a learning rate of 0.002'], [[(('parameter', 'epoch'), 4, 9), (('parameter', 'epochs'), 93, 99), (('v number', '20'), 18, 20)], 'The epoch size is 20, and we apply early-stop when dev loss has not been improving for three epochs']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 73, 79), (('parameter', 'learning rate'), 128, 141), (('parameter', 'epochs'), 191, 197), (('v number', '1.0'), 14, 17), (('v number', '120'), 69, 72), (('v number', '0.9'), 109, 112), (('v number', '0.1'), 145, 148), (('v number', '10'), 172, 174), (('v number', '40'), 181, 183), (('v number', '80'), 188, 190)], 'For Webvision 1.0, we trained an InceptionResNet-V2 from scratch for 120 epochs using SGD with a momentum of 0.9 and an initial learning rate of 0.1 , which was divided by 10 after 40 and 80 epochs refer to '], [[(('parameter', 'epochs'), 82, 88), (('parameter', 'learning rate'), 137, 150), (('parameter', 'epochs'), 194, 200), (('v number', '50'), 35, 37), (('v number', '60'), 79, 81), (('v number', '0.9'), 118, 121), (('v number', '0.01'), 154, 158), (('v number', '10'), 182, 184), (('v number', '30'), 191, 193)], 'For FOOD-101N, we trained a ResNet-50 with the ImageNet pretrained weights for 60 epochs using SGD with a momentum of 0.9 and an initial learning rate of 0.01 , which was divided by 10 after 30 epochs refer to '], [[(('parameter', 'batch size'), 37, 47), (('parameter', 'dropout'), 57, 64), (('parameter', 'weight decay'), 80, 92), (('v number', '64'), 51, 53), (('v number', '0.4'), 68, 71), (('v number', '0.001'), 96, 101)], 'Regardless of the dataset, we used a batch size of 64, a dropout of 0.4 , and a weight decay of 0.001 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 39, 44), (('parameter', 'batch size'), 62, 72), (('v number', '64'), 75, 77)], 'For all experiments, we initialize our model with _0= and SGD batch size b=64 '], [[(('parameter', 'K'), 35, 36), (('parameter', 'steps'), 68, 73)], 'In each round, we uniformly sample K devices at random, which run E steps of SGD in parallel'], [[(('artifact', 'system'), 18, 24), (('parameter', 'learning rate'), 44, 57), (('v number', '0.01'), 61, 65), (('v number', '0.996'), 93, 98)], 'For the prototype system, we use an initial learning rate _0=0.01 with a fixed decay rate of 0.996 '], [[(('artifact', 'system'), 19, 25), (('v number', '0.1'), 60, 63)], 'For the simulation system, we use decay rate _0} , where _0=0.1 and r is communication round index']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 83, 88), (('v number', '1'), 62, 63), (('v number', '0.05'), 68, 72)], 'All our experiments use -greedy scheme where is decayed from =1 to =0.05 over time steps'], [[(('parameter', 'learning rate'), 7, 20), (('v number', '0.0005'), 24, 30)], 'We use learning rate of 0.0005 with soft target updates for all experiments'], [[(('parameter', 'm'), 25, 26), (('v number', '0.005'), 8, 13), (('v number', '0.05'), 50, 54)], 'We use =0.005 for PP and m -step experiments and =0.05 for SC2 experiments'], [[(('artifact', 'gradient clipping'), 13, 30), (('v number', '10'), 74, 76)], 'We also used gradient clipping to restrict the norm of the gradient to be 10 .']] \n",
      "\n",
      "[[[(('parameter', 'K'), 25, 26), (('v number', '2.2'), 20, 23), (('v number', '2'), 27, 28), (('v number', '0.01'), 32, 36), (('v number', '0.09'), 42, 46), (('v number', '0.5'), 56, 59)], 'wfixed step size: q=2.2,;K=2,; =0.01,; _1=0.09,; _1= _2=0.5 RGF Runge Kutta disc'], [[(('parameter', 'K'), 23, 24), (('v number', '3'), 20, 21), (('v number', '2'), 25, 26), (('v number', '0.01'), 30, 34), (('v number', '0.09'), 40, 44), (('v number', '0.5'), 54, 57)], 'wfixed step size: q=3,;K=2,; =0.01,; _1=0.09,; _1= _2=0.5 RGF Runge Kutta disc'], [[(('parameter', 'K'), 23, 24), (('v number', '6'), 20, 21), (('v number', '2'), 25, 26), (('v number', '0.01'), 30, 34), (('v number', '0.09'), 40, 44), (('v number', '0.5'), 54, 57)], 'wfixed step size: q=6,;K=2,; =0.01,; _1=0.09,; _1= _2=0.5 RGF Runge Kutta disc'], [[(('parameter', 'K'), 24, 25), (('v number', '10'), 20, 22), (('v number', '2'), 26, 27), (('v number', '0.01'), 31, 35), (('v number', '0.09'), 41, 45), (('v number', '0.5'), 55, 58)], 'wfixed step size: q=10,;K=2,; =0.01,; _1=0.09,; _1= _2=0.5']] \n",
      "\n",
      "[[[(('artifact', 'model'), 88, 93), (('v number', '149'), 16, 19), (('v number', '99'), 51, 53), (('v number', '50'), 112, 114)], 'Data Out of the 149 images in the training subset, 99 of them are used for training the model and the remaining 50 are used as the validation set'], [[(('parameter', 'batch size'), 53, 63), (('v number', '1'), 66, 67)], 'The network is trained with single image input i.e., batch size = 1.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 20, 26), (('v number', '15'), 30, 32)], 'The total number of epochs is 15 for most models'], [[(('parameter', 'learning rate'), 12, 25), (('artifact', 'model'), 64, 69), (('v number', '1e-4'), 89, 93), (('v number', '3e-4'), 97, 101)], 'The initial learning rate of the cosine cycle is tuned for each model, which ranges from 1e-4 to 3e-4 '], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epoch'), 33, 38), (('parameter', 'learning rate'), 74, 87)], 'The learning rate in the warm up epoch is always one tenth of the initial learning rate of the cosine cycle'], [[(('parameter', 'batch size'), 0, 10), (('v number', '64'), 14, 16)], 'Batch size is 64 for all models']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 27, 32), (('parameter', 'layer'), 51, 56), (('v number', '200'), 69, 72)], 'The dimensions of the LSTM layer and the attention layer were set to 200'], [[(('parameter', 'layer'), 22, 27), (('v number', '2'), 39, 40)], 'The depth of the LSTM layer was set to 2'], [[(('parameter', 'dropout'), 28, 35), (('parameter', 'layers'), 61, 67)], 'For all methods, we applied Dropout to the input of the LSTM layers'], [[(('parameter', 'dropout'), 4, 11), (('v number', '0.3'), 30, 33)], 'All dropout rates were set to 0.3'], [[(('artifact', 'Adam'), 8, 12), (('parameter', 'learning rate'), 29, 42), (('v number', '0.001'), 46, 51)], 'We used Adam with an initial learning rate of 0.001 as our optimizer'], [[(('parameter', 'epoch'), 21, 26), (('v number', '20'), 38, 40)], 'The maximum training epoch was set to 20'], [[(('parameter', 'batch size'), 17, 27), (('parameter', 'epoch'), 114, 119), (('v number', '16'), 39, 41)], 'The maximum mini-batch size was set to 16, and the order of mini-batches was shuffled at the end of each training epoch']] \n",
      "\n",
      "[[[(('artifact', 'model'), 35, 40), (('v number', '90'), 159, 161)], 'In the same vein, we evaluate each model based on their performance on the SQuAD task.The scores on UQA correlate well with the scores on SQuAD, with close to 90 F1 for most models.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 14, 20), (('parameter', 'layer'), 58, 63), (('v number', '250'), 94, 97)], 'Both the LSTM layers from the decoder and the feedforward layer that processes the spans used 250 hidden units'], [[(('artifact', 'model'), 16, 21), (('v number', '50'), 141, 143), (('v number', '128'), 148, 151)], 'Versions of the model using learned word embeddings and character based CNN representations used the same parameters as for the POS tagger – 50 and 128, respectively.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 34, 38), (('parameter', 'learning rate'), 90, 103), (('v number', '0.001'), 107, 112), (('v number', '0.9'), 124, 127), (('v number', '0.999'), 139, 144)], 'For training the networks, we use Adam optimizer, whose parameters are empirically set as learning rate is 0.001 , beta1 is 0.9 , beta2 is 0.999 '], [[(('parameter', 'batch size'), 49, 59), (('v number', '64'), 63, 65)], 'Training data are split into mini-batches with a batch size of 64']] \n",
      "\n",
      "[[[(('artifact', 'model'), 111, 116), (('v number', '1'), 5, 6)], 'With 1 NVIDIA Tesla V100 GPU, it takes a couple of days to train with our implementation of the style transfer model'], [[(('parameter', 'batch size'), 9, 19), (('v number', '256'), 23, 26), (('v number', '30'), 57, 59)], 'We use a batch size of 256, a maximum sequence length of 30, a vocab size of 20K, and word-level tokenization'], [[(('parameter', 'steps'), 21, 26), (('parameter', 'steps'), 48, 53), (('v number', '100'), 13, 16), (('v number', '000'), 17, 20), (('v number', '100'), 44, 47)], 'We train for 100,000 steps and evaluate for 100 steps.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 7, 11), (('parameter', 'learning rate'), 38, 51), (('v number', '0.0001'), 55, 61)], 'We use Adam optimizer with an initial learning rate of 0.0001']] \n",
      "\n",
      "[[[(('artifact', 'method'), 17, 23), (('parameter', 'learning rate'), 36, 49), (('v number', '0.1'), 71, 74), (('v number', '0.001'), 78, 83)], 'The optimization method is SGDR and learning rate is set to decay from 0.1 to 0.001 '], [[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 61, 67), (('v number', '5'), 24, 25), (('v number', '100'), 57, 60)], 'We train the model with 5 cycles, each of which contains 100 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 0, 13), (('artifact', 'linear decay'), 28, 40), (('v number', '0.0001'), 16, 22), (('v number', '0.95'), 46, 50)], 'Learning rate = 0.0001 with linear decay rate 0.95 per 50k iterations'], [[(('parameter', 'batch size'), 0, 10), (('v number', '128'), 21, 24)], 'Batch size is set to 128']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 58, 63), (('v number', '8'), 186, 187), (('v number', '192'), 188, 191)], 'Pre-training: We train amber on the Wikipedia data for 1M steps first using the default hyper-parameters as mBERThttps:github.comgoogle-researchbert except that we use a larger batch of 8,192 sentence pairs, as this has proven effective in '], [[(('artifact', 'model'), 30, 35), (('parameter', 'steps'), 69, 74), (('v number', '2'), 91, 92), (('v number', '048'), 93, 96), (('v number', '15'), 181, 183)], 'We then continue training the model by our objectives for another 1M steps with a batch of 2,048 sentence pairs from Wikipedia corpus and parallel corpus which is used to train XLM-15 '], [[(('parameter', 'steps'), 107, 112), (('parameter', 'learning rate'), 127, 140), (('artifact', 'linear decay'), 155, 167), (('parameter', 'learning rate'), 175, 188), (('v number', '256'), 84, 87), (('v number', '1e-4'), 144, 148)], 'We set the maximum number of subwords in the concatenation of each sentence pair to 256 and use 10k warmup steps with the peak learning rate of 1e-4 and a linear decay of the learning rate']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 71, 76), (('parameter', 'layers'), 122, 128)], 'Following Gatys et al., we take the output of the fourth convolutional layer for content and the outputs of convolutional layers one through five for style ']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 59, 71), (('v number', '0.9'), 51, 54)], 'We train all models using SGD with a momentum of = 0.9 and weight decay'], [[(('parameter', 'batch size'), 20, 30), (('parameter', 'learning rate'), 64, 77), (('parameter', 'epochs'), 130, 136), (('parameter', 'weight decay'), 146, 158), (('v number', '100'), 34, 37), (('v number', '0.1'), 81, 84), (('v number', '0.1'), 97, 100), (('v number', '250'), 126, 129), (('v number', '0.0005'), 162, 168)], 'For MNIST, we use a batch size of 100 and train with an initial learning rate of 0.1 decaying by 0.1 at every 25k batches for 250 epochs, and use weight decay of 0.0005 '], [[(('parameter', 'batch size'), 25, 35), (('parameter', 'learning rate'), 69, 82), (('parameter', 'epochs'), 128, 134), (('parameter', 'epochs'), 143, 149), (('v number', '10100'), 10, 15), (('v number', '64'), 39, 41), (('v number', '0.1'), 86, 89), (('v number', '0.1'), 102, 105), (('v number', '160'), 139, 142)], 'For CIFAR-10100 we use a batch size of 64, and train with an initial learning rate of 0.1 decaying by 0.1 at the 80th and 120th epochs for 160 epochs'], [[(('parameter', 'weight decay'), 11, 23), (('v number', '0.0001'), 40, 46)], 'We set the weight decay parameter to be 0.0001 '], [[(('parameter', 'batch size'), 32, 42), (('parameter', 'weight decay'), 53, 65), (('parameter', 'steps'), 85, 90), (('parameter', 'learning rate'), 96, 109), (('parameter', 'learning rate'), 134, 147), (('parameter', 'steps'), 170, 175), (('v number', '32'), 46, 48), (('v number', '0.0005'), 66, 72), (('v number', '120'), 77, 80), (('v number', '000'), 81, 84), (('v number', '0.001'), 113, 118), (('v number', '80'), 151, 153), (('v number', '000'), 154, 157), (('v number', '100'), 162, 165), (('v number', '000'), 166, 169)], 'For Pascal VOC, we train with a batch size of 32 and weight decay 0.0005 for 120,000 steps at a learning rate of 0.001 decreasing the learning rate at 80,000 and 100,000 steps']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 169, 182), (('parameter', 'batch size'), 280, 290), (('parameter', 'epochs'), 312, 318), (('v number', '0.9'), 58, 61), (('v number', '0.9'), 74, 77), (('v number', '0.1'), 93, 96), (('v number', '0.9997'), 134, 140), (('v number', '0.001'), 156, 161), (('v number', '0.004'), 186, 191), (('v number', '4e-5'), 216, 220), (('v number', '0.7'), 254, 257), (('v number', '2.0'), 271, 274), (('v number', '32'), 294, 296), (('v number', '200'), 308, 311)], 'The training used RMSprop optimization with a momentum of 0.9, a decay of 0.9 and epsilon of 0.1; batch normalization with a decay of 0.9997 and epsilon of 0.001; fixed learning rate of 0.004; L2 regularization with 4e-5 weight; focal loss with alpha of 0.7 and gamma of 2.0; and batch size of 32 images and 200 epochs for training']] \n",
      "\n",
      "[[[(('parameter', 'learning rates'), 28, 42), (('v number', '1e-06'), 54, 59), (('v number', '5'), 64, 65), (('v number', '0.0001'), 66, 72)], 'The initial and the maximum learning rates are set to 1e-06 and 5 0.0001 respectively']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 33, 38), (('artifact', 'Adam'), 48, 52), (('v number', '100'), 29, 32)], 'The networks are trained for 100 epoch by using Adam algorithm to optimize the loss functions'], [[(('parameter', 'batch size'), 4, 14), (('v number', '4'), 25, 26), (('v number', '8'), 80, 81)], 'The batch size is set as 4 for the training of completion network and is set as 8 for the training of super-resolution network'], [[(('parameter', 'layers'), 19, 25), (('parameter', 'layers'), 126, 132), (('v number', '7'), 96, 97), (('v number', '5'), 124, 125)], 'The numbers of the layers in the encoding and decoding parts of the completion network are both 7 as shown in Fig.REF , and 5 layers are employed in the super-resolution network as shown in Fig.REF .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 59, 65), (('v number', '100'), 55, 58)], 'The models are initialized with weights pretrained for 100 epochs on the labeled training set'], [[(('artifact', 'model'), 18, 23), (('parameter', 'epochs'), 31, 37), (('parameter', 'learning rate'), 74, 87), (('v number', '10'), 28, 30), (('v number', '0.9'), 62, 65), (('v number', '0.001'), 91, 96)], 'We fine-tune each model for 10 epochs using SGD with momentum 0.9 , and a learning rate of 0.001 '], [[(('parameter', 'batch size'), 9, 19), (('v number', '128'), 23, 26)], 'We use a batch size of 128 for all scenarios, unless explicitly stated otherwise']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 75, 88), (('parameter', 'epochs'), 107, 113), (('parameter', 'epochs'), 166, 172), (('parameter', 'learning rate'), 177, 190), (('v number', '0.001'), 92, 97), (('v number', '25'), 104, 106), (('v number', '10'), 163, 165)], 'Aiming to train the base network, we use the SGD optimizer with an initial learning rate of 0.001, with 25 epochs as maximum number of training while during every 10 epochs the learning rate decays to half of the previous'], [[(('parameter', 'learning rate'), 84, 97), (('parameter', 'epochs'), 141, 147), (('parameter', 'learning rate'), 152, 165), (('parameter', 'epochs'), 212, 218), (('v number', '0.01'), 111, 115), (('v number', '20'), 138, 140), (('v number', '10'), 209, 211)], 'Similarly, for the attention module, the SGD optimizer was selected with an initial learning rate to be set at 0.01, at maximum number of 20 epochs and learning rate which decays to half of the previous every 10 epochs'], [[(('parameter', 'batch size'), 40, 50), (('v number', '256'), 54, 57)], 'We implement the two networks using the batch size of 256.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 42, 46), (('parameter', 'learning rate'), 62, 75), (('parameter', 'batch size'), 88, 98), (('v number', '1e-4'), 79, 83), (('v number', '64'), 102, 104)], 'The investigated models are trained using Adam optimizer with learning rate of 1e-4 and batch size of 64'], [[(('parameter', 'epochs'), 29, 35), (('parameter', 'epochs'), 178, 184), (('parameter', 'epochs'), 261, 267), (('v number', '100'), 39, 42), (('v number', '5'), 88, 89), (('v number', '41'), 160, 162), (('v number', '19'), 164, 166), (('v number', '21'), 168, 170), (('v number', '29'), 175, 177), (('v number', '23'), 243, 245), (('v number', '19'), 247, 249), (('v number', '23'), 251, 253), (('v number', '18'), 258, 260)], 'We set the initial number of epochs to 100 and the early stopping patience parameter to 5, causing DeepIrisNet, MobileNetV3, DenseNet, and ResNet to stop after 41, 19, 21 and 29 epochs, respectively on normalized iris training data, and after 23, 19, 23 and 18 epochs, respectively for the coarsely segmented iris images.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 18, 23), (('artifact', 'model'), 47, 52)], 'When retraining a model with frequency r , the model is optionally fine-tuned initially, then repeatedly fine-tuned on queries r, 2r, 3r, until the query budget is reached']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 32, 45), (('parameter', 'learning rate'), 133, 146)], 'For the first stage, an initial learning rate of is used which is later divided by following the standard step decay process for the learning rate']] \n",
      "\n",
      "[[[(('artifact', 'model'), 21, 26), (('parameter', 'layer'), 53, 58), (('parameter', 'layers'), 147, 153), (('parameter', 'activation'), 254, 264), (('parameter', 'layers'), 276, 282), (('parameter', 'layers'), 352, 358), (('parameter', 'activation'), 421, 431), (('parameter', 'layer'), 443, 448), (('v number', '6'), 169, 170), (('v number', '16'), 197, 199), (('v number', '128'), 369, 372)], 'We use four types of model in the experiments: multi-layer perceptron MLP , LeNet5 , a convolutional neural networks CNN1 with two 3x3 convolution layers the first with 6 channels, the second with 16 channals, each followed with 2x2 max pooling and ReLu activation and two FC layers, and a convolutional neural networks CNN2 with three 3x3 convolution layers each with 128 channels followed with 2x2 max pooling and ReLu activation and one FC layer']] \n",
      "\n",
      "[[[(('parameter', 'K'), 70, 71), (('v number', '9'), 34, 35), (('v number', '10'), 74, 76), (('v number', '0.3'), 137, 140), (('v number', '1'), 146, 147), (('v number', '168'), 247, 250), (('v number', '13'), 284, 286)], \"For example, when we condition on 9 properties in QM9, while sampling K = 10 values of _j for each _i when evaluating the loss, we set = 0.3 and =1 : under these values for any given from the training set we have a minimum of one and a maximum of 168 suggested 's, with an average of 13\"]] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 6, 12), (('v number', '5'), 4, 5)], 'for 5 epochs']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 76, 82), (('parameter', 'layer'), 102, 107), (('v number', '10'), 73, 75), (('v number', '20'), 87, 89)], 'We performed the comparison using force-complete data and a network with 10 layers and 20 neurons per layer network iii']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 6, 10), (('parameter', 'batch size'), 48, 58), (('v number', '64'), 69, 71)], 'Using Adam parameter optimization strategy, the batch size is set to 64'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.0001'), 28, 34)], 'The learning rate begins at 0.0001'], [[(('parameter', 'learning rate'), 70, 83), (('parameter', 'epoch'), 114, 119), (('parameter', 'epochs'), 135, 141), (('v number', '776'), 13, 16), (('v number', '10'), 98, 100), (('v number', '10'), 104, 106), (('v number', '17'), 111, 113), (('v number', '22'), 145, 147)], 'For the VeRi-776 dataset, MSM T17 dataset and market1501 dataset, the learning rate is divided by 10 at 10 and 17 epoch, and the total epochs is 22'], [[(('parameter', 'learning rate'), 31, 44), (('parameter', 'epoch'), 75, 80), (('parameter', 'epochs'), 96, 102), (('v number', '10'), 59, 61), (('v number', '50'), 65, 67), (('v number', '60'), 72, 74), (('v number', '65'), 106, 108)], 'For the VehicleID dataset, the learning rate is divided by 10 at 50 and 60 epoch, and the total epochs is 65.']] \n",
      "\n",
      "[[[(('parameter', 'T'), 28, 29), (('v number', '1.5'), 30, 33)], 'REF ,REF ,REF , temperature T=1.5 in Eq']] \n",
      "\n",
      "[[[(('parameter', 'm'), 53, 54), (('v number', '5'), 56, 57), (('v number', '0.2'), 81, 84), (('v number', '4.0'), 90, 93)], 'For the hyper-parameters, we use positive key number M^=5 , softmax temperature =0.2 and =4.0 in Eq.REF see definitions in section REF '], [[(('parameter', 'layer'), 16, 21), (('parameter', 'layer'), 35, 40), (('parameter', 'layer'), 82, 87), (('v number', '50'), 98, 100)], 'We attach a two-layer MLP Multiple Layer Perceptrons on top of the global pooling layer of ResNet-50 for generating the final embeddings'], [[(('parameter', 'batch size'), 4, 14), (('v number', '512'), 27, 30), (('v number', '8'), 77, 78)], 'The batch size is set to N=512 that enables applicable implementations on an 8-GPU machine'], [[(('parameter', 'epochs'), 21, 27), (('parameter', 'learning rate'), 44, 57), (('v number', '200'), 17, 20), (('v number', '0.06'), 64, 68)], 'We train JCL for 200 epochs with an initial learning rate of lr=0.06 and lr is gradually annealed following a cosine decay schedule .']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 10, 23), (('v number', '4'), 132, 133)], 'Number of hidden layers in encoder and decoder stacks, which are the combination of attention and feed-forward sublayers, is set to 4'], [[(('parameter', 'layer'), 28, 33), (('v number', '512'), 68, 71)], 'The dimensionality of inner layer in feed-forward network is set to 512.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 92, 98), (('v number', '20'), 89, 91)], 'All models are implemented using the Transformers package , and trained for a maximum of 20 epochs'], [[(('artifact', 'Adam'), 31, 35), (('artifact', 'linear warmup'), 51, 64)], 'Training is performed using an Adam optimizer with linear warmup '], [[(('parameter', 'batch size'), 19, 29), (('parameter', 'batch size'), 78, 88), (('v number', '16'), 33, 35), (('v number', '4'), 92, 93)], 'We also simulate a batch size of 16 using gradient accumulation and an actual batch size of 4'], [[(('parameter', 'learning rate'), 14, 27), (('parameter', 'learning rate'), 60, 73), (('v number', '2'), 31, 32), (('v number', '1e-05'), 33, 38), (('v number', '6.25'), 77, 81), (('v number', '1e-05'), 82, 87)], 'For GPT2, the learning rate is 2*1e-05 and for GPT we use a learning rate of 6.25*1e-05 '], [[(('parameter', 'K'), 20, 21), (('v number', '1'), 22, 23)], 'We retrieve the top k=1 inferences from memory.For GPT2 we use memory during training and decoding'], [[(('parameter', 'Version'), 26, 33), (('artifact', 'model'), 46, 51)], 'We use the 124M parameter version of the GPT2 model.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 39, 49), (('v number', '1024'), 53, 57), (('v number', '4000'), 107, 111)], 'For all our experiments, we maintain a batch size of 1024 and trained all the networks using an RTX Quadro 4000 GPU.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 9, 19), (('v number', '12'), 30, 32)], 'The mini-batch size is set to 12'], [[(('artifact', 'model'), 35, 40), (('v number', '10'), 11, 13)], 'We use L_1+10*L_ loss to train the model']] \n",
      "\n",
      "[[[(('parameter', 'K'), 102, 103), (('parameter', 'K'), 111, 112), (('v number', '12'), 96, 98), (('v number', '2'), 106, 107), (('v number', '2'), 121, 122)], 'Supposing the output dimension is O , we can derive the number of parameters in CDT as: NCDT=[R+12^-1+K R 2^]+[K+12^-1+O 2^]']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 82, 86), (('parameter', 'learning rate'), 135, 148), (('parameter', 'batch size'), 160, 170), (('v number', '30'), 35, 37), (('v number', '000'), 38, 41), (('v number', '0.5'), 105, 108), (('v number', '0.999'), 116, 121), (('v number', '0.0001'), 149, 155), (('v number', '256'), 171, 174)], 'The cGAN and CcGAN are trained for 30,000 iterations on the training set with the Adam optimizer with _1=0.5 and _2=0.999 , a constant learning rate 0.0001 and batch size 256.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 45, 50), (('parameter', 'layer'), 85, 90), (('v number', '64'), 111, 113)], 'For fairness, we use the same neural network model scale as the baseline models: two-layer neural network with 64 hidden units'], [[(('parameter', 'K'), 21, 22), (('parameter', 'K'), 43, 44), (('parameter', 'K'), 85, 86), (('v number', '10'), 45, 47), (('v number', '0.1'), 50, 53), (('v number', '10'), 87, 89), (('v number', '0.2'), 92, 95)], 'We also use the same K and as APPNP, i.e., K=10, =0.1 for three citation graphs, and K=10, =0.2 for co-authorship graph'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 33, 46), (('parameter', 'dropout'), 65, 72), (('artifact', 'model'), 89, 94), (('v number', '0.1'), 55, 58), (('v number', '0.0'), 98, 101)], 'We use the Adam optimizer with a learning rate of lr = 0.1 , The dropout rate for neural model is 0.0 ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 65, 71), (('parameter', 'batch size'), 84, 94), (('v number', '10'), 62, 64), (('v number', '4096'), 79, 83)], 'All models i.e., DIM, DCM, and NLU re-ranker were trained for 10 epochs with a 4096 batch size.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 127, 133), (('v number', '10'), 112, 114)], 'The models are trained until no improvement of the average log-likelihood on the validation set is observed for 10 consecutive epochs.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 52, 56), (('parameter', 'learning rate'), 84, 97), (('parameter', 'learning rate'), 147, 160), (('v number', '1.0'), 101, 104), (('v number', '3'), 185, 186)], 'We directly utilized the default implementations of Adam optimizer with the initial learning rate of 1.0 and an adaptive scheduler that halves the learning rate with patience parameter 3 based on the sum of the avgADE and avgFDE in validation'], [[(('parameter', 'batch size'), 4, 14), (('artifact', 'model'), 56, 61), (('parameter', 'batch size'), 97, 107), (('v number', '64'), 18, 20), (('v number', '4'), 111, 112)], 'The batch size is 64 for all baselines and the proposed model except AttGlobal-CAM-NF, where the batch size of 4 is used'], [[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 35, 41), (('artifact', 'model'), 109, 114), (('v number', '100'), 31, 34)], 'We train the model for maximum 100 epochs, however, the early stopping is applied when the over-fitting of a model is observed.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 11, 17), (('parameter', 'layers'), 70, 76), (('v number', '8'), 9, 10), (('v number', '512'), 21, 24), (('v number', '4'), 68, 69), (('v number', '128'), 80, 83)], \"We apply 8 layers of 512 hidden units for the `Base' component, and 4 layers of 128 hidden units for both `Static` and “Transient Uncertainty' components; see Figure REF \"], [[(('artifact', 'Adam'), 24, 28), (('parameter', 'batch size'), 60, 70), (('parameter', 'learning rate'), 94, 107), (('parameter', 'fold'), 132, 136), (('v number', '300'), 34, 37), (('v number', '000'), 38, 41), (('v number', '2048'), 74, 78), (('v number', '0.001'), 111, 116), (('v number', '150'), 143, 146), (('v number', '000'), 147, 150)], 'Models are trained with Adam over 300,000 iterations with a batch size of 2048 and an initial learning rate of 0.001 , decaying ten-fold every 150,000 iterations.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 4, 14), (('parameter', 'epochs'), 43, 49), (('v number', '2'), 18, 19), (('v number', '7'), 41, 42)], 'The batch size is 2 and we fine-tune for 7 epochs'], [[(('parameter', 'batch size'), 28, 38), (('parameter', 'epoch'), 67, 72), (('v number', '1'), 42, 43), (('v number', '1'), 65, 66)], 'For SQuAD pre-training, the batch size is 1 and we fine-tune for 1 epoch'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 54, 67), (('v number', '0.9'), 38, 41), (('v number', '0.999'), 42, 47), (('v number', '5e-5'), 71, 75)], 'We use the Adam optimizer with _1, _2=0.9,0.999 and a learning rate of 5e-5']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 118, 124), (('v number', '201'), 52, 55), (('v number', '201'), 114, 117)], 'In our experiment, we use a well-known CNN DenseNet-201 , which is a densely connected convolutional network with 201 layers'], [[(('parameter', 'm'), 32, 33), (('v number', '10'), 36, 38), (('v number', '10'), 39, 41), (('v number', '12'), 44, 46), (('v number', '12'), 47, 49), (('v number', '14'), 52, 54), (('v number', '14'), 55, 57), (('v number', '16'), 60, 62), (('v number', '16'), 63, 65), (('v number', '18'), 69, 71), (('v number', '18'), 72, 74)], 'The images are partitioned into m = 10 10 , 12 12 , 14 14 , 16 16 or 18 18 patches']] \n",
      "\n",
      "[[[(('artifact', 'model'), 27, 32), (('parameter', 'layer'), 180, 185), (('v number', '2018'), 83, 87), (('v number', '2048'), 214, 218)], 'We use the best-performing model from the project page trained on Google Landmarks 2018 data , consisting of a ResNet101 trunk followed by generalized-mean pooling and a whitening layer, which produces features of 2048 dimensions']] \n",
      "\n",
      "[[[(('artifact', 'system'), 4, 10), (('v number', '0.5'), 94, 97), (('v number', '0.2'), 102, 105), (('v number', '0.01'), 107, 111), (('v number', '1.0'), 117, 120), (('v number', '0.0'), 126, 129)], 'The system models described in the previous section are used with the following parameters : =0.5 , V=0.2, 0.01 , r_=1.0 , _0=0.0 [ms]'], [[(('parameter', 'm'), 36, 37), (('v number', '10'), 32, 34), (('v number', '120'), 64, 67)], 'The maximum sensing range is r_=10 [m] and its field of view is 120 degrees'], [[(('parameter', 'T'), 127, 128), (('v number', '0'), 60, 61), (('v number', '1'), 62, 63), (('v number', '2'), 64, 65), (('v number', '3'), 66, 67), (('v number', '0'), 74, 75), (('v number', '2'), 79, 80), (('v number', '2'), 82, 83), (('v number', '100'), 139, 142)], 'A set of motion primitives, or the action space, is = v, |v 0,1,2,3 [ms], 0, - 2, 2 [rads] and the time horizon of an episode, T is set to 100'], [[(('artifact', 'model'), 62, 67), (('artifact', 'model'), 83, 88), (('v number', '0.5'), 106, 109), (('v number', '3.5'), 156, 159)], 'In single-target scenarios, the noise constants of the target model and the belief model are set as q=q_b=0.5 , and the maximum target velocity is set as _=3.5 [ms]']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 4, 8), (('parameter', 'batch size'), 34, 44), (('parameter', 'epoch'), 64, 69), (('v number', '8'), 48, 49), (('v number', '50'), 80, 82)], 'The Adam optimizer is used with a batch size of 8 and the total epoch number of 50'], [[(('parameter', 'learning rates'), 4, 18), (('v number', '0.0001'), 66, 72), (('v number', '0.0004'), 77, 83)], 'The learning rates for the generator and discriminator are set to 0.0001 and 0.0004 , respectively'], [[(('parameter', 'm'), 6, 7), (('v range', '[0.03, 0.07]'), 32, 44)], 'REF , M is randomly expanded by [0.03, 0.07] of the image width.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('artifact', 'Adam'), 42, 46)], 'The proposed model, TICPan, trained using ADAM optimizer with default Pytorch parameters and weights were initialized with He normal initialization '], [[(('parameter', 'epochs'), 38, 44), (('parameter', 'learning rate'), 53, 66), (('parameter', 'epochs'), 113, 119), (('v number', '1000'), 33, 37), (('v number', '400'), 109, 112)], 'All experiments were trained for 1000 epochs and the learning rate was initialized with 8e^ with decay after 400 epochs'], [[(('parameter', 'layers'), 14, 20), (('parameter', 'dropout'), 55, 62), (('parameter', 'layer'), 63, 68), (('v number', '0.2'), 43, 46), (('v number', '0.5'), 80, 83)], 'The LeakyReLU layers parameter was set to =0.2 and the dropout layer was set to 0.5 .']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 128, 133), (('v number', '13'), 163, 165), (('v number', '500'), 166, 169), (('v number', '1'), 207, 208), (('v number', '350'), 209, 212)], 'Since the corpus for CWS is much larger than the corpus for NER, the former corpus is randomly sub-sampled during each training epoch, with each sample containing 13,500 sentences for training step one, and 1,350 sentences for training step two'], [[(('artifact', 'model'), 45, 50), (('parameter', 'epochs'), 100, 106), (('v number', '30'), 97, 99)], 'Models are trained until the F1 score of NER model converges on the validation dataset, or up to 30 epochs.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 32, 36), (('parameter', 'learning rate'), 54, 67), (('parameter', 'weight decay'), 79, 91), (('parameter', 'batch size'), 103, 113), (('v number', '0.0002'), 71, 77), (('v number', '0.2'), 95, 98), (('v number', '2048'), 117, 121)], 'For the first stage, we use the Adam optimizer with a learning rate of 0.0002, weight decay of 0.2 and batch size of 2048 examples'], [[(('parameter', 'learning rate'), 9, 22), (('artifact', 'linear warmup'), 48, 61), (('parameter', 'steps'), 70, 75), (('artifact', 'linear decay'), 108, 120), (('parameter', 'steps'), 137, 142), (('v number', '4000'), 65, 69), (('v number', '0.0002'), 87, 93), (('v number', '1.2'), 125, 128)], 'We use a learning rate schedule consisting of a linear warmup of 4000 steps to a value 0.0002 followed by a linear decay for 1.2 million steps'], [[(('parameter', 'learning rate'), 109, 122), (('parameter', 'steps'), 137, 142)], 'In the second stage, we retain the same settings for both rounds of leveraging synthetic data except for the learning rate and number of steps'], [[(('parameter', 'steps'), 46, 51), (('parameter', 'steps'), 104, 109), (('v number', '240'), 91, 94)], 'In the first round, we use the same number of steps, while in the second round we only use 240 thousand steps, a 15th of the original.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 25, 35), (('artifact', 'Adam'), 57, 61), (('parameter', 'learning rate'), 74, 87), (('v number', '100'), 39, 42), (('v number', '1'), 91, 92), (('v number', '0.001'), 93, 98)], 'Empirically, We set mini-batch size as 100, optimizer as Adam and initial learning rate as 1 0.001 '], [[(('parameter', 'layer'), 11, 16), (('v number', '4'), 38, 39)], 'We use two-layer encoder-decoder with 4 heads for two datasets']] \n",
      "\n",
      "[[[(('parameter', 'K'), 23, 24), (('v number', '16'), 49, 51)], 'The number of subspace K is set by experience to 16 for all modules.']] \n",
      "\n",
      "[[[(('parameter', 'fold'), 9, 13), (('v number', '5'), 7, 8)], 'We use 5-fold cross validation on the CT and MRI datasets, and normalized as zero mean and unit variance'], [[(('artifact', 'method'), 44, 50), (('artifact', 'model'), 114, 119)], 'We evaluated two variations of the proposed method: our proposed 3D Wasserstein Distance Guided Domain Adaptation model without content discriminator 3D-WDGDA, and 3D-WDGDA with content discriminator 3D-WDGDA^ .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 10, 15), (('v number', '4'), 39, 40)], \"Flair NER model was trained to predict 4 entities e.g., `Locations LOC', `miscellaneous MISC', `organizations ORG', and `persons PER'\"]] \n",
      "\n",
      "[[[(('parameter', 'steps'), 66, 71), (('v number', '8'), 30, 31), (('v number', '3.2'), 39, 42), (('v number', '12'), 63, 65), (('v number', '4.8'), 72, 75)], 'In the first part, we observe 8 frames 3.2 seconds and predict 12 steps 4.8 seconds'], [[(('artifact', 'model'), 17, 22), (('v number', '1.13'), 49, 53)], 'We implement all model variants using TensorFlow 1.13 '], [[(('parameter', 'learning rate'), 122, 135), (('parameter', 'dropout'), 170, 177), (('v number', '3.50'), 89, 93), (('v number', '16.04'), 111, 116), (('v number', '5e-3'), 139, 143), (('v number', '0.95'), 159, 163), (('v number', '0.80'), 181, 185)], 'The experiments are deployed in a leave-one-out on desktop Intel® XeonR CPU of frequency 3.50-GHz using Ubuntu 16.04 at a learning rate of 5e-3, decay rate of 0.95 and a dropout of 0.80']] \n",
      "\n",
      "[[[(('parameter', 'K'), 93, 94), (('artifact', 'model'), 133, 138), (('v number', '1'), 78, 79), (('v number', '50'), 97, 99)], 'After standardising the dataset to have zero mean and a standard deviation of 1, we generate K = 50 ground truth bands REF using the model driven approach in Section REF ']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 23, 30), (('parameter', 'layers'), 50, 56), (('parameter', 'activation'), 91, 101), (('v number', '0.1'), 39, 42)], 'PALM is trained with a dropout rate of 0.1 on all layers and attention weights, and a GELU activation function used as GPT'], [[(('parameter', 'learning rate'), 4, 17), (('artifact', 'linear warmup'), 39, 52), (('parameter', 'steps'), 72, 77), (('artifact', 'linear decay'), 82, 94), (('v number', '1e-5'), 28, 32)], 'The learning rate is set to 1e-5, with linear warmup over the first 10k steps and linear decay'], [[(('parameter', 'steps'), 69, 74), (('v number', '16'), 35, 37), (('v number', '64'), 107, 109), (('v number', '500'), 138, 141)], 'The pre-training procedure runs on 16 NVIDIA V100 GPU cards for 800K steps, with each minibatch containing 64 sequences of maximum length 500 tokens.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 86, 91), (('v number', '224'), 70, 73), (('v number', '224'), 74, 77)], \"Following the general mobile setting , we set the input image size to 224 224 and the model's FLOPs were constrained to below 600M\"], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.4'), 29, 32), (('v number', '0'), 47, 48)], 'The initial learning rate is 0.4 and decays to 0 by the cosine decay rule'], [[(('parameter', 'weight decay'), 9, 21), (('v number', '4e-5'), 25, 29)], 'Then the weight decay is 4e-5'], [[(('parameter', 'dropout'), 4, 11), (('v number', '0.2'), 20, 23)], 'The dropout rate is 0.2'], [[(('parameter', 'layer'), 56, 61), (('parameter', 'layers'), 109, 115), (('v number', '1'), 62, 63), (('v number', '3'), 65, 66), (('v number', '6'), 68, 69), (('v number', '8'), 71, 72), (('v number', '16'), 78, 80)], \"In order to fit the FDNAS net to ImageNet's image size, Layer 1, 3, 6, 8, and 16 are set to the downsampling layers.\"]] \n",
      "\n",
      "[[[(('artifact', 'L'), 131, 132), (('v number', '1'), 157, 158), (('v number', '1'), 163, 164)], 'To optimize the parameters of the policy network, we use Proximal Policy Optimization PPO with a clipped objective as shown below: L^ = }[minr_t , clipr_t , 1 - , 1 + ]']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 37, 49), (('v number', '0.9'), 25, 28), (('v number', '0.0001'), 60, 66)], 'The momentum is fixed as 0.9 and the weight decay is set to 0.0001'], [[(('parameter', 'learning rate'), 19, 32), (('parameter', 'learning rate'), 58, 71), (('v number', '1'), 89, 90)], 'We employ a \"poly\" learning rate policy where the initial learning rate is multiplied by 1-^ '], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.01'), 36, 40), (('v number', '0.9'), 65, 68)], 'The initial learning rate is set to 0.01 and the power is set to 0.9'], [[(('parameter', 'batch size'), 7, 17), (('artifact', 'model'), 38, 43), (('v number', '16'), 21, 23)], 'We use batch size of 16 and train our model for 40k iterations for NYUDv2 and 60k iterations for Cityscapes']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 32, 38), (('v number', '3'), 30, 31), (('v number', '45'), 110, 112)], 'Hyperparameter setup: We used 3 layers of transformer encoders and decoders with a maximum sequence length of 45 words']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 30, 36), (('v number', '6'), 76, 77)], 'More precisely, the number of layers in the encoder and in the decoder is N=6 '], [[(('parameter', 'layers'), 35, 41), (('v number', '8'), 14, 15)], 'We employ h = 8 parallel attention layers, or heads'], [[(('parameter', 'layer'), 67, 72), (('v number', '512'), 47, 50), (('v number', '2048'), 122, 126)], 'The dimensionality of input and output is d_ = 512 , and the inner-layer of a feed-forward networks has dimensionality d_=2048 .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 95, 100), (('v number', '12'), 185, 187)], 'For the sentence-level embedding extractor, the parameters of BERT framework are pretrainedThe model is available at https:storage.googleapis.combert_models2018_10_18cased_L-12_H-768_A-12.zip'], [[(('parameter', 'layer'), 238, 243), (('v number', '768'), 73, 76), (('v number', '1024'), 106, 110), (('v number', '256'), 224, 227), (('v number', '768'), 261, 264), (('v number', '256'), 280, 283)], 'The CLS output from BERT framework with regard to an input sentence is a 768-dim vectorWe did not use the 1024-dim embedding due to limited RAM on GPU, and the output dimension size of the sentence-level embedding is set to 256 with a FC layer that project the 768-dim vectors to 256-dim vectors'], [[(('parameter', 'layer'), 83, 88), (('v number', '360'), 11, 14), (('v number', '33'), 94, 96)], 'We use the 360 example sentences in the ACE annotation guideline and use a Softmax layer with 33 the number for event types labels and train a sub-framework to predict the existence of an event type in the guideline sentences'], [[(('parameter', 'layer'), 28, 33), (('parameter', 'layer'), 66, 71)], 'After we remove the softmax layer, we consider the output from FC layer as a sentence-level embedding that captures the features indicating the existence of event types.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 119, 129), (('parameter', 'learning rate'), 138, 151), (('parameter', 'epochs'), 185, 191), (('v number', '10'), 81, 83), (('v number', '237'), 99, 102), (('v number', '3264'), 131, 135), (('v number', '0.0011e-05'), 153, 163), (('v number', '1'), 193, 194), (('v number', '510'), 225, 228), (('v number', '0.600'), 243, 248), (('v number', '44'), 249, 251)], 'For the hyperparameters in self-adaptive ensemble scheme, based on the best Hits@10 on WN18RRFB15k-237 dev set, we set batch size =3264 , learning rate =0.0011e-05 , number of training epochs =1 , number of negative samples =510 , and margin =0.600.44 in hinge loss function.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 22, 28), (('parameter', 'layers'), 64, 70)], 'For LIF-SNN, all LIAF layers in the network are replaced by LIF layers'], [[(('parameter', 'layers'), 31, 37), (('parameter', 'layers'), 63, 69)], 'For ConvLSTM network, all LIAF layers are replaced by ConvLSTM layers']] \n",
      "\n",
      "[[[(('parameter', 'K'), 47, 48), (('v number', '24'), 51, 53)], 'Its underlying skeleton is defined as a set of K = 24 joints'], [[(('artifact', 'model'), 13, 18), (('parameter', 'epochs'), 26, 32), (('parameter', 'batch size'), 49, 59), (('parameter', 'epochs'), 84, 90), (('artifact', 'Adam'), 96, 100), (('parameter', 'learning rate'), 115, 128), (('v number', '10'), 23, 25), (('v number', '4'), 63, 64), (('v number', '0.001'), 134, 139)], 'We train the model for 10 epochs with an initial batch size of 4, doubled every two epochs with Adam optimizer and learning rate lr = 0.001 .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 64, 74), (('v number', '64'), 78, 80), (('v number', '50'), 108, 110)], 'For Binary classifiers, we use a single-layered recurrent unit, batch size of 64, hidden, and input size of 50 units'], [[(('parameter', 'learning rate'), 33, 46), (('v number', '0.005'), 50, 55), (('v number', '0.01'), 91, 95)], 'For LSTM and ONLSTM, the initial learning rate is 0.005, while for the GRU and DRNN, it is 0.01']] \n",
      "\n",
      "[[[(('artifact', 'model'), 29, 34), (('v number', '1.5'), 142, 145), (('v number', '2'), 146, 147), (('v number', '10'), 158, 160), (('v number', '000'), 161, 164), (('v number', '1'), 218, 219), (('v number', '10'), 229, 231), (('v number', '000'), 232, 235)], 'The time taken to train each model varies based on the number of training samples—as a rule of thumb, training the learned models takes about 1.5-2 hours per 10,000 samples, while training the fixed models takes about 1 hour per 10,000 samples'], [[(('artifact', 'model'), 20, 25), (('v number', '24'), 89, 91)], 'Training all of the model variants necessary to reproduce this paper in full takes about 24 hours'], [[(('artifact', 'model'), 15, 20), (('v number', '1'), 57, 58), (('v number', '000'), 59, 62)], 'Testing either model type takes only several minutes per 1,000 samples.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 74, 78), (('parameter', 'weight decay'), 102, 114), (('v number', '0.01'), 118, 122)], 'Our entire MVAF-Net network is end to-end trainable, and it is trained by ADAM optimizer with a fixed weight decay of 0.01'], [[(('parameter', 'learning rate'), 57, 70), (('v number', '3e-3'), 78, 82), (('v number', '10'), 104, 106), (('v number', '0.95'), 136, 140), (('v number', '0.85'), 144, 148)], 'The learning schedule is a one-cycle policy with the max learning rate set to 3e-3, the division factor 10, and the momentum range from 0.95 to 0.85'], [[(('parameter', 'batch size'), 9, 19), (('artifact', 'model'), 41, 46), (('parameter', 'epochs'), 65, 71), (('v number', '2'), 30, 31), (('v number', '40'), 62, 64)], 'The mini-batch size is set to 2, and the model is trained for 40 epochs']] \n",
      "\n",
      "[[[(('artifact', 'model'), 16, 21), (('parameter', 'Version'), 57, 64), (('parameter', 'layers'), 103, 109), (('v number', '6'), 101, 102), (('v number', '12'), 121, 123), (('v number', '768'), 161, 164)], 'For the RoBERTa model , we use the pre-trained distilled version proposed in sanh2019distilbert with 6 layers containing 12 attention heads and a hidden size of 768, as implemented by Wolf2019HuggingFacesTS'], [[(('artifact', 'AdamW'), 27, 32), (('parameter', 'learning rate'), 50, 63), (('parameter', 'weight decay'), 79, 91), (('v number', '0.001'), 67, 72), (('v number', '0.01'), 101, 105)], 'During training we use the AdamW optimizer with a learning rate of 0.001 and a weight decay value of 0.01 for both pretraining and fine-tuning'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'steps'), 71, 76), (('v number', '4000'), 58, 62)], 'The learning rate was scheduled as in NIPS20177181, using 4000 warm-up steps'], [[(('artifact', 'model'), 4, 9), (('parameter', 'epochs'), 138, 144), (('v number', '3'), 124, 125)], 'All model configurations are trained by early stopping if the performance of neither structure nor nuclearity improves over 3 consecutive epochs on the development dataset']] \n",
      "\n",
      "[[[(('parameter', 'K'), 11, 12), (('parameter', 'K'), 25, 26), (('v number', '1'), 8, 9)], 'p^}}_}|=1, K, } = _^ __}}k }} _};_h}|} =']] \n",
      "\n",
      "[[[(('artifact', 'model'), 233, 238), (('v number', '1'), 168, 169)], 'This technique lets the agent gradually learn the best moves to obtain victory: for instance, in the first phase it is very easy to win the game, as the enemy has only 1 HP and only one attack is needed to defeat it; in this way the model can learn to reach its objective without worrying too much about other variables']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 50, 60), (('v number', '64'), 61, 63)], 'The SIP subnet is first pre-trained on DIV2K data batch size 64, then the complete MVA+SIP network is fine-tuned end-to-end on the 3D training set'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 31, 44), (('v number', '0.0001'), 45, 51)], 'We use the Adam optimizer with learning rate 0.0001 '], [[(('parameter', 'batch size'), 31, 41), (('v number', '4'), 53, 54)], 'Due to memory limitations, the batch size was set to 4 for the multi-view dataset.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 13, 18), (('v number', '244'), 104, 107), (('v number', '2500'), 120, 124)], 'The proposed model was trained end-to-end to perform relative lane ID estimation on a set consisting of 244 sequence of 2500 image each 600k images']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 24, 30), (('v number', '4'), 22, 23)], 'First, the DAPnet has 4 layers of feature abstraction'], [[(('parameter', 'layers'), 23, 29), (('v number', '3'), 7, 8)], 'We use 3 convolutional layers for feature extraction on these groups, and the number of convolution kernels is gradually increased'], [[(('parameter', 'layers'), 22, 28), (('v number', '3'), 6, 7)], 'After 3 convolutional layers, we apply max-pooling to obtain the global features of each group L_1 '], [[(('parameter', 'layers'), 16, 22), (('v number', '4'), 14, 15)], 'The following 4 layers are feature propagation']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('artifact', 'Adam'), 54, 58), (('v number', '0.0001'), 33, 39), (('v number', '0.5'), 69, 72), (('v number', '0.999'), 84, 89)], 'The learning rate we employed is 0.0001 , whereas the Adam betas are 0.5 for _1 and 0.999 for _2 ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 87, 92), (('parameter', 'batch size'), 112, 122), (('parameter', 'learning rate'), 135, 148), (('v number', '128'), 123, 126), (('v number', '0.0001'), 152, 158)], 'We randomly initialize the weights following a Gaussian distribution, and optimize the model using RMSProp with batch size 128, with a learning rate of 0.0001']] \n",
      "\n",
      "[[[(('artifact', 'L'), 44, 45), (('parameter', 'K'), 64, 65), (('parameter', 'dropout'), 82, 89), (('v number', '100'), 46, 49), (('v number', '12'), 66, 68), (('v number', '0'), 95, 96)], 'We follow the same setup as in , with depth L=100 , growth rate k=12 Dense-BC and dropout rate 0'], [[(('parameter', 'weight decay'), 57, 69), (('v number', '0.9'), 46, 49), (('v number', '0.0001'), 92, 98)], 'Specifically, we train Dense-BC with momentum 0.9 and _2 weight decay with a coefficient of 0.0001 '], [[(('parameter', 'epochs'), 30, 36), (('parameter', 'epochs'), 86, 92), (('v number', '10'), 27, 29), (('v number', '10'), 48, 50), (('v number', '100'), 61, 64), (('v number', '100'), 82, 85)], 'For GTSRB, we train it for 10 epochs; for CIFAR-10 and CIFAR-100, we train it for 100 epochs'], [[(('parameter', 'batch size'), 36, 46), (('parameter', 'batch size'), 91, 101), (('v number', '64'), 47, 49), (('v number', '128'), 102, 105)], 'For in-distribution dataset, we use batch size 64; For outlier exposure with _}^} , we use batch size 128'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'learning rate'), 59, 72), (('v number', '0.1'), 29, 32)], 'The initial learning rate of 0.1 decays following a cosine learning rate schedule .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 28, 33), (('parameter', 'hidden layers'), 69, 82), (('v number', '12'), 66, 68), (('v number', '768'), 86, 89), (('v number', '12'), 100, 102)], 'We use the pre-trained BERT model [BERT-Base, Uncased], which has 12 hidden layers of 768 units and 12 self-attention heads to encode utterances and schema descriptions'], [[(('parameter', 'dropout'), 4, 11), (('v number', '0.1'), 27, 30)], 'The dropout probability is 0.1'], [[(('parameter', 'batch size'), 4, 14), (('v number', '8'), 25, 26)], 'The batch size is set to 8'], [[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 46, 59), (('v number', '1e-4'), 63, 67)], 'Adam is used for optimization with an initial learning rate of 1e-4']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 64, 70), (('parameter', 'layers'), 118, 124), (('parameter', 'layers'), 252, 258), (('v number', '1'), 149, 150), (('v number', '2'), 164, 165)], 'This is shown in equation REF where } , and } are the number of epochs that WUS phase i.e., only biases of the last x layers are updated; where x is 1 for 1L; x is 2 for 2L, and so on, and normal training phase i.e., both weights and biases of the all layers are updated are employed, respectively']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 61, 66), (('v number', '50'), 28, 30), (('v number', '3'), 59, 60)], 'Specifically, we use ResNet-50 as the base encoder, with a 3-layer MLP projection head'], [[(('parameter', 'K'), 99, 100), (('v number', '10'), 111, 113), (('v number', '0.7'), 133, 136)], 'We use the attraction strategy with max aggregation for false negative cancellation, while the top-k is set to 10 and a threshold of 0.7 is used for filtering the scores'], [[(('parameter', 'epochs'), 21, 27), (('parameter', 'batch size'), 53, 63), (('v number', '1000'), 16, 20), (('v number', '128'), 31, 34), (('v number', '4096'), 67, 71)], 'We pretrain for 1000 epochs on 128 Cloud TPUs with a batch size of 4096'], [[(('parameter', 'learning rate'), 33, 46), (('parameter', 'learning rate'), 90, 103), (('parameter', 'weight decay'), 111, 123), (('v number', '6.4'), 50, 53), (('v number', '1'), 127, 128), (('v number', '0.0001'), 129, 135)], 'We use the LARS optimizer with a learning rate of 6.4, a cosine schedule for decaying the learning rate, and a weight decay of 1 0.0001 .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 81, 91), (('v number', '16'), 95, 97), (('v number', '5'), 147, 148)], 'For training the localisation network, we follow the settings in , training with batch size of 16, where each sample in the batch in a sequence of 5 consecutive RGB frames'], [[(('artifact', 'Adam'), 11, 15), (('v number', '0.5'), 68, 71), (('v number', '0.999'), 76, 81)], 'We use the ADAM optimiser , with first and second momentum terms of 0.5 and 0.999 values respectively']] \n",
      "\n",
      "[[[(('artifact', 'model'), 21, 26), (('artifact', 'Adam'), 59, 63)], 'We trained the G-LBM model using gradient descent based on Adam optimization with respect to the parameters of the encoder and decoder, i.e., and , respectively'], [[(('parameter', 'learning rate'), 12, 25), (('artifact', 'gradient clipping'), 41, 58)], 'We employed learning rate scheduling and gradient clipping in the optimization setup'], [[(('parameter', 'epochs'), 133, 139), (('v number', '3'), 42, 43), (('v number', '40'), 61, 63), (('v number', '120'), 87, 90), (('v number', '500'), 129, 132)], 'Training was performed on batches of size 3 video clips with 40 consecutive frames i.e 120 video frames in every input batch for 500 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'weight decay'), 70, 82), (('v number', '5'), 86, 87), (('v number', '0.0001'), 88, 94), (('v number', '0.9'), 111, 114)], 'All networks are trained using stochastic gradient descent SGD with a weight decay of 5 0.0001 and momentum of 0.9'], [[(('parameter', 'epochs'), 46, 52), (('parameter', 'batch size'), 66, 76), (('v number', '10'), 9, 11), (('v number', '100'), 22, 25), (('v number', '300'), 42, 45), (('v number', '128'), 80, 83)], 'On CIFAR-10 and CIFAR-100, we trained for 300 epochs, with a mini-batch size of 128'], [[(('parameter', 'learning rate'), 12, 25), (('parameter', 'epochs'), 89, 95), (('v number', '0.1'), 36, 39), (('v number', '0.1'), 67, 70), (('v number', '120'), 77, 80), (('v number', '240'), 85, 88)], 'The initial learning rate is set to 0.1 and decayed by a factor of 0.1 after 120 and 240 epochs'], [[(('parameter', 'dropout'), 0, 7), (('parameter', 'layers'), 58, 64), (('parameter', 'dropout'), 73, 80), (('v number', '0.5'), 90, 93)], 'Dropout regularization is employed in the fully-connected layers, with a dropout ratio of 0.5.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 125, 130), (('v number', '64'), 141, 143)], 'In our experiments, we implement XDBoost with TensorFlow.Tensorflow: https:www.tensorflow.org The dimension of the embedding layer is set to 64'], [[(('artifact', 'Adam'), 32, 36), (('parameter', 'batch size'), 59, 69), (('parameter', 'learning rate'), 88, 101), (('v number', '1'), 73, 74), (('v number', '024'), 75, 78), (('v number', '0.0001'), 112, 118)], 'For the optimization we use the Adam optimizer with a mini-batch size of 1,024, and the learning rate is set to 0.0001'], [[(('parameter', 'layer'), 25, 30), (('parameter', 'activation'), 62, 72), (('parameter', 'activation'), 113, 123)], 'For all deep models, the layer depth is set to three, and all activation functions are ReLU, except for the last activation which is sigmoid'], [[(('parameter', 'layer'), 26, 31), (('v number', '128'), 35, 38), (('v number', '256'), 65, 68)], 'The number of neurons per layer is 128 for the Avazu dataset and 256 for the Taboola dataset']] \n",
      "\n",
      "[[[(('parameter', 'T'), 22, 23), (('v number', '64'), 47, 49), (('v number', '100'), 53, 56)], 'The number of LSTM or T-LSTM units were set to 64 or 100'], [[(('parameter', 'dropout'), 8, 15), (('parameter', 'layers'), 36, 42), (('v number', '0.0'), 64, 67), (('v number', '0.2'), 72, 75)], 'For the dropout rate for both dense layers, we tried the values 0.0 and 0.2 '], [[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rates'), 25, 39)], 'Adam optimizer estimates learning rates based on initial moments of the gradients'], [[(('parameter', 'learning rates'), 44, 58), (('artifact', 'LSTMs'), 140, 145), (('v number', '0.0001'), 59, 65), (('v number', '0.0002'), 67, 73), (('v number', '0.001'), 75, 80), (('v number', '0.002'), 82, 87), (('v number', '0.01'), 89, 93)], 'Furthermore, we tested a range of different learning rates 0.0001, 0.0002, 0.001, 0.002, 0.01 since this is known to have a large impact on LSTMs '], [[(('artifact', 'model'), 16, 21), (('parameter', 'epochs'), 30, 36), (('parameter', 'batch size'), 45, 55), (('v number', '150'), 26, 29), (('v number', '64'), 59, 61), (('v number', '25'), 101, 103)], 'We trained each model for 150 epochs, with a batch size of 64 and apply early stopping with patience 25 for regularization.']] \n",
      "\n",
      "[[[(('parameter', 'hidden layers'), 37, 50), (('parameter', 'layer'), 65, 70)], 'Assuming the deep neural network has hidden layers, and the i th layer has H_i neurons in DQN, VDQN, NoisyNet, and AVDQN'], [[(('parameter', 'layer'), 27, 32), (('parameter', 'layer'), 70, 75)], 'The dimension of the input layer is I and the dimension of the output layer of AVDQN and VDQN is n_A n_V and n_V , respectively, where n_A is the number of parameters of qQ ']] \n",
      "\n",
      "[[[(('artifact', 'model'), 35, 40), (('parameter', 'epochs'), 91, 97), (('v number', '80'), 88, 90)], 'For the temporal gated convolution model, we use Amsgrad as the optimizer and train for 80 epochs'], [[(('parameter', 'batch size'), 11, 21), (('v number', '1024'), 25, 29)], 'We set the batch size as 1024 in the training'], [[(('parameter', 'learning rate'), 50, 63), (('parameter', 'epoch'), 140, 145), (('v number', '0.001'), 90, 95), (('v number', '0.95'), 119, 123)], 'For Human3.6M, we adopt an exponentially decaying learning rate schedule, starting from = 0.001 with a shrink factor = 0.95 applied on each epoch.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 26, 30), (('parameter', 'batch size'), 36, 46), (('parameter', 'epochs'), 55, 61), (('parameter', 'learning rate'), 74, 87), (('parameter', 'epochs'), 128, 134), (('v number', '5'), 8, 9), (('v number', '256'), 47, 50), (('v number', '140'), 51, 54), (('v number', '0.001'), 91, 96), (('v number', '0.2'), 121, 124), (('v number', '40'), 135, 137), (('v number', '80'), 139, 141), (('v number', '120'), 147, 150)], 'For CNN-5, we train it by Adam with batch size 256 140 epochs; an initial learning rate of 0.001, decayed by a factor of 0.2 at epochs 40, 80, and 120'], [[(('parameter', 'weight decay'), 82, 94), (('parameter', 'epochs'), 140, 146), (('parameter', 'learning rate'), 159, 172), (('parameter', 'epochs'), 211, 217), (('v number', '0.0001'), 98, 104), (('v number', '0.9'), 132, 135), (('v number', '200'), 136, 139), (('v number', '0.1'), 176, 179), (('v number', '0.2'), 204, 207), (('v number', '60'), 218, 220), (('v number', '120'), 222, 225), (('v number', '160'), 231, 234)], 'For residual networks, We use the standard Stochastic Gradient Descent SGD with a weight decay of 0.0001 and a Nesterov momentum of 0.9 200 epochs; an initial learning rate of 0.1, decayed by a factor of 0.2 at epochs 60, 120, and 160'], [[(('parameter', 'm'), 15, 16), (('parameter', 'm'), 72, 73), (('v number', '4'), 17, 18), (('v number', '10'), 32, 34), (('v number', '10'), 74, 76), (('v number', '100'), 90, 93)], 'For KD, we set M=4 on the CIFAR-10, following the experiments , , , and M=10 on the CIFAR-100']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 63, 69), (('v number', '256'), 30, 33), (('v number', '2'), 61, 62), (('v number', '128'), 76, 79)], 'For SEQ2SEQ models, we used a 256 unit hidden size LSTM with 2 layers and a 128 unit input embedding dimension'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '4E-3'), 48, 52)], 'The learning rate we used for all the models is 4E-3'], [[(('parameter', 'layers'), 110, 116), (('v number', '512'), 27, 30), (('v number', '512'), 49, 52), (('v number', '2'), 85, 86), (('v number', '4'), 108, 109)], 'For Transformer, we used a 512 unit hidden size, 512 unit input embedding dimension, 2 attention header and 4 layers']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 41, 54), (('parameter', 'learning rate'), 109, 122), (('v number', '0.01'), 65, 69)], 'We use SGD as the optimizer, the initial learning rate is set to 0.01, and lambda poly is used to adjust the learning rate'], [[(('parameter', 'epochs'), 35, 41), (('parameter', 'epochs'), 88, 94), (('v number', '24'), 32, 34), (('v number', '12'), 85, 87)], 'For most models, we first train 24 epochs on the GLDv2 clean dataset, and then train 12 epochs on the GLDv2 cluster dataset'], [[(('parameter', 'epochs'), 25, 31), (('parameter', 'epochs'), 38, 44), (('v number', '12'), 22, 24), (('v number', '6'), 36, 37)], 'For ResNest200, it is 12 epochs and 6 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 8, 21), (('parameter', 'learning rate'), 34, 47), (('parameter', 'learning rate'), 70, 83), (('parameter', 'epoch'), 114, 119), (('v number', '1e-2'), 25, 29)], 'Initial learning rate is 1e-2 and learning rate decay rate is initial learning rate divided by the number of each epoch'], [[(('parameter', 'epochs'), 14, 20), (('parameter', 'batch size'), 31, 41), (('v number', '32'), 24, 26), (('v number', '32'), 45, 47)], 'The number of epochs is 32 and batch size is 32.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 55, 60), (('v number', '700'), 66, 69)], 'In our active learning setup, we begin by training our model on a 700-document subset of the full training set'], [[(('artifact', 'model'), 55, 60), (('artifact', 'model'), 114, 119)], 'After all documents have been labelled, we retrain our model on the full document set from scratch, resetting all model and trainer parameters.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 13, 17), (('parameter', 'learning rate'), 41, 54), (('v number', '0.0001'), 58, 64)], 'We have used Adam as an optimizer with a learning rate of 0.0001 ']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 19, 24), (('parameter', 'layer'), 58, 63)], \"The encoder's last layer is followed by a fully-connected layer that outputs a vector with dimension D \"], [[(('parameter', 'epochs'), 44, 50), (('v number', '1800'), 39, 43)], 'We trained each network for a total of 1800 epochs'], [[(('parameter', 'learning rate'), 51, 64), (('parameter', 'weight decay'), 112, 124), (('v number', '0.01'), 96, 100), (('v number', '0.001'), 144, 149)], 'For better network convergence, we employed cosine learning rate decay with an initial value of 0.01 as well as weight decay with a rate set to 0.001 '], [[(('parameter', 'dropout'), 9, 16), (('parameter', 'layers'), 105, 111), (('v number', '0.8'), 39, 42)], 'Finally, dropout regularization with a 0.8 keep probability value was applied to all the fully connected layers in the network.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 14, 19), (('v number', '16'), 26, 28)], 'We train each model using 16 Tesla V100-SXM2-16GB GPUs and following the implementation and parameters in the NVIDIA codebaseMore specifically, we adapt these scripts to our needs.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 49, 62), (('v number', '0.01'), 66, 70)], 'For training on both datasets we used an initial learning rate of 0.01 together with the stochastic gradient decent SGD optimizer'], [[(('parameter', 'weight decay'), 37, 49), (('v number', '0.0001'), 52, 58), (('v number', '0.9'), 74, 77)], 'The parameters for the optimizer are weight decay = 0.0001 and momentum = 0.9 '], [[(('parameter', 'epochs'), 24, 30), (('parameter', 'learning rate'), 36, 49), (('v number', '0.1'), 65, 68), (('v number', '1e-06'), 86, 91)], 'After each five hundred epochs, the learning rate was reduced by 0.1 until it reached 1e-06 when the training was stopped']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 53, 59), (('v number', '0.1'), 0, 3), (('v number', '500'), 49, 52)], '0.1, no explicit regularization, and trained for 500 epochs.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 68, 74), (('v number', '512'), 51, 54), (('v number', '2048'), 96, 100), (('v number', '8'), 135, 136)], 'Specifically, we used word embeddings of dimension 512, feedforward layers with inner dimension 2048, and multi-headed attentions with 8 heads.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 74, 79), (('parameter', 'epochs'), 98, 104), (('v number', '500'), 85, 88)], 'For each run, we set a fixed seed for random number generation, build the model, run 500 training epochs, and finally evaluate the classification accuracy on the non-training samples']] \n",
      "\n",
      "[[[(('artifact', 'model'), 12, 17), (('v number', '150'), 44, 47)], 'For the CNN model, we use a kernel width of 150 for all convolutions'], [[(('parameter', 'layer'), 86, 91), (('parameter', 'layer'), 114, 119), (('v number', '150'), 151, 154)], 'To insert pretrained entity embeddings, we replace the candidate document convolution layer, followed by a single layer MLP to reduce the embedding to 150 dimensions'], [[(('parameter', 'dropout'), 9, 16), (('parameter', 'layer'), 39, 44), (('v number', '0.2'), 67, 70)], 'We apply dropout to the word embedding layer with a probability of 0.2.']] \n",
      "\n",
      "[[[(('artifact', 'Triplet Loss'), 122, 134), (('v number', '2'), 30, 31)], 'The GoogleInception, C3D, and 2-D LSTM networks were constructed with Python using the Keras Library with contrastive and triplet loss functions'], [[(('artifact', 'Triplet Loss'), 33, 45), (('v number', '32'), 79, 81), (('v number', '16'), 87, 89)], 'To train the Siamese Network and Triplet Loss networks, we used batch sizes of 32 with 16 identities per batch and two imagessequences per identity'], [[(('artifact', 'Triplet Loss'), 81, 93), (('v number', '16'), 10, 12), (('v number', '16'), 32, 34), (('v number', '32'), 156, 158)], 'There are 16 positive pairs and 16 random negative pairs for Siamese Network and Triplet Loss Network employed semi-hard negative mining on each batch with 32 imagessequences'], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 46, 59), (('v number', '0.0001'), 63, 69), (('v number', '1'), 77, 78), (('v number', '0.99'), 88, 92), (('v number', '2'), 104, 105), (('v number', '0.99'), 115, 119)], 'The Adam optimizer was used from Keras with a learning rate of 0.0001 , beta 1 value of 0.99 , and beta 2 value of 0.99 across the various network architectures.']] \n",
      "\n",
      "[[[(('artifact', 'LSTMs'), 4, 9), (('parameter', 'hidden state'), 48, 60), (('parameter', 'hidden state'), 167, 179), (('v number', '512'), 69, 72), (('v number', '256'), 188, 191)], 'The LSTMs in the encoder-decoder network have a hidden state size of 512, and the LSTM block used to encode the contextual text in the pointer generator network has a hidden state size of 256'], [[(('parameter', 'dropout'), 17, 24), (('v number', '0.5'), 33, 36)], 'During training, dropout rate of 0.5 is applied on the video feature input, embedded word input, embedded context input, and all LSTM outputs'], [[(('artifact', 'Adam'), 35, 39), (('parameter', 'learning rate'), 58, 71)], 'The training is performed with the Adam optimizer using a learning rate of .']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 61, 67), (('v number', '18'), 41, 43), (('v number', '34'), 45, 47), (('v number', '50'), 49, 51), (('v number', '101'), 57, 60), (('v number', '1'), 94, 95)], 'The experiments are conducted on ResNets 18, 34, 50, and 101 layers , AlexNet , SqueezeNet v1.1 , and Deep Compression '], [[(('parameter', 'learning rates'), 105, 119), (('parameter', 'learning rate'), 217, 230), (('v number', '1'), 60, 61), (('v number', '0.01'), 123, 127), (('v number', '0.1'), 130, 133), (('v number', '1'), 136, 137), (('v number', '25'), 259, 261), (('v number', '50'), 264, 266), (('v number', '64'), 288, 290), (('v number', '128'), 295, 298)], 'A grid hyper-parameter search is conducted based on the Top-1 accuracy for all models, including initial learning rates in 0.01,{}0.1,{}1 , Stochastic Gradient Descent SGD and Adadelta optimizer, exponential and step learning rate decays with gamma values in 25,{}50 , and batch sizes of 64 and 128'], [[(('parameter', 'learning rate'), 42, 55), (('parameter', 'epoch'), 71, 76), (('parameter', 'weight decay'), 102, 114), (('v number', '50'), 68, 70), (('v number', '0.1'), 94, 97)], 'The Adadelta optimizer with Step adaptive learning rate step: every 50 epoch at gamma rate of 0.1 and weight decay of 10e^ is used'], [[(('parameter', 'epochs'), 14, 20), (('parameter', 'batch size'), 36, 46), (('v number', '200'), 24, 27), (('v number', '128'), 50, 53)], 'The number of epochs is 200 and the batch size is 128'], [[(('parameter', 'dropout'), 51, 58), (('v number', '0.5'), 74, 77)], 'For the other models, where applicable, the random dropout rate is set to 0.5'], [[(('parameter', 'epochs'), 67, 73), (('v number', '100'), 63, 66)], 'The early state convergence in REF is used with a threshold of 100 epochs']] \n",
      "\n",
      "[[[(('parameter', 'epoch'), 65, 70), (('parameter', 'learning rate'), 71, 84), (('v number', '30'), 62, 64)], 'We follow standard hyperparameters used for fine-tuning , , : 30 epoch learning rate 1e^ on SGD optimizer'], [[(('artifact', 'model'), 32, 37), (('parameter', 'epoch'), 108, 113), (('parameter', 'learning rate'), 127, 140), (('parameter', 'epoch'), 159, 164), (('v number', '160'), 104, 107), (('v number', '0.1'), 141, 144), (('v number', '0.1'), 178, 181), (('v range', '[81, 122]'), 165, 174)], 'as the authors train the pruned model with the standard hyperparameters used for training from scratch: 160 epoch with initial learning rate 0.1 and decays on epoch [81, 122] by 0.1.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 19, 24), (('artifact', 'model'), 129, 134)], 'The learned global model was sent to each device for further fine-tuning with local training data, and then the fine-tuned local model was tested on each device']] \n",
      "\n",
      "[[[(('artifact', 'model'), 12, 17), (('artifact', 'model'), 29, 34), (('parameter', 'layers'), 62, 68), (('parameter', 'layers'), 107, 113), (('v number', '300'), 53, 56)], 'The default model is an LSTM model comprising of two 300-unit layers, and a skip connection over both LSTM layers'], [[(('artifact', 'model'), 66, 71), (('parameter', 'layer'), 101, 106), (('v number', '300'), 13, 16)], 'The input is 300-dimension subword embedding ew_ learned with the model, and the output is a softmax layer'], [[(('parameter', 'layer'), 32, 37), (('parameter', 'layer'), 58, 63), (('parameter', 'dropout'), 72, 79), (('v number', '256'), 48, 51), (('v number', '0.2'), 68, 71)], 'The context encoder is a single layer LSTM with 256 units layer and 0.2 dropout'], [[(('parameter', 'activation'), 4, 14), (('artifact', 'model'), 15, 20), (('parameter', 'layer'), 28, 33), (('v number', '2'), 26, 27), (('v number', '128'), 44, 47)], \"The activation model is a 2-layer LSTM with 128 units, the LSTM's output is projected into a scalar\"], [[(('parameter', 'activation'), 13, 23), (('artifact', 'model'), 24, 29), (('v number', '514'), 52, 55), (('v number', '256'), 58, 61), (('v number', '256'), 62, 65), (('v number', '1'), 66, 67), (('v number', '1'), 68, 69)], 'The input to activation model has dimensionality of 514 = 256+256+1+1 see Eq'], [[(('parameter', 'dropout'), 11, 18), (('parameter', 'layers'), 32, 38), (('v number', '0.2'), 7, 10)], 'We use 0.2 dropout between LSTM layers'], [[(('artifact', 'model'), 23, 28), (('parameter', 'layer'), 55, 60), (('v number', '128'), 41, 44)], 'Finally, the attention model is a single 128-unit LSTM layer with its output projected into a scalar'], [[(('artifact', 'model'), 14, 19), (('v number', '514'), 32, 35)], \"The attention model's input has 514 dimensions see Eq\"], [[(('parameter', 'dropout'), 11, 18), (('parameter', 'layer'), 45, 50), (('v number', '0.2'), 7, 10)], 'We add 0.2 dropout before and after the LSTM layer.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 11, 15), (('parameter', 'batch size'), 33, 43), (('artifact', 'model'), 79, 84), (('v number', '32'), 47, 49), (('v number', '2500'), 91, 95)], 'We use the Adam optimizer with a batch size of 32 sentences and checkpoint the model every 2500 updates']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 80, 93), (('v number', '8'), 33, 34), (('v number', '8'), 135, 136), (('v number', '8'), 141, 142)], 'We use distributed training with 8 nodes for the final models, and scale up the learning rate by the number of distributed nodes e.g., 8 for 8-node training'], [[(('parameter', 'batch size'), 4, 14), (('v number', '256'), 28, 31)], 'The batch size is set to be 256 per node'], [[(('parameter', 'weight decay'), 25, 37), (('v number', '0.9'), 101, 104)], 'Additionally, we set the weight decay and momentum for batch normalization parameters to be zero and 0.9, respectively']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 13, 19), (('parameter', 'K'), 127, 128), (('v number', '12'), 10, 12), (('v number', '768'), 21, 24), (('v number', '12'), 38, 40), (('v number', '84'), 83, 85), (('v number', '12'), 129, 131)], 'Base uses 12 layers, 768 hidden size, 12 attention heads, local attention radius r=84 , and relative position maximum distance k=12 '], [[(('parameter', 'layers'), 14, 20), (('parameter', 'K'), 62, 63), (('v number', '24'), 11, 13), (('v number', '1024'), 22, 26), (('v number', '16'), 40, 42), (('v number', '169'), 52, 55), (('v number', '24'), 64, 66)], 'Large uses 24 layers, 1024 hidden size, 16 heads, r=169 , and k=24 ']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 50, 63), (('v number', '1'), 67, 68), (('v number', '0.0001'), 69, 75)], 'We use all the default parameters with an initial learning rate of 1 0.0001 '], [[(('artifact', 'Adam'), 4, 8), (('parameter', 'learning rate'), 43, 56)], 'The Adam algorithm adaptively computes the learning rate']] \n",
      "\n",
      "[[[(('parameter', 'steps'), 42, 47), (('v number', '750.000'), 34, 41), (('v number', '50'), 117, 119), (('v number', '100'), 126, 129)], 'The algorithm is set to train for 750.000 steps, depending on the size of the problem instance, the episode requires 50 up to 100 actions.']] \n",
      "\n",
      "[[[(('parameter', 'K'), 0, 1), (('v number', '36'), 2, 4), (('v number', '2048'), 61, 65), (('v number', '152'), 135, 138)], 'K=36 objects are detected by pretrained faster R-CNN , and a 2048 dimensional vector for each object is extracted by pretrained ResNet-152 '], [[(('parameter', 'batch size'), 4, 14), (('v number', '256'), 18, 21)], 'The batch size is 256']] \n",
      "\n",
      "[[[(('artifact', 'model'), 17, 22), (('v number', '512'), 43, 46), (('v number', '1024'), 52, 56), (('v number', '4'), 63, 64), (('v number', '2019'), 95, 99)], 'We replicate the model configuration embed=512, ffn=1024, head=4 as the baseline in Wu et al., 2019'], [[(('parameter', 'batch size'), 17, 27), (('parameter', 'dropout'), 47, 54), (('parameter', 'dropout'), 72, 79), (('v number', '8192'), 31, 35), (('v number', '0.1'), 58, 61), (('v number', '0.1'), 83, 86)], 'In addition, the batch size of 8192, attention dropout of 0.1, and relu dropout of 0.1 is used as suggested by '], [[(('parameter', 'learning rate'), 80, 93), (('parameter', 'learning rate'), 108, 121), (('v number', '0.0015'), 125, 131)], 'When training the standard Transformer from scratch, we follow the inverse_sqrt learning rate schedule with learning rate of 0.0015 and warmup of 8k'], [[(('parameter', 'learning rate'), 76, 89), (('v number', '0.0005'), 93, 99)], 'MT, LayerDrop, all of them are finetuned from the pre-trained baseline with learning rate of 0.0005 and warmup of 4k.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 55, 60), (('parameter', 'T'), 86, 87)], 'In this section, we present our algorithm to learn the model parameters _1, , _m and ^t _^} from the training data'], [[(('artifact', 'model'), 60, 65), (('parameter', 'layer'), 85, 90)], 'Without loss of generality, let us consider to optimize the model parameters for one layer'], [[(('parameter', 'T'), 54, 55), (('parameter', 'T'), 98, 99), (('v number', '2'), 95, 96), (('v number', '1'), 110, 111), (('v number', '1'), 117, 118), (('v number', '1'), 120, 121)], 'Therefore, the objective for optimizing _1, , _m and ^t _^} will be & _^} _^ - _^ _, _^ _^ _j ^2 .t & _^ _0 = 1 & _ -1, 1 .']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '1.1'), 39, 42)], 'Our model is implemented using Pytorch 1.1'], [[(('artifact', 'model'), 13, 18), (('artifact', 'Adam'), 90, 94), (('parameter', 'learning rate'), 148, 161), (('v number', '3e-4'), 162, 166)], 'We train our model on a ThinkStation P920 workstation with one NVIDIA GTX 1080Ti, and use Adam as a training optimization strategy with the initial learning rate 3e-4'], [[(('parameter', 'learning rate'), 110, 123), (('v number', '6e-4'), 134, 138)], 'Besides, the super convergence training strategy is employed to boost the training processing and the maximum learning rate is set to 6e-4.']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 4, 17), (('artifact', 'model'), 25, 30), (('v number', '2'), 41, 42), (('v number', '0.0001'), 43, 49)], 'The learning rate of our model is set to 2 0.0001 at first'], [[(('parameter', 'epochs'), 123, 129), (('parameter', 'learning rate'), 135, 148), (('v number', '2'), 99, 100), (('v number', '1e-05'), 101, 106), (('v number', '10'), 120, 122)], 'To restrain the slightly larger fluctuation at the beginning of the training process, it is set to 2 1e-05 at the first 10 epochs as a learning rate warmup strategy'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '10'), 24, 26), (('v number', '100'), 102, 105)], 'The learning rate drops 10 times once the validation accuracy does not increase over a large patience 100'], [[(('parameter', 'batch size'), 20, 30), (('artifact', 'model'), 52, 57), (('v number', '128'), 16, 19)], 'Mini-batch with 128 batch size is used to train our model'], [[(('parameter', 'layers'), 116, 122), (('v number', '8'), 40, 41), (('v number', '6'), 43, 44), (('v number', '4'), 46, 47), (('v number', '8'), 99, 100), (('v number', '6'), 102, 103), (('v number', '4'), 108, 109)], 'The dense block configuration is set to 8, 6, 4, where in the first, second and third dense block, 8, 6 and 4 dense layers are used, respectively']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 60, 66), (('parameter', 'layer'), 88, 93), (('v number', '5'), 51, 52)], 'All BPNNs trained in this section were composed of 5 BPNN-D layers followed by a BPNN-B layer and were trained to predict the natural logarithm of the number of satisfying solutions to an input formula in CNF form'], [[(('artifact', 'model'), 83, 88), (('parameter', 'counts'), 89, 95)], 'We evaluated the performance of our BPNN using benchmarks from , with ground truth model counts obtained using DSharp ']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 50, 60), (('parameter', 'epochs'), 76, 82), (('v number', '128'), 64, 67), (('v number', '500'), 72, 75)], 'On the DFDC Dataset, we trained our models with a batch size of 128 for 500 epochs'], [[(('parameter', 'batch size'), 73, 83), (('parameter', 'epochs'), 113, 119), (('v number', '32'), 87, 89), (('v number', '100'), 109, 112)], 'Due to the significantly smaller size of the DF-TIMIT dataset, we used a batch size of 32 and trained it for 100 epochs'], [[(('artifact', 'Adam'), 8, 12), (('parameter', 'learning rate'), 30, 43), (('v number', '0.01'), 47, 51)], 'We used Adam optimizer with a learning rate of 0.01 ']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 6, 12), (('v number', '20'), 14, 16)], 'Stack Epochs: 20'], [[(('parameter', 'batch size'), 0, 10), (('v number', '256'), 12, 15)], 'Batch size: 256'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 21, 34), (('parameter', 'epochs'), 46, 52), (('v number', '0.001'), 35, 40), (('v number', '5'), 54, 55)], 'Optimizer: Adam with learning rate 0.001 Auto Epochs: 5'], [[(('parameter', 'batch size'), 0, 10), (('v number', '128'), 12, 15)], 'Batch size: 128'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 21, 34), (('parameter', 'epochs'), 45, 51), (('v number', '0.001'), 35, 40), (('v number', '210'), 53, 56)], 'Optimizer: Adam with learning rate 0.001 C_E Epochs: 210'], [[(('parameter', 'batch size'), 0, 10), (('v number', '256'), 12, 15)], 'Batch size: 256'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 21, 34), (('parameter', 'epochs'), 122, 128), (('parameter', 'epochs'), 136, 142), (('v number', '0.0001'), 47, 53), (('v number', '0.00001'), 70, 77), (('v number', '0.000008'), 82, 90), (('v number', '126'), 110, 113), (('v number', '168'), 118, 121), (('v number', '210'), 144, 147)], 'Optimizer: Adam with learning rate starting at 0.0001 , decreasing to 0.00001 and 0.000008 respectively after 126 and 168 epochs Luring Epochs: 210'], [[(('parameter', 'batch size'), 0, 10), (('v number', '256'), 12, 15)], 'Batch size: 256'], [[(('artifact', 'Adam'), 11, 15), (('parameter', 'learning rate'), 21, 34), (('parameter', 'epochs'), 122, 128), (('v number', '0.0001'), 47, 53), (('v number', '0.00001'), 70, 77), (('v number', '0.000008'), 82, 90), (('v number', '126'), 110, 113), (('v number', '168'), 118, 121)], 'Optimizer: Adam with learning rate starting at 0.0001 , decreasing to 0.00001 and 0.000008 respectively after 126 and 168 epochs']] \n",
      "\n",
      "[[[(('parameter', 'learning rate'), 12, 25), (('artifact', 'model'), 97, 102), (('parameter', 'batch size'), 140, 150), (('v number', '0.001'), 36, 41), (('v number', '0.1'), 63, 66), (('v number', '12'), 154, 156)], 'The initial learning rate is set to 0.001 with a step decay of 0.1 after 12k iterations and each model is trained for 32k iterations with a batch size of 12']] \n",
      "\n",
      "[[[(('artifact', 'L'), 68, 69), (('artifact', 'L'), 105, 106), (('parameter', 'm'), 199, 200), (('v number', '40'), 70, 72), (('v number', '20'), 107, 109), (('v number', '4'), 151, 152), (('v number', '36'), 201, 203)], 'As previously, we only consider the Count-or-Memorization task with l=40 , the Add-or-Multiply task with l=20 , the Hierarchical-or-Linear task with d=4 and the Composition-or-Memorization task with M=36 ']] \n",
      "\n",
      "[[[(('parameter', 'K'), 70, 71), (('v number', '1000'), 63, 67), (('v number', '4'), 72, 73), (('v number', '0.75'), 77, 81), (('v number', '4'), 99, 100), (('v number', '2'), 114, 115), (('v number', '0'), 120, 121), (('v number', '2'), 124, 125), (('v number', '0.1'), 133, 136), (('v number', '0.5'), 143, 146)], 'Recall that we consider the following parameters as default: n=1000 , k=4 , =0.75 , | | = | |= n n^4 and F_= ^ , ^2,F_= 0, ^2 with = 0.1 and = 0.5 .']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 34, 40), (('parameter', 'batch size'), 84, 94), (('parameter', 'learning rate'), 106, 119), (('v number', '100'), 30, 33), (('v number', '8'), 98, 99), (('v number', '1e-07'), 123, 128), (('v number', '0.001'), 141, 146)], 'All networks are trained over 100 epochs with Stochastic Gradient Descent SGD and a batch size of 8, with learning rate of 1e-07 for OFT and 0.001 for our network and VED']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 120, 125), (('v number', '1'), 129, 130)], 'We apply two architectural modifications to the standard implementation: we reduce the channel number of the very first layer to 1 since we use single-channel magnitude spectrograms and we vary the number of output neurons to match the classes to the task.']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 125, 130), (('parameter', 'epochs'), 165, 171), (('v number', '20'), 162, 164)], 'Before training each network, we initialize them with pre-trained weights on ImageNet and fine-tune the last fully-connected layer on the dataset of interest for 20 epochs'], [[(('parameter', 'weight decay'), 108, 120), (('v number', '0.95'), 57, 61), (('v number', '5'), 121, 122), (('v number', '1e-05'), 123, 128)], 'To train AlexNet , we use SGD as optimizer with momentum 0.95 and L2 regularization on network weights with weight decay 5 1e-05 '], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.001'), 29, 34), (('v number', '0.95'), 75, 79)], 'The initial learning rate is 0.001 , exponentially decayed with decay rate 0.95 '], [[(('artifact', 'Adam'), 26, 30), (('parameter', 'weight decay'), 35, 47), (('v number', '18'), 7, 9), (('v number', '1e-06'), 48, 53)], 'ResNet-18 is trained with Adam and weight decay 1e-06 '], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0001'), 29, 35)], 'The initial learning rate is 0.0001 ']] \n",
      "\n",
      "[[[(('parameter', 'Version'), 55, 62), (('parameter', 'steps'), 82, 87)], 'We train } initialized from PubMedBERT for the English version with 100K training steps'], [[(('parameter', 'Version'), 28, 35), (('parameter', 'steps'), 78, 83)], 'We also train cross-lingual version } initialized from mBERT with 1M training steps'], [[(('parameter', 'K'), 16, 17), (('parameter', 'steps'), 66, 71), (('v number', '128'), 18, 21), (('v number', '8'), 75, 76)], 'A batch-size of k=128 relation triplets and gradient accumulation steps of 8 are used for training'], [[(('parameter', 'm'), 56, 57), (('v number', '8'), 58, 59)], 'In each mini-batch, we set the count of repeat triplets m=8 '], [[(('artifact', 'AdamW'), 7, 12), (('parameter', 'steps'), 71, 76), (('parameter', 'learning rate'), 95, 108), (('v number', '10000'), 65, 70), (('v number', '2e-5'), 90, 94)], 'We use AdamW as the optimizer with a linear warm-up in the first 10000 steps to a peak of 2e-5 learning rate that decayed to zero linearly']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('artifact', 'Adam'), 31, 35), (('parameter', 'learning rate'), 95, 108), (('parameter', 'epochs'), 125, 131), (('v number', '2.0'), 70, 73), (('v number', '0.001'), 112, 117), (('v number', '20'), 122, 124)], 'The model is trained using the Adam optimizer with default tensorflow 2.0 settings and initial learning rate of 0.001 for 20 epochs'], [[(('parameter', 'learning rate'), 4, 17), (('parameter', 'epochs'), 55, 61), (('parameter', 'epoch'), 74, 79), (('v number', '2'), 45, 46), (('v number', '2'), 53, 54), (('v number', '15'), 80, 82)], 'The learning rate is annealed by a factor of 2 every 2 epochs starting at epoch 15'], [[(('parameter', 'epoch'), 4, 9), (('artifact', 'model'), 53, 58), (('v number', '50'), 22, 24), (('v number', '256'), 41, 44), (('v number', '12'), 80, 82), (('v number', '12'), 90, 92), (('v number', '12'), 102, 104), (('v number', '12'), 118, 120), (('v number', '12'), 128, 130), (('v number', '12'), 140, 142)], 'One epoch consists of 50 batches of size 256 and the model takes the history of 12 points 12 month; w=12 and predicts 12 points 12 month; H=12 ahead in one shot']] \n",
      "\n",
      "[[[(('artifact', 'model'), 4, 9), (('v number', '1'), 101, 102), (('v number', '1'), 109, 110)], 'The model parameters W are learned by minimizing the cross entropy loss: = - _^ _^_s } y_i^j p_i^j + 1-y_i^j 1-p_i^j.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 156, 166), (('parameter', 'dropout'), 174, 181), (('parameter', 'learning rate'), 198, 211), (('v number', '128'), 151, 154), (('v number', '32'), 170, 172), (('v number', '0.1'), 190, 193), (('v number', '5e-5'), 215, 219)], 'The parameters configuration of the binary classification tasks, namely, stance classification and evidence detection, was: maximum sequence length of 128, batch size of 32, dropout rate of 0.1 and learning rate of 5e-5'], [[(('artifact', 'model'), 5, 10), (('parameter', 'epochs'), 34, 40), (('v number', '10'), 31, 33)], 'Each model was fine-tuned over 10 epochs, using a cross-entropy loss function'], [[(('artifact', 'model'), 15, 20), (('parameter', 'batch size'), 124, 134), (('parameter', 'dropout'), 144, 151), (('parameter', 'learning rate'), 170, 183), (('v number', '100'), 117, 120), (('v number', '32'), 138, 140), (('v number', '0.1'), 160, 163), (('v number', '2e-5'), 187, 191)], 'The regression model for argument quality prediction, similar to the one used by , used a maximum sequence length of 100, a batch size of 32, a dropout rate of 0.1 and a learning rate of 2e-5'], [[(('artifact', 'model'), 5, 10), (('parameter', 'epochs'), 33, 39), (('v number', '3'), 31, 32)], 'Each model was fine-tuned over 3 epochs, using a mean-squared-error loss function'], [[(('artifact', 'model'), 18, 23), (('parameter', 'epoch'), 38, 43)], 'In all cases, the model from the last epoch was selected for evaluation.']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 14, 18), (('parameter', 'learning rate'), 42, 55), (('parameter', 'epochs'), 86, 92), (('artifact', 'model'), 136, 141), (('v number', '2'), 58, 59), (('v number', '5'), 84, 85)], 'We employ the Adam optimizer with initial learning rate = 2 , fine-tune for at most 5 epochs, and use early-stopping to select the best model'], [[(('parameter', 'batch size'), 9, 19), (('parameter', 'batch size'), 85, 95), (('v number', '32'), 23, 25), (('v number', '16'), 99, 101)], 'We use a batch size of 32 for experiments that do not use contrastive training and a batch size of 16 for those using contrastive training to establish a fair comparison.']] \n",
      "\n",
      "[[[(('parameter', 'layers'), 80, 86), (('parameter', 'layers'), 114, 120), (('v number', '4'), 95, 96)], 'The transformers used for language and multimodal modeling have fully connected layers of size 4 d_ and attention layers of size d_ , in the notation of , '], [[(('parameter', 'layers'), 52, 58), (('parameter', 'layers'), 83, 89)], 'For math, image, and video modeling we scale the FC layers to d_ and the attention layers to d_4 '], [[(('parameter', 'weight decay'), 25, 37), (('v number', '0.05'), 41, 45)], 'For math alone we used a weight decay of 0.05 ']] \n",
      "\n",
      "[[[(('artifact', 'BiLSTM'), 167, 173), (('parameter', 'layers'), 174, 180), (('parameter', 'dropout'), 182, 189), (('parameter', 'layers'), 204, 210), (('artifact', 'BiLSTM'), 214, 220), (('artifact', 'Adam'), 234, 238), (('parameter', 'learning rate'), 254, 267), (('v number', '50'), 88, 90), (('v number', '200'), 120, 123), (('v number', '0.2'), 195, 198), (('v number', '0.3'), 268, 271)], 'More specifically, we find the following hyper parameters for the deep learning models: 50 samples per each mini-batch; 200 hidden dimensions for all feed forward and BiLSTM layers; dropout rate 0.2; two layers of BiLSTM and GCN; and Adam optimizer with learning rate 0.3'], [[(('parameter', 'Version'), 61, 68), (('v number', '768'), 83, 86)], 'Note that for pre-trained word embeddings we use the uncased version of BERT_ with 768 dimensions.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 84, 94), (('artifact', 'Adam'), 114, 118), (('v number', '180'), 24, 27), (('v number', '32'), 61, 63), (('v number', '16'), 98, 100), (('v number', '384'), 101, 104), (('v number', '2'), 131, 132), (('v number', '0.0001'), 133, 139), (('v number', '0.99'), 147, 151), (('v number', '0.999'), 159, 164)], 'Models were trained for 180 thousand parameter updates using 32 NVIDIA V100 GPUs, a batch size of 16,384, and the Adam optimizer = 2 0.0001 , _1 = 0.99 , _2 = 0.999 ']] \n",
      "\n",
      "[[[(('artifact', 'Adam'), 32, 36), (('parameter', 'learning rate'), 56, 69), (('parameter', 'batch size'), 83, 93), (('v number', '1e-4'), 70, 74), (('v number', '16'), 97, 99)], 'The network is trained with the Adam optimizer with the learning rate 1e-4 and the batch size of 16']] \n",
      "\n",
      "[[[(('artifact', 'model'), 10, 15), (('artifact', 'Adam'), 34, 38), (('v number', '0'), 68, 69), (('v number', '0.99'), 74, 78)], 'Our final model was trained using Adam optimizer with parameters _1=0, _2=0.99 .']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 11, 21), (('parameter', 'learning rate'), 36, 49), (('parameter', 'epochs'), 129, 135), (('parameter', 'epochs'), 156, 162), (('v number', '128'), 25, 28), (('v number', '0.1'), 73, 76), (('v number', '10'), 97, 99), (('v number', '80'), 112, 114), (('v number', '120'), 122, 125), (('v number', '160'), 152, 155)], 'We adopt a batch size of 128, and a learning rate LR is initially set to 0.1 and then decayed by 10 at both the 80-th and 120-th epochs among the total 160 epochs, as in .']] \n",
      "\n",
      "[[[(('artifact', 'method'), 78, 84), (('v number', '8'), 19, 20)], 'A small network of 8 cells is trained via the FGSM-based adversarial training method in Eq'], [[(('parameter', 'batch size'), 13, 23), (('parameter', 'epochs'), 64, 70), (('v number', '64'), 27, 29), (('v number', '16'), 54, 56), (('v number', '50'), 61, 63)], 'REF with the batch size as 64 and initial channels as 16 for 50 epochs'], [[(('parameter', 'weight decay'), 48, 60), (('v number', '0.9'), 36, 39), (('v number', '3'), 61, 62), (('v number', '0.0001'), 63, 69)], 'The SGD optimizer with the momentum 0.9 and the weight decay 3 0.0001 is used'], [[(('artifact', 'method'), 13, 19), (('v number', '0.3'), 46, 49), (('v number', '1'), 50, 51)], 'The proposed method is implemented in PyTorch 0.3.1 and all the experiments are conducted in Tesla V100S GPUs with 32G CUDA memory.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 56, 63), (('parameter', 'batch size'), 81, 91), (('v number', '256'), 47, 50), (('v number', '0.1'), 72, 75), (('v number', '16'), 95, 97)], 'Across models, we set all hidden dimensions to 256, the dropout rate to 0.1, and batch size to 16'], [[(('artifact', 'Adam'), 12, 16), (('parameter', 'learning rate'), 34, 47), (('parameter', 'steps'), 99, 104), (('v number', '3e-4'), 51, 55), (('v number', '3'), 76, 77), (('v number', '8'), 93, 94), (('v number', '000'), 95, 98)], 'We used the Adam optimizer with a learning rate of 3e-4 , l_2 constraint of 3, and warmup of 8,000 steps']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 7, 14), (('v number', '0.1'), 29, 32), (('v number', '0.1'), 53, 56)], 'We add dropout to embeddings 0.1 and label smoothing 0.1'], [[(('artifact', 'Adam'), 31, 35), (('parameter', 'batch size'), 53, 63), (('v number', '4096'), 67, 71)], 'AR models are trained with the Adam optimizer with a batch size of 4096 tokens'], [[(('parameter', 'learning rate'), 12, 25), (('v number', '0.0002'), 29, 35), (('v number', '30'), 58, 60), (('v number', '4'), 68, 69)], 'The initial learning rate is 0.0002, and it is reduced by 30% after 4 checkpoints without validation perplexity improvement'], [[(('artifact', 'Adam'), 33, 37), (('parameter', 'learning rate'), 51, 64), (('parameter', 'batch size'), 81, 91), (('parameter', 'steps'), 129, 134), (('v number', '0.0005'), 68, 74), (('v number', '64'), 95, 97), (('v number', '800'), 98, 101), (('v number', '300'), 121, 124), (('v number', '000'), 125, 128)], 'All NAR models are trained using Adam with initial learning rate of 0.0005 and a batch size of 64,800 tokens for maximum 300,000 steps.Our preliminary experiments and prior work show that NAR models require larger training batches than AR models']] \n",
      "\n",
      "[[[(('artifact', 'model'), 83, 88), (('v number', '2'), 69, 70)], 'We down-sample the speech to 16k Hz sampling frequency, The tacotron 2 part in our model is trained with vanilla setups except setting frequency to match our speech'], [[(('parameter', 'learning rate'), 4, 17), (('v number', '0.001'), 30, 35), (('v number', '0.05'), 66, 70)], 'The learning rate is fixed to 0.001 and the loss weight of NML is 0.05.']] \n",
      "\n",
      "[[[(('artifact', 'model'), 40, 45), (('artifact', 'model'), 65, 70), (('v number', '0.41'), 74, 78)], 'When all 35k images are used to train a model, the PSM EPE of XL model is 0.41 on \"finalpass\"'], [[(('parameter', 'batch size'), 44, 54), (('v number', '320'), 30, 33), (('v number', '960'), 34, 37), (('v number', '8'), 58, 59), (('v number', '320'), 88, 91)], 'We considered random crops of 320 960 and a batch size of 8, and a maximum disparity of 320'], [[(('parameter', 'm'), 20, 21), (('artifact', 'Adam'), 43, 47), (('parameter', 'learning rate'), 75, 88), (('parameter', 'm'), 159, 160), (('parameter', 'm'), 166, 167), (('v number', '1.42'), 15, 19), (('v number', '1.3'), 155, 158), (('v number', '1.4'), 162, 165)], 'We trained for 1.42 M iterations using the Adam optimizer, starting from a learning rate of 4e^ , dropping it to 1e^ , then to 4e^ , then to 1e^ after 1M, 1.3 M, 1.4 M iterations respectively']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 10, 15), (('parameter', 'layer'), 62, 67), (('v number', '6'), 8, 9), (('v number', '96.6'), 28, 32), (('v number', '4'), 60, 61), (('v number', '82.4'), 80, 84)], 'All the 6-layer models have 96.6 million parameters and the 4-layer models have 82.4 million parameters'], [[(('artifact', 'Adam'), 26, 30), (('artifact', 'linear warmup'), 48, 61), (('parameter', 'learning rate'), 73, 86), (('parameter', 'learning rate'), 111, 124), (('parameter', 'learning rate'), 157, 170), (('v number', '1e-4'), 90, 94), (('v number', '0.1'), 174, 177), (('v number', '14000'), 181, 186), (('v number', '19000'), 194, 199)], 'We train our models using Adam optimizer with a linear warmup and with a learning rate of 1e-4 and a staircase learning rate schedule, where we multiply the learning rate by 0.1 at 14000 and at 19000 iterations'], [[(('parameter', 'epochs'), 40, 46), (('v number', '36'), 13, 15), (('v number', '100'), 36, 39), (('v number', '2'), 50, 51), (('v number', '12'), 77, 79), (('v number', '96'), 110, 112), (('v number', '768'), 121, 124)], 'We train for 36.1K total iterations 100 epochs on 2 NVIDIA Titan XP GPUs for 12 hours and use a batch-size of 96 and d = 768 as dimensionality for encoding all multi-modal features']] \n",
      "\n",
      "[[[(('artifact', 'L'), 70, 71), (('parameter', 'layers'), 135, 141), (('artifact', 'L'), 145, 146), (('v number', '12'), 72, 74), (('v number', '768'), 78, 81), (('v number', '12'), 85, 87), (('v number', '512'), 272, 275)], 'Following the previous practice, we pre-train ERNIE-Gram in base size L=12, H=768, A=12 , Total Parameters=110MWe donate the number of layers as L , the hidden size as H and the number of self-attention heads as A ., and set the length of the sequence in each batch up to 512 tokens'], [[(('artifact', 'Adam'), 7, 11), (('v number', '0.9'), 20, 23), (('v number', '0.999'), 28, 33), (('v number', '1e-06'), 36, 41)], 'We use Adam with _1=0.9, _2=0.999, =1e-06 for optimizing'], [[(('parameter', 'batch size'), 52, 62), (('parameter', 'learning rate'), 97, 110), (('parameter', 'steps'), 136, 141), (('v number', '256'), 73, 76), (('v number', '4'), 118, 119)], 'For pre-training on base-scale English corpora, the batch size is set to 256 sequences, the peak learning rate is 1e -4 for 1M training steps, which are the same settings as BERT'], [[(('parameter', 'batch size'), 69, 79), (('parameter', 'learning rate'), 108, 121), (('parameter', 'steps'), 140, 145), (('v number', '5112'), 83, 87), (('v number', '4'), 129, 130)], 'As for the pre-training on large-scale English corpora underway, the batch size is 5112 sequences, the peak learning rate is 4e -4 for 500K steps'], [[(('parameter', 'batch size'), 41, 51), (('parameter', 'learning rate'), 79, 92), (('v number', '256'), 55, 58), (('v number', '4'), 100, 101)], 'For pre-training on Chinese corpora, the batch size is 256 sequences, the peak learning rate is 1e -4']] \n",
      "\n",
      "[[[(('parameter', 'layer'), 8, 13), (('parameter', 'layer'), 87, 92), (('parameter', 'layer'), 118, 123), (('parameter', 'layer'), 152, 157), (('v number', '6105'), 29, 33), (('v number', '16'), 34, 36), (('v number', '2'), 37, 38), (('v number', '6105'), 56, 60), (('v number', '16'), 94, 96), (('v number', '2'), 129, 130)], 'A multi-layer perceptron MLP 6105-16-2 corresponding to 6105 nodes for the input first layer, 16 nodes for the hidden layer, and 2 nodes for the output layer was used for classification'], [[(('parameter', 'fold'), 15, 19), (('artifact', 'cross-validation'), 20, 36), (('v number', '5'), 13, 14)], 'We performed 5-fold cross-validation subject-wise splitting, and each entry of the input vectors was normalized by training set mean and standard deviation std within each site'], [[(('artifact', 'Adam'), 0, 4), (('parameter', 'learning rate'), 43, 56), (('parameter', 'epochs'), 93, 99), (('parameter', 'epoch'), 124, 129), (('v number', '1e-5'), 57, 61), (('v number', '12'), 77, 79), (('v number', '20'), 90, 92)], 'Adam optimization was applied with initial learning rate 1e-5 and reduced by 12 for every 20 epochs and stopped at the 50th epoch'], [[(('parameter', 'steps'), 17, 22), (('parameter', 'epoch'), 31, 36), (('parameter', 'batch size'), 52, 62), (('v number', '60'), 40, 42), (('v number', '60'), 113, 115)], 'We set the total steps of each epoch as 60, and the batch size of each site was the number of training data over 60.']] \n",
      "\n",
      "[[[(('parameter', 'epochs'), 42, 48), (('artifact', 'Adam'), 81, 85), (('parameter', 'learning rate'), 103, 116), (('parameter', 'batch size'), 158, 168), (('v number', '90'), 39, 41), (('v number', '0.001'), 123, 128), (('v number', '0.9'), 134, 137), (('v number', '0.999'), 146, 151), (('v number', '100'), 172, 175)], 'Each Auto-Encoder has been trained for 90 epochs using the MSE loss function and Adam optimiser with a learning rate of lr=0.001, _1 =0.9 and _2 =0.999 and a batch size of 100.']] \n",
      "\n",
      "[[[(('parameter', 'dropout'), 9, 16), (('parameter', 'dropout'), 88, 95), (('v number', '0.3'), 20, 23)], 'We use a dropout of 0.3 for both the WMT and IWSLT datasets and linearly scale down the dropout ratio when shrinking the dimension of the embeddings for the WMT datasets'], [[(('artifact', 'Adam'), 19, 23), (('parameter', 'learning rate'), 47, 60), (('parameter', 'learning rate'), 102, 115), (('v number', '1e-07'), 147, 152), (('v number', '0.001'), 156, 161)], 'Same as , we apply Adam optimizer and a cosine learning rate schedule , for the WMT models, where the learning rate is first linearly warm up from 1e-07 to 0.001 followed by a cosine annealing with a single cycle'], [[(('parameter', 'dropout'), 16, 23), (('parameter', 'layer'), 42, 47), (('parameter', 'layer'), 101, 106)], 'We decrease the dropout ratio for the FFN layer by half in our Lite Transformer due to the flattened layer.']] \n",
      "\n",
      "[[[(('parameter', 'batch size'), 31, 41), (('parameter', 'K'), 95, 96), (('v number', '128'), 52, 55), (('v number', '32'), 88, 90), (('v number', '4'), 100, 101), (('v number', '32'), 125, 127), (('v number', '4'), 143, 144)], 'For reasonable comparison, the batch size is set to 128 to match TriNet by setting P to 32 and K to 4, thus a batch contains 32 identities and 4 images for each identity']] \n",
      "\n",
      "[[[(('artifact', 'L'), 172, 173), (('v number', '1'), 174, 175)], 'We note that the multinomial distribution does not show a clear maximum because of the too few examples provided by the user compared to the large number r of values _l^j; l=1,...,r ']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for info in extracted_info:\n",
    "    # how to deal with too much p and v???\n",
    "    print(extracted_info[info],'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
