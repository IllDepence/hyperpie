{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3659090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869dcfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines of data:  444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph_index</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>annotation_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tsa</td>\n",
       "      <td>2005.00512</td>\n",
       "      <td>0</td>\n",
       "      <td>Extracting information about entities and thei...</td>\n",
       "      <td>{'entities': {'a1': {'id': 'a1', 'type': 'a', ...</td>\n",
       "      <td>[{'@context': 'http://www.w3.org/ns/anno.jsonl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tsa</td>\n",
       "      <td>2005.00512</td>\n",
       "      <td>1</td>\n",
       "      <td>Creating datasets for information extraction a...</td>\n",
       "      <td>{'entities': {'a2': {'id': 'a2', 'type': 'a', ...</td>\n",
       "      <td>[{'@context': 'http://www.w3.org/ns/anno.jsonl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tsa</td>\n",
       "      <td>2005.00512</td>\n",
       "      <td>3</td>\n",
       "      <td>To overcome the annotation challenges for larg...</td>\n",
       "      <td>{'entities': {'a5': {'id': 'a5', 'type': 'a', ...</td>\n",
       "      <td>[{'@context': 'http://www.w3.org/ns/anno.jsonl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tsa</td>\n",
       "      <td>2005.00512</td>\n",
       "      <td>6</td>\n",
       "      <td>we introduce SciREX, a dataset that evaluates\\...</td>\n",
       "      <td>{'entities': {'a4': {'id': 'a4', 'type': 'a', ...</td>\n",
       "      <td>[{'@context': 'http://www.w3.org/ns/anno.jsonl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tsa</td>\n",
       "      <td>2005.00512</td>\n",
       "      <td>7</td>\n",
       "      <td>In recent years, there has been multiple attem...</td>\n",
       "      <td>{'entities': {}, 'relations': {}}</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_id  document_id  paragraph_index  \\\n",
       "0          tsa   2005.00512                0   \n",
       "1          tsa   2005.00512                1   \n",
       "2          tsa   2005.00512                3   \n",
       "3          tsa   2005.00512                6   \n",
       "4          tsa   2005.00512                7   \n",
       "\n",
       "                                                text  \\\n",
       "0  Extracting information about entities and thei...   \n",
       "1  Creating datasets for information extraction a...   \n",
       "2  To overcome the annotation challenges for larg...   \n",
       "3  we introduce SciREX, a dataset that evaluates\\...   \n",
       "4  In recent years, there has been multiple attem...   \n",
       "\n",
       "                                          annotation  \\\n",
       "0  {'entities': {'a1': {'id': 'a1', 'type': 'a', ...   \n",
       "1  {'entities': {'a2': {'id': 'a2', 'type': 'a', ...   \n",
       "2  {'entities': {'a5': {'id': 'a5', 'type': 'a', ...   \n",
       "3  {'entities': {'a4': {'id': 'a4', 'type': 'a', ...   \n",
       "4                  {'entities': {}, 'relations': {}}   \n",
       "\n",
       "                                      annotation_raw  \n",
       "0  [{'@context': 'http://www.w3.org/ns/anno.jsonl...  \n",
       "1  [{'@context': 'http://www.w3.org/ns/anno.jsonl...  \n",
       "2  [{'@context': 'http://www.w3.org/ns/anno.jsonl...  \n",
       "3  [{'@context': 'http://www.w3.org/ns/anno.jsonl...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('./data/v2/tsa_processed.json')\n",
    "print('lines of data: ',len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbb90de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number and distribution of document:\n",
      " 2211.14208    69\n",
      "2005.00512    61\n",
      "2210.10073    51\n",
      "2212.06522    51\n",
      "1808.09602    48\n",
      "2205.02033    44\n",
      "2211.15202    37\n",
      "2005.11184    35\n",
      "2203.05325    30\n",
      "2211.15196    18\n",
      "Name: document_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = df['document_id'].value_counts()\n",
    "print('number and distribution of document:\\n', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be7c711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of paragraph with relations:  27\n",
      "number of relations:  212\n"
     ]
    }
   ],
   "source": [
    "# only save the dataframe with at least one relation\n",
    "joint_dataset = {}\n",
    "\n",
    "num_parg = 0\n",
    "num_rels = 0\n",
    "\n",
    "for idx in df.index:\n",
    "    text = df['text'][idx] \n",
    "    annotation = df['annotation'][idx]\n",
    "    relations = annotation['relations']\n",
    "\n",
    "    if relations:\n",
    "        # save\n",
    "        id = str(df['document_id'][idx]) + '_paragraph' + str(df['paragraph_index'][idx])\n",
    "        info = {\n",
    "                'id': id,\n",
    "                #'phase': 'train',\n",
    "                #'paragraph': df['paragraph_index'][idx],\n",
    "                'text': df['text'][idx],\n",
    "                'entity': annotation['entities'],\n",
    "                'relation': annotation['relations'],\n",
    "        }\n",
    "        joint_dataset[id] = info\n",
    "        num_parg += 1\n",
    "        num_rels += len(relations)\n",
    "\n",
    "print('number of paragraph with relations: ',num_parg)\n",
    "print('number of relations: ',num_rels)\n",
    "\n",
    "# sampling from none 'relation': \n",
    "\n",
    "# None as a type? nearly half\n",
    "# joint_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6108357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fold', '40') ([26, 28], [29, 33])\n",
      "('cross-validation', 'fold') ([29, 33], [34, 50])\n",
      "('cross-validation', 'fold') ([29, 33], [34, 50])\n",
      "('64', 'hyperparameter search') ([111, 132], [161, 163])\n",
      "('batch size', '64') ([161, 163], [147, 157])\n",
      "('\\\\lbrace 1e-5, 2e-5, 3e-5\\\\rbrace', 'hyperparameter search') ([111, 132], [189, 220])\n",
      "('learning rates', '\\\\lbrace 1e-5, 2e-5, 3e-5\\\\rbrace') ([189, 220], [168, 182])\n",
      "('\\\\lbrace 8,16,64,128\\\\rbrace', 'hyperparameter search') ([111, 132], [246, 272])\n",
      "('epochs number', '\\\\lbrace 8,16,64,128\\\\rbrace') ([246, 272], [226, 239])\n",
      "('0.01', 'hyperparameter search') ([111, 132], [350, 354])\n",
      "('weight decay coefficient', '0.01') ([350, 354], [322, 346])\n",
      "('6%', 'hyperparameter search') ([111, 132], [306, 308])\n",
      "('steps', '6%') ([306, 308], [312, 317])\n",
      "('linear warmup', 'steps') ([312, 317], [278, 291])\n",
      "('1e-5', 'each dataset') ([462, 474], [450, 454])\n",
      "('learning rates', '1e-5') ([450, 454], [168, 182])\n",
      "('8', '1,000 elements datasets') ([493, 516], [480, 481])\n",
      "('weight decay coefficient', '8') ([480, 481], [322, 346])\n",
      "('64', '100-element datasets') ([525, 545], [518, 520])\n",
      "('weight decay coefficient', '64') ([518, 520], [322, 346])\n",
      "('128', '20-element datasets') ([558, 577], [550, 553])\n",
      "('weight decay coefficient', '128') ([550, 553], [322, 346])\n",
      "('\\\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\\\rbrace', 'for all methods') ([109, 124], [138, 176])\n",
      "('\\\\beta', '\\\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\\\rbrace') ([138, 176], [86, 91])\n",
      "('m', '\\\\lbrace 1, 3 ,5, 7, 9\\\\rbrace') ([325, 353], [319, 320])\n",
      "('Triplet Loss', 'm') ([319, 320], [253, 265])\n",
      "('\\\\tau', '\\\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\\\rbrace') ([424, 462], [415, 419])\n",
      "('SupCon Loss', '\\\\tau') ([415, 419], [378, 389])\n",
      "('\\\\tau', '\\\\lbrace 5, 25, 1,000, 2000\\\\rbrace') ([546, 579], [415, 419])\n",
      "('SoftTriple loss,', '\\\\tau') ([415, 419], [483, 499])\n",
      "('\\\\gamma', '\\\\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\\\rbrace') ([598, 640], [587, 593])\n",
      "('SoftTriple loss,', '\\\\gamma') ([587, 593], [483, 499])\n",
      "('\\\\lambda', '\\\\lbrace 1,3,3.3,4,6,\\\\\\\\8,10\\\\rbrace') ([660, 693], [648, 655])\n",
      "('SoftTriple loss,', '\\\\lambda') ([648, 655], [483, 499])\n",
      "('\\\\delta', '\\\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\\\rbrace') ([716, 757], [705, 711])\n",
      "('SoftTriple loss,', '\\\\delta') ([705, 711], [483, 499])\n",
      "('\\\\lbrace 1, 3 ,5, 7, 9\\\\rbrace', 'grid search') ([270, 281], [325, 353])\n",
      "('\\\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9\\\\rbrace', 'grid search') ([270, 281], [424, 462])\n",
      "('\\\\lbrace 5, 25, 1,000, 2000\\\\rbrace', 'grid search') ([270, 281], [546, 579])\n",
      "('\\\\lbrace 0.01, 0.03, 0.05, 0.07, 0.1\\\\rbrace', 'grid search') ([270, 281], [598, 640])\n",
      "('\\\\lbrace 1,3,3.3,4,6,\\\\\\\\8,10\\\\rbrace', 'grid search') ([270, 281], [660, 693])\n",
      "('\\\\lbrace 0.1, 0.3, 0.5, 0.7, 0.9, 1\\\\rbrace', 'grid search') ([270, 281], [716, 757])\n",
      "('\\\\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\\\rbrace', 'grid search') ([812, 823], [858, 918])\n",
      "('\\\\delta', '\\\\lbrace 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 3, 5\\\\rbrace') ([858, 918], [705, 711])\n",
      "('ProxyNCA Loss', '\\\\delta') ([705, 711], [767, 780])\n",
      "('\\\\lbrace 16, 32, 64, 128\\\\rbrace', 'grid search') ([812, 823], [1023, 1053])\n",
      "('\\\\alpha', '\\\\lbrace 16, 32, 64, 128\\\\rbrace') ([1023, 1053], [1012, 1018])\n",
      "('ProxyAnchor Loss', '\\\\alpha') ([1012, 1018], [928, 944])\n",
      "('\\\\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\\\rbrace', 'grid search') ([812, 823], [1075, 1116])\n",
      "('\\\\delta', '\\\\lbrace 0, 0.1, 0.3, 0.5, 0.7, 0.9\\\\rbrace') ([1075, 1116], [1064, 1070])\n",
      "('ProxyAnchor Loss', '\\\\delta') ([1064, 1070], [928, 944])\n",
      "('T', '1.7') ([175, 178], [13, 14])\n",
      "('1.7', 'achieves the highest mean test accuracy') ([128, 167], [175, 178])\n",
      "('40', 'demonstrate the mitigation of oversmoothing') ([52, 95], [317, 319])\n",
      "('layers', '40') ([317, 319], [320, 326])\n",
      "('GNN', 'layers') ([320, 326], [310, 313])\n",
      "('CRPSE', 'K') ([382, 383], [16, 21])\n",
      "('CRPSE', 'K') ([382, 383], [16, 21])\n",
      "('1, 5 or 10', 'usually set as') ([444, 458], [459, 469])\n",
      "('1.96', 'In this study') ([177, 190], [206, 210])\n",
      "('\\\\alpha', '1.96') ([206, 210], [66, 72])\n",
      "('6.0', 'In this study') ([177, 190], [213, 216])\n",
      "('\\\\beta', '6.0') ([213, 216], [83, 88])\n",
      "('prefix beam search decoding', '\\\\alpha') ([162, 168], [66, 93])\n",
      "('prefix beam search decoding', '\\\\beta') ([184, 189], [66, 93])\n",
      "('1024', 'At the test time') ([206, 222], [273, 277])\n",
      "('beam-width', '1024') ([273, 277], [259, 269])\n",
      "('Prefix beam search decoding', 'beam-width') ([259, 269], [224, 251])\n",
      "('model', '\\\\lambda _{\\\\text{E}}') ([219, 238], [52, 57])\n",
      "('model', '\\\\lambda _{\\\\text{R}}') ([264, 283], [52, 57])\n",
      "('model', '\\\\lambda _{\\\\text{C}}') ([305, 324], [52, 57])\n",
      "('beam pruning', '\\\\lambda _{\\\\text{C}}') ([518, 537], [32, 44])\n",
      "('beam pruning', '\\\\lambda _{\\\\text{R}}') ([549, 568], [32, 44])\n",
      "('hidden state', '128') ([188, 191], [194, 206])\n",
      "('BiLSTM', 'hidden state') ([194, 206], [176, 182])\n",
      "('dropout', '0.2') ([228, 231], [217, 224])\n",
      "('hidden layers', 'two') ([298, 301], [302, 315])\n",
      "('feedforward networks', 'hidden layers') ([302, 315], [261, 281])\n",
      "('hidden layers', '128') ([335, 338], [302, 315])\n",
      "('feedforward networks', 'hidden layers') ([302, 315], [261, 281])\n",
      "('feedforward networks', 'dropout') ([217, 224], [261, 281])\n",
      "('activation', 'gelu') ([344, 348], [349, 359])\n",
      "('feedforward networks', 'activation') ([349, 359], [261, 281])\n",
      "('hidden state', '128') ([545, 548], [194, 206])\n",
      "('feedforward networks', 'hidden state') ([194, 206], [261, 281])\n",
      "('epochs', '30') ([613, 615], [616, 622])\n",
      "('1e-3', 'for all non BERT weights') ([671, 695], [649, 653])\n",
      "('learning rate', '1e-3') ([649, 653], [657, 670])\n",
      "('Adam', 'learning rate') ([657, 670], [629, 633])\n",
      "('2e-5', 'for BERT weights') ([705, 721], [700, 704])\n",
      "('learning rate', '2e-5') ([700, 704], [657, 670])\n",
      "('patience value', '7') ([770, 771], [752, 766])\n",
      "('Adam', 'patience value') ([752, 766], [629, 633])\n",
      "('7', 'on the validation') ([772, 789], [770, 771])\n",
      "('model', 'dropout') ([217, 224], [136, 141])\n",
      "('model', 'epochs') ([616, 622], [136, 141])\n",
      "('200', 'we test with the following hyperparameter configurations') ([16, 72], [87, 90])\n",
      "('epochs', '200') ([87, 90], [91, 97])\n",
      "('Adam', 'epochs') ([91, 97], [108, 112])\n",
      "('dimensionality of \\\\(\\\\mathbf {H}\\\\)', '{32, 64, 128, 256}') ([203, 221], [128, 161])\n",
      "('{32, 64, 128, 256}', 'we test with the following hyperparameter configurations') ([16, 72], [203, 221])\n",
      "('1.0 to 4.0', 'we test with the following hyperparameter configurations') ([16, 72], [248, 258])\n",
      "('T', '1.0 to 4.0') ([248, 258], [225, 226])\n",
      "('1e-4 to 2e-2', 'we test with the following hyperparameter configurations') ([16, 72], [293, 305])\n",
      "('learning rate', '1e-4 to 2e-2') ([293, 305], [264, 277])\n",
      "('Adam', 'learning rate') ([264, 277], [108, 112])\n",
      "('0 to 0.01', 'we test with the following hyperparameter configurations') ([16, 72], [327, 336])\n",
      "('weight decay', '0 to 0.01') ([327, 336], [311, 323])\n",
      "('Adam', 'weight decay') ([311, 323], [108, 112])\n",
      "('method', 'dimensionality of \\\\(\\\\mathbf {H}\\\\)') ([128, 161], [8, 14])\n",
      "('method', 'T') ([225, 226], [8, 14])\n",
      "('K', '5') ([485, 486], [467, 468])\n",
      "('CRPSE', 'K') ([467, 468], [140, 145])\n",
      "('5', 'an example') ([120, 130], [485, 486])\n",
      "('system', 'k') ([197, 198], [461, 467])\n",
      "('learning rates', '[3\\\\mathrm {e}{-5}, 5\\\\mathrm {e}{-5}, 7\\\\mathrm {e}{-5}]') ([285, 339], [264, 278])\n",
      "('AdamW', 'learning rates') ([264, 278], [249, 254])\n",
      "('epoch', '1') ([363, 364], [365, 370])\n",
      "('linear warmup', 'epoch') ([365, 370], [346, 359])\n",
      "('epoch', '60') ([422, 424], [365, 370])\n",
      "('linear decay', 'epoch') ([365, 370], [385, 397])\n",
      "('batch size', '4') ([535, 536], [521, 531])\n",
      "('max norm', '1') ([585, 586], [573, 581])\n",
      "('gradient clipping', 'max norm') ([573, 581], [548, 565])\n",
      "('AdamW', 'batch size') ([521, 531], [249, 254])\n",
      "('50', 'During training and development set evaluation') ([746, 792], [871, 873])\n",
      "('k', '50') ([871, 873], [803, 804])\n",
      "('400', 'For test set evaluation') ([992, 1015], [1038, 1041])\n",
      "('k', '400') ([1038, 1041], [803, 804])\n",
      "('\\\\le 100', 'relation extraction performance improves proportional to the entity recall') ([223, 297], [306, 313])\n",
      "('k', '\\\\le 100') ([306, 313], [100, 101])\n",
      "('k', 'higher') ([348, 354], [100, 101])\n",
      "('higher', 'the improvement slows down') ([317, 343], [348, 354])\n",
      "('k', '50') ([418, 420], [100, 101])\n",
      "('50', 'during training') ([460, 475], [418, 420])\n",
      "('Version', '11') ([232, 234], [224, 231])\n",
      "('COCI (OpenCitations Index of Crossref open DOI-to-DOI references)', 'Version') ([224, 231], [30, 95])\n",
      "('\\\\sigma', 'T') ([113, 114], [8, 14])\n",
      "('L', '\\\\alpha _1') ([750, 759], [737, 738])\n",
      "('L', '\\\\alpha _2') ([774, 783], [737, 738])\n",
      "('EMA', 'm') ([182, 183], [62, 65])\n",
      "('L', '\\\\alpha') ([113, 119], [82, 83])\n",
      "('0', 'becomes segment ensemble') ([18, 42], [9, 10])\n",
      "('m', '0') ([9, 10], [7, 8])\n",
      "('50', 'for all datasets') ([316, 332], [313, 315])\n",
      "('max training epoch', '50') ([313, 315], [291, 309])\n",
      "('32', 'for OntoNotes 5.0') ([405, 422], [402, 404])\n",
      "('16', 'for CoNLL03, Webpage, and Twitter') ([364, 397], [361, 363])\n",
      "('training batch size', '16') ([361, 363], [338, 357])\n",
      "('training batch size', '32') ([402, 404], [338, 357])\n",
      "('1e-5', 'for CoNLL03 and Webpage') ([457, 480], [452, 456])\n",
      "('learning rate', '1e-5') ([452, 456], [428, 441])\n",
      "('2e-5', 'for OntoNotes 5.0 and Twitter') ([491, 520], [486, 490])\n",
      "('learning rate', '2e-5') ([486, 490], [428, 441])\n",
      "('1', 'for CoNLL03') ([610, 621], [587, 588])\n",
      "('epochs', '1') ([587, 588], [603, 609])\n",
      "('2', 'OntoNotes 5.0') ([623, 636], [590, 591])\n",
      "('epochs', '2') ([590, 591], [603, 609])\n",
      "('12', 'Webpage') ([638, 645], [593, 595])\n",
      "('epochs', '12') ([593, 595], [603, 609])\n",
      "('6', 'Twitter') ([651, 658], [601, 602])\n",
      "('epochs', '6') ([601, 602], [603, 609])\n",
      "('0.9', 'for all datasets') ([747, 763], [743, 746])\n",
      "('confidence threshold', '0.9') ([743, 746], [704, 724])\n",
      "('0.995', 'for dataset CoNLL03') ([895, 914], [814, 819])\n",
      "('m', '0.995') ([814, 819], [805, 806])\n",
      "('0.995', 'OntoNotes 5.0') ([916, 929], [821, 826])\n",
      "('m', '0.995') ([821, 826], [805, 806])\n",
      "('0.99', 'Webpage') ([931, 938], [828, 832])\n",
      "('m', '0.99') ([828, 832], [805, 806])\n",
      "('0.995', 'Twitter') ([944, 951], [834, 839])\n",
      "('m', '0.995') ([834, 839], [805, 806])\n",
      "('0.8', 'for dataset CoNLL03') ([895, 914], [869, 872])\n",
      "('\\\\sigma _2', '0.8') ([869, 872], [846, 855])\n",
      "('0.995', 'OntoNotes 5.0') ([916, 929], [874, 879])\n",
      "('\\\\sigma _2', '0.995') ([874, 879], [846, 855])\n",
      "('0.8', 'Webpage') ([931, 938], [881, 884])\n",
      "('\\\\sigma _2', '0.8') ([881, 884], [846, 855])\n",
      "('0.75', 'Twitter') ([944, 951], [890, 894])\n",
      "('\\\\sigma _2', '0.75') ([890, 894], [846, 855])\n",
      "('sigma _2', '0') ([351, 352], [342, 350])\n",
      "('m', '0') ([407, 408], [405, 406])\n",
      "('0', 'investigate the effectiveness of different student ensemble methods') ([3, 70], [351, 352])\n",
      "('0', 'investigate the effectiveness of different student ensemble methods') ([3, 70], [407, 408])\n",
      "('0.995', 'investigate the parameter influence of fine-grained ensemble') ([846, 906], [962, 967])\n",
      "('m', '0.995') ([962, 967], [405, 406])\n",
      "('0.75', 'investigate the parameter influence of fine-grained ensemble') ([846, 906], [987, 991])\n",
      "('sigma _2', '0.75') ([987, 991], [342, 350])\n",
      "('counts', '500') ([172, 175], [176, 182])\n",
      "('W&B Sweeps', 'counts') ([176, 182], [122, 132])\n",
      "('500', 'hyperparameter search') ([81, 102], [172, 175])\n",
      "('hidden layers', '2') ([193, 194], [195, 208])\n",
      "('FFNNs', 'hidden layers') ([195, 208], [182, 187])\n",
      "('dimensional hidden layers', '150') ([212, 215], [147, 172])\n",
      "('FFNNs', 'dimensional hidden layers') ([147, 172], [182, 187])\n",
      "('dropout', '0.5') ([315, 318], [288, 295])\n",
      "('dropout', '0.4') ([284, 287], [288, 295])\n",
      "('FFNNs', 'dropout') ([288, 295], [182, 187])\n",
      "('variational dropout', '0.4') ([240, 243], [244, 263])\n",
      "('LSTMs', 'variational dropout') ([244, 263], [277, 282])\n",
      "('0.5', 'input embeddings') ([335, 351], [315, 318])\n",
      "('0.4', 'relation extraction') ([501, 520], [490, 493])\n",
      "('\\\\lambda _{\\\\text{R}}', '0.4') ([490, 493], [470, 489])\n",
      "('beam pruning', '\\\\lambda _{\\\\text{R}}') ([470, 489], [387, 399])\n",
      "('0.3', 'coreference resolution') ([441, 463], [430, 433])\n",
      "('\\\\lambda _{\\\\text{C}}', '0.3') ([430, 433], [410, 429])\n",
      "('beam pruning', '\\\\lambda _{\\\\text{C}}') ([410, 429], [387, 399])\n",
      "('system', 'dropout') ([288, 295], [4, 10])\n",
      "('dimensional hidden layers', '200') ([143, 146], [147, 172])\n",
      "('BiLSTM', 'dimensional hidden layers') ([147, 172], [131, 137])\n",
      "('layer', '1') ([123, 124], [125, 130])\n",
      "('BiLSTM', 'layer') ([125, 130], [131, 137])\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "# len(joint_dataset)\n",
    "# define the 3 type of relation: ap, pv, vc\n",
    "# relation_entity_dic = {}\n",
    "\n",
    "rel_to_entity = defaultdict(list)\n",
    "\n",
    "entity_pair_dict = {}\n",
    "st_relation_list = []\n",
    "\n",
    "entity_type = defaultdict(list)\n",
    "\n",
    "for paragraph in joint_dataset.keys():\n",
    "    for rel in joint_dataset[paragraph]['relation']:\n",
    "        text = joint_dataset[paragraph]['text']\n",
    "        text = ' '.join(text.splitlines())\n",
    "        \n",
    "        rel_context = joint_dataset[paragraph]['relation'][rel]\n",
    "\n",
    "        # entity name (like entity type v1 p1 a1...)\n",
    "        source = rel_context['source']\n",
    "        target = rel_context['target']\n",
    "        # entity informstion \n",
    "        entity_context = joint_dataset[paragraph]['entity']\n",
    "        s_entity = entity_context[source]\n",
    "        t_entity = entity_context[target]\n",
    "        st_relation = target[0] + source[0]\n",
    "\n",
    "        # entity name\n",
    "        s_positon_start = s_entity['surface_forms'][0]['start']\n",
    "        s_positon_end = s_entity['surface_forms'][0]['end']\n",
    "        t_positon_start = t_entity['surface_forms'][0]['start']\n",
    "        t_positon_end = t_entity['surface_forms'][0]['end']\n",
    "        s_name = text[s_positon_start: s_positon_end] \n",
    "        t_name = text[t_positon_start: t_positon_end]\n",
    "        entity_pair = (t_name, s_name)\n",
    "        entity_pair_dict[entity_pair] = st_relation\n",
    "        rel_to_entity[st_relation].append(entity_pair)\n",
    "\n",
    "        entity_type[source[0]].append(s_name) \n",
    "        entity_type[target[0]].append(t_name)\n",
    "\n",
    "        print(entity_pair,([s_positon_start,s_positon_end], [t_positon_start,t_positon_end]))\n",
    "print(len(entity_pair_dict))\n",
    "# print(entity_pair_dict)\n",
    "\n",
    "# Basic rule for generate data\n",
    "# First, if (there are numbers in sentences):\n",
    "#     Next if (a, p) in sentences:\n",
    "#         Next sentences exist {'rel': (entity_1, entity_2)}\n",
    "\n",
    "# print(set(st_relation_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ba3628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of a type:  28\n",
      "number of p type:  44\n",
      "number of c type:  38\n"
     ]
    }
   ],
   "source": [
    "a_entity = set(entity_type['a'])\n",
    "p_entity = set(entity_type['p'])\n",
    "c_entity = set(entity_type['c'])\n",
    "print('number of a type: ', len(a_entity))\n",
    "print('number of p type: ', len(p_entity))\n",
    "print('number of c type: ', len(c_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bba9ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ap type:  54\n",
      "number of pv type:  77\n",
      "number of vc type:  64\n"
     ]
    }
   ],
   "source": [
    "ap_rel = set(rel_to_entity['ap'])\n",
    "pv_rel = set(rel_to_entity['pv'])\n",
    "vc_rel = set(rel_to_entity['vc'])\n",
    "print('number of ap type: ', len(ap_rel))\n",
    "print('number of pv type: ', len(pv_rel))\n",
    "print('number of vc type: ', len(vc_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d843af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ap_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5461c4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Adam', 'epochs'),\n",
       " ('Adam', 'learning rate'),\n",
       " ('Adam', 'patience value'),\n",
       " ('Adam', 'weight decay'),\n",
       " ('AdamW', 'batch size'),\n",
       " ('AdamW', 'learning rates'),\n",
       " ('BiLSTM', 'dimensional hidden layers'),\n",
       " ('BiLSTM', 'hidden state'),\n",
       " ('BiLSTM', 'layer'),\n",
       " ('COCI (OpenCitations Index of Crossref open DOI-to-DOI references)',\n",
       "  'Version'),\n",
       " ('CRPSE', 'K'),\n",
       " ('EMA', 'm'),\n",
       " ('FFNNs', 'dimensional hidden layers'),\n",
       " ('FFNNs', 'dropout'),\n",
       " ('FFNNs', 'hidden layers'),\n",
       " ('GNN', 'layers'),\n",
       " ('L', '\\\\alpha'),\n",
       " ('L', '\\\\alpha _1'),\n",
       " ('L', '\\\\alpha _2'),\n",
       " ('LSTMs', 'variational dropout'),\n",
       " ('Prefix beam search decoding', 'beam-width'),\n",
       " ('ProxyAnchor Loss', '\\\\alpha'),\n",
       " ('ProxyAnchor Loss', '\\\\delta'),\n",
       " ('ProxyNCA Loss', '\\\\delta'),\n",
       " ('SoftTriple loss,', '\\\\delta'),\n",
       " ('SoftTriple loss,', '\\\\gamma'),\n",
       " ('SoftTriple loss,', '\\\\lambda'),\n",
       " ('SoftTriple loss,', '\\\\tau'),\n",
       " ('SupCon Loss', '\\\\tau'),\n",
       " ('Triplet Loss', 'm'),\n",
       " ('W&B Sweeps', 'counts'),\n",
       " ('\\\\sigma', 'T'),\n",
       " ('beam pruning', '\\\\lambda _{\\\\text{C}}'),\n",
       " ('beam pruning', '\\\\lambda _{\\\\text{R}}'),\n",
       " ('cross-validation', 'fold'),\n",
       " ('feedforward networks', 'activation'),\n",
       " ('feedforward networks', 'dropout'),\n",
       " ('feedforward networks', 'hidden layers'),\n",
       " ('feedforward networks', 'hidden state'),\n",
       " ('gradient clipping', 'max norm'),\n",
       " ('linear decay', 'epoch'),\n",
       " ('linear warmup', 'epoch'),\n",
       " ('linear warmup', 'steps'),\n",
       " ('method', 'T'),\n",
       " ('method', 'dimensionality of \\\\(\\\\mathbf {H}\\\\)'),\n",
       " ('model', '\\\\lambda _{\\\\text{C}}'),\n",
       " ('model', '\\\\lambda _{\\\\text{E}}'),\n",
       " ('model', '\\\\lambda _{\\\\text{R}}'),\n",
       " ('model', 'dropout'),\n",
       " ('model', 'epochs'),\n",
       " ('prefix beam search decoding', '\\\\alpha'),\n",
       " ('prefix beam search decoding', '\\\\beta'),\n",
       " ('system', 'dropout'),\n",
       " ('system', 'k')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c13f77b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation with at least two sets(a p v): 46\n",
      "paragraph with at least two sets(a p v): 13\n"
     ]
    }
   ],
   "source": [
    "## full triples\n",
    "apv_set = 0\n",
    "paragraph_set = 0\n",
    "gt_data = []\n",
    "for paragraph in joint_dataset.keys():\n",
    "    params2nums = {'a':0, 'p':0, 'v':0, 'c':0}\n",
    "    for rel in joint_dataset[paragraph]['relation'].keys():\n",
    "        source = joint_dataset[paragraph]['relation'][rel]['source'][0]\n",
    "        target = joint_dataset[paragraph]['relation'][rel]['target'][0]\n",
    "        params2nums[source] += 1\n",
    "        params2nums[target] += 1\n",
    "    if params2nums['a']!=0 and params2nums['p']!=0 and params2nums['v']!=0:\n",
    "        paragraph_set += 1\n",
    "        params2nums.pop('c')\n",
    "        min_ = min(params2nums.values())\n",
    "        #print(params2nums, min_)\n",
    "        apv_set += min_\n",
    "        # save data with 2 sets (a p v)\n",
    "        gt_data.append(joint_dataset[paragraph])\n",
    "#         print(params2nums)\n",
    "\n",
    "print('relation with at least two sets(a p v):', apv_set)\n",
    "print('paragraph with at least two sets(a p v):', paragraph_set)\n",
    "# print('left: ', len(gt_data))\n",
    "# print(gt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d62b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to JSON file\n",
    "with open(\"ap_rel.json\", \"w\") as json_file:\n",
    "    json.dump(list(ap_rel), json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
