[["feedforward networks", "activation"], ["system", "dropout"], ["prefix beam search decoding", "\\alpha"], ["ProxyNCA Loss", "\\delta"], ["method", "dimensionality of \\(\\mathbf {H}\\)"], ["SoftTriple loss,", "\\lambda"], ["Adam", "weight decay"], ["Adam", "learning rate"], ["Triplet Loss", "m"], ["ProxyAnchor Loss", "\\delta"], ["COCI (OpenCitations Index of Crossref open DOI-to-DOI references)", "Version"], ["feedforward networks", "hidden state"], ["ProxyAnchor Loss", "\\alpha"], ["BiLSTM", "dimensional hidden layers"], ["prefix beam search decoding", "\\beta"], ["SupCon Loss", "\\tau"], ["model", "\\lambda _{\\text{R}}"], ["SoftTriple loss,", "\\tau"], ["beam pruning", "\\lambda _{\\text{C}}"], ["Adam", "patience value"], ["system", "k"], ["CRPSE", "K"], ["FFNNs", "dimensional hidden layers"], ["AdamW", "batch size"], ["L", "\\alpha _1"], ["model", "\\lambda _{\\text{C}}"], ["feedforward networks", "dropout"], ["beam pruning", "\\lambda _{\\text{R}}"], ["model", "epochs"], ["\\sigma", "T"], ["linear warmup", "steps"], ["FFNNs", "dropout"], ["model", "\\lambda _{\\text{E}}"], ["gradient clipping", "max norm"], ["Adam", "epochs"], ["EMA", "m"], ["linear decay", "epoch"], ["feedforward networks", "hidden layers"], ["AdamW", "learning rates"], ["linear warmup", "epoch"], ["BiLSTM", "layer"], ["method", "T"], ["LSTMs", "variational dropout"], ["FFNNs", "hidden layers"], ["L", "\\alpha"], ["Prefix beam search decoding", "beam-width"], ["model", "dropout"], ["SoftTriple loss,", "\\gamma"], ["cross-validation", "fold"], ["SoftTriple loss,", "\\delta"], ["L", "\\alpha _2"], ["BiLSTM", "hidden state"], ["W&B Sweeps", "counts"], ["GNN", "layers"]]