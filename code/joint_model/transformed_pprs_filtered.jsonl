{"id": "1507.01422", "categories": "cs.CV cs.LG cs.NE", "paragraphs": ["The limited amount of training data for our architecture made overfitting a significant challenge, so we used different techniques to minimize its effects.\nFirstly, we apply norm constraint regularization for the maxout layers [7]. Secondly, we use data augmentation technique by mirroring all images.\nWe also tested a dropout layer [9] after the first fully connected layer, with a dropout ratio of 0.5 (50% of probability to set a neuron\u2019s output value to zero).\nHowever, this did not make much of a difference, so it is not included to the final model.\n", "The weights in all layers are initialized from a normal Gaussian distribution with zero mean and a standard deviation of 0.01, with biases initialized to 0.1.\nGround truth values that we used for training are saliency maps with normalized values between 0 and 1.\n", "For validation control purposes, we split the training partitions of iSUN and SALICON datasets into 80% for training and the rest for real time validation.\nThe network was trained with stochastic gradient descent (SGD) and Nesterov momentum SGD optimization method that helps the loss function to converge faster.\nThe learning rate was changing over time; it started with a higher learning rate 0.03 and decreased during the course of training until 0.0001.\nWe set 1,000 epochs to train a separate network for each dataset.\nFigures REF  and REF  present the learning curves for the iSUN and SALICON models, respectively.\n{FIGURE}{FIGURE}"]}
{"id": "1506.05929", "categories": "cs.CV", "paragraphs": ["All networks were trained using an effective training procedure (cf. [0]), with the values of the learning rate, momentum, and weight decay hyperparameters being \\(10^{-2}\\) , \\(0.9\\) , and \\(5 \\cdot 10^{-4}\\)  respectively. Whenever the error on the validation set stopped decreasing the learning rate was decreased by a factor 10. To deal with the increased size of the images the number of images per batch was adjust to fit into memory.\n"]}
{"id": "1501.05759", "categories": "cs.CV", "paragraphs": ["Unless otherwise specified we train all our models using the following\nparameters. Feature channels are HOG+LUV only. The final classifier\nincludes 4096 level-2 decision trees (L2, 3 stumps per tree), trained\nvia vanilla discrete Adaboost. Each tree is built by doing exhaustive\ngreedy search for each node (no randomization). The model has size\n\\(60\\hspace{0.0pt}\\times \\hspace{0.0pt}120\\ \\mbox{pixels}\\) , and is built\nvia four rounds of hard negative mining (starting from a model with\n32 trees, and then 512, 1024, 2048, 4096 trees). Each\nround adds \\(10\\,000\\)  additional negatives to the training set. The\nsliding window stride is \\(6\\ \\mbox{pixels}\\)  (both during hard negative\nmining and at test time).\n"]}
{"id": "1511.04587", "categories": "cs.CV cs.LG", "paragraphs": ["We provide parameters used to train our final model. We use a network of depth 20. Training uses batches of size 64. Momentum and weight decay parameters are set to 0.9 and \\(0.0001\\) , respectively.\n", "We train all experiments over 80 epochs (9960 iterations with batch size 64). Learning rate was initially set to 0.1 and then decreased by a factor of 10 every 20 epochs. In total, the learning rate was decreased 3 times, and the learning\nis stopped after 80 epochs. Training takes roughly 4 hours on GPU Titan Z.\n"]}
{"id": "1511.04491", "categories": "cs.CV cs.LG", "paragraphs": ["We use 16 recursions unless stated otherwise. When unfolded, the longest chain from the input to the output passes 20 conv. layers (receptive field of 41 by 41). We set the momentum parameter to 0.9 and weight decay to 0.0001. We use 256 filters of the size \\(3 \\times 3\\)  for all weight layers. Training images are split into 41 by 41 patches with stride 21 and 64 patches are used as a mini-batch for stochastic gradient descent.\n", "Learning rate is initially set to 0.01 and then decreased by a factor of 10 if the validation error does not decrease for 5 epochs. If learning rate is less than \\(10^{-6}\\) , the procedure is terminated. Training roughly takes 6 days on a machine using one Titan X GPU.\n"]}
{"id": "1509.07308", "categories": "cs.CL", "paragraphs": ["Training Data. To induce bilingual word embeddings as well as to be directly comparable with baseline representations from prior work, we use a dataset comprising a subset of comparable Wikipedia data available in three language pairs [106], [107]Available online: people.cs.kuleuven.be/\\(\\sim \\) ivan.vulic/software/: (i) a collection of 13,\u00a0696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18,\u00a0898 Italian-English Wikipedia article pairs (IT-EN), and (iii) a collection of 7,\u00a0612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations of each other. To be directly comparable to prior work in the two evaluation tasks [106], [107], we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger [87] is used for POS tagging and lemmatization. After the preprocessing steps vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair, and the training corpora are quite small: ranging from approximately 1.5M tokens for NL-EN to 4M for ES-EN. Exactly the same training data and vocabularies are used to train all representation models in comparison (both from Group I and Group II, see Section\u00a0).\n", "Trained BWESG Models To test the effect of random shuffling in the merge and shuffle BWESG strategy, we have trained the BWESG model with 10 random corpora shuffles for all three training corpora. We also train BWESG with the length-ratio shuffle strategy. All parameters are set to default suggested parameters for SGNS from the word2vec package: stochastic gradient descent (SGD) with a linearly decreasing global learning rate of 0.025, 25 negative samples, subsampling rate \\(1e-4\\) , and 15 epochs.\n", "We have varied the number of dimensions \\(d=100,200,300\\) . We have also trained BWESG with \\(d=40\\)  to be directly comparable to readily available sets of BWEs from prior work [12]. Moreover, to test the effect of window size on the final results, i.e., the number of positives used for training, we have varied the maximum window size \\(cs\\)  from 4 to 60 in steps of 4.We remind the reader that we slightly abuse terminology here, as the BWESG windows do not include the locality component any more.\n", "For Basic-MuPTM and Association-MuPTM, as in [105], a bilingual latent Dirichlet allocation (BiLDA) model was trained with \\(K=2000\\)  topics and the standard values for hyper-parameters: \\(\\alpha = 50/K\\) , \\(\\beta =0.01\\)  [92]. Post-hoc semantic space pruning was employed with the pruning parameter set to 200 for Basic-MuPTM and to 2000 for Association-MuPTM. We refer the reader to the relevant paper for more details.\n", "Baseline Representations: Group II  All baseline BWE models were trained with the same number of dimensions as BWESG: \\(d=100,200,300\\) . Other model-specific parameters were taken as suggested in prior work.\n", "For BiCVM, we use the tool released by the authors.https://github.com/karlmoritz/bicvm We train an additive model, with hinge loss margin \\(mrg=d\\)  as in the original paper, batch size of 50, and noise parameter of 10. All models were trained with 200 iterations.\n", "For Mikolov, we train two monolingual SGNS models using the original word2vec package, SGD with a global learning rate of 0.025, 25 negative samples, subsampling rate \\(1e-4\\) , and 15 epochs. The seed lexicon required to learn the mapping between two monolingual spaces is exactly the same as for Traditional-PPMI.\n", "For BilBOWA, we use SGD with a global learning rate 0.15 for trainingSuggestions for parameter values received through personal correspondence with the authors. The software is available online: https://github.com/gouwsmeister/bilbowa, 25 negative samples, subsampling rate \\(1e-4\\) , and 15 epochs. For BilBOWA and Mikolov, we vary the window size the same way as in BWESG.\n"]}
{"id": "1512.09272", "categories": "cs.CV", "paragraphs": ["The model training is based on stochastic gradient descent (SGD) that involves: 1) the use of a learning rate of \\(0.01\\)  that gradually (automatically computed based on the number of epochs set for training) decreases after each epoch until it reaches \\(0.0001\\) ; 2) a momentum set at \\(0.9\\) , 3) weight decay of \\(0.0005\\) , and 4) data augmentation by rotating the pair of patches by 90, 180, and 270 degrees, and flipping the images horizontally and vertically (, augmented 5 times: 3 rotations and 2 flippings)\u00a0[35].\nThe training set for the triplet and siamese networks consists of a set of \\(250,000\\)  triplets, which are sampled randomly from the aforementioned set of \\(500,000\\)  pairs of matching and non-matching image patches, where it is important to make sure that the triplet contains one pair of matching image patches and one patch that belongs to a different class of this pair.\nThe mini-batch of the SGD optimisation consists of 250 triplets (randomly picked from this \\(250K\\)  set of triplets), which is used to compute the global loss in (REF )\u00a0and\u00a0(REF ).\nOur Matlab implementation takes \\(\\approx 56\\)  hours for training a model and processes \\(16K\\)  images/sec during testing on a GTX 980 GPU.\n", "The triplet networks TNet-TLoss and TNet-TGLoss use the three towers of ConvNets (see Fig.\u00a0REF ) to learn an embedding of size 256 (we choose this number of dimensions based on the feature dimensionality of the models in\u00a0[35], which also have 256 dimensions before the fully connected layer). During testing, only one tower is used (all three towers are in fact the same after training) to compute the embedded features, which are compared based on the \\(\\ell _2\\)  norm of the distance between these embedded features.\nThe network weights for the TNet-TLoss network are initialised randomly and trained for 100 epochs, whereas the weights for the TNet-TGLoss network are trained for 50 epochs after being initialised using the weights from TNet-TLoss network trained for 50 epochs (the initialisation from the TNet-TLoss model trained with early stopping provided a good initialisation for TNet-TGLoss).\nThis number of epochs for training is decided based on the convergence obtained in the training set with respect to the loss function.\nMoreover, the margin parameter \\(m=0.01\\)  in (REF ) and \\(\\gamma =1\\) , \\(t=0.4\\)  and \\(\\lambda =0.8\\)  in (REF ) are estimated via cross validation.\nFor the siamese networks SNet-GLoss and CS-SNet-GLoss, the weights are randomly initialised and trained for 80 epochs (again, based on the convergence of the training set). Finally, \\(m=1\\)  and\n\\(\\lambda =1\\)  in (REF ) are also estimated via cross validation.\n{FIGURE}"]}
{"id": "1611.02064", "categories": "cs.CV", "paragraphs": ["Throughout the experiments, we have fixed the learning rate to be \\(0.0001\\)  and RMSprop\u00a0[12] optimization algorithm is used with momentum fixed at \\(0.7\\) . Our model is trained for\n60 epochs with a batch size of 32.\n"]}
{"id": "1611.01487", "categories": "cs.CL", "paragraphs": ["To train our models, we used the train portion of the datasets as-is and evaluated on the test portion the model which performed best on the development portion of the dataset, without conducting any specific pre-processing steps on the data. We train the models for a maximum of 100 epochs over the training set. To avoid long training time, we trained the model for 20 epochs for datasets larger than 50k examples, and for 5 epochs for datasets larger than 200k examples. The models were implemented using the python bindings of the dynet toolkit.https://github.com/clab/dynet\n", "We trained the network by optimizing the expected output sequence likelihood using cross-entropy loss as mentioned in equation REF . For optimization we used ADADELTA [42] without regularization. We updated the weights after every example (i.e. mini-batches of size 1). We used the dynet toolkit implementation of an LSTM network with two layers for all models, each having 100 entries in both the encoder and decoder. The character embeddings were also vectors with 100 entries for the CELEX experiments, and with 300 entries for the SIGMORPHON and Wiktionary experiments.\n", "The morpho-syntactic attribute embeddings were vectors of 20 entries in all experiments. We did not use beam search while decoding for both the hard and soft attention models as it is significantly slower and did not show clear improvement in previous experiments we conducted. For the character level alignment process we use the implementation provided by the organizers of the SIGMORPHON2016 shared task.https://github.com/ryancotterell/sigmorphon2016\n"]}
{"id": "1606.02003", "categories": "cs.CL", "paragraphs": ["In training of the\nneural networks, we limit the source and target vocabularies to the most frequent 30K words in both Chinese and\nEnglish, covering approximately \\(97.7\\%\\)  and \\(99.3\\%\\)  of the two corpora respectively.\nThe dimensions of word embedding is 512 and the size of the hidden\nlayer is 1024.\nThe dimemsion of each cell in \\(\\mathbf {M}^\\textsc {B}\\)  is set to 1024 and the number of cells \\(n\\)  is set to 8.\n"]}
{"id": "1606.07659", "categories": "cs.LG cs.IR", "paragraphs": ["We train a one-hidden layer autoencoders with hyperbolic tangent transfer functions. The layers have 600 hidden neurons. Weights are randomly initialized with a uniform law\n\\(\\mathbf {W_{ij}} \\sim {\\cal U}\\left[-1/\\sqrt{n}, 1/\\sqrt{n}\\right]\\) .\nThe latent dimension of the low rank matrix of tags/friendships is set to 50. Hyperparamenters were are fine-tuned by a genetic algorithm and the final learning rate, learning decay and weight decay are respectively set to \\(0.7\\) , \\(0.3\\)  and \\(0.5\\) . \\(\\alpha \\) , \\(\\beta \\)  and masking ratio are set to 1, \\(0.5\\)  and \\(0.25\\) .\n"]}
{"id": "1608.07636", "categories": "cs.LG stat.ML", "paragraphs": ["\n\\(\\theta _{\\max }\\in \\lbrace 0,1\\rbrace \\) : We can identify \\(A+q_0B+D\\)  and \\(q_1B-AD\\) . Note that for \\(\\theta _{\\max }=0\\) , we have \\(q_0=1\\)  and \\(q_1=0\\) .\n\n\\(\\theta _{\\max }\\ge 2\\) : We can identify \\(A+q_0B+D\\) , \\(q_1B-AD\\) , and the matrix \\(B\\)  up to a scalar multiple.\n\n\n"]}
{"id": "1612.05386", "categories": "cs.CV", "paragraphs": ["In our system, the dimensions of the encoded question/image/fact features (\\(d\\)  in Eq.\u00a0REF ) and the hidden layers of the LSTM and co-attention models (\\(h\\)  in Eq.\u00a0REF ) are set to 512.\nFor facts, the subject/relation/object entities are embedded to 128/128/256 dimensional vectors respectively and concatenated to form 512d vectors.\nWe used two layers of LSTM model.\nFor the MLP in Eq.\u00a0(REF ), the dimensions of \\(\\mathbf {h}^w\\)  and \\(\\mathbf {h}^p\\)  are also 512, while the dimension of \\(\\mathbf {h}^q\\)  is\nset to 1024 for the VQA dataset and 2048 for the Visual Genome dataset.\nFor prediction, we take the top 3000 answers for the VQA dataset and the top 5000 answers for the Visual Genome dataset.\nThe whole system is implemented on the Torch7\u00a0[5]The code and pre-trained models will be released upon the acceptance of the paper. and trained end-to-end but with fixed CNN features.\nFor optimzation, the RMSProp method is used with a base learning rate of \\(2\\times 10^{-4}\\) \nand momentum \\(0.99\\) . The model is trained for up to 256 epochs until the validation error has not improved in the last 5 epochs.\n"]}
{"id": "1604.02426", "categories": "cs.CV", "paragraphs": ["Our training samples are derived from the dataset used in the work of Schonberger\u00a0et al. \u00a0[39], which consists of 7.4 million images downloaded from Flickr using keywords of popular landmarks, cities and countries across the world.\nThe clustering procedure\u00a0[36] gives \\(19,546\\)  images to serve as query seeds.\nThe extensive retrieval-SfM reconstruction\u00a0[46] of the whole dataset results in \\(1,474\\)  reconstructed 3D models.\nRemoving overlapping models leaves us with 713 3D models containing \\(163,671\\)  unique images from the initial dataset.\nThe initial dataset contained on purpose all images of Oxford5k and Paris6k datasets.\nIn this way, we are able to exclude 98 clusters that contain any image (or their near duplicates) from these test datasets.\n", "Each training and validation tuple contains 1 query, 1 positive and 5 negative images.\nThe pool of candidate positives consists of \\(k=100\\)  images with closest camera centers to the query.\nIn particular, for method \\(m_3\\) , the inliers overlap threshold is \\(t_i=0.2\\) , and the scale change threshold \\(t_s=1.5\\) .\nHard negatives are re-mined 3 times per epoch, i.e. roughly every \\(2,000\\)  training queries.\nGiven the chosen queries and the chosen positives, we further add 20 images per cluster to serve as candidate negatives during re-mining.\nThis constitutes a training set of \\(22,156\\)  images and it corresponds to the case that all 3D models are included for training.\n"]}
{"id": "1609.05722", "categories": "cs.CV", "paragraphs": ["Taking into account the fact that generic natural image priors have an interesting effect only at medium noise levels\n[20], we conduct our\ntraining experiments based on the images with relatively high peak value. In this study, we choose \\(\\text{peak} = 40\\) .\n", "As we focus on the problem of Poisson denoising for natural images in a general sense,\nFoE prior models should be trained based on natural image datasets.\nWe conducted our training experiments using the training images from the BSDS300 image\nsegmentation database\u00a0[1].\nWe used the whole 200 training images, and randomly sampled one \\(128 \\times 128\\) \npatch from each training image,\nresulting in a total of 200 training samples. The gray value range of all the\nimages was first\nscaled with peak value \\(\\text{peak} = 40\\) , and then corrupted with\nPoisson noise using the Matlab function\npoissrnd.\n", "Concerning the model capacity of the trained FoE models,\nfollowing a common setup of filters for the FoE model in [5],\nwe learned 48 filters with dimension \\(7 \\times 7\\) . As we focused on mean-zero filters,\nwe made use of a modified DCT-7 basis (the atom with constant entries is excluded) to construct\nour learned filters. We initialized the filters using the modified DCT-7 basis.\nAll filters have the same norm and weight, which are 0.1 and 1, respectivelyAs shown in previous work [5], the training process is not sensitive to the initialization..\n"]}
{"id": "1608.05604", "categories": "cs.CL", "paragraphs": ["For both the reader and the decoder networks, we choose a one-layer\nLSTM network with 1,000 memory cells. The attention network is a\none-layer feedforward network. For the loss estimator \\(U\\) , we use a\nbidirectional LSTM with 20 memory cells. Input data is split into\nsequences of 50 tokens, which are used as the input sequences for\nNEAT, disregarding sentence boundaries. Word embeddings have 100\ndimensions, are shared between the reader and the attention network,\nand are only trained during the training of the reader. The\nvocabulary consists of the 10,000 most frequent words from the\ntraining corpus. We trained NEAT on the training set of the\nDaily Mail section of the corpus described by\n, which consists of 195,462 articles from\nthe Daily Mail newspaper, containing approximately 200 million\ntokens. The recurrent networks and the attention network were each\ntrained for one epoch. For initialization, weights are drawn from the\nuniform distribution. We set \\(\\alpha = 5.0\\) , \\(\\gamma = 5.0\\) , and used\na constant learning rate of \\(0.01\\)  for \\(A\\) .\n"]}
{"id": "1610.02692", "categories": "cs.CL cs.CV cs.MM", "paragraphs": ["We set the batch size to be the maximum that we could save in the GPU RAM, having a value of just 32 samples. We need to consider that we also need to fit the compiled model (its weights) in the GPU RAM and this is very expensive as some of our layers, and thus its weights, are huge as we will see now.\n", "For the question maximum length we have taken the length of the largest question in the training subset. This parameter is used in the last LSTM so it knows when it has seen the whole question and can output its state. We found that for the training set, the maximum question length is 22. The questions that have a smaller length have been left-padded with 0, so the input is 'inactive' and then it is activated with the question tokens. The network has been configured to ignore these padding zeros.\n", "The vocabulary size is crucial for the softmax layer as this will set the number of neurons of this layer. A value of 20.000 was chosen as it is quite common and respects the tradeoff between number of words (which give more flexibility to the model) and number of weights to train (time consuming, training problems, memory constraints).\n", "For this model we chose the number of LSTM hidden units and the embedding size to be the same, with a value of 100. We used this value for simplicity and due to some experience of a team member regarding these parameters.\n"]}
{"id": "1603.06067", "categories": "cs.CL", "paragraphs": ["\nAs mentioned above, Eq.\u00a0(REF ) and () show that the non-compositional embeddings are mainly updated when \\(\\alpha (p)\\)  is close to 0, and vice versa.\nThe partial derivative \\(\\frac{\\partial {J}}{\\partial {\\mathbf {c}(p)}}\\)  is used to update the model parameters in the composition function via the backpropagation algorithm.\nAny differentiable composition functions can be used in our method.\n"]}
{"id": "1602.07776", "categories": "cs.CL cs.NE", "paragraphs": ["For the discriminative model, we used hidden dimensions of 128 and 2-layer LSTMs (larger numbers of dimensions reduced validation set performance). For the generative model, we used 256 dimensions and 2-layer LSTMs. For both models, we tuned the dropout rate to maximize validation set likelihood, obtaining optimal rates of 0.2 (discriminative) and 0.3 (generative). For the sequential LSTM baseline for the language model, we also found an optimal dropout rate of 0.3. For training we used stochastic gradient descent with a learning rate of 0.1. All parameters were initialized according to recommendations given by [22].\n"]}
{"id": "1612.01202", "categories": "cs.CV", "paragraphs": ["Training Databases. We train our system using the 3DDFA data of [54]. The 3DDFA data provides projection and 3DMM model parameters for the Basel [34] + FaceWarehouse [6] model for each image of the 300W database. We use the topology defined by this model to define our UV space and rasterize the images to obtain per-pixel ground truth UV coordinates. Our training set consists of the LFPW trainset, Helen trainset and AFW, thus 3148 images\nthat are captured under completely unconstrained conditions\nand exhibit large variations in pose, expression, illumination,\nage, etc.\nMany of these images contain multiple faces, some of which are not annotated. We deal with this issue by employing the out-of-the-box DPM face detector of Mathias et al.\u00a0[31] to obtain the regions that contain a face for all of the images. The detected regions that do not overlap with the ground truth landmarks do not contribute to the loss. For training and testing, we have rescaled the images such that their largest side is 800 pixels.\n", "CNN Training. For the dense regression network, we adopt a ResNet101\u00a0[18] architecture with dilated convolutions (atrous)\u00a0[9], [28], such that the stride of the CNN is 8. We use bilinear interpolation to upscale both the \\(\\hat{q}\\)  and \\(\\hat{r}\\)  branches before the losses. The losses are applied at the input image scale and back-propagated through interpolation. We apply a weight to the smooth \\(L1\\)  loss layers to balance their contribution. In our experiments, we have used a weight of 40 for quantized\u00a0(\\(d=0.1\\) ) and a weight of 70 for non-quantized regression, which are determined by a coarse cross validation. We initialize the training with a network pre-trained for the MS COCO segmentation task\u00a0[26]. The new layers are initialized with random weights drawn from Gaussian distributions. Large weights of the regression losses can be problematic at initialization even with moderate learning rates. To cope with this, we use initial training with a lower learning rate for a warm start for a few iterations. We then use a base learning rate of \\(0.001\\)  with a polynomial decay policy for \\(20k\\)  iterations with a batch size of 10 images. During training, each sample is randomly scaled with one of the ratios \\([0.5, 0.75, 1, 1.25, 1.5]\\)  and cropped to form a fixed \\(321\\times 321\\)  input image.\n"]}
{"id": "1606.01561", "categories": "cs.CV", "paragraphs": ["We train faster R-CNN networks built on the VGG16\u00a0[20] and AlexNet\u00a0[15], pretrained on the ImageNet-1k\u00a0[3] classification dataset. VGG16 has sixteen convolution layers \u00a0[20] and AlexNet has only five\nconvolutional layers\u00a0[15]. Rather than using the convolution features of the last pooled layer (as is done in the original faster R-CNN paper), we use features from convolutional layers that are the bottom layers of the previous pooling layer. For VGG16 it is conv4_3. We resize the \\(roi pooling\\)  window size accordingly. For VGG16 the window size changes from 7x7 to 13x13. We also reduce the feature stride by a factor of two for avoiding one pooling stage. For VGG16 and AlexNet, fully connected layers are used as the R-CNN branch. The weights in these layers are initialized with random gaussian noise. As the standard procedure introduced in faster R-CNN, we randomly sample 128 positive and 128 negative \\(roi\\)  proposals per batch to train the R-CNN layer. For all the experiments, we use initial learning rate of 0.0005, step size 50000 and momentum 0.9. A total of 70K iterations are run during R-CNN training starting from imagenet pre-trained weights for the convolution layers.\n"]}
{"id": "1611.03718", "categories": "cs.CV cs.LG", "paragraphs": ["The weights for the Deep Q-network were initialized from a normal distribution. For learning, we used Adam optimizer [6] with a learning rate of 1e-6 to avoid that the gradients explode. We trained each model for 50 epochs.\n"]}
{"id": "1602.01255", "categories": "cs.CV", "paragraphs": ["All networks were trained using an effective training procedure (cf.\n[0]), with the values of the learning rate, momentum, and\nweight decay hyperparameters being \\(10^{-2}\\) , \\(0.9\\) , and \\(5 \\cdot 10^{-4}\\) \nrespectively. Whenever the error on the validation set stopped decreasing the\nlearning rate was decreased by a factor 10.\n"]}
{"id": "1601.02225", "categories": "cs.CV cs.DS", "paragraphs": ["In this section, parameter quantization is described, separately. Parameter \\(\\theta \\)  varies in range \\([0,\\pi )\\)  to cover all directions. For this purpose two points \\(C\\)  and \\(O\\)  were used to represent orientation. The angle between the line passing from points \\(C\\)  and \\(O\\)  and the horizontal axis of coordinate system, forms \\(\\theta \\) . Suppose that square \\(S\\)  is \\(m\\times n\\) . Point \\(C\\)  is fixed and located at pixel \\(C=(\\lfloor \\frac{m}{2} \\rfloor ,\\lceil \\frac{n}{2} \\rceil )\\) . Location of point \\(O\\)  varies and can be any pixel in set \\(D_O=\\lbrace (0,y)|y=0,\\ldots ,n-1\\rbrace \\cup \\lbrace (x,n-1)|x=0,\\ldots ,m-1\\rbrace \\) . Thus, the number of difference state of parameter \\(\\theta \\)  is equal to \\(m+n-1\\) .\n"]}
{"id": "1612.02482", "categories": "cs.CL cs.LG cs.NE", "paragraphs": ["The error function used was the sampled SoftMax loss to ensure a large target vocabulary could be accommodated [15]. A zoomed inset graph (Fig. REF ) has been used to visualize the values of the error function for the RNNSearch + Word2Vec and RNNMorph models with 4 hidden layers. It can be seen that the RNNMorph model is consistently better in terms of the perplexity values through the time steps.\n{FIGURE}"]}
{"id": "1610.09300", "categories": "cs.LG math.OC stat.ML", "paragraphs": ["Let \\(Q,Q\\in \\mathbb {R}^{m\\times m}_+\\)  and assume \\(0 \\le Q_{i,j}\\le Q_{i,j}\\)  for every \\(i,j\\in [m]\\) , then \\(\\rho (Q)\\le \\rho (Q)\\) , see Corollary 3.30 [2]. It follows that \\(\\rho (A)\\)  in Theorem REF  is increasing w.r.t. \\(\\rho _u,\\rho _w,\\rho _x\\)  and the number of hidden units \\(n_1\\) . Moreover, \\(\\rho (A)\\)  is decreasing w.r.t. \\(p_u,p_w\\)  and in particular, we note that for any fixed architecture \\((n_1,\\alpha ,\\rho _u,\\rho _w)\\)  it is always possible to find \\(p_u,p_w\\)  large enough so that \\(\\rho (A)<1\\) . Indeed, we know from the Collatz-Wielandt formula (Theorem 8.1.26 in [9]) that \\(\\rho (A)=\\rho (A^T)\\le \\max _{i\\in [K+1]}(A^Tv)_i/v_i\\)  for any \\(v\\in \\mathbb {R}^{K+1}_{++}\\) . We use this to derive lower bounds on \\(p_u,p_w\\)  that ensure \\(\\rho (A)<1\\) . Let \\(v=(p_w-1,\\ldots ,p_w-1,p_u-1)\\) , then \\((A^Tv)_i < v_i\\)  for every \\(i\\in [K+1]\\)  guarantees \\(\\rho (A)<1\\)  and is equivalent to\n\\(p_w> 4(K+1)\\xi _1+3 \\qquad \\text{and}\\qquad p_u >2(K+1)(\\Vert  \\alpha  \\Vert _{\\infty }+2\\xi _2)-1,\\) \n"]}
{"id": "1602.08210", "categories": "cs.LG cs.NE", "paragraphs": ["For each experiment,\nwe use Adam\u00a0 for optimization, and conduct\na grid search\non the learning rate in \\(\\lbrace 10^{-2}, 10^{-3},10^{-4},10^{-5}\\rbrace \\) .\nFor \\(tanh\\)  RNNs,\nthe parameters are initialized with samples from a uniform distribution.\nFor the LSTM networks we adopt a similar initialization scheme, while\nthe forget gate biases are chosen by the grid search on \\(\\lbrace -5, -3, -1, 0, 1, 3, 5\\rbrace \\) .\nWe employ early stopping during training, and the batch size is set to 50.\n"]}
{"id": "1605.08618", "categories": "cs.LG stat.ML", "paragraphs": ["Do we need to include actual prior knowledge when using VI approaches?\nNo, the main advantage in relying on a training that is based on variational Bayesian inference is, that the introduction of the distributions over the model parameters prevents us from running into local minima. Especially those that arise when a component (or a state) collapses over a single observation (or multiple observations with identical characteristics). In that case the variance approaches 0 (and the mean \\(\\infty \\)  since \\(\\int p(x) \\mathrm {d}x = 1\\) ) and would normally dramatically increase the likelihood for the model \u2013 this is also known as singularity (and one of the known drawbacks of normal EM).\nUsing the 2nd order approaches prevents this by a low density in the parameter space where the variance approaches 0 (Therefore \\(p(x|\\sigma )\\rightarrow \\infty \\)  is attenuated by a low density for \\(p(\\sigma )\\) ).\n"]}
{"id": "1611.06624", "categories": "cs.LG cs.CV", "paragraphs": ["All the parameters used in the optimizer are the same as those of the\noriginal WGAN. Specifically, we used the RMSProp optimizer\n[40] with the learning rate of \\(0.00005\\) .\nAll the weights in the temporal generator and the discriminator are initialized with HeNormal [7], and the weights in the image generator are initialized with the uniform distribution within a range of \\([-0.01, 0.01]\\) .\nChainer [41] was used to implement all models and for experiments.\n", "For comparison, we employed the conventional clipping method and the SVC\nto train models with the WGAN. In the conventional clipping method,\nwe carefully searched clipping parameter \\(c\\)  and confirmed that\nthe best value is \\(c = 0.01\\) . We set \\(n_D\\)  to 1 for the both methods.\n"]}
{"id": "1610.01030", "categories": "cs.CL cs.CY cs.LG", "paragraphs": ["Before performing the online learning, we assume that an initial model \\(\\theta _0\\)  exists. In our case, we train the initial model using all the datasets from CrisisNLP except the Nepal earthquake. For online training, we sort the Nepal labeled data based on the time stamp of the tweets. This brings the tweets in their posting order. Next, the dataset \\(D\\)  is divided at each time interval \\(d_t\\)  in which case \\(D\\)  is defined as: D = \\(\\sum _{t=1}^T d_t\\)  where \\(d_t= 200\\) .\nFor each time interval \\(t\\) , we divide the available labeled dataset into a train set (70%), dev set (10%), and a test set (20%)\nusing ski-learn toolkit's module [18], which ensured that the class distribution remains reasonably balanced in each subset.\n", "Based on the data splitting strategy mentioned above, we start online learning to train a binary and a multi-class classifier. For the binary classifier training, we merge all the informative classes to create one general Informative class. We train CNN models by optimizing the cross entropy in Equation REF  using the gradient-based online learning algorithm ADADELTA [26].Other algorithms (SGD, Adagrad) gave similar results. The learning rate and\nthe parameters were set to the values as suggested by the authors.\nThe maximum number of epochs was set to 25. To avoid overfitting, we use dropout [22] of hidden units and early stopping based on the accuracy on the validation set.\\(l_1\\)  and \\(l_2\\)  regularization on weights did not work well. We experimented with \\(\\lbrace 0.0, 0.2, 0.4, 0.5\\rbrace \\)  dropout rates and \\(\\lbrace 32, 64, 128\\rbrace \\)  minibatch sizes.\nWe limit the vocabulary (\\(V\\) )\nto the most frequent \\(P\\%\\)  (\\(P\\in \\lbrace 80, 85, 90\\rbrace \\) ) words in the training corpus. The word vectors in \\(L\\)  were initialized with the pre-trained embeddings.\nWe use rectified linear units (ReLU) for the activation functions (\\(f\\) ), \\(\\lbrace 100, 150, 200\\rbrace \\)  filters each having window size (\\(L\\) ) of \\(\\lbrace 2, 3, 4\\rbrace \\) , pooling length (\\(p\\) ) of \\(\\lbrace 2,3, 4\\rbrace \\) , and \\(\\lbrace 100, 150, 200\\rbrace \\)  dense layer units. All the hyperparameters are tuned on the development set.\n"]}
{"id": "1609.04337", "categories": "cs.CV cs.AI", "paragraphs": ["We collected stereo image pairs using a PointGrey BumbleBee2 BB2-03S2C-25 wide-angle color stereoscopic camera, with focal length \\(f=2.5mm\\) , baseline distance \\(B=120mm\\)  and resolution \\(640 \\times 480\\)  at 25 frames per second. The images were rectified using the Triclops proprietary PointGrey middleware. The camera was mounted on a TurtleBot 2 mobile robot base, which was manually controlled in an office environment to collect data. A total of 6301 frames were captured, corresponding to 4 minutes and 10 seconds of video.\n"]}
{"id": "1301.2840", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["Different to [1], our models are trained in an unsupervised fashion on the available patches. We train on one scene (400,000 randomly selected patches from this scene) and evaluate the performance on the test set of every scene. This allows us to investigate the self-taught learning paradigm [31]. We also train on all three scenes jointly (represented by 1.2 million image patches) and then evaluate again every scene individually.\n"]}
{"id": "1311.7251", "categories": "cs.CV cs.LG cs.NE", "paragraphs": ["\nFor each location \\(q\\)  in the image matrix, extract its disk-shaped neighborhood from each of the \\(K\\)  images \\(\\tilde{f}_i\\) , \\(i=1,...,K\\) . The radius of the disk is set to 3 pixels (containing 29 pixels).\n\nCompose a set of inputs for the ANN by stacking the pixel intensities from the \\(K\\)  neighborhoods into one vector. Normalize this vector in the training stage (discussed below).\n\nApply the ANN to produce a set of output values, which are the intensity values in the disk-shaped neighborhood of \\(q\\)  in the image \\(\\hat{f}\\) . This disk has the same radius of 3 pixels.\n\nBy this design, each pixel in the output image is covered by 29 disk-shaped patches; its final value is computed by averaging all those contributions.\n\n", "We detail now on the several of the steps in the list above. In the training stage, the neural network is tuned to minimize the discrepancy between true values in each output vector and those produced by the network from the set of noisy inputs. A vector of inputs is built, as described above, for a location \\(q\\)  in a reference image \\(f\\)  from a training set, using data from noisy reconstructions. The corresponding vector of outputs is the disk-shaped neighborhood of \\(q\\)  in the reference image. Thus, for each image \\(f\\)  we produce the set \\(\\tilde{f}_1, ...,\\tilde{f}_K\\)  using pre-defined FBP filters and sample them to build the training dataset. The image is sampled on a cartesian grid, choosing every third pixel \\(q\\)  both in horizontal and vertical directions. The pair of input and output vectors for the neural networks is an example used in the training process. Examples from all the training images \\(f\\)  are put in one pool. A portion of this pool, having a very low variance in the inputs vector, is discarded (specifically, the threshold is set to \\(10^{-6}\\)  times the maximal variance). Those examples correspond to regions of air, since no constant patch in any kind of tissue can be observed in the noisy FBP images. This step leads to an empirical improvement in the performance of the ANN.\n"]}
{"id": "cs/0001020", "categories": "cs.CL", "paragraphs": ["\n\\(\\underline{z}=h_0.tag, h_0.word, h_{-1}.tag, h_{-1}.word\\)  for\norder 4;\n\n\\(\\underline{z}=h_0.tag, h_0.word, h_{-1}.tag\\)  for order 3;\n\n\\(\\underline{z}=h_0.tag, h_0.word\\)  for order 2;\n\n\\(\\underline{z}=h_0.tag\\)  for order 1;\n\n", "The higher order events \u2014 closer to the root of the linear\ninterpolation scheme in Figure\u00a0REF  \u2014 become more\nand more diverse during the first estimation stage, as opposed to the\nlower order events. This shows that the \u201cN-best\u201d parses for a given\nsentence change from one iteration to the next. Although the E0 counts\nwere collected from \u201c1-best\u201d parses \u2014 binarized treebank parses\n\u2014 the increase in number of maximal order types from E0 to E1 \u2014\ncollected from \u201cN-best\u201d, N = 10 \u2014 is far from dramatic, yet higher\nthan that from E1 to E2 \u2014 both collected from \u201cN-best\u201d parses.\n"]}
{"id": "1911.10470", "categories": "cs.CL", "paragraphs": ["To use the pre-trained BERT models, we used the public code base, pytorch-transformers,https://github.com/huggingface/pytorch-transformers. written in PyTorch.https://pytorch.org/.\nFor optimization, we used the code base's implementation of the Adam optimizer\u00a0[11], with a weight-decay coefficient of \\(0.01\\)  for non-bias parameters.\nA warm-up strategy in the code base was also used, with a warm-up rate of \\(0.1\\) .\nMost of the settings follow the default settings.\nTo train our recurrent retriever, we set the learning rate to \\(3\\cdot 10^{-5}\\) , and the maximum number of the training epochs to three.\nThe mini-batch size is four; a mini-batch example consists of a question with its corresponding paragraphs.\nTo train our reader model, we set the learning rate to \\(3\\cdot 10^{-5}\\) , and the maximum number of training epochs to two.\nEmpirically we observe better performance with a larger batch size as discussed in previous work\u00a0[17], [25], and thus we set the mini-batch size to 120.\nA mini-batch example consists of a question with its evidence paragraphs.\nWe will release our code to follow our experiments.\n"]}
{"id": "1909.08103", "categories": "cs.CL cs.SD eess.AS", "paragraphs": ["The rest of the 571 hrs of 3,207 lecture recordings\n(excluding the same speaker's lectures in the evaluation sets) were used for AM and language model (LM) training.\nWe generated two-speaker mixed speech for training data in accordance with the following protocol.\n", "\nPrepare a list of speech samples (= main list).\n\nShuffle the main list to create a second list under the constraint that the same speaker does not appear in the same line in the main and second lists.\n\nMix the audio in the main and second lists one-by-one with a specific signal-to-interference ratio (SIR). For training data, we randomly sampled an SIR as follows.\n\nIn 1/3 probability, sample the SIR from a uniform distribution between -10 and 10 dB.\n\nIn 1/3 probability, sample the SIR from a uniform distribution between 10 and 60 dB. The transcription of the interference speaker was set to null.\n\nIn 1/3 probability, sample the SIR from a uniform distribution between -60 and -10 dB. The transcription of the target speaker was set to null.\n\n\nThe volume of each mixed speech was randomly changed to enhance robustness against volume difference.\n\n", "We trained a TS-AM consisting of a convolutional neural network (CNN),\ntime-delay NN (TDNN) ,\nand long short-term memory (LSTM) , as shown in fig:ts-am.\nThe input acoustic feature for the network was a 40-dimensional FBANK without normalization.\nA 100-dimensional i-vector was also extracted and used for the target-speaker embedding\nto indicate the target speaker.\nFor extracting this i-vector, we randomly selected an utterance of the same speaker.\nWe conducted 8 epochs of training on the basis of LF-MMI,\nwhere the initial learning rate was set to 0.001 and exponentially decayed to 0.0001 by the end of the training.\nWe applied \\(l2\\) -regularization and CE-regularization \nwith scales of 0.00005 and 0.1, respectively.\nThe leaky hidden Markov model coefficient was set to 0.1.\nA backstitch technique  with a backstitch scale of 1.0 and backstitch interval of 4 was also used.\n", "For comparison, we trained another TS-AM without the auxiliary loss.\nWe also trained a \u201cclean AM\u201d using clean, non-speaker-mixed speech.\nFor this clean model, we used a model architecture without the auxiliary output branch,\nand an i-vector was extracted every 100 msec for online speaker/environment adaptation.\n"]}
{"id": "1910.14388", "categories": "cs.LG stat.ML", "paragraphs": ["For all the baselines and Generative Graph Transformer variations, we run the same hyper-parameter search on: learning rate, batch size, weight decay, and \\(\\lambda \\)  coefficient. In all the experiments we use the Adam optimizer [31] with parameters \\(\\eta \\in [3 \\cdot 10^{-4}; 5 \\cdot 10^{-4}]\\) , \\(\\beta _1 = 0.9\\) , \\( \\beta _2 = 0.999\\)  and \\(\\epsilon = 10^{-8}\\) . For all the models, the batch size is set to 64, except for the RNN where we find 16 to be better. Weight decay parameters in the range \\([10^{-5}, 5\\cdot 10^{-5}]\\)  are found to be optimal for regularization. For the \\(\\lambda \\)  hyper-parameter, we notice best performance in the range \\([0.30, 0.70]\\) , and we set \\(\\lambda =0.5\\)  in our experiments. We also try different sizes for the output of the node-wise GRU in GraphRNN, and for the number of decoder blocks and heads in the GGT.\n"]}
{"id": "1911.09994", "categories": "cs.CL cs.AI", "paragraphs": ["After each hidden layer, a dropout layer of \\(0.5\\)  probability for regularization is added. Regularization helps in over-fitting of the model. Then each epoch of the training phase is optimized using the Adam optimizer [13]. Adam is a momentum based gradient descent optimization technique. We are using a mini-batch of size 128 pairs in each training epoch. The first hidden layer has 512 units and the second hidden layer has 128 units. We use Rectified Linear Unit \\((relu)\\)  activation functions in both the hidden layers and Sigmoid for the last layer.\n"]}
{"id": "1906.00891", "categories": "cs.CV cs.SY eess.SY", "paragraphs": ["The network architecture used in our experiment is composed of four convolutional layers, four pooling layers and three fully connected layers, as shown in Fig. REF .\nThe network was trained by the stochastic gradient descent algorithm [31]. \\(L_2\\)  regularization with a weight decay 0.0001 was adopted to prevent overfitting.\nThe learning rate was set as 0.001 and the training was stopped after 40 epochs. The implementation of CNN-DC was based on Tensorflow [32].\nThe training was conducted on a Intel Xeon E5-2690 CPU with a TITAN Xp GPU.\n"]}
{"id": "1905.00921", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["We implement the model in PyTorch [13]. All of the experiments are conducted on an Amazon AWS p3.16xlargehttps://aws.amazon.com/ec2/instance-types/p3/ cluster with 8 Tesla V100 GPUs. For initial training, we train the model for 20 epochs with learning rate 0.001, batch size 512. For the continuous domain adaptation, we add the new domains in a random order. Each domain data will be trained independently one-by-one for 10 epochs, with learning rate 0.01 and batch size 128. For both training procedures, we use Adam as the optimizer. The development data is used to pick the best model in different epoch runs. We evaluate the classification accuracy on the test set.\n"]}
{"id": "1908.04577", "categories": "cs.CL", "paragraphs": ["We used documents from English Wikipedia (2,500M words) and BookCorpus\u00a0[34] as pre-training data, following the preprocessing and the WordPiece tokenization from \u00a0[5]. The maximum length of input sequence was set to 512.\n", "We ran Adam with learning rate of 1e-4, \\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\) , L2 weight decay of 0.01, learning rate warm-up over the first 10% of the total steps, and linear decay of the learning rate. We set a dropout probability of 0.1 for every layer. The gelu activation\u00a0[9] was used as done in GPT\u00a0[19].\n", "StructBERTBase: \\(L=12\\) , \\(H=768\\) , \\(A=12\\) , Number of parameters\\(=110\\) M\n", "StructBERTLarge: \\(L=24\\) , \\(H=1024\\) , \\(A=16\\) , Number of parameters\\(=340\\) M\n"]}
{"id": "1908.05408", "categories": "cs.CL", "paragraphs": ["All the baselines are implemented by PyTorch. One-hot input tokens are embedded into a 64-dimensional space. The goals are encoded by \\(GRU^{(g)}\\)  with a hidden layer of size 64. The sizes of hidden states in input utterance encoder \\(GRU^{(u)}\\) , \\(GRU^{(c)}\\)  and looking-ahead module \\(GRU^{(l)}\\) , \\(h_k^{(l)}\\) , are all set to 256. A stochastic gradient descent method is employed to optimize the model with a mini-batch size of 32 for supervised learning, an initial learning rate of 1.0, momentum with \\(\\mu =0.1\\) , and clipping gradients 0.5 in \\(L^2\\)  norm. The best model is chosen from the processing of training the model for 400 epochs. After that, the learning rate decays by a factor of 2 for every epoch. The initial hyper-parameters setting in the loss function (Equation (11)) is \\(\\alpha =0.05\\)  and \\(\\beta =1.0\\) . Words that appear in the training dataset for less than 5 times are replaced with the `unknown' (\\(\\left<unk\\right>\\) ) token. A validation dataset is employed to choose the optimal hyper-parameters.\n"]}
{"id": "1901.05049", "categories": "cs.LG cs.DC cs.SD eess.AS stat.ML", "paragraphs": ["All training tools generate both the training model and the solver definition files automatically. We have trained the CNN and DS_CNN models using Bonseyes-Caffe, [4]. These tools import the output generated in the MFCC generation step using the training dataset where the extracted MFCC features and labels are packed all together into an HDF5 file. Training is carried out with a multinomial logistic loss and Adam optimizer [65] over a batch of 100 MFCC samples (since our input sample size is \\(40\\times 32\\) , we opt to use a relatively big batch size). The batch size and number of iterations are specified in the workflow files that control the execution of the tools. Each model is trained for 40K iterations following a multi-step training strategy. The initial learning rate is \\(5\\times 10^{-3}\\) . With every step of 10K iterations, learning rate drops to 30% of the previous step.\n"]}
{"id": "1902.04574", "categories": "cs.CL", "paragraphs": ["For training, we use the Adam [38] optimizer with decaying learning rate, as implemented in TensorFlow [39]. We start with the following values: learning rate \\(\\eta = {5e-04}\\) , exponential decay rate for the 1st and the 2nd momentum \\(\\beta _1 = [round-precision=1]{0.9}\\)  and \\(\\beta _2 = {0.999}\\) , and constant for prevention of division by zero \\(\\epsilon = {1e-7}\\) .\nThen, we decay the learning after each epoch by a factor of 0.99. We also apply dropout with a probability of 0.1, and L2 weight decay on all trainable variables with \\(\\lambda = {3e-7}\\) . We train each model for 42K steps with a batch size of 64. We found these values by running a grid search on a dev set (extracted as a fraction of the training data) and using the values suggested in [0], where applicable.\n"]}
{"id": "1902.00293", "categories": "cs.CV", "paragraphs": ["ERFNet\u00a0[27] is used as the network architecture. The last layer is adapted to output two feature maps, one for each ego-lane line. In both the cross-entropy and end-to-end experiments, we train for 350 epochs on a single GPU with image resolution of 256x512, batch size of 8, and Adam\u00a0[18] with a learning rate of 1e-4. As a simple data augmentation technique the images are randomly flipped horizontally. In the end-to-end experiments, we use a fixed transformation matrix H to transform the weighted pixel coordinates to the ortho-view.\nNote that the input image itself is not transformed to the ortho-view, although that would also be an option.\nThe system is implemented in PyTorch\u00a0[25].\n"]}
{"id": "1905.05979", "categories": "cs.CL", "paragraphs": ["We follow the setup of Transformer base model\u00a0[29]. More precisely, the number of layers in the base encoder, base decoder and CADed is \\(N=6\\) . We employ \\(h = 8\\)  parallel attention layers, or heads. The dimensionality of input and output is \\(d_{model} = 512\\) , and the inner-layer of a feed-forward networks has dimensionality \\(d_{ff}=2048\\) .\n"]}
{"id": "1901.03559", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["We use the V-trace actor-critic algorithm described by [7], with 4 GPUs for each learner and 200 actors generating trajectories.\nWe reduce variance and improve stability by using \\(\\lambda \\) -returns targets (\\(\\lambda =0.97\\) ) and a smaller discount factor (\\(\\gamma =0.97\\) ). This marginally reduces the maximum performance observed, but increases the stability and average performance across runs, allowing better comparisons.\nFor all experiments, we use a BPTT (Backpropagation Through Time) unroll of length 20 and a batch size of 32.\nWe use the Adam optimizer [19]. The learning rate is initialized to 4e-4 and is annealed to 0 over 1.5e9 environment steps with polynomial annealing. The other Adam optimizer parameters are \\(\\beta _1=0.9, \\beta _2=0.999, \\epsilon =\\) 1e-4.\nThe entropy and baseline loss weights are set to 0.01 and 0.5 respectively. We also apply a \\(\\mathcal {L}^2\\)  norm cost with a weight of 1e-3 on the logits, and a \\(\\mathcal {L}^2\\)  regularization cost with a weight of 1e-5 to the linear layers that compute the baseline value and logits.\n"]}
{"id": "1901.03788", "categories": "cs.CL", "paragraphs": ["\nIn BOW, we put words that appear more than twice into the dictionary.\n\nFor SVM, parameters \\(C\\)  and \\(g\\)  are searched via five-fold cross validations from {0.1, 1, 5, 10, 100} and {0.01, 0.1, 1, 5, 10}, respectively. For LR, the codes in MATLAB are used, and all the parameters are set to default.\n\nFor deep models, the word embedding dimension is set to 300 by GloVe [20].\n\nIn both IAN\\(^+\\)  and Semi-IAN, the \\(\\rho \\) -hot encoding [21] is used in the lexical and option embeddings. In \\(\\rho \\) -hot encoding, the size \\(k\\)  is searched in [ 1, 2, 4, \\(\\cdots \\) , 16]; the parameter \\(\\rho \\)  is searched in [0.1, 0.2, \\(\\cdots \\) , 1].\n\n"]}
{"id": "1909.10363", "categories": "cs.CV", "paragraphs": ["For both the synthetic and real datasets, we train the Shadow Transfer networks for 50 epochs with a learning rate of 2e-4 and a batch size of 2. To accommodate the requirement of the U-net encoder-decoder, the input images are resized to 512x512.\nThe SunEst-CNN networks are initialized using the weights of a VGG-16 network trained to perform scene classification on Places-365. They are trained for 20 epochs at a learning rate of 1e-5, batch size of 2. The input images are resized to 256x256 to accommodate the architecture requirements.\nThe two state of the art multi-domain to multi-domain transfer methods, ComboGAN and StarGAN, are trained on CARLA-sun using the training hyperparameters given in the paper and respective github repositories.\nAll networks were train on a single Titan X GPU.\n"]}
{"id": "1908.10940", "categories": "cs.CL cs.LG", "paragraphs": ["The sentence encoder has a shared 200k token multilingual vocabulary with 10k OOV buckets. For each token, we also extract character n-grams (\\(n=[3, 6]\\) ) hashed to 200k buckets. Word token items and character n-gram items are mapped to 320 dim. character embeddings. Word and character n-gram representations are summed together to produce the final input token representation. The encoder is a 3-layer Transformer with hidden size of 512, filter size of 2048, and 8 attention heads.\nWe train for 40M steps using an SGD optimizer with batch size K=100 and learning rate \\(0.003\\) .\nDuring training, the word and character embeddings are scaled by a gradient multiplier of 25.\n"]}
{"id": "1910.03177", "categories": "cs.LG cs.CL stat.ML", "paragraphs": ["For all the plain NSE models, we have truncated the article to a maximum of 400 tokens and the summary to 100 tokens. For the hierarchical NSE models, articles are truncated to have a maximum of 20 sentences and 20 words per sentence each. Shorter sequences are padded with `PAD` tokens. Since the factored models have lemma, PoS tag and the separator `|` for each word, sequence lengths should be close to 3 times the non-factored counterparts. For practical reasons of memory and time, we have used 800 tokens per article and 300 tokens for the summary.\n", "For all the models, including the pointer-generator model, we use a vocabulary size of 50,000 words for both source and target. Though some previous works [14] have used large vocabulary sizes of 150,000, since our models have a copy mechanism, smaller vocabulary is enough to obtain good performance. Large vocabularies increase the computation time. Since memory plays a prominent role in retrieval and update, it is vital to start with a good initialization. We have used 300-dimensional pre-trained GloVe [18] word-vectors to represent the input sequence to a model. Sentence memories are initialized with GloVe word-vectors of all the words in that sentence. Document memories are initialized with vector representations of all the sentences where a sentence is represented with the average of the GloVe word-vectors of all its words. All the models are trained using the Adam optimizer with the default learning rate of 0.001. We have not applied any regularization as the usage of dropout, and \\(L_{2}\\)  penalty resulted in similar performance, however with a drastically increased training time.\n"]}
{"id": "1909.01326", "categories": "cs.CL cs.AI", "paragraphs": ["paragraph40ex plus.2ex minus.2ex-1emBERT We use the pretrained uncased version of BERT-Base (12 layers) with mostly default parameters, except that we use a max sequence length of 50 and train for 5 epochs.\nparagraph40ex plus.2ex minus.2ex-1emLSTM We use a two-layer LSTM with 100 units each, followed by a linear layer with a softmax activation. We use Adam as the optimization function. For other parameters, we try to use values comparable to those of the BERT model, except that we need to train for 20 epochs.\n"]}
{"id": "1912.01966", "categories": "cs.CV cs.LG", "paragraphs": ["As convolutional neural network, we use a densly-connected model (DenseNet) with 121 layers [4]. We load the ImageNet pretrained weights before starting the training. The input image is accordingly normalized and provided in the three input channels. The output layer of the network is reduced such that one sigmoidal unit is returned. During training we use the Adam optimizer [5] (\\(\\beta _1\\)  = 0.9, \\(\\beta _2\\)  = 0.999) and a learning rate of \\(10^{-4}\\) . We stop the training and jump back to the best epoch if the validation accuracy does not improve after a patience of 8 epochs. We apply the binary cross-entropy function to predict the loss. Each batch is filled with 16 examples. The performance on the test set is evaluated with the area under ROC curve (AUC).\n"]}
{"id": "1909.10649", "categories": "cs.CL cs.IR cs.LG", "paragraphs": ["The pre-training input sequences are generated with default parameters and use whole work masking (if a word composed of multiple subword units is masked, all of its subword units are masked and have to be predicted in Masked Language Modeling task). The models are trained for 1,000,000 steps. We use a learning rate of 1e-4, learning rate warmup over the first 10,000 steps followed by a linear decay of the learning rate.\n", "For BERT Base models, the weights are initialized with the checkpoint of Multilingual BERT Base. We use a batch size of 128 and sequences of 512 tokens the entire training. This training takes 4 days on a TPUv3-8 instance and performs about 8 epochs over the training data.\n", "For BERT Large, the weights are initialized with the checkpoint of English BERT Large. Since it is a bigger model with longer training time, we follow the instructions of [6] and use sequences of 128 tokens in batches of size 256 for the first 900,000 steps and then sequences of 512 tokens and batch size 128 for the last 100,000 steps. This training takes 7 days on a TPUv3-8 instance and performs about 6 epochs over the training data.\n"]}
{"id": "1903.11626", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["All adversarial attacks in this paper were optimized to maximize the cross entropy loss or the CW surrogate objective with 40 iterations of PGD. Following previous works [26] and [21], during adversarial training, we trained on adversarial images only. That is, we did not mix natural and adversarial images. We describe the adversarial training procedure and attack settings used in each section.\n"]}
{"id": "1906.03787", "categories": "cs.CV", "paragraphs": ["We use the publicly available adversarial training pipelinehttps://github.com/facebookresearch/ImageNet-Adversarial-Training to train all models with different strategies on ImageNet. We select ResNet-152 [10] as the baseline network, and apply projected gradient descent (PGD) [19] as the adversarial attacker to generate adversarial examples during training. The hyper-parameters of the PGD attacker are: maximum perturbation of each pixel \\(\\epsilon \\)  = 16, attack step size \\(\\alpha \\)  = 1, number of attack iterations N = 30, and the targeted class is selected uniformly at random over the 1000 ImageNet categories. We initialize the adversarial image by the clean counterpart with probability = 0.2, or randomly within the allowed \\(\\epsilon \\)  cube with probability = 0.8. All models are trained for a total of 110 epochs, and we decrease the learning rate by 10\\(\\times \\)  at the 35-th, 70-th, and 95-th epoch.\n", "By following the parameter settings listed in the ALP paperFor easier implementation, we apply 48 GPUs (which can be distributed over 6 8-GPU machines) for adversarial training, instead of using the original number, i.e., 50 GPUs., we can train a ResNet-101 with an accuracy of 38.1% against PGD-10. The ResNet-101 performance reported in the ALP paper is 30.2% accuracy against an attack suiteThis attack suite contains 8 different attackers, including PGD-10. However, due to the vague description of parameter settings in this attack suite, we are not able to reproduce it.. This \\(\\scriptstyle \\sim \\) 8% performance gap is possibly due to different attacker settings in evaluation. However, by evaluating this model against PGD-2000, we are able to obtain a similar result that reported in [5], i.e., [5] reports ALP obtains 0% accuracy, and in our implementation the accuracy is 2.1%.\n"]}
{"id": "1912.04663", "categories": "cs.CV", "paragraphs": ["We use the Adam optimizer with learning rate of \\(10^{-4}\\) . The mini batch size is set to 64. Training loss is averaged in each mini batch. We use \\(80\\%\\)  of the 3D models in ShapeNet for training, \\(10\\%\\)  for validation, and the rest for testing.\n"]}
{"id": "1912.04711", "categories": "cs.CV cs.HC", "paragraphs": ["On all training instances, we used a uniform distribution for weights initialization and Adam optimizer with a learning rate of 0.0001 for optimization. Using single NVIDIA Titan X we were able to use a batch size of 115 and took approximately two days to train each individual model. All models were implemented using the Pytorch framework [39].\n"]}
{"id": "1905.13570", "categories": "cs.LG cs.AI cs.NE stat.ML", "paragraphs": ["We briefly discuss training differences between BFVI and other methods here. For the RNN-based methods, no bidirectional training is involved, so we only minimized the (forward) filtering ELBO (for F-Mask, F-Skip) or the smoothing ELBO (for B-Mask, B-Skip). While we also tried to use the multimodal training paradigm (i.e. minimizing the sum of \\(L^{1:M}\\)  and \\(L^m\\) ) for the RNN-based methods, the effects of this were mixed: On the spirals dataset, performance dropped very sharply with the multimodal paradigm, whereas performance increased on the Weizmann video dataset. As such, the results we report in the main manuscript use the multimodal training paradigm only for the video dataset. For the spirals dataset, we only minimize \\(L^{1:M}\\) , with all inputs provided, but not \\(L^m\\) , which is computed with only modality \\(m\\)  provided as input. Finally, we used a higher learning rate (0.02) for BFVI on the spirals dataset because we noticed slow convergence with the lower rate of 0.01. Increasing the learning rate to 0.02 for the other methods hurt their performance.\n"]}
{"id": "1907.02848", "categories": "cs.LG cs.CL", "paragraphs": ["Open-Subtitles: Additionally, we show results with the unannotated Open-Subtitles dataset [35] (we randomly sample up to 2 million dialogs for training and validation). We tag the dataset with dialog attributes using pre-trained classifiers.\n", "Model Details: We use two-layer GRUs [3] for both encoder and decoders with hidden sizes of 512.\nWe restrict the vocabulary for both the datasets to top 25000 frequency occurring tokens.\nThe dialog attribute classifier for dialog attributes is a simple 2-layer MLP with layer sizes of 256, and 10 respectively.\nWe use the rectified linear unit (ReLU) as the non-linear activation function for the MLPs and use dropout rate of \\(0.3\\)  for the token embeddings, hidden-hidden transition matrices of the encoder and decoder GRUs.\n", "Training Details: We ran our experiments in Nvidia Tesla-K80 GPUs and optimized using the ADAM optimizer with the default hyper-parameters used in [19], [20].\nAll models are trained with batch size 128 and a learning rate \\(0.0001\\) .\n"]}
{"id": "1907.03089", "categories": "cs.CV", "paragraphs": ["All the experiments are implemented with PyTorch 1.1.0, CUDA 9.0 and CuDNN 7. The networks run 200 epochs on single Tesla V100 GPU, using Adam optimizer with weight decay of 2e-4 and momentum of 0.9. The initial learning rate is 5e-4 and we adopt \"ploy\" learning rate policy with power of 0.9. We set batch size to 16 to fit our GPU memory. In order to alleviate the unbalanced categories, we adopt cross entropy loss with weight \\(W_{class}=\\frac{1}{log(P_{class}+c)}\\)  and we set \\(c\\)  to 1.12.\n"]}
{"id": "1911.09338", "categories": "cs.CV", "paragraphs": ["Model settings. The voice embedding network and face embedding network are pre-trained by VoxCeleb2 and VGGFace2 respectively. Margin \\(m\\)  for Triplet Loss is set to 1, and scale \\(s\\)  for L2 normalization is set to 128. Adam Optimizer is adopted in this paper. The total number of learning steps is 70k. The learning rate of FC layer for \u201cstep\\(<\\) 20k\u201d, \u201c20k\\(<\\) step\\(<\\) 40k\u201d, \u201c40k\\(<\\) step\\(<\\) 60k\u201d and \u201cstep\\(>\\) 60k\u201d is \\(10^{-3},10^{-4},10^{-5},10^{-6}\\)  respectively. The learning rate of face embedding network is fixed to \\(10^{-6}\\) .\n"]}
{"id": "1903.00827", "categories": "cs.LG stat.ML", "paragraphs": ["The difficulty level of L2R environment is set to be 2 for all of our experiments in this paper, which means that there are three stumbling blocks with random sizes and positions in each episode. To handle the high-dimensional observation vector and action vector, we specially design the network architecture depicted in Figure REF . Adam [12] is adopted to train the agent networks with a learning rate of \\(3e^{-4}\\) . We use mini-batch size \\(N=96\\) , discount factor \\(\\gamma =0.99\\) , soft update rate \\(\\tau =1e^{-3}\\) , and size of replay buffer \\(\\mathcal {M}=10e^{6}, \\mathcal {M}_H=5\\times 10^{4}\\)  . Specially, we tune the probability \\(\\rho \\)  of sampling from \u201cHMemory\u201d in \\([0.05, 0.25]\\)  according to the number of interaction threads.\n", "In MuJoCo environments, we adopt fully connected networks with hidden sizes of (256, 256, 128) and (256, 128) to build the actor and critic respectively. And we use a learning rate of \\(1e^{-4}\\)  and a mini-batch size of 128. Other hyper-parameters keep the same settings of agents on L2R task.\n"]}
{"id": "1910.10223", "categories": "cs.CV", "paragraphs": ["For all experiments, we use \\(b_\\gamma = l_\\gamma = 10^{-2}\\)  and \\(\\mu = 10^{-1}\\) . \\(p(\\pi \\vert x_2)\\)  and \\(r(\\pi )\\)  are both modeled as Gaussian\ndistributions of unit variance. The latter has a mean of zero and \\(E_\\pi \\) \nestimates the mean of the first. We use the reparameterization trick\n[28] to obtain low variance estimates of the gradient. Depending on\nthe dataset, we implement the negative log-likelihood \\(\\mathcal {L}_{\\text{rec}}\\)  with a \\(l_2\\) \nloss (sprites), a perceptual loss [24] (norb) or a perceptual\nloss together with a discriminator loss as in [10],\nweighted by \\(10^{-3}\\)  (remaining datasets).\n"]}
{"id": "1910.10307", "categories": "cs.LG stat.ML", "paragraphs": ["To train OSVM classifiers for detection and finding the OODL, we used the rbf kernel and training error \\(\\textit {nu}=0.001\\) . The rbf kernel gave us the best results in comparison to other kernels such as linear or poly. We used temperature scale \\(T=1000\\)  for the ODIN approach since it was deemed as the optimal value based in the original paper [17]. The perturbation magnitude \\(\\varepsilon \\)  for our approach and ODIN was optimized to minimize FPR at \\(95\\%\\)  TPR by having access to randomly selected \\(20\\%\\)  of OOD datasets. We used F-MNIST and TinyImageNet datasets to find the OODLs. The OODL was fixed for every ID dataset and its associated model. The OODL for the MNIST dataset was the second convolutional layer. The OODLs for VGG-16 trained on CIFAR-10 and CIFAR-100 were the second convolutional layer and the first max-polling layer, respectively. The OODL for ResNet trained on CIFAR-10 and CIFAR-100 were the thirteenth and ninth residual layers, respectively. Details of parameters for training models are in supplemental material.\n"]}
{"id": "1909.13550", "categories": "cs.LG stat.ML", "paragraphs": ["\nbatch size of 256\n\nAdamW optimizer [19] with initial learn rate of \\( 0.01 \\)  and \\( \\beta _{1} = 0.9, \\beta _{2} = 0.999 \\)\n\nweight decay of \\( 0.01 \\)\n\nnegative-log likelihood (cross entropy) loss\n\nreduce-on-plateau learn rate scheduler (patience of 10 epochs) with factor of \\( 0.1 \\)\n\nadditional validation set is randomly extracted from the training set (5000 samples)\n\ndropout with probability of \\( 0.5 \\)  before the last linear layer was used in all models during training\n\nin MC dropout, \\( N = 25 \\)  forward passes with dropout probability of \\( 0.5 \\)  were performed\n\n"]}
{"id": "1909.13302", "categories": "cs.CL", "paragraphs": ["The detailed parameters of the model are as follows: Both of the source embedding and the target embeddings have 512 dimensions and use the same BPE vocabulary. We share the weights of decoder's input and output embeddings. Both of the encoder and decoder have 6 multi-head layers and 8 attention heads. The size of inner layer at each multi-head layer is 2048. We use Adam optimizer to train transformer model with the inverse squared root schedule which will decay the learning rate based on the inverse square root of the warm-up steps. The learning rate initialize with \\(5 \\times 10^{-4}\\)  and warm-up during the first 4,000 steps. In order to train transformer adequately, we use a batch size of 32,000 tokens and fine-tune the model on labeled data for 30,000 steps. Dropout is applied at a ratio of 0.3. The Loss function we use is the Edit-weighted MLE objective [18] and the factor \\(\\Lambda \\)  is set to 1.2.\n"]}
{"id": "1912.11683", "categories": "cs.CV cs.LG eess.IV", "paragraphs": ["All our models are trained in PyTorch [28]. We use the Adam optimizer [17] with default params and learning rates between 1e-3 and 1e-4. We anneal the learning rate once the loss curves start to plateau.\n", "We use the Runge-Kutta (RK-45) adaptive solver and the adjoint sensitivity method provided in [5]. We set the relative error tolerance to zero and explore absolute error tolerances between 1e-3 and 1e-5.\n", "The kidney segmentation experiments were conducted on a single NVIDIA DGX-1 with 8 GPUs. The salient object detection experiments on UNet and NodeUNet were conducted on a single NVIDIA DGX-1 with 8 GPUs, and the experiments on NodeStack were conducted on 4 NVIDIA DGX-1 nodes with 32 GPUs total. We used the largest possible batch size given memory constraints.\n"]}
{"id": "1901.03991", "categories": "cs.CV", "paragraphs": ["DRAW\u00a0 We follow the approach described in [20]https://github.com/ericjang/draw.\nThe model architecture as well as about the used objectives can be found in [20].\nWe did not change any of the building blocks of draw and train the network as proposed with the Adam optimizer, the momentum set to \\(\\beta _1=0.9\\) .\nThe learning rate is set to 0.0005.\nTraining converges after approximately 300 epoch.\nAn epoch corresponds to \\(\\frac{S_T}{S_B}\\)  gradient updates, \\(S_T\\)  denotes dataset size and \\(S_B\\)  is the mini-batch size.\nAll shown samples, we generate from a normal distribution \\(z \\sim \\mathcal {N}(0,1)\\) .\nWe take the same number of dimension for the latent space as suggested in the paper: \\(z \\in \\mathbb {R}^{100}\\) .\nIn this experiment we enabled the attention mechanism and set the read and write window size to \\(3\\times 3\\)  and \\(5\\times 5\\)  respectively.\nThe number of glimpses or equivalently defined as time steps, is set to 64.\nFor both recurrent networks, encoder and decoder, we choose the hidden vectors \\(h^{enc}\\)  and \\(h^{dec}\\)  to have dimension 256.\n"]}
{"id": "1901.07821", "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "paragraphs": ["where we use \\(\\sigma = 2/L\\) . Uniformly distributed noise \\(\\mathcal {U}(-\\tfrac{a}{2},\\tfrac{a}{2})\\)  is added to the encoder output before it is passed on to the decoder, with \\(a = 2 / (L-1)\\) .\n"]}
{"id": "1905.04919", "categories": "cs.LG stat.ML", "paragraphs": ["In the searching stage, we train a small network stacked by 8 cells using BayesNAS with different \\(\\lambda _w\\) . This network size is determined to fit into a single GPU. Since we cache the feature maps in memory, we can only set batch size as 18. The optimizer we use is SGD optimizer with momentum 0.9 and fixed learning rate 0.1. Other training setups follow DARTS and SNAS (Appendix REF ). The search takes about 3 hours on a single GPUAll the experiments were performed using NVIDIA TITAN V GPUs.\n", "In the searching stage, we set batch size to 32 and learning rate to 0.1. We use the same optimizer as for proxy search. The \\(\\lambda \\)  of BayesNAS for each possible path is set to \\(1\\times 10^{-2}\\) .\n", "The network parameters are optimized using momentum SGD, with initial learning rate \\(\\eta _{\\mathbf {\\theta }} = 0.1\\) , momentum 0.9, and weight decay \\(1 \\times 10^{-4}\\) . The batch size employed is 16 and the initial number of channels is 16.\n", "The network is trained with batch size 128, SGD optimizer with weight decay \\(3\\times 10^{-4}\\) , momentum 0.9 and initial learning rate 0.1, which is decayed using cosine annealing.\n"]}
{"id": "1906.05381", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["A PyTorch implementation is available (see acknowledgements). All experiments use the same hyperparameters, and many were set according to the best-performing seq2seq model in [15]. The input and output sequence encoders are two-layer biLSTMs with \\(m=200\\)  hidden units per layer, producing \\(m\\)  dimensional embeddings. The output decoder is a two-layer LSTM also with \\(m=200\\) . Dropout is applied with probability 0.5 to each LSTM and symbol embedding. A greedy decoder is effective due to SCAN's determinism [15].\n", "Networks are meta-trained for 10,000 episodes with the ADAM optimizer [14]. The learning rate is reduced from 0.001 to 0.0001 halfway, and gradients with a \\(l_2\\) -norm greater than 50 are clipped. With my PyTorch implementation, it takes less than 1 hour to train meta seq2seq on SCAN using one NVIDIA Titan X GPU (regular seq2seq trains in less than 30 minutes). All models were trained five times with different random initializations and random meta-training episodes.\n{FIGURE}"]}
{"id": "1906.03221", "categories": "cs.CL", "paragraphs": ["Model hyperparameters were tuned on the development set. We used the\nAdagrad optimizer [7] with an initial\nlearning rate of\u00a00.15, decayed by 0.97 for every epoch after the\u00a04th\nepoch. We used truncated BPTT [38]\nof length 100 and made use of input feeding [23]. We\nsummarize the hyperparameters of the RotoWire and MLB models\nin the Appendix. All models were implemented on a fork of OpenNMT-py\n[18].\n"]}
{"id": "1908.09936", "categories": "cs.LG cs.CL stat.ML", "paragraphs": ["We have as trainable parameters the parameters of our Bregman module\u2013with the exception of the step size \\(\\epsilon \\) \u2013the threshold \\(\\tau \\) , and the Bi-LSTM parameters.\nFor the initialization of the Bi-LSTMs, we used Xavier [5], and a recurrent dropout of \\(0.5\\)[4]. They have all an input size of 400, and a hidden size of 200.\nThe Bregman module was initalized with random orthonormal matrices [16], and with an input size of 512, output size of 1, and a depth of 10.\nThe threshold is initialized to \\(\\tau = 0.75\\) , and we use Adam [7] as our optimizer. We trained the model for 50 epochs, with an early termination of 10 epochs.\nWe maintained the maximum number of hypernyms fixed at 9, the depth of the Wordnet graph at 2, and the features at 10.\n"]}
{"id": "1906.01543", "categories": "cs.CL", "paragraphs": ["All input text is lower-cased and tokenised, numbers with 5 or more digits get their digits replaced by a wildcard symbol #, while words longer than 16 characters are replaced by a wildcard token LONGWORD. Sentence boundary tokens <S> and </S> are added to each sentence. The vocabulary consists of the unigrams that occur at least 10 times in a random 1M subset of the Reddit training set \u2013this results in a total of 105K unigrams\u2013 plus the 200K most frequent bigrams in the same random subset.\n", "The following training setup refers to the final Reddit model, illustrated in Figure\u00a0REF , and used in fine-tuning. The model is trained by SGD setting the initial learning rate to 0.03, and then decaying the learning rate by 0.3x every 1M training steps after the first 2.5M steps. Similar to learning rate scaling by the batch size used in prior work [18], [12], we scale the unigram and bigram embedding gradients by the batch size. The batch size is 500, and attention projection dimensionality is 64.\n"]}
{"id": "1906.01502", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["All models were fine-tuned with a batch size of 32, and a maximum sequence length of 128 for 3 epochs. We used a learning rate of \\(3\\mathrm {e}{-5}\\)  with learning rate warmup during the first \\(10\\%\\)  of steps, and linear decay afterwards. We also applied \\(10\\%\\)  dropout on the last layer. No parameter tuning was performed.\nWe used the BERT-Base, Multilingual Cased checkpoint from https://github.com/google-research/bert.\n"]}
{"id": "1906.01363", "categories": "cs.CV cs.LG", "paragraphs": ["We did a train:validation:test split as 70:20:10, stratified on the basis of the number of classes corresponding to each input. Adam optimizer with a learning rate of 5x\\(10^{-4}\\)  is used. Training is done on batches, with a batch size of 64, and is continued for 600 epochs. The model used for testing is the one saved at the epoch with the highest validation accuracy.\n"]}
{"id": "1905.03721", "categories": "cs.CV", "paragraphs": ["All RNNs used as encoder or decoder are 2-layer LSTMs with 300-dimensional hidden states.\nAction predictor and price decoder networks have the same network architecture, a 4-layer fully-connected network with ReLU activation functions. We also applied a dropout with a rate of 0.3 to all parts of our architecture.\n", "Parameters of the models are optimised using Adam with the learning rate set to 1e-3 in first 20 epochs and then decayed to 1e-4 for another 320 epochs. The batch size is set to 128 in all experiments.\n", "Except for hierarchical dialogue encoder and the language decoder which are trained together, we trained other modules separately during the supervised learning process. Afterwards, for RL, we only optimised the action predictor and price decoder parameters for 5000 episodes using a learning rate of 1e-4.\n"]}
{"id": "1907.04666", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["We describe here the hyperparameters used to train the models. The autoencoders are constituted of one layer of 100 LSTM neurons for the encoder and the decoder. For the KISSME version, the encodings are then projected into a 50-dimensional space, and the distance matrix, which thus has also dimension 50, was updated with the closed-form every 30 epochs.\nThese parameters were determined after preliminary tests where deeper architectures and higher dimensional spaces were tested. Models are trained with 20 similar pairs for each time slot and the same total number of dissimilar pairs for a total of 960 training pairs coming from 12 different days of data. The training was stopped based on the loss computed on the validation set which contains three days of data i.e., 72 sequences. The testing set is composed of 15 days or 360 sequences. The data in the training set were rescaled between -1 and 1 and the same parameters were applied on the validation and testing sets. A learning rate of 0.001 was used and divided by 10 if the loss did not decrease anymore during 10 epochs. A batch size of 50, a margin of 1 for the contrastive loss and of 0.5 for the cosine loss were chosen. We also observed that changing to zero 30% of the values of the training sequences sliglty improved the results as suggested in [27].\n"]}
{"id": "1910.02366", "categories": "cs.LG cs.NE stat.ML", "paragraphs": ["We treat the filters as the neurons to split for convolutional neural networks.\nFor example, consider a convolutional layer with \\(n_{out} \\times n_{in} \\times k \\times k\\)  parameters, where \\(n_{out}\\)  denotes the number of output channels and \\(n_{in}\\)  the number of input channels and \\(k\\)  the filter size.\nWe treat it as \\(n_{out}\\)  neurons, and each neuron has a parameter of size \\(n_{in} \\times k \\times k\\) . To apply our methods,\nwe start with a small variant of the MobileNet and VGG19,\nand gradually grow the network by splitting the (convolutional) neurons with the most negative splitting indexes following Algorithm REF .\nFor MobileNet, we construct the initial network by keeping the size of the\nfirst convolution layer as the same (=32) as the original MobileNet\nand setting the number of depthwise and pointwise channels to be 16.\nFor VGG19, we set the number of channels of the initial network to be 16 for all layers.\n", "We start with a very narrow network and progressively grow it using splitting steepest descent.\nWe build our initial narrow network based on the DS-CNN architecture proposed in [28], by reducing the number of channels in each layer to 16.\nThe backbone DS-CNN model consists of one regular convolution layer and five depthwise and pointwise convolution layers [9]. We refer the reader to [28]\nfor more information. At each splitting stage,\nwe increase the number of channels by a percentage of 30% using the approach\ndescribed in Algorithm\u00a0REF .\nWe use the same hyper-parameters for training and evaluation as in [28].\n"]}
{"id": "1912.00042", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["We train on ImageNet32 and ImageNet64 for \\(200,000\\)  iterations with mini batches of size 64, and a learning rate of 0.0001 using the Adam optimizer [21]. The high-resolution image \\(x_{hr}\\)  either the original \\(32 \\times 32\\)  or \\(64 \\times 64\\)  original input images. The flow architecture is build with \\(L=2\\)  levels and \\(K=8\\) .\n{FIGURE}"]}
{"id": "1902.09631", "categories": "cs.CV", "paragraphs": ["Optimization was performed with the adam\u00a0[20] optimizer with a learning rate of \\(0.0002\\) , \\(\\beta _1=0.5\\) , \\(\\beta _2=0.9\\) . Gradient descent was alternated between generator and discriminator, with the discriminator receiving real and generated images in distinct batches.\n"]}
{"id": "1912.07183", "categories": "cs.CV", "paragraphs": ["Our training setup is similar to Edge-connect [17]. PyTorch is used for implementation. All models are trained using \\(256 \\times 256\\)  images with a batch size of eight. The Adam optimiser with \\(\\beta _1 = 0\\)  and \\(\\beta _2 = 0.9\\)  is used. The initial learning rate of generator is set to \\(10^{-4}\\) . It is divided by 10 when the generator loss value stops decreasing. In total, the learning rate is divided by 10 twice. As we used a WGAN, the learning rate of the discriminator is five times that of generator. In large datasets the convergence appears within 200,000 steps, while in small datasets the final convergence appears within 85,000 steps.\n"]}
{"id": "1905.10464", "categories": "cs.CL", "paragraphs": ["We trained the word2vec modelWe train using https://github.com/tmikolov/word2vec. using the CBOW algorithm with window size of 10, negative sampling of 10, and minimum count of 10;\nthe GloVe modelWe train using https://github.com/stanfordnlp/GloVe. with windows size of 10 and minimum count of 10;\nand the FastText modelWe train using https://github.com/facebookresearch/fastText. using the CBOW algorithm with word n-gram of 5, window size of 5, and negative sampling of 10.\n"]}
{"id": "1903.09171", "categories": "cs.LG cs.AI", "paragraphs": ["To initialize the model structure, the only information required is the number and types of inputs and outputs together with their corresponding dimensions. In our case, we can define \\(I=\\lbrace i_0\\rbrace \\)  and \\(O=\\lbrace o_0, o_1, o_2\\rbrace \\) , where \\(i_0=(v_{i_0}, t_{i_o})\\) , \\(|v_{i_0}| = 28\\times 28=784\\)  and \\(t_{i_0} = \\text{Numeric}\\) . \\(o_0=(\\psi _0, f_{o_0})\\)  where \\(|\\psi _0^0|=32\\) , and \\(t_{\\psi _0^0}=\\text{Numeric}\\) , \\(o_1=(\\psi _1, f_{o_1})\\)  where \\(|\\psi _1^0|=10\\) , and \\(t_{\\psi _1^0}=\\text{Discrete}\\) , and \\(o_2=(\\psi _2, f_{o_2})\\) , where \\(|\\psi _2^0|=28\\times 28=784\\)  and \\(t_{\\psi _2^0}=\\text{Samples}\\) . \\(f_{o_0}=f_{o_1}=f_{o_2}=\\Lambda \\) .\n"]}
{"id": "1902.02502", "categories": "cs.LG stat.ML", "paragraphs": ["Latent representations \\(s_{k}\\)  can be updated by either gradient descent with a learnable learning rate or an RNN which imitate the behavior of gradient descent. For the sake of notational simplicity, outputs of the neural networks \\(f_{\\phi }(s_k)_m\\)  and \\(g_{\\psi }(s_k)\\)  are denoted by \\(f_{k,m}\\)  and \\(g_k\\) , respectively. If using gradient descent as the update rule, latent representations \\(s_k\\)  with \\(1 \\le k < K\\)  are updated by\n\\(s_k^{(t+1)} = s_k^{(t)} + \\eta _s \\sum _{m}\\Big (\\gamma _{m,k}^{(t)} \\frac{\\partial \\log {p_{m,k}}}{\\partial g_k} \\frac{\\partial g_k}{\\partial s_k} \\\\+ \\sum _{k^{\\prime } \\ge k}{\\gamma _{m,k^{\\prime }}^{(t)} \\frac{\\partial \\log {\\pi _{m,k^{\\prime }}}}{\\partial f_{k,m}} \\frac{\\partial f_{k,m}}{\\partial s_k}}\\Big )\\bigg |_{s_k = s_k^{(t)}}\\) \n"]}
{"id": "1902.02441", "categories": "cs.LG cs.RO stat.ML", "paragraphs": ["Due to limited compute resources, a full hyperparameter search was not feasible. A few parameters were evaluated during training for 24 hours each. Discount factors between 0.95 and 0.99 were evaluated. Trials showed the agent learned to walk faster using a value range of [0.96, 0.976]. Different rates of learning rate per step were evaluated. Evaluating mini-batch sizes from 32 to 128 showed that a higher value was more beneficial.\n"]}
{"id": "1908.11503", "categories": "cs.LG stat.ML", "paragraphs": ["Our aggregate network applies 2 search depth (i.e., 2-hops) with output dimension of 1024 and 512, respectively. We perform batch normalization after each output layer, followed with ReLU activation function. As for multi-head attention module, two dense layers respectively followed by tanh and LeakyReLU\u00a0[43] activations are developed for both class-level and instance-level attention. In the relation kernel module, we use a two-layer MLP with batch normalization and ReLU activation for adjacency matrix building, whose input and output dimensions are consistent with the output of the aggregate network and adjacency matrix size, respectively. GCN module is composed of 2 graph convolutional layers with output channel dimensionality of 512 and 128, respectively.\nOur whole TGG model is trained end-to-end via ADAM\u00a0[20] optimizer with learning rate 0.001 and weight decay 0.0005. The batch size is set to be 128 for all datasets and we use validation sets for early stopping. Both \\(\\lambda _1\\)  and \\(\\lambda _2\\)  in Eq.(REF ) are set to be 0.5. We implement our TGG by PyTorchhttps://pytorch.org/ and the source code of our work is available at: https://github.com/zcrwind/tgg-pytorch.\n"]}
{"id": "1911.07559", "categories": "cs.CV", "paragraphs": ["We train the FFA-Net in RGB channels and augment the training dataset with randomly rotated by 90,180,270 degrees and horizontal flip.\nThe 2 hazy-image patches with the size \\(240\\times 240\\)  are extracted as FFA-Net\u2019s input. The whole network is trained for \\(5\\times 10^5\\) , \\(1\\times 10^6\\)  steps on indoor and outdoor images respectively. We use Adam optimizer, where \\(\\beta 1\\)  and \\(\\beta 2\\)  take the default values of 0.9 and 0.999, respectively.\n", "The initial learning rate is set to \\(1\\times 10^{-4}\\) ,\nwe adopt the cosine annealing strategy [7] to adjust the learning rate from the initial value to 0\nby following the cosine function. Assume the total number of batches is \\(T\\) ,\\(\\eta \\)  is the initial\nearning rate, then at batch \\(t\\) , the learning rate \\( \\eta _{t}\\)  is computed as:\n\\( \\eta _{t}=\\frac{1}{2}(1+\\cos (\\frac{t\\pi }{T}))\\eta \\qquad \\mathrm {(9)}\\) \n"]}
{"id": "1909.06887", "categories": "cs.CV", "paragraphs": ["To learn our descriptor, we use one \\(S^2\\)  convolution layers and three \\(\\operatorname{SO}(3)\\)  convolution layers with constant number of channels, 40, while the bandwidths is set to 24 for the first three layer and 4 for the last one, which results in a descriptor with 512 entries.\nThe architecture of our decoder is made of 4 fully-connected layers, with ReLU non-linearities on the first three layers and tanh on the final output layer.\nThe network is trained with mini-bacthes of size 32 by using ADAM [14]. The starting learning rate is set to 0.001 and is decayed every 4000 iterations. We train the network for 14 epochs.\n"]}
{"id": "1905.10073", "categories": "cs.LG stat.ML", "paragraphs": ["We used the Adam optimizer\u00a0[20] with the first momentum set to 0.9 and the second momentum set to 0.999. Weight decay was set to \\(5*10^{-4}\\)  for the convolutions and to \\(10^{-8}\\)  for the decision trees. The batch size was set to 400 and each batch was always balanced in terms of available classes. This means that in each batch each class was represented 40 times. The initial learning rate was set to \\(10^{-2}\\)  and reduced by \\(10^{-1}\\)  after each 100 epochs until it reached \\(10^{-4}\\) . For the learning rate of \\(10^{-4}\\)  we continued the training for additional 1000 epochs and selected the best result. For data augmentation we used random noise in the range of 0-30% of the image resolution.\n", "We used the Adam optimizer\u00a0[20] with the first momentum set to 0.9 and the second momentum set to 0.999. Weight decay was set to \\(5*10^{-5}\\)  for the convolutions and to \\(10^{-10}\\)  for the decision trees. The batch size was set to 50 with the same batch balancing approach as for the MNIST dataset. For CIFAR this means each batch consisted of five examples per class. The initial learning rate was set to \\(10^{-2}\\)  and reduced by \\(10^{-1}\\)  after each 500 epochs until it reached \\(10^{-5}\\) . For the learning rate of \\(10^{-5}\\)  we continued the training for additional 1000 epochs and selected the best result. For data augmentation we used random cropping of \\(24 \\times 24\\)  patches, random color offsets, random color distortion, flipping the image horizontally and vertically as well as random noise in the range of 0-20% of the image resolution. Additionally, we overlayed patches of the same class with an intensity of up to 20%.\n"]}
{"id": "1905.10095", "categories": "cs.LG stat.ML", "paragraphs": ["\nNetwork architecture. The number of shared and specific graph convolutional layers are both 1, shared hidden size is 64, and specific hidden size is 16.\n\nInitialization. The node feature matrix can be initialized randomly, or by other embedding methods, we initialize it as the identity matrix.\n\nGradient normalization. We normalize the gradient of shared parameter \\(\\mathbf {\\Theta }_s\\)  of each domain, and then use the normalized gradient to calculating \\(\\alpha \\)  in Eq.\u00a0(REF ). The normalized gradient of domain \\(d\\)  is \\(\\mathbf {G}_d / \\left(\\left\\Vert  \\mathbf {G}_d \\right\\Vert _2 \\cdot L_d \\right)\\) , where \\(\\mathbf {G}_d = \\frac{\\partial L_d\\left(\\mathbf {\\Theta }_s, \\mathbf {\\Theta }_d\\right)}{\\partial \\mathbf {\\Theta }_s}\\)  is the unnormalized gradient.\n\nOther hyper-parameters. The number of negative samples is 2; the embedding dimension is 16; the dropout of shared graph convolutional layers is 0.3 and that is 0.1 of specific graph convolutional layers; the batch size is 256 and we train the model for a maximum of 10 epochs using Adam.\n\n", "\nMF. It is implemented using LibMFhttps://www.csie.ntu.edu.tw/\u00a0cjlin/libmf/.\n\nDeepWalk. The length of context window is 5; the length of random walk is 20; the number of walks per node is 50.\n\nLINE. The number of negative samples is 2.\n\nnode2vec. The length of context window is 5; the length of random walk is 20; the number of walks per node is 50; the number of negative samples is 2; \\(p\\)  is 1 and \\(q\\)  is 0.25.\n\nGCN. The number of graph convolutional layers is 1.\n\nmGCN. The initial general representation size is 64, other parameter settings are the same as\u00a0[21], and we train the model for a maximum of 20 epochs using Adam.\n\nDMGE (\\(\\alpha \\) ). Considering that both domains are important, we set the weight \\(\\alpha \\)  to 0.5; the other parameter settings are the same as DMGE.\n\n"]}
{"id": "1911.09839", "categories": "cs.LG stat.CO stat.ML", "paragraphs": ["\nThe latent parameter \\(z_i\\)  at \\(i \\in \\lbrace 1,\\ldots ,m\\rbrace \\)  has the fixed value \\(e_i\\)  in \\(\\lbrace 0,1\\rbrace ^m\\)  that has 1 at the \\(i\\) -th position and 0 everywhere else. Formally, this means that the prior \\(P(z_{1:m})\\)  is the Dirac distribution at \\((e_1,e_2,\\ldots ,e_m)\\) .\n\nThe random variable \\(x_j\\)  at \\(j \\in \\lbrace 1,\\ldots ,n\\rbrace \\)  consists of two parts,\n\\(x_j^S \\in \\mathcal {X}_S\\)  for the latent state and \\(x_j^O \\in \\mathcal {X}_O\\)  for the observed value.\nThus, \\(x_j = (x_j^S,x_j^O)\\)  and \\(\\mathcal {X}= \\mathcal {X}_S \\times \\mathcal {X}_O\\) .\n\nThe probability distribution \\(P_\\phi (x_j \\,|\\,x_{1:j-1},z_i)\\)  is parameterised by \\(\\phi \\in \\mathbb {R}^p\\)  for some \\(p\\) , and has the form\n\\(P_\\phi (x_j \\,|\\, x_{1:j-1},z_i) = P_\\phi (x_j^O \\,|\\,x_j^S,z_i) P_\\phi (x_j^S \\,|\\,x_{1:j-1}^S,z_i).\\) \nTypically, \\(P_\\phi \\)  is defined using a neural network, and \\(\\phi \\)  denotes the weights of the network.\n\n"]}
{"id": "1907.03548", "categories": "cs.CV eess.IV", "paragraphs": ["In an end-to-end training manner, we updated the weights of all networks using the Adam optimizer with an initial learning rate of \\(1e^{-4}\\) , and the batch size of training was 8. All networks were trained up to 100 epochs, where the learning rate was fixed in the first 60 epochs and then linearly reduced to \\(1e^{-6}\\) . In the early phase, the synthetic images were blurry, so \\(\\lambda _{shape}\\)  was set to 0 at the beginning and linearly increased to 100 at 60 epoch. In the end, we used the model trained at the 100th epoch to perform on testing data.\n"]}
{"id": "1905.00561", "categories": "cs.CV", "paragraphs": ["Models: R(2+1)D-d [14]Source code: https://github.com/dutran/R2Plus1D is the fundamental architecture used for pre-training, where \\(d\\)  denotes model depth = \\(\\lbrace 18, 34, 101, 152\\rbrace \\) . As in [29], we construct models of depth \\(>\\ 34\\)  by replacing simple temporal blocks with bottleneck blocks for computational feasibility. We direct the reader to the supplementary material for details.\n", "Training Details: \nVideo frames are down-sampled to a resolution of \\(128\\times 171\\)  and each video clip is generated by cropping a random patch of size \\(112\\times 112\\)  from a frame. Video clips of either 8 or 32 frames are used in our experiments, and temporal jittering is also applied to the input. Synchronous stochastic gradient descent (SGD) is used to train our models on 128 GPUs across 16 machines using caffe2 [9]. When 32 frames per input video clip are considered, each GPU processes 6 videos at a time (due to memory constraints), while 16 videos are processed at a time when 8 frames per video clip are considered. Batch normalization (BN) is applied to all convolutional layers and the statistics [33] are computed on each GPU. All pre-training experiments process \\(490M\\)  videos in total. Learning rate is set following the linear scaling procedure proposed in [27] with a warmup. An initial learning rate of \\(0.192\\)  is used which is divided by 2 at equal steps such that the total number of learning rate reductions is 13 over the course of training.\n{FIGURE}"]}
{"id": "1904.02823", "categories": "cs.CV", "paragraphs": ["We use fully convolutional VGG-style networks for CIFAR-10 and SVHN, and ResNet for CIFAR-100. All of them use the ADAM optimizer\u00a0[25] as suggested by Hubara et al.\u00a0[21]. For the BNN trained with distribution loss (BNN-DL), we compute the loss with the activations prior to each binarized activation function (\\(i.e.\\) , \\(Sign\\)  function that uses \\(HardTanh\\)  for gradient computation). Unless noted otherwise, we set the coefficient \\(k_\\epsilon \\)  to be 1, 0.25 and 0.25 for \\(L_D\\) , \\(L_S\\)  and \\(L_M\\) , respectively, and set \\(\\lambda \\)  to be 2. To show the statistical significance, all the experiments for CIFAR-10 and SVHN are averaged over five experiments with different parameter initialization seeds. The details of the network structure and training scheme for each dataset is as follows:\n"]}
{"id": "1911.01678", "categories": "cs.CL cs.AI", "paragraphs": ["\\(\\bullet \\)DEFT: This is a recently released dataset for DE [26]. DEFT consists of two categories of definitions: a) Contracts: involving 2,433 sentences from the 2017 SEC contract filing with 537 definitional and 1906 non-definitional sentences. Besides terms and definitions, this corpus has an additional type qualifier. It indicates the words/phrases specifying the conditions, dates or locations in which the definitions are valid for the terms. We also use the BIO tagging schema for this type. 2) Textbook: involving 21,303 sentences from the publicly available textbooks in different domains, including biology, history, and physics. This corpus contains 5,964 definitional and 15,339 non-definitional sentences.\n", "For all the datasets, we use the standard data splits to ensure a comparable comparison with the prior work. We fine tune the model parameters on the validation set of the DEFT Contract dataset and fix the detected parameters to train and evaluate the models on the other datasets for consistency. The parameters we found include: 50 dimensions for the POS embeddings; 200 dimensions for the LSTM and GCN hidden vectors and all the feed forward neural networks in the model; \\(a=1,b=1,c=1,\\alpha =1, \\beta =10, \\gamma =1\\)  and \\(\\eta =1\\)  for the trade-off parameters; \\(U=3\\)  for the latent labels in the semantic consistency module; and the learning rate of 0.003 for the Adam optimizer. We use the pre-trained word embeddings GloVe with 300 dimensions from [24] to initialize the model. Finally, to assess how well our model could benefit from the pre-trained contextualized word embeddings, we also perform an additional experiment where BERT [7] is used to initialize the word embeddings in the model.\n"]}
{"id": "1906.00654", "categories": "cs.LG cs.SD eess.AS stat.ML", "paragraphs": ["For all experiments, we optimize our models using Adam[21]. The batch size is set to 100, and there are 10 - 15 batches per epoch for each task. To train the classifier, we use an initial learning rate equal to 5e-4 and we train it for 300 epochs by minimizing the cross-entropy loss for each task. Moreover, in order to train the generator, we use an initial learning rate of 1e-3 and train it for 1700 epochs for each task. The autoencoder loss is the binary cross-entropy for each time-frequency bin between the original spectrogram and the reconstruction. The loss for the variational autoencoder is the sum of binary cross-entropy and KL-Divergence between the modeled distribution and unit Gaussian.\n"]}
{"id": "1904.00824", "categories": "cs.CV cs.GR stat.ML", "paragraphs": ["RA. Six classes shown in Figure\u00a0REF  are used for this experiment on training images synthesized using local BRDFs with environment maps for reflection approximation. In order to train the object detector we used \\(12K\\)  frames. We set \\(batch\\_size = 8\\) , from which we determine the number of steps per epoch \\(steps = 1500\\) . Training is stopped when no further improvement takes place. For RA the process was stopped after 55 epochs.\n", "DR. In the second experiment we trained another detector on the DR images. A simple indoor scene is randomized in order to apply domain randomization to our data set as described in Section . For synthesis of frames we also used local shading. We train the object detector with \\(38K\\)  images and we set \\(batch\\_size = 8\\)  leading to 4750 steps per epoch. This task was terminated after 39 epochs.\n"]}
{"id": "1910.08914", "categories": "cs.CV eess.IV", "paragraphs": ["We use the Adam\u00a0[16] optimizer with momentum parameters \\(\\beta _1=0.5, \\beta _2=0.999\\) . We update one step for either \\(G\\)  or \\(D\\)  alternatively, and batch size is set to 8.\nEither the first or the second training stage lasts 100 epochs with an initial learning rate \\(lr_G=0.0001\\)  for the generator and \\(lr_D=0.0004\\)  for the discriminator, while the third stage lasts 50 epochs with initial learning rates \\(lr_G=0.00001\\)  and \\(lr_D=0.00004\\) .\nThe learning rates decay at the halfway point of each stage.\nThe entire training process takes about seven days on eight GeForce GTX 1080Ti GPUs.\n"]}
{"id": "1902.01382", "categories": "cs.CL", "paragraphs": ["Third, we consider a semi-supervised setting where we also leverage monolingual data on the target side using\nthe standard back-translation training protocol\u00a0[31]: we train a backward MT system, which we use to translate monolingual target sentences to the source language.\nThen, we merge the resulting pairs of noisy (back-translated) source sentences with the original target sentences and add them as additional parallel data for training source-to-target MT system. Since monolingual data is available for both languages, we train backward MT systems in both directions and repeat the back-translation process iteratively\u00a0[14], [19].\nWe consider up to two back-translation iterations.\nAt each iteration we generate back-translations using beam search, which has been shown to perform well in low-resource settings\u00a0[8]; we use a beam width of 5 and individually tune the length-penalty on the dev set.\n"]}
{"id": "1904.04764", "categories": "cs.CL cs.AI cs.LG cs.SD", "paragraphs": ["We train the end-to-end TTS system with a high-quality American English speech database used in 2011 Blizzard Challenge, which has 16 hours of speech recorded by a single female speaker. We train these models for 200,000 iterations with a batch size of 128 distributed across 4 GPUs with synchronous updates, using \\(L1\\)  loss and Adam optimizer with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\)  and a learning rate of \\(10^{-3}\\)  exponentially decayed to \\(10^{-5}\\)  after \\(50,000\\)  iterations. In this study, we use factored parser [14] of the Stanford Parser [27] to extract syntactic trees.\n"]}
{"id": "1904.04775", "categories": "cs.CL cs.LG cs.SD", "paragraphs": ["We use Tacotron2 [9] as TTS model, include WaveNet as vocoder for all experiments. We use one-hot feature as input, which contains phonemes, punctuation and the blank between two adjacent words. The model output is an 80-channel Mel spectrum (12.5 ms frame shift, 50 ms frame length), one frame at a time. The model structure of the discriminator has been shown in Fig.REF , which has 1536-dim input, 512-dim hidden size, and 1-dim output.\n", "All models are trained with a batch size of 128 sequences. We train these models using the Adam optimizer with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\) . The learning rate is exponentially decayed from \\(10^{-3}\\)  to \\(10^{-5}\\)  after 50,000 iterations. The TF model trained with 100,000 steps is set to be the baseline model. In SS, TF-GAN and SS-GAN training, we adopt the TF model trained with 50,000 steps as the pre-trained model, and train it for another 50,000 steps with these algorithms. The scheduled sampling strategy is to use real data with a linear decay, from probability 1 to 0.5, in the first 50,000 steps. We set the initial learning rate \\(lr_g = 10^{-3}, lr_d = 10^{-3}\\) , adversarial weight \\(\\alpha = 10^{-3}\\)  for GAN-based algorithms. The range of the required discriminator accuracy is set to \\(75\\% \\sim 97\\%\\) .\n"]}
{"id": "1903.05807", "categories": "cs.CV", "paragraphs": ["We train our model for 50 epochs with batch size 32. We use Adam optimizer [12] with initial learning rate 0.01, \\(\\beta _1\\)  0.9 and \\(\\beta _2\\)  0.999. Batch normalization [9] is applied before activation functions in all layers except the last layer. Leaky ReLU with a fixed leakiness parameter 0.2 is used as activation functions. Dropout [20] with keep ratio 0.7 is applied on last three fully-connected layers.\n"]}
{"id": "1910.12129", "categories": "cs.CL", "paragraphs": ["Even though on the small datasets we work with we do not necessarily expect the Transformer model to perform better than recurrent neural networks, we chose this model for its significantly faster training, without sacrificing the performance. For our experiments a small 2-layer Transformer with 8 heads proved to be sufficient. The input tokens are encoded into embeddings of size 256, and the target sequences were truncated to 60 tokens. The model performed best with dropout values of 0.2. For training of the Transformer models we used the Adam optimizer with a custom learning rate schedule including a brief linear warm-up and a cosine decay.\n"]}
{"id": "1906.09925", "categories": "cs.LG eess.SP stat.ML", "paragraphs": ["We employ a variety of masks based on the dependencies we want to capture in the dataset. We use three different kinds of the mask in our evaluation (mask A1 and B1 for mask 1, mask A2 and B2 for mask 2, mask A3 and B3 for mask 3).The masks 1, 2 and 3 for filter dimension 5 is shown in Fig. REF  (c, d, e) respectively, where the middle element in Fig. REF  (c, d, e) is 0 for mask A and 1 for mask B.\n"]}
{"id": "1909.06092", "categories": "cs.CL cs.AI", "paragraphs": ["Augmented Bias Specifications. We first augment the bias specifications using a similarity-specialized embedding space produced by [23]Available at: https://tinyurl.com/y273cuvk. based on the en fastText embeddings [1]. For WEAT T8, we augment the target and attribute lists with \\(k=4\\)  nearest neighbours of each term. As the initial lists of WEAT T1 are longer than those of T8, we use \\(k=2\\)  with T1. We train all debiasing models using bias specifications containing only the augmentation terms (i.e., without the initial bias specification terms); we use the initial terms for testing.\n"]}
{"id": "1907.13196", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["Recognising that \\(W(\\mathbf {\\phi }_{0}) = 0\\)  (the distance between the same probability densities), and \\(\\nabla _{\\mathbf {\\phi }} W(\\mathbf {\\phi }_{0}) = 0\\)  since \\(\\mathbf {\\phi }_{0}\\)  minimises \\(W(\\mathbf {\\phi })\\) , we can simplify the Hessian approximation by writing:\n\\(W(\\mathbf {\\phi }) \\approx \\frac{1}{2} (\\mathbf {\\phi } - \\mathbf {\\phi }_{0})^{\\mathsf {T}} \\nabla ^{2}_{\\mathbf {\\phi }} W(\\mathbf {\\phi }_{0})(\\mathbf {\\phi } - \\mathbf {\\phi }_{0}).\\) \n"]}
{"id": "1907.13268", "categories": "cs.CV", "paragraphs": ["For both synthetic and real experiments, our EMP-Net model is trained with a batch size of 16, where every instance within a batch is a sequence of 5 consecutive frames resized to \\(120 \\times 160\\) . The RGB and depth data were scaled to between \\([0,1]\\) . The buffer size of the SSMM is \\(b=4\\)  and the number of extracted point-embeddings is \\(N_r=4800\\) . The temperature parameter was tested with values between \\(\\tau =[10^{3},10^{6}]\\) , where we found the model to be fairly invariant to this value. In all of the experiments shown \\(\\tau =10^{5}\\) . The embedding distance function is defined as the \\(L2\\)  distance, \\(d_{\\phi }=\\left\\Vert a-b\\right\\Vert _2\\) . \\(\\lambda _t=0.02\\)  and \\(\\lambda _R=5\\)  are chosen to maintain the same ratio as in\u00a0[22]. We use the ADAM optimiser\u00a0[23], using the default first and second moment terms of 0.9 and 0.999 values respectively. We use a learning rate of \\(10^{-3}\\)  and train for 10 epochs.\n"]}
{"id": "1901.06484", "categories": "cs.CV", "paragraphs": ["The network depth is usually defined as the length of the longest path from the input to the output [17], [30]. According to the entire structure of the proposed model, the depth of our CSSFN network is given by:\n\\(D = n[1 + m \\times (q + 1)] + s + 6,\\) \n", "where \\(s\\)  denotes the depth of the upscale modula and depends on the specific value of the scaling factor \\(r\\) . Specifically, \\(s = 1\\)  for \\(r = 2\\)  or \\(r = 3\\) , and \\(s = 2\\)  for \\(r = 4\\) . The first \u201c1\u201d in (REF ) corresponds to the compression layer at the beginning of each CSSFB, and the second one denotes the extension layer at the end of each CSSFU.\n"]}
{"id": "1903.06482", "categories": "cs.CV cs.LG", "paragraphs": ["Both the depth and semantic predictions are jointly trained using groundtruth data.\nIn addition to the reconstruction losses discussed in the following sections, the variational setup requires a KL-divergence based loss on the latent space [20].\nIn order to avoid a degrading latent space, we employ a KL annealing strategy [3], [33] where we gradually increase the weights of the KL terms from 0 after 2 training epochs.\nFinally, the weights of semantic vs. depth reconstruction losses are trained in an adaptive manner to account for task-dependent uncertainty [17].\nIn all of our experiments, we train\nthe whole network in an end-to-end manner using the Adam optimiser [19] with an initial learning rate of \\(10^{-4}\\)  and a weight decay of\n\\(10^{-4}\\) . The ResNet-50 is initialised using ImageNet pre-trained\nweights, and all other weights are initialised using He 's method [10].\n", "For depth images, as in [2],\nthe raw depth values \\(d\\)  are first transformed via a hybrid\nparametrisation called proximity, \\(p= a/(a+d)\\) ,\nwhere \\(a\\)  is the average depth value, which is set to 2m in all\nof our experiments. In this way, we can handle raw depth values\nranging from 0 to \\(+\\infty \\)  and assign more precision\nto regions closer to the camera. An \\(L_{1}\\)  loss function\ntogether with data dependent Homoscedastic uncertainty [16]\nis used as the reconstruction error:\n\\(L_{\\phi ,\\theta }\\left(d\\right) & = \\sum _{i=1}^{N}\\left[\\frac{\\left|\\widetilde{p}_{i}-p_{i}\\right|}{b_{i}}+\\log \\left(b_{i}\\right)\\right],\\) \n"]}
{"id": "1908.02391", "categories": "cs.CV", "paragraphs": ["We use Inception-V3 as a backbone for our model. In particular, we take the convolutional layers and initialize them with weights from a standard network pre-trained on ImageNet. The final descriptors are further max-pooled and \\(\\ell _2\\)  normalized. The descriptor size is \\(2,048\\) . The model is trained using the ADAM optimizer, with the initial learning rate \\(10^{-4}\\) , and with a learning rate decay of \\(0.9\\)  each \\(50\\textrm {k}\\)  iterations. The images for person re-ID are resized to \\(192 \\times 384\\)  pixels, and \\(256 \\times 256\\)  for Stanford Online Products. At test time, we extract representations and compare them using dot product.\n"]}
{"id": "1907.01922", "categories": "cs.CV", "paragraphs": ["In our proposed model, we used the encoder architecture in\u00a0[3] as the backbone for both CNNs.\nWe adopted the initialization strategy of this work\u00a0[7] to initialize the weights of all convolutional layers.\nMoreover, we set the initial learning rate as \\(1e^{-4}\\) , periodically reduced it by multiplying with \\(0.1\\) , and stopped the learning process after 100 epochs.\nWe employed the Adam optimizer\u00a0[9] with the first momentum of 0.9, the second momentum of 0.999, and a weight decay of \\(0.0001\\)  to minimize the loss (see Eq.\u00a0REF ) of the whole network.\nOur network was implemented using the Keras toolbox with a Tensorflow backend and we set the mini-batch size as one.\n"]}
{"id": "1907.01791", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["All our proposed models are trained with backpropagation, and gradient-based optimization is performed using Adam [16]. In all experiments, we set the character LSTM hidden size to 64 and word embedding LSTM hidden size to 128. We use 300-dimension GloVe vectors [27] for the benchmark datasets and in-house embeddings for the Alexa dataset, which are trained with Wikipedia data and live utterances spoken to Alexa. Character embedding dimensions and dropout rate are set to 100 and 0.5 respectively. Minimax optimization in adversarial training was implemented via the use of a gradient reversal layer [9], [22]. The models are implemented with the TensorFlow library [0].\n", "For benchmark data, the models are trained using an early-stop strategy with\nmaximum epoch set to 50 and patience (i.e., number of epochs with no\nimprovement on the dev set for both SF and IC) to 6. In addition, the benchmark dataset has varied size vocabularies across its datasets. To give equal importance to each of them, \\(\\alpha _i^j\\)  (see Equation\u00a0REF ) is proportional to \\(1/n\\) , where \\(n\\)  is the training set size of task \\(j\\)  in group \\(i\\) . We are able to train on CPUs, due to the low values of \\(n\\) .\n", "For Alexa data, optimal hyperparameters are determined on the 80 development skills and applied to the training and evaluation of the 90 test skills. \\(\\alpha _i^j\\)  is here set to 1 as all skills have \\(10,000\\)  training utterances sampled from the respective developer-defined skill grammars [17]. Here, training was done using GPU-enabled EC2 instances (p2.8xlarge).\n"]}
{"id": "1909.06515", "categories": "cs.CL cs.SD eess.AS", "paragraphs": ["For the B\u00e9rard architecture, we use the Adam optimizer\u00a0[22] with a learning rate of 0.001.\nFor the smaller AST Librispeech task, we use a minibatch size of 16000 frames to help convergence.\nFor other tasks, we use a minibatch size of 96,000 frames except for the vggtransformer where we use 72,000 frames (to avoid memory issues).\nWe also use delayed updates\u00a0[23] in order to keep the same effective batch size and avoid GPU out-of-memory errors.\nAll experiments are conducted on 8 GPUs.\nFor other architectures than B\u00e9rard, we use ADADELTA\u00a0[24] with a learning rate of 1 and we normalize the loss per utterance instead of per token.\nThese hyperparameters were chosen based on preliminary experimentation on the ASR Librispeech task.\n"]}
{"id": "1910.13676", "categories": "cs.CV cs.AI", "paragraphs": ["As mentioned in section  we use our modified version of Pointnet++ for training and testing the effectiveness of synthetic data. We train the model on point clouds with and without color information denoted as RGB-Di-* and Di-* respectively. Here * represents the type of training dataset, i represents the number of training classes (excluding unlabelled class), RGB stands for Red, Green, Blue channel of image and D stands for Depth information. We trained all models until saturation, and used Adam optimizer with learning rate of \\(0.001\\) , momentum of \\(0.9\\)  and learning rate decay of \\(0.7\\)  . All models are trained on a sample size of 8192 points per batch.\n"]}
{"id": "1905.12349", "categories": "cs.CV", "paragraphs": ["The ILSVRC 2012 classification dataset consists of 1.28 million images for training, and 50,000 images for validation, from 1000 different classes.\nWe train networks on the training set and report the top-1 and top-5 accuracy on the validation set.\nWe train our models using Pytorch, and optimization is performed using synchronous Stochastic Gradient Descent (SGD) algorithm with a momentum of \\(0.9\\)  and a batch size of 256.\nFollowing the MobileNets\u00a0,  setup, we use a initial learning rate of 0.045, and a learning rate decay rate of 0.98 per epoch.\n", "The CIFAR-100 dataset consists of colored natural images with \\(32 \\times 32\\)  pixels.\nThe training and testing sets contain 50000 and 10000 images, respectively.\nWe train networks on the training set and report the average accuracy of three executions of the experiments on the testing set.\n", "We train all these models using Pytorch and optimize them using synchronous SGD algorithm with the momentum of \\(0.9\\)  and the batch size of 128.\nAll the models are trained in 150 epochs from scratch.\nThe initial learning rate is set to \\(0.01\\)  and decreased by a factor of 10 every 80 epochs.\n"]}
{"id": "1904.09764", "categories": "cs.CV cs.LG", "paragraphs": ["We use CIFAR-100 dataset\u00a0[15] to perform the analysis. The dataset contains 60,000 32x32 color images in 100 different classes (600 images per class). It is split into training set and test set with the ratio of 5:1. All models in this section are trained 90 epochs with random horizontal flip as data augmentation\u00a0[16], for preprocessing, we normalize the data using the channel means and standard deviations as in\u00a0[16]. The networks are updated with ADAGRAD\u00a0[4] optimizer with learning rate set to 0.1 and decreases to 0.01 from 45th epoch on wards.\n"]}
{"id": "1910.07747", "categories": "cs.LG cs.HC stat.ML", "paragraphs": ["Exponential linear units (ELU) were used as a nonlinear function in our network. In addition to the two networks, i.e., \\(T_\\text{g}\\)  and \\(T_\\text{l}\\) , we applied a batch normalization [32]. blackFurthermore, we applied an \\(l_2\\) -regularization with a coefficient of black\\(0.1\\)  and a dropout [33] with a rate of \\(0.5\\)  to prevent over-fitting. We trained models by using a RAdam [34] with a learning rate of \\(10^{-3}\\)  by exponentially decreasing \\(0.99\\)  per epoch, where the dimension of a mini-batch size was 40. Regarding the hyper-parameters \\(\\alpha \\) , \\(\\beta \\)  and \\(\\gamma \\)  in Eq. (REF ), we chose \\(\\lbrace \\alpha , \\beta , \\gamma \\rbrace \\)  as \\(\\lbrace 0.5,0.3,0.5\\rbrace \\)  for all cases.\n"]}
{"id": "1911.12815", "categories": "cs.LG cs.AR cs.ET eess.SP", "paragraphs": ["We set \\(N_i=1,2,3\\)  to get three distinct resolution configurations in each pipeline stage in our experiments.\nFor each stage, we train different NN models and each NN model is trained via stochastic gradient descent with the Adam optimizer using TensorFlow\u00a0.\nThe weight precision \\(A_\\textit {R}\\)  during training is set to be 1\\(\\sim \\) 7-bit.\nThe batch size is 4096, and the projection step is performed every 256 iterations.\nWe train for a total of 2\\(\\times 10^4\\)  iterations for each sub-ADC model and residue model, varying the learning rate from \\(10^{-3}\\)  to \\(10^{-4}\\)  across the iterations.\n"]}
{"id": "1902.11109", "categories": "cs.CV cs.CL", "paragraphs": ["In experiments, we set word embedding dimension, model dimension and latent vector dimension all to 128. The number of attention layer \\(L=4\\) , number of heads \\(H=8\\)  each with dimension \\(d_{h}=64\\) . Consider the size of our dataset and vocabulary size, we set our model parameters to a relatively small value compared to Transformers.\n", "All parameters are initialised from Gaussian Distribution \\(N(0, 0.02)\\) . We employ vanilla GAN to train our model, learning rate is set to \\(5 \\times 10^{-6}\\)  as we find a larger learning rate leads to instability very quickly. Adam optimiser is employed with beta1 equals 0.5.\n"]}
{"id": "1910.12906", "categories": "cs.CV cs.RO", "paragraphs": ["\u00a0\nFor training STEP-Gen, we use a batch size of 8 and train for 150 epochs. We use the Adam optimizer\u00a0[25] with an initial learning rate of \\(0.1\\) , which decreases to \\(\\frac{1}{10}\\) -th of its current value after 75, 113 and 132 epochs. We also use a momentum of \\(0.9\\)  and and weight-decay of \\(5\\times 10^{-4}\\) .\n", "For training STEP, we use a split of \\(7:2:1\\)  for training, validation and testing sets. We use a batch size of 8 and train for 500 epochs using the Adam optimizer\u00a0[25] with an initial learning rate of \\(0.1\\) . The learning rate decreases to \\(\\frac{1}{10}\\) -th of its current value after 250, 375 and 438 epochs. We also use a momentum of \\(0.9\\)  and and weight-decay of \\(5\\times 10^{-4}\\) .\nAll our results were generated on an NVIDIA GeForce GTX 1080 Ti GPU.\n"]}
{"id": "1905.07473", "categories": "cs.LG math.OC stat.ML", "paragraphs": ["We train separate 2-layer LSTMs with a embedding input layer and a linear output-layer to both the fixed- and variable- copy tasks by minimizing the cross-entropy loss.\nThe embedding dimension is set to 6 and hidden and cell dimensions of the LSTM layers are set to 50.\nWe train \\(\\theta \\)  using SGD\nusing a batchsize of \\(S=64\\)  and a fixed learning rate of \\(\\gamma = 1.0\\) \nwith fixed TBPTT \\(K \\in [5, 10, 15, 20, 30]\\)  and our adaptive TBPTT method \\(\\delta \\in [0.9, 0.5, 0.1]\\) , \\(W = 100\\) , \\(K_0 = 15\\)  and \\([K_{\\min }, K_{\\max }] = [2,100]\\) .\nWe set \\(W = 100\\) , \\(K_0 = 15\\)  and \\([K_{\\min }, K_{\\max }] = [2,100]\\)  for Algorithm\u00a0.\n", "For both the PTB and Wiki2 corpus,\nwe train 1-layer LSTMs with a word embedding layer input and a linear output layer.\nThe embedding dimension, hidden state, and cell state dimensions are all 900 for the PTB following\u00a0() and 512 for the Wiki2 corpus following\u00a0().\nWe use a batchsize of \\(S=32\\)  and a fixed learning rate of \\(\\gamma = 10\\) \nfor fixed TBPTT \\(K \\in [10, 50, 100, 200, 300]\\)  and our adaptive TBPTT method \\(\\delta \\in [0.9, 0.5, 0.1]\\) .\nWe set \\(W = 400\\) , \\(K_0 = 100\\)  and \\([K_{\\min }, K_{\\max }] = [10,400]\\)  for Algorithm\u00a0.\n"]}
{"id": "1906.04980", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["Here we describe experimental details for unsupervised NMT setup. We use the English tokenizer from Moses\u00a0[19], and use FastBPE (https://github.com/glample/fastBPE) to split into subword units, with a vocabulary size of 60000. The architecture uses a 4-layer transformer encoder and 4-layer transformer decoder, where one layer is language specific for both the encoder and decoder, the rest are shared. We use the standard hyperparameter settings recommended by [23]. The models are initialised with random weights, and the input word embedding matrix is initialised using FastText vectors [3] trained on the concatenation of the \\(C\\)  and \\(Q\\)  corpora. Initially, the auto-encoding loss and back-translation loss have equal weight, with the auto-encoding loss coefficient reduced to \\(0.1\\)  by 100K steps and to 0 by 300k steps. We train using 5M cloze questions and natural questions, and cease training when the BLEU scores between back-translated and input questions stops improving, usually around 300K optimisation steps. When generating, we decode greedily, and note that decoding with a beam size of 5 did not significantly change downstream QA performance, or greatly change the fluency of generations.\n"]}
{"id": "1905.11062", "categories": "cs.LG stat.ML", "paragraphs": ["We use the Glorot uniform initializer [11] for the weights of encoder-decoder networks. The codebook is initialized by the uniform unit scaling. All models are trained using Adam optimizer [16] with learning rate 3e-4 and evaluate the performance after 40000 iterations with batch size 32. Early stopping at 10000 iterations is applied by soft VQ-VAE on SVHN and CIFAR-10 datasets.\n"]}
{"id": "1910.11552", "categories": "cs.LG stat.ML", "paragraphs": ["According to [15], the selection of hyper parameters greatly influences the performance of the algorithms. For LS-SVM with the popular Gaussian kernel (i.e., \\(K(u,v)={\\rm exp}(-\\delta ||u-v||^{2})\\) ), it mainly has two hyper parameters which are the cost parameter \\(C\\)  (preventing the model from being too complex) and kernel parameter \\(\\delta \\) . ELM using Gaussian kernel, has three hyper parameters, the cost parameter \\(C\\) , kernel parameter \\(\\delta \\)  and number of nodes \\(L\\) . For the proposed GNN with R-WDD, it also has three hyper parameters which are the regularizer \\(\\gamma \\) , the parameter for GPS \\(\\lambda \\)  and the number of neurons \\(L\\) . In this paper, the value of \\(\\lambda \\)  is fixed to \\(0.05\\) . Also, the number of nodes for ELM and the proposed GNN with R-WDD for binary and multi-classification problems are fixed to 1000.\n"]}
{"id": "1905.02244", "categories": "cs.CV", "paragraphs": ["We train our models using synchronous training setup on\n4x4 TPU Pod [23] using standard tensorflow RMSPropOptimizer with 0.9 momentum. We use the initial learning rate of 0.1, with batch size 4096 (128 images per chip), and learning rate decay rate of 0.01 every 3 epochs. We use dropout of 0.8, and l2 weight decay 1e-5 and the same image preprocessing as Inception\u00a0[41]. Finally we use exponential moving average with decay 0.9999. All our convolutional layers use batch-normalization layers with average decay of 0.99.\n"]}
{"id": "1911.09071", "categories": "cs.CV cs.LG q-bio.NC", "paragraphs": ["We trained at a batch size of 4096 using SGD with Nesterov momentum of 0.9 and weight decay of \\(8 \\times 10^{-5}\\)  and performed evaluation using an exponential moving average of the training weights computed with decay factor 0.9999. The learning rate schedule consisted of 10 epochs of linear warmup to a maximum learning rate of 1.6, followed by exponential decay at a rate of 0.975 per epoch. For all conditions we randomly horizontally flipped images and performed standard Inception-style color augmentation.\n"]}
{"id": "1909.13701", "categories": "cs.CV", "paragraphs": ["For all the experiments we use images resized to 256\\(\\times \\) 512 (due to our limited memory size), and hyper-parameters \\(\\lbrace w_1, w_2, w_3, \\lambda _{cyc}, \\lambda _{ste}\\rbrace  = \\lbrace 0.5, 0.7, 1, 10, 0.05\\rbrace \\) . To accommodate the stereo pair requirement, our translation network is simply modified to have two exactly similar weight-sharing pipelines, each working on an image from the pair. We use a buffer of 50 previously translated pairs to reduce model oscillations\u00a0[3], [25]. All networks use Adam\u00a0[15] solver with its parameters \\(\\lbrace \\beta _1, \\beta _2\\rbrace =\\lbrace 0.5, 0.999\\rbrace \\) . We use a batch size of 4, and keep a constant learning rate of 0.0002 for all the networks for the first-half of the training epochs, and then linearly decay it to zero over the second-half.\n"]}
{"id": "1911.04947", "categories": "cs.LG stat.ML", "paragraphs": ["The imitation learning model acts as a policy network during the next training phase which uses PPO. A replica of the same network is created for value function estimation, with the output layer of size 1 instead of 6. We refrain from using the same CNN layers to approximate the value function as this creates aberration during the initial phase of learning and often leads to policy forgetting and degradation. We also avoid using any regularization technique such as dropout while training using PPO, as this leads to significant increase in KL divergence between trained policies.\n"]}
{"id": "1911.05045", "categories": "cs.CV cs.LG", "paragraphs": ["In all the experiments we use Stochastic Gradient Descent (SGD) with 0.9 momentum, cross entropy loss as loss function and L2 regularization.\nAll other training details are provided along with the open-sourced code (Section\u00a0REF ).\n"]}
{"id": "1902.02401", "categories": "cs.LG stat.ML", "paragraphs": ["For our CNN, we use 300-dimensional Word2Vec\u00a0[10], [11], [12] word embeddings trained on GoogleNews datasethttps://code.google.com/p/word2vec, and 128 feature maps with filter width {2, 3, 4}. We set maximum word lengths of 50 and 500 for claims and documents respectively; these values are greater than the length for most claims and documents in the target train data. For the BOW model, we keep the hyper-parameters and features the same as the baseline model\u00a0[15].\nOur models are trained using the Adam optimizer, and \\(20\\%\\)  of the training data is set aside as validation data. In the models with a domain adaptation (DA) component, equal amounts of both source and target data are randomly selected at each epoch during training. Finally, we fine-tune all the hyper-parameters of our models on validation data which contains equal amounts of source and target data.\n"]}
{"id": "1911.11536", "categories": "cs.LG stat.ML", "paragraphs": ["The best results were obtained with batch-size \\(b = 128\\)  and epochs \\(e = 40\\) . \\(Nadam\\) \u00a0[13] is used as optimiser and the mean squared error (MSE) as loss-function. When using bigger batch-sizes, unwanted jumps in the training loss were observed regularly, and when using smaller batch-sizes, overfitting occured early during training.\n{FIGURE}"]}
{"id": "1911.11763", "categories": "cs.CV", "paragraphs": ["Model Parameters:\u00a0 The Keypoint Encoder MLP has 5 layers, mapping positions to dimensions of size \\((32,64,128,256,D)\\) , yielding 100k parameters. Each layer has the three projection matrices, and an extra \\({\\bf W}^O\\)  to deal with the multi-head output. The message update MLP has 2 layers and maps to dimensions \\((2D,D)\\) . Both MLPs use BatchNorm and ReLUs. Each layer has 0.66M parameters. SuperGlue has 18 layers, with a total of 12M parameters.\n{FIGURE}"]}
{"id": "1905.07967", "categories": "cs.CV cs.RO", "paragraphs": ["The dataset from section REF  was split into training and test set with a \\(4:1\\)  ratio. As a result, 3725 images were used for training and 931 for testing. The networks were trained with Tensorflow using an Nvidia GeForce GTX 1080 Ti graphics card.\n"]}
{"id": "1905.07826", "categories": "cs.CV", "paragraphs": ["The model was built on tensorflow 1.8 framework and implemented using Python 3.6. The code is open-sourced and can be found at \\(github.com/hyuna915/video_segmentation\\). In addition to original code to pre-process image, isolate objects, construct U-Net, and SegNet graph, we used the davis-2017 code repository for model evaluation and OSVOS code repository for constructing OSVOS layer.\n", "Our model was trained on N1-HighMem-8 instance, with v8CPU, 52 GB memory, on Google Cloud Compute Platform. We've also attached NVIDIA Tesla P100 GPU with 16GB memory to the virtual machine. Due to the sheer amount of model structure/parameters we have experimented upon, in total 5 GPUs are used for this project.\n"]}
{"id": "1911.08522", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["For all the experiments, we use the GloVe word embedding of size 300\u00a0[8]. The encoder and decoder are bidirectional LSTMs of 3-layers with a state size of 256. The dimension of all hidden layers is 256. We initialized all weights with samples from a normal distribution with mean zero and a standard deviation of 0.0001. We minimize the loss function in Equation\u00a0REF  with the Adam optimizer\u00a0[5], with a learning rate of \\(0.005\\) . We also applied regular dropout after the input and output of each LSTM with keep probability of \\(95.0\\%\\)  for SMTD and \\(50.0\\%\\)  for CamRest. We identified the overwrite probability \\(\\epsilon \\)  by random search in \\([0.01, 5]\\)  and fixed it at \\(0.1\\)  after evaluating in the held-out validation dataset.\n"]}
{"id": "1910.05577", "categories": "cs.CV cs.CL", "paragraphs": ["For the default setting on ImageNet, we use \\(224\\times 224\\)  random resized cropping and random horizontal flipping for data augmentation. Then we standardize the data with mean and variance per channel. We use a traditional cross-entropy loss to train all the networks with a batch size of 256 on 8 GPUs by SGD, a weight decay of 0.0001, and a momentum of 0.9 for 100 epochs. We start from a learning rate of 0.1 and decrease it by a factor of 10 every 30 epochs. The last normalization layers in the module are zero-initialized to make the gates start from constants. All the extra layers in Context-Gated Convolution have a learning rate ten times smaller than convolutional kernels.\n", "For the advanced setting, we also use mixup\u00a0[55] for data augmentation, and we follow\u00a0[22] to use learning rate warm-up in the first 5 epochs of training. We train the networks with the cosine learning rate schedule\u00a0[22] for 120 epochs. The other hyper-parameters are set to be same with the default setting.\n", "For CIFAR-10, we use \\(32\\times 32\\)  random cropping with a padding of 4 and random horizontal flipping. We use a batch size of 128 and train on 1 GPU. We decrease the learning rate at the 81st and 122nd epochs, and halt training after 164 epochs.\nFor the ablation study, the result is an average of 3 runs.\n"]}
{"id": "1909.11287", "categories": "cs.CL cs.AI", "paragraphs": ["\nSequence to sequence. For SEQ2SEQ, we adopt one layer LSTMs as encoder and decoder. For Key-Value Retrieval dataset, hidden size is placed at 512 and the dropout rate is 0.3. On dataset bAbI, the hidden size and dropout rate are 128 and 0.1 for task 3, 256 and 0.1 for task 4 and 5. Learning rates are set to 0.001 for bAbI and 0.0001 for DSTC 2 and Key-Value Retrieval dataset.\n\nSEQ2SEQ + Attention. We adopt the attention mechanism [12] commonly used in neural machine translation. On dataset bAbI, hidden size and the dropout rate are 256 and 0.1 for task 3 and 4, 128 and 0.1 for task 5. For Key-Value Retrieval dataset, hidden size and dropout rate are 512 and 0.3. On the DSTC 2 task, hidden size is set to 353 and word embedding size is 300 (same with original work).\n\nMem2Seq. Except 128 in task 3, hidden size in other tasks is 256. The dropout rate is set to 0.2 in task 3, 4 and Key-Value Retrieval dataset, 0.1 in task 5 and DSTC 2 dataset. We adopt three hops in DSTC 2 and Key-Value Retrieval dataset.\n\nHMNs with context-free only (HMNs-CFO). To test the performance of context-aware memory, we apply other context-free to encode dialogue history instead of context-aware memory in HMNs. All the other structure and parameter settings are the same as HMNs in this model.\n\n"]}
{"id": "1909.11229", "categories": "cs.CV cs.LG", "paragraphs": ["We utilized the pose estimation toolbox called DeepLabCut\u00a0[32], [37], [17], and added MobileNetV2\u00a0[43] and EfficientNet backbones\u00a0[46] to the ResNets\u00a0[12] that were present, as well as adding imgaug for data augmentation\u00a0[18]. The TensorFlow\u00a0[0]-based network architectures could be easily exchanged while keeping data loading, training, and evaluation consistent. The feature detectors in DeepLabCut consist of a backbone followed by deconvolutional layers to predict pose scoremaps and location refinement maps (offsets), which can then be used for predicting the pose while also providing a confidence score. As previously, for the ResNet backbones we utilize an output stride of 16 and then upsample the filter banks with deconvolutions by a factor of two to predict the heatmaps and location-refinement at 1 / 8th of the original image size scale\u00a0[17], [32]. For MobileNetV2\u00a0[43], we configured the output-stride as 16 (by changing the last stride 2 convolution to stride 1).\n", "The baseline EfficientNet model was designed by Tan et al.\u00a0[46] through a neural architecture search to optimize for accuracy and inverse FLOPS.\nFrom B0 to B6, compound scaling is used to increase the width, depth, and resolution of the network, which directly corresponds to an increase in ImageNet performance\u00a0[46].\nWe used the AutoAugment pretrained checkpoints from TensorFlow as well as adapted the EfficientNet's output-stride to 16 (by changing the (otherwise) last stride 2 convolution to stride 1).\n", "The training loss is defined as the cross entropy loss for the scoremaps and the location refinement error via a Huber loss with weight \\(0.05\\) \u00a0[32]. The loss is minimized via ADAM with batch size 8\u00a0[20]. For training, a cosine learning rate schedule, as in \u00a0[22] with ADAM optimizer and batchsize 8 was used; we also performed augmentation, using imgaug\u00a0[18], with random cropping and rotations. Initial learning rates and decay target points were cross-validated for MobileNetV2 \\(0.35\\)  and \\(1.0\\) , ResNet-50, EfficientNet B0, B3, and B5 for the pretrained and from scratch models (see Supplementary Material). For each model that was not cross validated (MobileNetV2 \\(0.5\\)  and \\(0.75\\) , ResNet-101, EfficientNet B1, B2, B4, B6), the best performing training parameters from the most similar cross validated model was used (i.e. the cross validated EfficientNet-B0 schedule was used for EfficientNet-B1; see Supplementary Material). For MobileNetV2s, we trained the batch normalization layers too (this had little effect on task performance for MobileNetV2-\\(0.35\\) ). Pretrained models were trained for 30k iterations (as they converged), while models from scratch were trained for 180k iterations. From scratch variants of the architectures used He-initialization\u00a0[11], while all pretrained networks were initialized from their ImageNet trained weights.\n"]}
{"id": "1904.02306", "categories": "cs.CL", "paragraphs": ["For the morphological tagger, we use the baseline implementation from P18-1247. This implementation uses an input layer and linear layer dimension of 128 and a 2-layer LSTM with a hidden layer dimension of 256. The Adam [18] optimizer is used for training and a dropout rate [32] of 0.3 is enforced during training. The tagger was trained for 10 epochs.\n", "For the lemmatizer, we use a 2-layer biLSTM encoder and a 1-layer LSTM decoder\nwith 400 hidden units. The dimensions of character and tag embedding are 200\nand 40, respectively. We enforce a dropout rate of 0.4 in the embedding and encoder LSTM layers. The\nlemmatizer is also trained with Adam and the learning rate is 0.001. We halve\nthe learning rate whenever the development log-likelihood increases and we perform early-stopping when the learning rate reaches \\(1\\times 10^{-5}\\) . We apply gradient clipping\nwith a maximum gradient norm of 5.\n"]}
{"id": "1910.11161", "categories": "cs.CL cs.AI", "paragraphs": ["The four models including the proposed model (THRED) are all encoder-decoder models. We use the bidirectional LSTM as the encoder part and the unidirectional LSTM as the decoder part. All models have the dimensional size of 500 in the hidden layers. The size of the latent variable \\(z\\)  is \\(d_z=100\\) . The size of the dense topic features in the (NMF) dense topic matrix is \\(d_t=40\\) . For each dataset, we pick top 20000 frequent tokens to make the vocabulary. We train the models with the learning rate of 0.0002. The best validated networks are saved in 400000 training epochs. We also improve the results using Beam Search [18] which samples best-first candidate tokens at each inference step. And we set the Beam number as 5.\n"]}
{"id": "1908.11024", "categories": "cs.CV", "paragraphs": ["Encoder/target network: In the classification experiment in Section REF , ResNet50 was used as an encoder network and the reduced AlexNet was used as a target network. The reduced AlexNet which consists of five convolutional layers and three FC layers, but the number of kernels in each layer are all reduced to about 1/2 to 1/4 of the ones in AlexNet [15]. Here the size of convolutional kernels is set to 3\\(\\times \\) 3. Then, in the object detection experiment, VGG16 was used as an encoder network and SSD300 [18] was used as a target network. In the knowledge distillation experiment in Section REF , ResNet26 was used as an encoder network and the reduced AlexNet was used as a target network.\n", "Training details: Each numerical value in all experimental results is the average value of three trials. Each iteration is 2,000 based on the batch size of 64, and it performs up to 50 epochs. Especially, in the case of the object detection, 120 epochs. We employ stochastic gradient descent (SGD) [30] with momentum 0.9 as an optimizer. Also we used the TensorFlow library for model construction as well as training.\n"]}
{"id": "1902.05300", "categories": "cs.CV", "paragraphs": ["with \\(\\lambda = 0.0001\\) .\nThe network is trained using the RMSProp algorithm (see for example\nhttp://www.cs.toronto.edu/~\ntijmen/csc321/slides/lecture_slides_lec6.pdf as referred to in ) with minibatch size 100,\nlearning rate \\(0.00002\\) , momentum 0, and decay \\(0.9\\) . The number of training\nepochs is 100.\n", "We used the same values for \\(\\alpha ,\\beta , \\gamma \\)  and \\(\\tau \\)  as in\n, in particular, \\(\\alpha =15\\) , \\(\\beta = 0.1\\) , \\(\\gamma = 0.0025\\)  and \\(\\tau =1\\) . The generator and the discriminator network were jointly trained by\nalternating gradient optimization. In particular, the Adam \noptimizer was adopted, with initial learning rate \\(0.0001\\) , momentum \\(0.5\\) , and\nminibatch size 25. The learning rate was halved every 5 epochs.\nWe applied the same early stopping rule as given in their\nimplementationhttps://github.com/nebulaV/DAGAN. This is based\non measuring the \\(\\mathcal {L}_{\\text{iMSE}}\\)  loss between the training set\nand validation set. We used the early stopping number 10. In total this\nresulted in 15 epochs of training.\n", "The network weights were initialized using He initialization  and the Adam \noptimizer was used for training. This optimizer takes as input a learning rate (step size) \\(\\alpha \\) , and two\nexponential decay parameters \\(\\beta _1\\)  and \\(\\beta _2\\)  related to a momentum\nterm. We refer to  for further explanations of these parameters.\nThe network was trained with \\(\\alpha = 10^{-4}\\) , \\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\)  and batch size equal 10.\n", "The network weights were provided by the authors of  and obtained\nbased on the training procedure as described in their paper . The loss function\nused to train the networks is the \\(\\ell ^2\\)  difference between the network\noutput and the ground truth, and the networks are trained using the stochastic\ngradient descent algorithm with momentum. The learning rate varies from \\(0.01\\)  to \\(0.001\\) , whereas the momentum is set to \\(0.99\\) ,\nand the minibatch size is equal to 1. During training, gradients are clipped\nto the interval \\([-I_{\\mathrm {max}}, I_{\\mathrm {max}}]\\)  with \\(I_{\\mathrm {max}}= 10^{-2}\\) , to prevent the divergence of the cost function. The networks are\ntrained for 101 epochs, and the code used to implement the networks is written in\nMatLab using the library MatConvNethttp://www.vlfeat.org/matconvnet.\n", "with \\(\\epsilon = 10^{-12}\\) .\nThe network parameters that minimize the loss function are determined using the\ninertial incremental proximal gradient (IIPG) optimizer (see\n,  for details). Optimization is performed for 1000\nepochs, with a step size of \\(10^{-3}\\) . Training data is arranged into\nminibatches of size 5. In the original paper, the batch size was set to 10, but\ndue to memory limitations we had to adjust this.\n"]}
{"id": "1902.05392", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["\nwhere \\(N\\)  is the number of images in the burst, \\(S\\)  is the set of kernel sizes, \\(\\beta \\)  and \\(\\alpha \\)  are the hyperparameters controlling the weight decay, \\(t\\)  is the training step, and \\(\\lambda _1 + \\lambda _2 = 1\\) .\nPlease note that during training we discard in-place addition of kernels to enable better convergence. However, once the network is well trained, the in-place addition can help speed up inference.\n"]}
{"id": "1911.07034", "categories": "cs.CV", "paragraphs": ["We train our LISA framework by following the training strategies of Mask R-CNN implemented on Facebook Detectron2\u00a0[47].\nSpecifically, we adopt the weights of ResNeXt-101-FPN\u00a0[27], [48] trained on ImageNet\u00a0[6] to initialize the parameters of the backbone network, and train our framework on two GeForce GTX 1080 Ti GPUs (four images per GPU) for 40\\(k\\)  training iterations.\nWe set the base learning rate as 1e-4, adopt a warm-up\u00a0[10] strategy to linearly increase the learning rate to 1e-3 during the first 1,000 iterations, keep the learning rate as 1e-3, and stop the learning after 40\\(k\\)  iterations.\nWe re-scale the input images, such that the longer side is less than 1,333 and the shorter side is less than 800 without changing the image aspect ratio.\nLastly, we randomly apply horizontal flips on the images for data augmentation.\n{FIGURE}"]}
{"id": "1905.04509", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["We train every model via stochastic gradient descent (SGD) with Nesterov momentum of weight 0.9 without dampening. We use a cosine shape learning rate schedule [6] which starts from \\(0.1\\)  and decreases gradually to 0 throughout the training. We set a weight decay of \\(10^{-4}\\) , except for the spatial shifting biases \\(\\mathbf {b}\\)  in which \\(10^{-5}\\)  is used instead.\nDuring the training, we call dealloc and realloc at each epoch for the half of the total epochs.\nThis is mainly for two reasons: (a) usually, most of dealloc is done before that time, and (b) we found this makes the training less sensitive on the choice of \\(\\gamma \\) .\nWhen a spatial bias is re-initialized, we sample a point from \\([-1.5, 1.5]\\times [-1.5, 1.5]\\)  pixels uniformly.\n"]}
{"id": "1904.02210", "categories": "cs.CL cs.LG", "paragraphs": ["We used 80-dimensional log Mel filterbank features with 3-dimensional pitch features. We tuned hyperparameters for these models using one Aymara reading.CMU Wilderness reading ID: AYMSBU. We found that a 4 layer encoder, 1 layer decoder with 768 for the encoder hidden size and projections, decoder hidden size, and attention hidden size yielded equal-best results with deeper models. These settings were then used for training the models used in our experiments.\n"]}
{"id": "1910.08470", "categories": "cs.CV cs.GR", "paragraphs": ["As mentioned above, we use the Darkening video sequence for training the model and the Light Switch video for testing.\nAll models are trained with the same parameters. The initial learning rate is \\(lr=0.001\\)  and is reduced by a factor of \\(0.1\\)  if the model does not improve for 2 epochs. The training process ends after 5 epochs of no improvements. For optimisation, the Adam optimiser [28] is selected with betas \\(b_1=0.9\\)  and \\(b_2=0.999\\) .\nFinally, the batch size is set to 1.\n"]}
{"id": "1910.08486", "categories": "cs.CL", "paragraphs": ["We initialize word embeddings with 128-d vectors and fine-tune\nthem during training. Concepts share the same embeddings with the words. The vocabulary size was set to 150k for both the source and target text. The hidden state size was set to 256.\nThe vocabulary size is increased from around 602 to 2216 concepts w.r.t the different number (\\(k=1,\\cdots , 5\\) ) of concept candidates for each word.\nNote that the generated concepts with UNKs were subsequently deleted.\nOur code is available on https://github.com/wprojectsn/codes, and the vocabularies and candidate concepts are also included.\n", "We trained our models on a single GTX TITAN GPU machine.\nWe used the Adagrad optimizer with a batch size of 64 to minimize the loss.\nThe initial learning rate and the accumulator value were set to 0.15 and 0.1, respectively. We used gradient clipping with a maximum gradient norm of 2.\nAt the time of decoding, the summaries were produced through a beam search of size 8.\nThe hyper-parameter settings were \\(\\lambda = 0.99\\) , \\(\\gamma =0.1\\) , \\(\\pi = 2.92\\)  on DUC-2004 and \\(\\pi =1.68\\)  on Gigaword. We trained our concept pointer generator for 450k iterations yielded the best performance, then took the optimization using RL rewards for RG-L at 95K iterations on DUC-2004 and at 50K iterations on Gigaword. We took the distance-supervised training at 5K iterations on DUC-2004 and at 6.5K iterations on Gigaword.\n"]}
{"id": "1910.13136", "categories": "cs.CV", "paragraphs": ["For the training process, 4 \\(\\times \\)  1080Ti GPUs are used; and the test is carried on with a single GPU. The Adam solver is used with parameters \\(\\beta _1\\)  = 0.9, \\(\\beta _2\\)  = 0.999, and \\(\\epsilon \\)  = \\(10^{-8}\\) . The batch size is set to 32, with the learning rate is set to 0.001. The model is trained on the generated dataset for 80 epochs. During the test process, it takes 0.27 seconds on average to fuse an image pairs of size 520 \\(\\times \\)  520.\n"]}
{"id": "1907.12949", "categories": "cs.CV", "paragraphs": ["Images were resized to 128x96 pixels in order to smooth noise and reduce both training time and memory requirements.\nJoint annotation was performed using a custom-built annotation tool, publicly available onlinehttps://github.com/roccopietrini/pyPointAnnotator.\nTo build the ground-truth masks, we selected \\(r\\)  equal to 6 pixels.\n", "For training the detection and regression network, we set an initial learning rate of 0.01 with a learning decay of 10% every 10 epochs, and a momentum of 0.98. We used a batch size of 16 and for both the networks the number of epochs was set to 100.\nWe selected the best model as the one that maximized the accuracy on the validation set (training/validation split = 0.3).\n"]}
{"id": "1910.05069", "categories": "cs.CL", "paragraphs": ["We leveraged a BFS method to search valid logical forms for questions in training data. The buffer size in BFS is set to 1000. Both embedding and hidden sizes in the model are set to \\(300D\\) , and no pretrained embeddings are loaded for initialization, and the positional encodings are randomly initialized and learnable. The head number of multi-head attention is 6 and activation function inside \\(\\operatornamewithlimits{FFN}(\\cdot )\\)  is \\(\\operatornamewithlimits{Gelu}(\\cdot )\\)  [12].\nWe used Adam [14] to optimize the loss function defined in Eq.(REF ) where \\(\\alpha \\)  is set to \\(1.5\\) , and learning rate is set to \\(10^{-4}\\) . The training batch size is 128 for 6 epochs. And we also employed learning rate warmup within the first \\(1\\%\\)  steps and linear decay within the rest. The source codes are available at https://github.com/taoshen58/MaSP.\nMore details of our implementation are described in Appendix \n"]}
{"id": "1905.02019", "categories": "cs.CL cs.AI cs.LG stat.ML", "paragraphs": ["\nWhen \\(HiddenSize = 150\\) , \\(Dropout = 0.2\\)  is a good choice. Increase will lead to high variance model and decrease will lead to over-fitting in early stage\n\nIncrease \\(HiddenSize\\)  from 150 to 250 doesn't improve the performance much.\n\n\\(EmbeddingSize = 100\\)  has a reasonable good performance. Increase embedding size from 100 to 200 doesn't affect model performance much.\n\n"]}
{"id": "1905.01995", "categories": "cs.CL cs.AI", "paragraphs": ["The model word embeddings are initialized with the 300-dimensional pre-trained vectors provided by Glove [58]. We update network weights by using the Adam [59] optimizer with learning rate 0.001. The hidden layers of Bi-LSTM and Bi-GRU have size 100. In the semantic matching model, Dropout is set to 0.1.\n", "We evaluate our method on the SimpleQuestions dataset which contains N = 21.687 questions and the corresponding triples. For each question we follow the procedure described in Section REF  to find whether adding char-level encoding or adding self-attention mechanism will help the matching of question and target fact, and which scoring method can get the answer of the question more accurately.\n"]}
{"id": "1903.00138", "categories": "cs.CL", "paragraphs": ["For the transformer model, we use token embeddings and hidden size of dimension 512, and the encoder and decoder have 6 layers and 8 attention heads. For the inner layer in the positionwise feed-forward network, we use 4096. Similar to previous models we set the dropout to 0.2. A 50,000 vocabulary for the input and output tokens are collected from the training data. In total, this model has 97M parameters.\n", "Models are optimized with Nesterov\u2019s Accelerated Gradient [21]. We set the learning rate with 0.002, the weight decay 0.5, the patience 0, the momentum 0.99 and minimum learning rate 10-4. During training, we evaluate the performance on the development set for every epoch.\n", "Almost the same architecture and hyper-parameters are used when pre-training using unlabeled data, except the \\(\\Lambda \\)  parameter for edit-weighted loss. We set \\(\\Lambda =3\\)  when we train the denoising auto-encoder, and set \\(\\Lambda \\in [1, 1.8]\\)  when we train GEC models.\n", "During decoding, we use a beam-size of 12 and normalize model scores by length. We do not use reranking when evaluating the CoNLL-2014 data sets. But we rerank the top 12 hypothesizes using the language model trained on Common Crawl [16] for the JFLEG test sets.\n"]}
{"id": "1902.03538", "categories": "cs.LG stat.ML", "paragraphs": ["For adversarial training, we apply the PGD\u00a0[31] attack to find adversarial samples.\nUnless otherwise specified, we set the perturbation magnitude \\(\\Delta \\)  to be 76 for MNIST and 4 for the other three datasets. (The color scale of each channel is between 0 and 255.)\nFollowing the settings in [31], we set PGD attack iteration numbers \\(n\\)  to be 16 for MNIST and 7 for the other three datasets.\nWe follow [29] to set PGD attack step size \\(\\alpha \\)  to be \\(\\min (\\Delta +4,1.25\\Delta )/n\\) .\nWe train ATMC for 50, 150, 150, 80 epochs on MNIST, CIFAR10, CIFAR100 and SVHN respectively.\n"]}
{"id": "1909.02322", "categories": "cs.CL", "paragraphs": ["For all experiments, our model used word embeddings with\n128\u00a0dimensions, pretrained using GloVe [32]. We\nset the dimensions of all hidden vectors to\u00a0256 and the batch size to\u00a08.\nFor decoding summaries, we use a length-normalized beam search with beam size of\u00a05.\nWe applied dropout\n[38] at a rate of\u00a00.5. The model was trained\nusing the Adam optimizer [21] with default parameters and \\(l_2\\)  constraint [16]\nof\u00a02. We performed early stopping based on model performance on the\ndevelopment set. Our model is implemented in PyTorchOur code\ncan be downloaded from xxx.yyy.zzz..\n"]}
{"id": "1902.08830", "categories": "cs.CL", "paragraphs": ["Across all simulations we trained BCF to induce \\(K=40\\)\ncategories and \\(G=50\\)  feature types which are shared across\ncategories. We ran the Gibbs sampler for 1,000 iterations, and report\nthe final most likely representation. We trained BayesCat on the same\ninput stimuli as BCF, with the following parameters: the number of\ncategories was set to\u00a0\\(K=40\\) , and the hyperparameters to\u00a0\\(\\alpha =0.7,\\beta =0.1\\) , and \\(\\gamma =0.1\\) . From the learnt representations, we\ninduced \\(G=50\\)  global feature types as described above. Again results\nare reported as averages over 10\u00a0runs of 1,000\u00a0iterations of the Gibbs\nsampler. The co-occurrence model induces \\(K=40\\) categories,\nand, subsequently, \\(G=5\\) feature types for each category.\n"]}
{"id": "1912.09551", "categories": "cs.CV cs.AI cs.CL cs.LG", "paragraphs": ["We have extracted the CONV-5 image feature of the pre-trained VGG-19 CNN model for the LSTM + Q+ I+ Attention baseline model. Since submission, we have updated our model and used the CONV-5 image feature of the pre-trained Resnet-152 CNN model for MCB to get state of the art result. We trained the differential attention model using joint loss in an end-to-end manner. We have used RMSPROP optimizer to update the model parameter and configured hyper-parameter values to be as follows: learning rate =0.0004 , batch size = 200, alpha = 0.99 and epsilon=1e-8 to train the classification network . In order to train a triplet model, we have used RMSPROP to optimize the triplet model model parameter and configure hyper-parameter values to be: learning rate =0.001 , batch size = 200, alpha = 0.9 and epsilon=1e-8. We have used learning rate decay to decrease the learning rate on every epoch by a factor given by:\n\\(Decay\\_factor=exp\\left(\\frac{log(0.1)}{a*b} \\right)\\) \n", "where value of a=1500 and b=1250 is set empirically. The selection of training controlling factor(\\(\\nu \\) ) has a major role during training. If \\(\\nu \\) =1 means updating the triplet and classification network parameter at the same rate. If \\(\\nu \\)  \\(\\gg \\)  1 means updating the triplet net more frequently as compare to the classification net. Since triplet loss decreases much lower then classification loss, we fixed the value of \\(\\nu \\)  \\(\\gg \\)  1 that is a fixed value of \\(\\nu \\) =10.\n"]}
{"id": "1906.08031", "categories": "cs.LG cs.CV math.OC stat.ML", "paragraphs": ["Theoretically Derived Learning Rates.\nThe determination of the learning rate has a significant impact on the convergence of optimization algorithms. Various scheduling schemes come up, e.g. [25], [37], as the later additionally suggests a way for obtaining an empirical upper bound on the learning rate. In section REF , multiple learning rates \\(\\eta ^*_c\\)  are suggested for minimizing the regret bound (REF ), as \\(c\\in \\lbrace N, R\\rbrace \\)  represents normal and reduction cells respectively.\nFor example, for CIFAR10 with 50%:50% train-validation split, 50 search epochs, gradient clipping of 1, 6 normal cells and 2 reduction cells both of 8 experts for each forecaster, (REF ) yields \\(\\eta ^*_N=\\) 7.5e-4 and \\(\\eta ^*_R=\\) 1.3e-3.\n"]}
{"id": "1907.05794", "categories": "cs.CV", "paragraphs": ["We train ACTNET on a subset of Google Landmarks dataset (GLD) [37], which contains 1 million images depicting 15K unique landmarks (classes). In GLD the number of images per class is highly unbalanced, some classes contain thousands of images, while for around 8K classes only 20 or fewer images are present. Furthermore, the GLD contains a non-negligible fraction of images unrelated to the landmarks. For this reason, we preprocess the GLD using the RVDW global descriptor and SIFT based RANSAC [31], to obtain a clean dataset containing 200K images belonging to 2K classes. Finally, we remove all images that overlap with the test datasets. We refer this clean dataset as Google Landmarks Retrieval dataset (GLRD).\n"]}
{"id": "1912.06598", "categories": "cs.CL", "paragraphs": ["For cache-based models, we fix the size of the two caches to 100 words each. The scoring feed-forward network has hidden dimensions 1000 and 500 and the gate feed-forward network has hidden dimensions 500 and 200, following the configurations reported in [8]. The cache word embeddings are shared with the Transformer decoder. During training we provide the real topic of the target sentence for half of the training data, and for the other half we provide the topic projected from the source, in order for the model to learn from but not be over-reliant on gold (source) topics. During training we also use the real target side sentences to load the dynamic cache. At inference time, the topic is a projection from source to target and the dynamic cache is loaded with words from previously translated sentences.\n"]}
{"id": "1903.05285", "categories": "cs.CV", "paragraphs": ["We use ShiftResNet-20 and ShiftResNet-56 with expansion rate 6 as two representatives for ablation study. We train these networks by two GPUs with mini-batch 128 and base learning rate 0.1. As the same with [36], the learning rate decays by a factor of 10 after 32k and 48k iterations, and the training stops after 64k iterations. Specifically, we stop the training of SSL after 48k iterations in order to fix the learned shift pattern. For data augmentation, only horizontal flipping and random cropping are adopted. We use L2 regularization to shift values in the following experiments since we find that the result of L2 regularization is slightly better than L1.\n", "In the experiments on ImageNet, we use SGD to train the networks with mini-batch 1024, weight decay 0.00004 and momentum 0.9. Training is started by a learning rate 0.6 with linear decaying policy and is stopped after 480 epochs, while the training of SSL is stopped after 240 epochs. The entire training iteration is comparable with [31], [21], [29], [25]. For data augmentation, we scale the short-side of images to 256 and adopt \\(224\\times 224\\)  random crop as well as horizontal flip to augment the training dataset. Also, to further rich the training images, more image of distortions are provided as used in Inception training [32], [9]. But it will be withdrawn in last several epochs. At the validation phase, we only center crop the feeding resized images to \\(224\\times 224\\)  and present the results with single-view approach.\n"]}
{"id": "1906.01861", "categories": "cs.LG stat.ML", "paragraphs": ["GraphRNN and GraphRNN-S were trained using a single GPU for 96000 iterations with batch size 32.\nFor the molecular dataset, considering its relatively large dataset size, we increased the number of iterations to 1536000.\n", "For the convenience of implementation, we started the generation process from seed subgraphs with the number of nodes \\(N_{\\min }\\) .\nFor the synthetic graph datasets, we trained our models using a single GPU for 100 epochs with batch size 32, and we used \\(r=2\\)  and \\(N_{\\min } = 10\\) .\nFor the molecular dataset, we trained our models using 2 GPUs for 20 epochs with batch size 256, and we used \\(r=7\\)  and \\(N_{\\min } = 5\\) .\nFor the ego dataset, we trained our models using 2 GPUs for 25 epochs, and we used \\(r=2\\)  and \\(N_{\\min } = 12\\) .\nWe used batch size 4 for GRAM and batch size 8 for GRAM-A, GRAM-B, and GRAM-AB.\nFor the protein dataset, we trained our models using 2 GPUs for 25 epochs, and we used \\(r=2\\)  and \\(N_{\\min } = 12\\) .\nWe used batch size 4 for GRAM and batch size 32 for GRAM-A, GRAM-B, and GRAM-AB.\nDue to the code dependencies, Tesla P100 was used for DeepGMG, GraphRNN, and GraphRNN-S, and Tesla V100 was used for GRAM and its variants.\nEmpirically, the choice between the two gave no significant difference in computation time for our cases.\n", "In evaluation, we reported an average of 3 runs for each score.\nTo calculate GK-MMD score for the molecular dataset, we used 100 samples from the generated graphs and 100 samples from the test set for fast evaluation.\nWe reported an average of 10 runs.\n"]}
{"id": "1907.06713", "categories": "cs.CV", "paragraphs": ["Our implementation is based on the Tensorpack framework\u00a0[33]. We used the re-implemented version of Mask R-CNN in Tensorpack as the baseline, which shows better mask \\(AP\\)  than the original paper. The pretrained model is publicly available from the Tensorpack model zoo. Image centric training\u00a0[13] is applied so that the images are resized to 800 pixels on the shorter edge, 1333 on the longer edge, without changing the aspect ratio. Each image has 512 sampled RoIs, and their positive to negatives ratio is 1:3. We use 8 Titan RTX GPUs in training and single image per GPU for 360000 iterations. The learning rate is 0.02, weight decay is 0.0001, and momentum is 0.9. Other configurations are the same as Mask R-CNN. The RPN is trained separately and do not share the weights with Mask R-CNN. In addition, all ablation studies are tested based on the ResNet-50-FPN backbone for faster training/testing speed.\n{FIGURE}"]}
{"id": "1909.08981", "categories": "cs.LG stat.ML", "paragraphs": ["Our baseline model contains 38,780 embeddings in a 32D space, generated by discretizing 13,233 unique variable labels, with 20 bins per continuous variable. We utilize a GRU [16] of depth 1 with a sigmoid output activation, layer normalization applied to all linear projections [17], dropout regularization [18] (\\(p=0.5\\) ) on both the aggregated embedding and the hidden states, as well as a weight decay coefficient of 0.001 [19]. We assess the effect of using summation, simple averaging and a masked softmax (over a separate token weight vector) as aggregation functions on model performance. We note that the risk of unnormalized aggregation, such as summation, is that the model learns to count the number of readings taken in an hour and correlate this with patient outcome.\n"]}
{"id": "1903.07402", "categories": "cs.CL", "paragraphs": ["The data set generated as in section 2 are of small batches (we call them batch units) which are able to fit in GPU memory, but larger batch size provides better performance for Transformer, \u201ctokens_optm\u201d is applied to support large batch sizes, the training script will forward and backward many batch units, and update parameters with an optimizer step until it has collected gradient with more than \u201ctokens_optm\u201d number of tokens on the target side.\n"]}
{"id": "1906.05721", "categories": "cs.CV eess.IV", "paragraphs": ["MobileNet and MNasNet models are trained in TensorFlow using asynchronous training on GPU using the standard RMSPropOptimizer with both decay and momentum set to 0.9. We use 16 GPU asynchronous workers, and a batch size of 96. We use an initial learning rate of 0.045, and learning rate decay rate of 0.98 per epoch. All the convolutional layers use batch normalization with average decay of 0.99. Using the floating-point checkpoints, we then train models with 8-bit representation of weights and activations by using quantization-aware training\u00a0[33], [3] with a learning rate of \\(10^{-5}\\)  and learning rate decay of 0.9 per epoch. Since we only classify two classes in Visual Wake Words dataset,\nwe shrink the last convolutional layer in MobileNet V2 and MNasNet models.\n"]}
{"id": "1906.05795", "categories": "cs.LG eess.SP stat.ML", "paragraphs": ["Different methods have been used for the model training and optimization. Firstly, all the channels described previously are concatenated into one fully-connected network, dealing with all the obtained feature maps concurrently. Secondly, all the activation layers used are PReLU, initialized with he_normal [13], [31]. Thirdly, the dropout has been parametrized according to the strategy of the annealing dropout, from a rate of 0.5 to a rate of 0.0 after 100 epochs. Concerning the losses, we used the categorical_crossentropy or binary_crossentropy for the classification model, and mean_squared_error for the auto-encoder structure. Adadelta was used for optimization with an initial learning rate of 1.0.\n"]}
{"id": "1910.13890", "categories": "cs.CL", "paragraphs": ["All models are implemented using gated recurrent units (GRU) [10], and have a single-layer bi-RNN encoder.\nThe source sides of the data used for training all NMT models, and the target sides of the data used in training the subword-level NMT models are segmented using BPE with 16,000 merge rules.\nWe implement all decoders using a comparable number of GRU parameters, including 3-layer stacked-GRU subword and character-level decoders, where the attention is computed after the 1st layer [3] and a 3-layer hierarchical decoder which implements the attention mechanism after the 2nd layer.\nAll models use an embedding dimension and GRU size of 512.\nLMM uses the same hierarchical GRU architecture, where the middle layer is augmented using 4 multi-layer perceptrons with 256 hidden units. We use a lemma vector dimension of 150, 10 inflectional features (See \u00a7REF  for experiments conducted to tune the feature dimensions) and set the regularization constant to \\(\\rho =0.4\\) . All models are trained using the Adam optimizer [17] with a batch size of 100, dropout rate of 0.2, learning rate of 0.0004 and learning rate decay of 0.8, applied when the perplexity does not decrease at a given epoch.Perplexity is the exponentiated average negative log-likelihood per segment (BPE, or character) that a model assigns to a dataset. It corresponds to the model's average surprisal per time step. Translations are generated with beam search with a beam size of 5, where the hierarchical models implement the hierarchical beam search algorithm [1].\n"]}
{"id": "1901.04547", "categories": "cs.CV", "paragraphs": ["We use ADAM\u00a0 to optimize the networks in\nAlgorithm\u00a0REF , with the learning rate of \\(10^{-4}\\) . We apply\nweight decay constraining the \\(\\ell _2\\) -norm of the weight parameters of the\nnetwork, where the hyper-parameter for the decay is empirically set to\n\\(10^{-4}\\) . To prevent the networks from overfitting, we train at max 1000\niterations per round, and at max 3 epochs for the samples in current experience\nmemory. In addition, we only keep examples from the ten most recent rounds of\noptimization.\n", "For MCTS, the number of repetitions of simulation is 10. The minimax-backup\nparameter \\(\\alpha \\)  is set to \\(0.5\\)  and \\(C_{puct}\\)  is set to \\(1.0\\) . The\nparameter for exploration extent is\n\\(\\epsilon =0.25\\) .\n"]}
{"id": "1904.08301", "categories": "cs.CL", "paragraphs": ["We initialize all parameters of the model randomly. Embedding vectors of dimension 128 are drawn from \\(U(0.05,0.05)\\)  and the LSTM weights (neurons: 128) and weights of the feed forward output layers are sampled from a Glorot uniform distribution [17]. For future work, initializing the embedding layer with pre-trained vectors could further increase the performance. In this work, however, we learn all parameters from the given data. We fit our model using Adam [25] (learning rate: 0.001) on the training data over 20 epochs with mini batches of size 16. We apply early stopping according to the maximum Pearson's \\(\\rho \\)  (with regard to Smatch F1) on the development data.\n\\( \\rho =\\frac{\\sum ^n _{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum ^n _{i=1}(x_i - \\bar{x})^2} \\sqrt{\\sum ^n _{i=1}(y_i - \\bar{y})^2}}\\) \nquantifies the linear relationship between predicted scores (\\(x_1,...,x_n\\) ) and true scores (\\(y_1,...,y_n\\) ).\n"]}
{"id": "1904.08105", "categories": "cs.CV", "paragraphs": ["Two Bi-LSTM layers with 800 cells each form the RNN. To avoid overfitting, a dropout with the rate of \\(0.3\\)  is used between and after the layers. In addition, the gradients of the LSTMs are clipped to a value of 1. Thereby gradient exploding is avoided.\n"]}
{"id": "1912.05510", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["For the discrete action environment (Tetris and VizDoom), the RL algorithm used is deep Q-learning\u00a0[24] with a target Q network. For the Humanoid domains, we use TRPO\u00a0[33]. For Tetris and the Humanoid domains, the policies are parameterized by fully connected neural networks, while VizDoom uses a convolutional network. The encoders and decoders of the VAEs used for VizDoom and Humanoid experiments are implemented as fully connected networks over the same buffer observations as above. The coefficient for the KL-divergence term in the VAE loss was 0.1 and 1.0 for the VizDoom and Humanoid experiments, respectively.\n"]}
{"id": "1912.05421", "categories": "cs.LG stat.ML", "paragraphs": ["The goal of our experiments is not to set new benchmarks for predicting number and geographic word classes in text, but instead to demonstrate that the NSLM approach can be used to improve popular neural models of language by enhancing the inductive bias. Thus, we adopt a standard language model architecture as our primary baseline, an RNN with LSTM cells and hyper-parameters corresponding to medium 650 dimensional models [25]. All models converged within 20 training epochs. During training, the softmax is computed using the full vocabulary, except for the Wikitext-103 model which uses a sampled-softmax [8] with a sampling rate of 2,500. Distributions used by the micro-models are learned on the training sets.\n", "[leftmargin=*]\nA traditional LSTM (NNLM) is used to assign probabilities to word classes over the vocabulary \\(V_W\\)  (Eq. REF ).\n\nA hierarchical NNLM (HNLM) is used to assign probabilities to both word classes and members of the class vocabulary \\(V_C\\)  (Eq. REF ). This evaluates how the benefit of working with smaller vocabularies for numbers and geographic locations may offset potentially poor accuracy of class probability estimates.\n\nCharacter RNNs (CRNN) are used to assign probabilities to word lass tokens by predicting one character of a token at a time. Separate CRNNs were trained for each class and probability was assigned to each token using the chain rule [9].\n\nNeural cache models [6] consider locality in language model results by up-weighting the probability of target words for repetitions in a historical window. Neural-cache was applied to the three other neural baselines. Step size, ensembling factor \\(\\lambda _{Cache}\\)  and temperature \\(\\theta _{Cache}\\)  were set to 500, 0.25 and 0.75, respectively, after tuning on the validation set.\n\n"]}
{"id": "1912.05457", "categories": "cs.LG eess.SP", "paragraphs": ["The batch size of the tested data is set as 64. The number of steps of historical data incorporated in the GMN model will have an influence on the prediction performance. Hence,the GMNs with 6-steps, 8-steps, and 10-steps of historical data are tested in the experiments, i.e. the \\(n\\)  in Equations REF  and REF  are set as 6, 8, and 10. In the following sections, we denoted these GMN models as GMN-6, GMN-8, and GMN-10, respectively. The corresponding SGMN models with different steps are denoted as SGMN-6, SGMN-8,and SGMN-10, respectively. The decay parameter \\(\\gamma \\)  is set as 0.9 in the experiments. For the RNN-based baseline models, including GRU, GRU-I, GRU-D, LSTM, LSTM-I, and LSTM-M, their input sequences all have 10 time steps.\n"]}
{"id": "1904.00198", "categories": "cs.CV", "paragraphs": ["As for network settings, the normal setups of ResNet on CIFAR-10 [15] are used, since CIFAR-10 has similar input as our approach. The stochastic gradient descent (SGD) is applied to train the models, with softmax loss function. 15 percents of the training examples are randomly handout for verification. The initial learning rate is set to 0.001, and reduced after 80, 120, 160, 180 epochs. 200 epochs are used in total, and the batch size is 128. Therefore, there are about 15000 iterations during one single epoch in total.\n"]}
{"id": "1909.09505", "categories": "cs.LG cs.RO eess.SP", "paragraphs": ["The output normalized from \\(-1\\)  to 1 was multiplied by a coefficient and converted into translation gains, rotation angles, and curvature gains as follows.\nTo set the range of the translation gains within 0.86 and 1.26, the coefficient for the output of the translation gains was set to 0.2, and then 1.06 was added.\nFor the rotation angle, the output multiplied by 180 and modulo 360 was used.\nTo set the range of the curvature gains within \\(-0.1333\\)  and 0.1333, the coefficient for the curvature gains output was set to 0.1333.\nHere, the positive and negative values indicate that the curvature is applied in the clockwise and counterclockwise directions, respectively.\n", "In RL, the learning time is generally inversely proportional to the CPU performance.\nBy using a PC equipped with an Intel Core i7-8750H CPU, the learning time of these models was approximately 2 h.\nAdditionally, the online execution time of the network was approximately 0.301 ms per frame, which is only 1.81% of the duration of a frame in a 60 frames-per-secound environment.\nThis execution time is less enough to run the trained agent in practical VR contents with very low overhead.\n"]}
{"id": "1901.07124", "categories": "cs.CV", "paragraphs": ["To train the network we randomly split the training set of GTSRB into two sets, 35309 images for training and 3900 for validation.\nFor testing we use the provided test set that holds 12630 images.\nWe crop and resize all images to 50\\(\\times \\) 50.\n", "For the STN/ICSTN module, denoting convolution layers with \\(c\\)  channels as \\(C(c)\\) , ReLU activations as \\(R\\) , and max-pooling layers as \\(P\\) , our network is: \\(C(4)RC(8)RPC(16)RPC(32)RPC(1024)\\) .\nAll convolutional layers use 7\\(\\times \\) 7 kernels.\nWe apply max-pooling over each channel of the last feature map to produce one feature vector of size 1024.\nWe then apply a fully connected layer with 48 neurons and ReLU activations, followed by another fully connected layer that maps to transformation parameters \u2013 translation, scale, and rotation in our experiments.\n", "For the classification network, we use a simple network with a single hidden layer with 128 neurons and ReLU activations.\nWe choose a simple architecture on purpose in order to prevent the network relying\non the increased capacity of a large classification network and learn spatial invariance and effectively ignoring the STN.\n{FIGURE}", "We use ADAM\u00a0[19] as the optimizer, and choose 10\\(^{-5}\\)  and 10\\(^{-3}\\)  as the learning rates for the STN/ICSTN modules and the classifier respectively.\nWe train the models from scratch with a batch size of 64, and set the maximum number of iterations to 300k. We use early stopping if the model shows no improvement on the validation split within the last 80k iterations.\n"]}
{"id": "1908.11834", "categories": "cs.CV cs.LG eess.IV", "paragraphs": ["Optimization We use ADADELTA\u00a0[36] with default hyper-parameters (rho=9e-1, eps=1e-6, weight decay=0) to minimize the aforementioned loss function.\nGradients are estimated using mini-batches with 512 images that are randomly sampled from training set.\nWe train 6 epochs in total.\nWe initialize the learning rate to \\(1.0\\)  for the first 4 epochs, \\(0.1\\)  for the fifth epoch and \\(0.01\\)  for the sixth epoch. All experiments are performed on a Ubuntu machine with 4 NVIDIA TITAN Xp graphics cards, each with 12GB memory.\n"]}
{"id": "1910.00054", "categories": "cs.LG cs.CL cs.IR stat.ML", "paragraphs": ["For a fair comparison, all the MIL-* models have the same parameter configuration as MILNET (Section 5.3 in \u00a0[1]).\nFor all models using word embeddings (i.e., Seg-*, Rev-*, MIL-*), we initialize the word embeddings using 300-dimensional (\\(k=300\\) ) pre-trained word2vec embeddings\u00a0[21].\nFor the CNNs we use kernels of size 3, 4, and 5 words, 100 feature maps per kernel, stride of size 1, and max-over-time pooling to get fixed-size segment encodings (resulting in \\(\\ell =300\\) ).\nFor the forward and backward GRUs we use hidden vectors with 50 dimensions (\\(n=2 \\cdot 50 = 100\\) ), while for the attention mechanism we use vectors of 100 dimensions (\\(m=100\\) ).\nWe use dropout (with rate 0.5) on the word embeddings and the internal GRU states.\nWe use L2 regularization for the softmax classifier.\n", "For the *-BoW classifiers, the review text is encoded as a bag-of-words vector including n-grams (for n=1, 2, and 3) and each term is weighted using the Term Frequency-Inverse Document Frequency (TF-IDF) statistic\u00a0[20].\nFor the Rev-* and MIL-* classifiers, we use the same model parameter configuration as in Section\u00a0REF .\nWe initialize the word embeddings using 300-dimensional pre-trained word2vec embeddings.\n"]}
{"id": "1907.02413", "categories": "cs.CV", "paragraphs": ["To evaluate on OCT datasets, all models were first trained on Cirrus\nfor 4500 iterations. Then on Spectralis, the trained models were first\nfine-tuned on the training set for 200 iterations, then evaluated\non the test sets. When training on the Cirrus and Spectralis datasets,\nto increase data diversity, in each iteration \\(12\\sim 18\\)  slices were\nrandomly chosen to form a batch from the 30 central slices of the\ninput image.\n", "On the CRC-MSI dataset, there is significant domain gap between the\ntraining and test images. Hence 2% of the original test images were\nmoved to the training set (these 2domain gap. In particular, all models were trained on the training set for one epoch (LR=0.01),\nand then fine-tuned on the tuning set for two epochs (LR=0.01, 0.004).\n"]}
{"id": "1907.00710", "categories": "cs.CL cs.AI cs.IR cs.LG", "paragraphs": ["We use the Python-based natural language toolkit NLTK to perform tokenization. Entities in dialog sessions are recognized via heuristic rules plus database entries. All counts, time and reference numbers are replaced with the \\(\\left<\\text{value\\_count}\\right>\\) , \\(\\left<\\text{value\\_time}\\right>\\)  and \\(\\left<\\text{domain\\_reference}\\right>\\)  tokens respectively. To reduce data sparsity further, all tokens are transformed to lowercase letters. The stop words are chosen using tf\u2013idf [47]. The number of topics \\(K\\)  is set to 20. All tokens that appear less than 5 times in the corpus are replaced with the \\(\\left<\\text{UNK}\\right>\\)  token. We follow the {S,U,S'} utterance \u201ctriples\u201d structure as [2] in our experiments, which means we aim to generate the system utterance S' by observing the former 1 turn of system utterance S and user utterance U.\n"]}
{"id": "1901.03447", "categories": "cs.CV", "paragraphs": ["Our method can be viewed as a way of training a network containing both encoders and a generator, such that the generator is effectively a portion of a GAN. The network accepts a source texture \\(S\\)  as input. A global encoder \\(E^g(S)\\)  encodes \\(S\\)  into a latent vector \\(z^g\\) , which can also be viewed as a latent tensor with spatial size \\(1\\times 1\\) . A local encoder \\(E^l(S)\\)  encodes the source texture into a latent tensor \\(z^l\\) , which has a spatial size that is a factor \\(m\\)  smaller than the size of the input texture: we use \\(m = 4\\) . The generator \\(G(z^l,z^g)\\)  ningcolorconcatenates \\(z^l\\)  and \\(z^g\\) , and can decode these latent tensors back into a texture patch, so that ideally \\(G(E^l(S),E^g(S))=S\\) , which encompasses the reconstruction task. Our generator is fully convolutional, so that it can generate output textures of arbitrary size: the output texture size is directly proportional to the size of the local tensor \\(z^l\\) . A discriminator \\(D^{\\text{rec}}\\)  is part of the reconstruction loss. An identical but separately trained discriminator \\(D^{\\text{itp}}\\)  evaluates the realism of interpolation.\n", "We show the full training setup in Figure\u00a0REF . We will also explain ningcolorour setup in terms of formulas here. As is shown in the upper-left of Figure\u00a0REF , the network is given two real source texture images \\(S_1\\)  and \\(S_2\\)  from the real texture dataset \\(\\mathcal {S}\\) . Each local encoder \\(E^l\\)  encodes \\(S_i\\)  (\\(i \\in \\lbrace 1, 2\\rbrace \\) ) to a local latent tensor \\(z^l_i = E^l(S_i)\\) . Meanwhile, each global encoder \\(E^g\\)  encodes \\(S_i\\)  to a global latent vector \\(z^g_i\\) , denoted as \\(z^g_i = E^g(S_i)\\) . These latent variables are shown in green and blue boxes in the upper-left of Figure\u00a0REF .\n", "We implement the random shuffling by row and column swapping over several scales from coarse to fine. For this coarse to fine process, we use scales that are powers of two: \\(s_i = 2^i\\)  for \\(i=0, 2, \\ldots , n\\) . We set the coarsest scale \\(n\\)  to give a scale \\(s_n\\)  that is half the size of the local tensor \\(z^l_i\\) . For each scale \\(s_i\\) , we define a grid over the tiled latent tensor \\(T(z^l)\\) , where each grid cell has size \\(s_i \\times s_i\\) . For each scale \\(s_i\\) , we then apply a random shuffling on cells of the grid for that scale: we denote this by \\(P_{i}\\) . This shuffling proceeds through grid rows first in top-down and then bottom-up order: each row is randomly swapped with the succeeding row with probability 0.5. Similarly, this is repeated on grid columns, with column swapping from left to right and right to left. Thus, the entire shuffling operation is:\n\\(P\\big (T(z^l_i)\\big ) = P_{0}\\circ P_{1}\\circ \\cdots \\circ P_{n}\\big (T(z^l_i)\\big )\\) \n"]}
{"id": "1907.11565", "categories": "cs.CV", "paragraphs": ["For ST Multinomial and ST Gumbel-Softmax we tested learning-rate in ({1e-4, 5e-4, 1e-3, 5e-3}) and decay in ({0.7, 0.75, 0.8}). For reinforce, we tested learning-rate in ({5e-4, 5e-3, 1e-2, 5e-2}) and decay in ({0.7, 0.8, 0.9}). For PSST Gumbel softmax and for PSST Multinomial we used the same hyperparameters that were found to be best for ST Multinomial and ST Gumbel-Softmax, and we did not tuned them further (learning rate of \\(5e-3\\)  and decay \\(0.75\\) ).\n", "To select the best learning rate within the above sets, we trained a full curve of recall-vs-cider for each value and selected the rate that maximized the recall@10 for CIDEr=1.125. We repeated the same procedure for the decay ({0.7, 0.75, 0.8}), together yielding the best rate = \\(5e-3\\) , and the best decay = \\(0.75\\) . For Straight-through Gumbel Softmax we tested temperatures of \\(\\tau \\in \\lbrace 0.5, 1, 3, 5, 7 ,10\\rbrace \\) , and found \\(\\tau = 1\\)  was best.\n", "For PSST Gumbel softmax and PSST Multinomial we tested \\(\\rho \\)  in {0, 0.25, 0.5, 0.75, 1}, and show performance as a function of \\(\\rho \\)  in Fig.\u00a0REF . When training with reinforce, we tested several baselines for reducing variance of the estimator (greedy, ground truth, no baseline) and report results obtained with the ground truth baseline which worked best.\nAt test time, we used a beam search of size 2 for generating captions, as in [27].\n"]}
{"id": "1905.13358", "categories": "cs.CL cs.CV", "paragraphs": ["The training setup of the navigation agent is identical to Fried:2018:Speaker. The agent learns to map the natural language instruction \\(\\mathcal {X}\\)  and the initial visual scene \\(v_1\\)  to a sequence of actions \\(a_{1..T}\\) . Language instructions \\(\\mathcal {X}=x_{1..n}\\)  are initialized with pre-trained GloVe word embeddings Pennington:2014:GloVe and encoded using a bidirectional RNN Schuster1997BidirectionalRN. At each time step \\(t\\) , the agent perceives a 360-degree panoramic view of its surroundings from the current location. The view is discretized into \\(m\\)  view angles (\\(m=36\\)  in our implementation, 3 elevations x 12 headings at 30-degree intervals). The image at view angle \\(i\\) , heading angle \\(\\phi \\)  and elevation angle \\(\\theta \\)  is represented by a concatenation of the pre-trained CNN image features with the 4-dimensional orientation feature [sin \\(\\phi \\) ; cos \\(\\phi \\) ; sin \\(\\theta \\) ; cos \\(\\theta \\) ] to form \\(v_{t,i}\\) . As in Fried:2018:Speaker, the agent is trained using student forcing where actions are sampled from the model during training, and supervised using a shortest-path action to reach the goal state.\n"]}
{"id": "1905.13164", "categories": "cs.CL cs.AI", "paragraphs": ["In all abstractive models, we apply dropout (with probability\nof\u00a0\\(0.1\\) ) before all linear layers; label\nsmoothing\u00a0[34] with smoothing\nfactor\u00a0\\(0.1\\)  is also used. Training is in traditional\nsequence-to-sequence manner with maximum likelihood estimation.\nThe optimizer was Adam\u00a0[17] with learning rate of\n\\(2, \\beta _1 = 0.9\\) , and \\(\\beta _2 = 0.998\\) ; we also applied\nlearning rate warmup over the first \\(8,000\\)  steps, and decay as in\u00a0[35]. All transformer-based models had 256\u00a0hidden units; the\nfeed-forward hidden size was\u00a0\\(1,024\\)  for all layers. All models\nwere trained on 4 GPUs (NVIDIA TITAN Xp) for \\(500,000\\)  steps. We\nused gradient accumulation to keep training time for all models\napproximately consistent. We selected the 5 best checkpoints\nbased on performance on the validation set and report averaged\nresults on the test set.\nDuring decoding we use beam search with beam size\u00a05 and length\npenalty with\u00a0\\(\\alpha = 0.4\\)  [38]; we decode until\nan end-of-sequence token is reached.\n"]}
{"id": "1905.13205", "categories": "cs.LG quant-ph", "paragraphs": ["We train all of our networks using the Adam method for stochastic optimization\u00a0[74], with \\(\\beta _1=0.5\\)  for all trained variables, \\(\\beta _2=0.9\\)  for our Boltzmann machines, and \\(\\beta _2=0.999\\)  for our generator and discriminator. As in previous works involving QBMs\u00a0[27], [26], we take all \\(_a=2\\) . During training on synthetic data, we set the learning rate for both of our Boltzmann machines to \\(10^{-3}\\)  and use \\(k=5\\)  Gibbs steps. Each Boltzmann machine has 8 visible units and 2 hidden units, which are sufficient for approximating the studied Bernoulli distribution.\n", "When training on the MNIST and CIFAR-10 data sets, we consider Boltzmann machines with 32 visible units and 8 hidden units. For consistency, we also set the dimension of the noise distribution for DCGAN to be 32. We use a learning rate of \\(2\\times 10^{-4}\\)  for both our generator and discriminator, and a learning rate of \\(10^{-3}\\)  for our Boltzmann machines with \\(k=5\\)  Gibbs steps. Furthermore, we initialize our weights using Xavier initialization\u00a0[75] and initialize our biases to zero. In order to help the discriminator learn in the early stages of trainig, we use soft and noisy labels where a random number between 0 and \\(0.1\\)  is used instead of 0 labels (fake images) and a random number between \\(0.9\\)  and 1 is used instead of 1 labels (real images). Each model is trained for 30 epochs, where an epoch represents one full pass through the training data.\n"]}
{"id": "1906.07093", "categories": "cs.CL cs.LG cs.SD eess.AS", "paragraphs": ["For multilingual adversarial training, we use 2 bidirectional LSTM layers each with 200 memory units in each direction as the shared layers, 2 bidirectional LSTM layers of 200 cells each direction as AM layers for each language, and 1 bidirectional LSTM layer with 128 cells each direction for LID. The AM layers of a particular language will be updated only when the corresponding training data is presented. In training of the adversarial layers, we removed silence frames which does not contain any language information. An adversarial weight (\\(\\lambda \\) ) of 1 is multiplied with the reversed gradients. We have also tried other weights and that does not significantly change the performance. Our training is done in parallel on CPUs using asynchronous stochastic gradient descent (ASGD) training described in [2], [4], [3], [6]. We choose the best model to be the one with highest frame accuracy on the development set. The frame accuracy is calculated by uniformly sampling utterances from different languages. For decoding, a standard 5-gram language model is trained.\n"]}
{"id": "1906.06874", "categories": "cs.CV eess.IV", "paragraphs": ["Different from DBPN \u00a0[9] which has different structures and configurations for different up-sampling enlargement, the proposed HBPN network uses the same structure as shown in Figure \u00a0REF . In UBP and DBP blocks, we use \\(6\\times 6\\)  convolution filters with two striding and two padding for down- and up-sampling. For shortcut connections, we use \\(3\\times 3\\)  convolution filters with one striding and 1 padding. We initialize the weights based on \u00a0[11]. The testing data include Set5 \u00a0[2], Set14 \u00a0[33], BSD100 \u00a0[1], Urban100 \u00a0[12] and Manga109 \u00a0[22] on \\(2\\times \\) , \\(4\\times \\)  and \\(8\\times \\)  SR enlargement.\n"]}
{"id": "1905.09418", "categories": "cs.CL", "paragraphs": ["We follow the setup of Transformer base model\u00a0[31]. More precisely, the number of layers in the encoder and in the decoder is \\(N=6\\) . We employ \\(h = 8\\)  parallel attention layers, or heads. The dimensionality of input and output is \\(d_{model} = 512\\) , and the inner-layer of a feed-forward networks has dimensionality \\(d_{ff}=2048\\) .\n"]}
{"id": "1911.06172", "categories": "cs.CL", "paragraphs": ["The Bi-GRU and feed-forward models were trained with a batch size of 80 and a learning rate of 0.1. The models were optimised using momentum-based SGD (\\(\\beta = 0.9\\) ). The Bi-GRU models were trained with 2 GRU layers of 512 neurons each, with 50% dropout. The character-level Bi-GRU models had a character embedding dimension of 100.\n"]}
{"id": "1902.00842", "categories": "cs.CV cs.RO", "paragraphs": ["We train our neural network using Stochastic Gradient Descent using the Adam optimizer with binary crossentropy as a loss function. We use a variation of the \"poly\" learning rate decay policy proposed in [11]:\n\\(LR(\\text{epoch}) = 0.0006 \\cdot \\left(\\frac{1000 - \\text{epoch}}{1000}\\right)^{0.9}\\) \n"]}
{"id": "1910.01738", "categories": "cs.LG cs.RO stat.ML", "paragraphs": ["\nA set of instances of tasks \\(T^i\\) , \\(i\\in \\lbrace 1,\\dots ,K\\rbrace \\) , and for each of them a set of demonstrations \\(\\mathcal {D}^i = \\lbrace (I_{t-1}^i, I_t^i, \\mathbf {a}^i_t)\\rbrace _{t}\\) .\n\nA randomly initialized neural network following the architecture described in Figure\u00a0REF  with weights \\(\\theta \\) .\n\n", "\\(epoch=1\\)  to \\(M\\) \n\\(step=1\\)  to \\(N\\) \nPick randomly \\(i \\in \\lbrace 1,\\dots ,K\\rbrace \\) .\nPick a random subset of samples to form a batch \\(\\mathcal {B}^i\\subset \\mathcal {D}^i\\) .\nAdjust \\(\\theta \\)  with a gradient descent step on the following expression to minimize:\n\\(\\mathbb {E}_{\\mathcal {B}^i}\\left[\\Vert  \\psi ^i(\\varphi _t^i, \\Delta \\varphi ^i_t) - \\mathbf {a}^i_t \\Vert ^2_2\\right].\\) \n", "An instance of this task is parameterized by the position of a goal\nthat the end-effector of the robot must reach within some margin of error\n(and in limited time).\nWe use as raw inputs grayscale images of \\(64 \\times 64\\)  pixels.\nTo increase the difficulty of the learning, in some cases we\nadd a randomly moving distractor and Gaussian noise, as shown on Figure\u00a0REF .\n", "Generating demonstrations:\nFor simplicity, we allow the target policies to have access to the ground truth state of the robot \\((\\mathbf {q}, \\mathbf {\\dot{q}})\\) , where \\(\\mathbf {q}\\)  is the configuration of the robot arm, represented as a vector of size 4: \\((\\cos \\alpha _1, \\sin \\alpha _1, \\cos \\alpha _2, \\sin \\alpha _2)\\) , \\(\\alpha _1\\)  and \\(\\alpha _2\\)  being respectively\nthe first and second joint angles in radians.\nTo construct the target policies, we run the Hindsight Experience Replay (HER) RL algorithm [35] that also exploits the\n\\((x,y)\\)  parameters of the goal position.\nIt returns a parameterized policy capable of producing reaching motions to any reachable goal position.\nNote that the purpose of our method is to generate state representation\nfrom (possibly noisy) inputs that are hard to exploit (such as raw images), so access to the robot state is only given to the target policies that generate demonstrations.\nThe demonstration data used to train SRLfD consists of 16 different instances of the reaching task, with for each of them 262 trajectories computed from various initial positions using the target policy\nobtained with HER.\n"]}
{"id": "1910.06044", "categories": "cs.LG cs.CR cs.MA stat.ML", "paragraphs": ["For the impact of different batch sizes shown in Figure\u00a0REF , we can observe that success rate hits the bottom (\\(82\\%\\) ) when the batch size is set to relatively small, and reaches a relatively high level (\\(94\\%\\) ) with batch size at 20. As we know, the batch size in training should not be set too small as it may lead to more calculation iterations and cause the model perform bad. Hence, we think Class Sniffing should be effective under common batch size settings.\n"]}
{"id": "1910.06262", "categories": "cs.CL cs.CY", "paragraphs": ["The language modelling LSTM network consists of 2-layers with 1024 hidden units and an equally sized character embedding space. The parameters were trained using Adam with a learning rate of \\(2\\cdot 10^{-3}\\) , a decay of \\(0.95\\) , gradient norm clipping of 5, and dropout probability \\(0.2\\)  for the inputs and the hidden layers.\n"]}
{"id": "1910.06251", "categories": "cs.CV cs.LG", "paragraphs": ["ReLU was used for activation throughout the experiments. For tasks with output at every timestep such as the language modeling problem in Subsection REF , the recurrent weights of all layers were initialized uniformly in the range \\([0, \\@root (T-t) \\of {\\gamma }]\\) , while for tasks such as classification where only the output at the last time step in the last layer is used, the recurrent weights of the last layer were initialized uniformly in the range \\([\\@root (T-t) \\of {\\epsilon }, \\@root (T-t) \\of {\\gamma }]\\)  as described in Subsection REF . The recurrent weights are constrained to be in the range of \\(|u_n|\\le \\@root (T-t) \\of {\\gamma }\\)  for IndRNN with ReLU as analysed in Subsection REF . This is especially important in processing long sequences where a small change of recurrent weights may significantly change the gradients. In our experiments, \\(\\gamma \\)  is simply set to \\(1.0\\)  for long sequences since \\(\\@root (T-t) \\of {\\gamma }\\)  is already close to 1 when \\(T\\)  is very large. For short sequences such as 20 steps, the constraint can also be removed as the gradient exploding problem is not very severe in such cases. We have also conducted an ablation study on \\(\\gamma \\) , which also verifies that the performance is not very sensitive to the setting of \\(\\gamma \\)  as long as it is in a reasonable range (values in \\([1,10]\\)  for example).\n", "To accelerate training, batch normalization was used except in the simple adding problem. Moreover, for classification tasks where the whole sequence is processed for output at the final time step, the statistics used in the batch normalization layer were obtained based on the samples at all time steps, while for other tasks such as language modeling which cannot access information from future time steps, the statistics are obtained based on all the samples in each time step. When dropout is applied, the dropout mask is shared over time to avoid the clipping of long memory. Weight decay of \\(10^{-4}\\)  is used for the weight parameters (without applying to the recurrent weight and bias). All the networks were trained using the Adam optimization method [50] with initial learning rate \\(2\\times 10^{-4}\\) . The learning rate is reduced by a factor of 5 when the accuracy (or loss) on the validation data no longer improves (drops) (with patience set to 100).\n", "For the densely connected IndRNN, the network shape was simply following the conventional denseCNN [36]. Each dense layer (the non-linear transformation function) consists of two composite functions as shown in Fig. REF  and produces k feature maps, termed as the growth rate. The first composite function is called the bottleneck layer and the number of neurons is set to four times (4*k) the growth rate. For each transition layer, the number of the neurons is set to be the half (50%) of the input feature maps. The dense block configuration is set to (8,6,4), where in the first, second and third dense block, 8, 6 and 4 dense layers are used, respectively. This keeps a relatively similar number of neurons in each dense block. Note that it is different from the denseCNN [36] because the tasks in the following experiments do not concern pooling (which reduces the size of the features). For the whole network, one IndRNN layer with six times (6*k) the growth rate are used to process the input first before going through the following dense layers. In the following, the residual IndRNN and the densely connected IndRNN are noted as res-IndRNN and dense-IndRNN, respectively.\n{FIGURE}"]}
{"id": "1909.05357", "categories": "cs.CL cs.LG stat.ML", "paragraphs": ["We follow the same architecture as proposed in [12] for the LSTM-Autoencoder. In LSTM, we set the input embedding dimension as 100 and hidden as 200. We use RMSprop as the optimizer with a learning rate of 0.001. We train the LSTM with a batch size of 32 and 100 epochs. For Autoencoder, we set the hidden size as 20. We use Adam as the optimizer with a learning rate of 0.001. We train the model with a batch size of 32 for 10 epochs.\n", "For vanilla CNN, we use the most common CNN architecture used in NLP tasks, where the convolutional layer on top of word embedding has 128 filters followed by a ReLU and max pooling layer before the final softmax. We use Adam as the optimizer with a learning rate of 0.001. We train the model with a batch size 64 for 100 epochs.\n", "Our proposed model O-Proto uses the similar CNN architecture, the optimizer and the learning rate in the previous Vanilla CNN. The input word embeddings are pre-trained by 1-billion-token Wikipedia corpus. We set the batch size as 10. In Eq. REF , REF , REF  and REF , \\(\\alpha \\) , \\(\\beta \\) , \\(\\gamma \\) , \\(\\mathcal {M}_1\\)  and \\(\\mathcal {M}_2\\)  are hyper-parameters, which we fix \\(\\beta \\) , \\(\\gamma \\)  as 1.0 by default, and set \\(\\alpha \\) , \\(\\mathcal {M}_1\\)  and \\(\\mathcal {M}_2\\)  as 10.0, 0.4 and 0.8 according to the meta-valid performance of Amazon dataset. The sentence encoder, CNN, has 200 filters, followed by a tanh and mean pooling layer before the final regression layer. The maximum length of tokens per example is 40, and any words out of this range will be discarded.\nDuring training, we set the size of sampled negative labels Step 3 (section 3.1) to at most four, so there will be maximum five labels involved in a training step (1 positive, 4 negative). The supporting set size for each label are 20.\n"]}
{"id": "1910.01335", "categories": "cs.CL", "paragraphs": ["In our setup, for both the encoder and decoder, the dimension of word embeddings and hidden states was set to 200. We adopted pre-trained word embedding Glove [18], and out-of-vocabulary words and segment labels were initiated with random vectors. Embedding weight sharing strategy was applied by sharing the same embedding matrix \\(W_{emb}\\)  for both encoder and decoder. This sharing significantly reduced parameter size and boosted the performance by reusing the semantic and syntactic information in one embedding space [21]. The learning rate was fixed to \\(0.0001\\)  and the batch size was set to 32. We adopted gradient clipping with a maximum gradient norm of \\(2.0\\) . Adam algorithm [32] was used for stochastic optimization, with \\(\\beta _1=0.9,\\beta _2=0.99\\) . The vocabulary size was \\(10K\\) . We limited source contents to 300 tokens and the decoding length to 100 tokens. We adopted early-stop strategy with validation in each training epoch. During testing, we set the beam search size to 5.\n{FIGURE}{FIGURE}"]}
{"id": "1910.01221", "categories": "cs.CV cs.MM eess.IV", "paragraphs": ["[!b]\nAdversarial training of ROMark Combined\n[l]\nBatch size: \\(b\\) , Learning Rate: \\(\\gamma _{\\beta }\\) ,\\(\\gamma _{\\theta }\\) ,\\(\\gamma _{\\phi }\\) , Attack functions: \\(N_1,...,N_K\\) \nRandomly initialize the networks: \\(D_{\\phi }\\) , \\(E_{\\theta }\\)  and \\(C_{\\beta }\\) .\nRandomly sample message batch \\(M\\)  of batch size \\(b\\) .\nSelect K integers: \\(k_{1},...,k_{i},...,k_{K}\\) , where \\(K\\)  is the number of types of attacks and \\(\\sum _{i=1}^{K}k_{i}=b\\) \nRead minibatch \\(B=\\lbrace x_1,...,x_b\\rbrace \\)  from training set.\nGenerate the watermarked minibatch \\(B_{wm}=\\lbrace E_{\\theta }(x_i,m_i): x_i \\in B, m_i \\in M\\rbrace \\)   Separate the minibatch \\(B_{wm}\\)  into \\(K\\)  subsets \\(\\lbrace B^{1}_{wm},...,B^{K}_{wm}\\rbrace \\)  where each contains \\(k_{i}\\)  images\nLoad severity ranges of attacks: \\(S_1,...,S_K\\) \ni = 1, 2,..., K\nSearch the worst-case \\(s^{*}_{i}\\)  by: \\(s^{*}_{i} = \\operatornamewithlimits{arg\\,max}_{s \\in S_i}\\sum _{ x^{wm} \\in B^{i}} L(m, D_{\\phi }(N_i(x^{wm}, s))) \\) \nCalculate the worst-case attacked image batch \\(B^{i}_{att} = \\lbrace N_{i}(x^{k}_{wm}, s^{*}_{i}): x^{k}_{wm} \\in B^{i}_{wm} \\rbrace \\) \nGenerate attacked minibatch \\(B_{att}=\\lbrace B^{1}_{att} ,...,B^{K}_{att} \\rbrace  \\) \nFeed \\(B_{att}\\)  into decoder, and then do one step training step:\n\u00a0\u00a0Updating discriminator C:\n"]}
{"id": "1906.04726", "categories": "cs.CL", "paragraphs": ["We fit each regression model's parameters by L-BFGS. We then evaluate the model's fitness by measuring its held-out data likelihood\u2014that is, the probability it assigns to the \\(y_{ij}\\)  values for held-out intents \\(i\\) . Here we use the previously fitted \\(d_j\\)  and \\(\\sigma \\)  parameters, but we must newly fit \\(n_i\\)  values for the new \\(i\\)  using MAP estimates or posterior means.\nA full comparison of our models under various conditions can be found in app:goodness-of-fit-plot. The primary findings are as follows.\nOn Europarl data (which has fewer languages), Model 2 performs best. On the Bible corpora, all models are relatively close to one another, though the robust Model 2L gets more consistent results than Model 2 across data subsets. We use MAP estimates under Model 2 for all remaining experiments for speed and simplicity.Further enhancements are possible: we discuss our \u201cModel 3\u201d in app:model-3, but it did not seem to fit better.\n"]}
{"id": "1911.11899", "categories": "cs.CL", "paragraphs": ["For a fair and rational comparison with baselines and competitive approaches, we set most of the hyper-parameters by following prior works [9], [3], and also use 50D word embedding and 5D position embedding released by [8], [3] for initialization, where the dimension of \\(d_h\\)  equals to 150. The filters number of CNN \\(d_c\\)  equals to 230 and the kernel size \\(m\\)  in CNN equals to 3. In output layer, we employ dropout [18] for regularization, where the drop probability is set to \\(0.5\\) . To minimize the loss function defined in Eq.REF , we use stochastic gradient descent with initial learning rate of \\(0.1\\) , and decay the learning rate to one tenth every 100K steps.\n"]}
{"id": "1911.11952", "categories": "cs.LG cs.CL stat.ML", "paragraphs": ["AllenNLP [9] and PyTorch [17] are used as the development and experimentation environments. ADAM optimizer [14] with learning rate of \\(10^{-4}\\)  is used for training. Transformer encoder consists of 1 layer and 8 attention heads. Projection, feedforward and, hidden dimensions of the encoder are 256, 128 and, 128, respectively. Target vocabulary size is pruned to include only the top 5000 frequent tokens, tokenized by WordPiece. Number of decoding steps is limited to maximum input sequence length of 13 tokens. Target embedding dimension is set to 768. During evaluation decoding, Beam search of size 16 is used. Each of the models have approximately 7 million parameters. We chose this set of parameters such that the baseline model would perform well on the dataset.\nModels are trained for 20 epochs, and the best model is chosen based on Max-BLEU score on the development set.\n"]}
{"id": "1912.05687", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["To tune the competitive models, we did a grid search on the following hyper parameters for\neach model using hold-out for partitioning the data. We randomly partition both NCI60 and GDSC data to 80 % training, 10 % validation and 10 % test. The same training, validation and test datasets were used for all models. The hyper parameters were tuned using the training and validation sets. The test sets were hold out for evaluating the final performance of each tuned model.\n", "For unbiased evaluation of machine learning models, nested cross-validation [50] (CV) is often considered where an inner CV is used for model selection (hyper parameter selection) and an outer CV is used for evaluating the model tuned by the inner CV. However, nested CV is often extremely computationally intensive and thus we considered a training-validation-test (hold-out) approach where the hyper-parameters are tested on the validation set and the selected model is evaluated based on the separate test set. We used training-validation-test approach as the sample size is relatively large and thus, both training-validation-test and nested CV approaches are expected to provide similar results for comparing different modeling approaches. To illustrate the similar behavior, we compared the results of nested CV and training-validation-test (hold-out) approach using three cell lines randomly selected from the NCI60 dataset. As the results provided in figure REF  indicates, the difference in the results are minimal. On the other hand, to compare the time complexity of the two approaches, we selected one cell line with a fixed CNN architecture. Within the CNN architecture, a grid of hyper parameters was defined. In a single run (architecture) that takes 48 hours on Texas Tech University high performance computer center, 50 different models can be tried using training-validation-test, whereas only 4 models can be tried using nested cross-validation. Given the time complexity and similar performance of the two approaches, we used training-validation-test approach for hyper parameter selection and model evaluation as we could search considerably larger space of hyper parameters.\n"]}
{"id": "1909.11835", "categories": "cs.LG stat.ML", "paragraphs": ["Training setup.\nThe surrogate has been trained with an Adam optimizer [17] regarding to this adaptive loss with the following parameters for the optimizer: learning rate of \\(10^{-5}\\) , \\(\\beta _1 = 0.99\\) , \\(\\beta _2 = 0.99\\) , \\(\\epsilon =10^{-8}\\)  and for the adaptive loss: \\(\\lambda _k = 0.01\\) , \\(\\gamma _k\\)  = \\(0.5\\)  and \\(k_0 = 0.001\\) .\nThe generator has also been trained with an Adam optimizer configured similarly to the surrogate's with the following parameters for the optimizer: learning rate of \\(10^{-5}\\) , \\(\\beta _1 = 0.99\\) , \\(\\beta _2 = 0.99\\)  and \\(\\epsilon =10^{-8}\\) .\nThese parameters are fairly standard and yield good results for the problems the GAMIN was confronted to.\nRemark that an extensive hyperparameter search is a non-trivial process in the black-box setting.\n"]}
{"id": "1903.10930", "categories": "cs.CV", "paragraphs": ["For all our experiments, we train the TPP-PHOCNet for \\(100\\,000\\)  iterations with an initial learning rate of \\(10^{-4}\\) , which is divided by 10 after \\(70\\,000\\)  iterations.\nWe use Adam optimization with a mini-batch size of 10, hyperparameters \\(\\beta _1=0.9\\)  and \\(\\beta _2=0.999\\)  and a weight decay of \\(5\\cdot 10^{-5}\\) .\nAnalogue to subsec:sigtdo, we apply two dropout layers during training.\nFurthermore, a simple data augmentation strategy is used as we apply a random affine transformation to all input images at training time.\n", "The metaclassifiers are trained for \\(25\\,000\\)  iterations with an initial learning rate of \\(10^{-2}\\) , which is divided by 10 after \\(10\\,000\\) , \\(15\\,000\\)  and \\(20\\,000\\)  iterations.\nFor optimization, we use the same Adam optimizer as for the TPP-PHOCNet with a weight decay of \\(5\\cdot 10^{-4}\\) .\n"]}
{"id": "1907.10471", "categories": "cs.CV", "paragraphs": ["Our model is trained stage-by-stage to save GPU memory. The first stage consists of 3D semantic segmentation and proposal generation, while the second is for box prediction. For the first stage, we use ADAM [12] optimizer with an initial learning rate 0.001 for the first 80 epochs and then decay it to 0.0001 for the last 20 epochs. Each batch consists of 16 point clouds evenly distributed on 4 GPU cards.\nFor the second stage, we train 50 epochs with batch size 1. The learning rate is initialized as 0.001 for first 40 epochs and is then decayed by 0.1 in every 5 epochs.\nFor each input point cloud, we sample 256 proposals, with ratio 1:1 for positives and negatives.\nOur implementation is based on Tensorflow [1]. For the box prediction network, a proposal is considered positive if its maximum 3D IoU with all ground-truth boxes is higher than 0.55 and negative if its maximum 3D IoU is below 0.45 during training the car model. The positive and negative 3D IoU thresholds are 0.5 and 0.4 for the pedestrian and cyclist models.\nBesides, for the IoU branch, we only train on positive proposals.\n"]}
{"id": "1909.10801", "categories": "cs.LG q-fin.ST stat.ML", "paragraphs": ["The models are implemented in PyTorch and trained using Adam [8] and a learning rate cosine decay schedule from \\(6e^{-4}\\)  down to \\(3e^{-4}\\) . To avoid overfitting uninformative noisy patterns in stale data input sequence length is set to 30 days. In addition, to enable a fair comparison and avoid additional overfitting we employ an early stopping scheme based on training loss that is motivated by different convergence times of different models. We use a static testing approach with a long period of 446 out-of-sample trading days to test stability of the learned trading strategy under turbulent market conditions and a wider distributional shift between in-sample and out-of-sample data. PyTorch code for models and training procedure is included in the supplementary material.\n"]}
{"id": "1912.12814", "categories": "cs.CV cs.NE", "paragraphs": ["\u00a0\u00a0\u00a0Following DARTS\u00a0[19], half of the CIFAR-10 training data are held out as the validation\nset for architecture search.\nA small network consisting of 8 cells is trained using RC-DARTS for 50 epochs, with batch size 64\n(for both the training and validation iterations) and the initial number of channels 16.\nMomentum SGD and Adam optimizer are used to optimize the weights \\(w\\)  and \\(\\theta \\)  iteratively in unconstrained\ntraining step.\nIn architectural projection step (ref. Eq.\u00a0(REF )),\nwe use Adam optimizer with initial learning rate of 3e-4, momentum of \\((0.5, 0.999)\\) , and no weight decay.\nThe number of iterations in phase I \\(e_u\\)  is set to 150 (i.e. 0.5 epoch);\nThe number of iteration in phase II \\(e_p\\)  is set to 500. The lower/upper bounds for constraints of #params and FLOPs are 1.8e5/2.0e5 and 2.8e7/3.3e7 respectively. These numbers are decided by experiments.\nAll experiments run with PyTorch using NVIDIA V100 GPUs.\n"]}
{"id": "1912.10667", "categories": "cs.CV", "paragraphs": ["Since the image tiles are too large to be fed through a deep CNN due to limited GPU memory, we randomly extract image patches of size of 256\\(\\times \\) 256 pixels as the training set. Following standard practice, we only use horizontal and vertical flipping as data augmentation during training. For testing, the whole image is split into \\(256\\times 256\\)  patches with a stride of 256. Then, the predictions of all patches are concatenated for evaluation.\n"]}
{"id": "1905.03197", "categories": "cs.CL", "paragraphs": ["Adam\u00a0[21] with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\)  is used for optimization. The learning rate is 3e-5, with linear warmup over the first \\(40,000\\)  steps and linear decay. The dropout rate is \\(0.1\\) . The weight decay is \\(0.01\\) . The batch size is 330.\nThe pre-training procedure runs for about \\(770,000\\)  steps. It takes about 7 hours for \\(10,000\\)  steps using 8 Nvidia Telsa V100 32GB GPU cards with mixed precision training.\n"]}
{"id": "1905.03282", "categories": "cs.LG cs.CR cs.IR stat.ML", "paragraphs": ["Problem 1 (the non-differentiability of \\(\\varphi (\\cdot )\\) ): The STCA ternarization operator \\(\\varphi (\\cdot )\\)  is not differentiable with respect to \\(\\bf x\\) . Therefore, one can envision several approaches to approximate \\(\\varphi (\\cdot )\\)  by some differentiable surrogate function, as for example by a hard-thresholding operator that preserves the same mutual information as for the ternarization operator for a range of sparsity levels \\(S_x\\)  [17], or by considering a linear approximation, i.e., \\(\\varphi (AW {\\bf x})=AW {\\bf x}\\) , that yields to:\n", "Problem 2 (model prior for real data): The above assumed \\(\\ell _2\\) -norm regularizer works only for the synthetic i.i.d Gaussian data. In the case of real images, it is too restrictive. The class of sparsification priors is also relatively restrictive in view of a single overcomplete shallow representation. Instead, recent works [18], [19] suggested using a generative model \\({\\bf x} = g_{\\theta _G} ({\\bf z})\\) , where \\(g_{\\theta _G} (\\cdot )\\)  is a generator of GAN or decoder of VAE trained on corresponding data \\(\\lbrace {\\bf x}_i\\rbrace _{i=1}^N\\) , where \\(N\\)  denotes the number of training samples. In this case, the reconstruction problem reduces to:\n"]}
{"id": "2109.08364", "categories": "cs.CV", "paragraphs": ["In our experiment, we set the number of N in Fig.\u00a0REF  to 5 and adopt 4 heads for self-attention. Different from the feature dimension value of 64 or 128 in prior works \u00a0[40], [37], we set the middle feature dimension of the model to 96 for GraFormer with a dropout rate of 0.25.\nWe adopt Adam \u00a0[14] optimizer for optimization with an initial learning rate of 0.001 and mini-batches of 64. For Human3.6M, we multiply the learning rate by 0.9 every 75000 steps.\nFor hand datasets, the learning rate decays by 0.9 every 30 epochs.\nWe train GraFormer for 50 epochs on Human3.6M, 900 epochs on Obman and GHD and 3000 epochs on FHAD.\n"]}
{"id": "2101.01165", "categories": "cs.CV cs.AI", "paragraphs": ["For training our networks, we gather equal number of real and fake videos from the mentioned datasets, and create train and test subsets with a 70%-vs-30% split (unless otherwise is noted in the results section). We use the following hyper-parameters determined empirically: adam optimizer with a learning rate of \\(1e-04\\) , 0.3 dropout probability, batch size of 32, and 0.2 threshold for leaky ReLU layers. We train the models for 100 epochs and validate every 10 epochs.\n"]}
{"id": "2108.01375", "categories": "cs.CV cs.LG", "paragraphs": ["In [5], the authors are using as input to the TCN the raw 3D skeleton points. We have used a similar setup, but have also tested the system performance when it receives as input the angles between different joints. For 3D skeleton points setup, we take the computed (X, Y, Z) values of each skeleton joint and concatenate all values to form a skeleton feature. A skeleton feature per frame is a 66 dimensional vector obtained by multiplying the number of joints (which is 22) with the data per point (which is 3) as it can be seen in Figure REF .\n{FIGURE}", "For the Res-TCN parameters, we used the same configuration as proposed by [5]: stochastic gradient descent with nesterov acceleration with a momentum of 0.9, all convolution layers have applied a \\(L1\\)  regularizer with a weight of \\(10^{-4}\\) , and to prevent overfitting a dropout of 0.5 is applied after every ReLU.\n"]}
{"id": "2103.00737", "categories": "cs.LG", "paragraphs": ["For each training program, our meta-algorithm used \\(2^{15}\\)  samples from the analytic (for \\(\\mathsf {gauss}\\) ) or approximate (for the rest, by HMC) posterior\ndistribution for the program.Except for \\(\\mathsf {rb}\\) ; see the discussion on Rosenbrock models in\nAppendix\u00a0.\nSimilarly, our meta-algorithm computed the marginal likelihood analytically (for \\(\\mathsf {gauss}\\) ) or approximately (for the rest) using layered adaptive importance\nsampling [26] where the proposals were defined by an HMC chain. We performed mini-batch training; a single gradient update was done with a training program and\na mini batch of size \\(2^{12}\\)  (out of \\(2^{15}\\)  samples for the program).\nWe used Adam [20] with its hyperparameters\n\\(\\lbrace \\beta _1=0.9,\\,\\beta _2=0.999,\\,\\textrm {weight\\_decay}=0\\rbrace \\) , and the initial learning rate was set to \\(0.001\\) .\nWhen the average training loss converged enough, the training stopped.\nWe repeated the same experiments three times using different random seeds.\n", "Results  Fig.\u00a0REF  shows the training and test losses for \\(\\mathsf {gauss}\\) , \\(\\mathsf {hierl}\\) , and \\(\\mathsf {milky}\\)  under three random seeds. The losses for the other\nmodel classes are in\nAppendix\u00a0.\nThe training loss was averaged over the training set and 8 batch updates, and the test loss over the test set.\nThe training losses in all three experiments decreased rapidly, and more importantly, these decreases were accompanied by the downturns\nof the test losses, which shows that the learnt parameters generalised to the test programs well. The later part of\nFig.\u00a0REF  shows cases where the test loss increases. This was because the loss of only\na few programs in the test set (of 50 programs) became large. Even in this situation, the losses of the rest remained small.\n"]}
{"id": "2106.09171", "categories": "cs.LG cs.CV cs.SD eess.AS", "paragraphs": ["LiRA-Supervised\u00a0\u00a0\u00a0In LiRA-Supervised, we train word-level (Fig.\u00a0[fig:variationsd]2a) and sentence-level lip-reading models (Fig.\u00a0[fig:variationsd]2d) from scratch. In particular, for the task of word-level lip-reading, we add a MS-TCN followed by a linear classifier with an output dimension of 500 on top of the encoder like\u00a0. A cross-entropy loss is employed to optimise the whole model using Adam with decoupled Weight decay (AdamW)\u00a0 with \\(\\beta _{1}=0.9\\) , \\(\\beta _{2}=0.999\\) , \\(\\epsilon =10^{-8}\\)  and a weight decay of 0.01 for 80 epochs with a batch size of 32. The initial learning rate is set to 0.0003. For the task of sentence-level lip-reading, we use 12 multi-head attention blocks (\\(d^{{\\rm ff}}=2048\\) , \\(n^{{\\rm head}}=4\\) , \\(d^{{\\rm q}}=256\\) , \\(d^{{\\rm k}}=256\\) , \\(d^{{\\rm v}}=256\\) ) together with a linear layer on the top of conformer blocks like\u00a0. Following\u00a0, we use a combination of CTC and cross-entropy loss to train a hybrid CTC/Attention architecture for 50 epochs with a batch size of 8. In this case, we use Adam\nwith \\(\\beta _{1}=0.9\\) , \\(\\beta _{2}=0.98\\)  and \\(\\epsilon =10^{-9}\\)  with the first 25\u00a0000 steps for warm-up. The initial learning rate is set to 0.0004. At the decoding phase, we use a beam size of 20 for beam search.\nDuring decoding, we also apply a transformer-based language model trained on LRS2, LRS3, and Librispeech 960h\u00a0 (16.2 million words in total). Due to graphic memory limitations, we exclude utterances with more than 600 frames during training.\n", "LiRA-Frozen\u00a0\u00a0\u00a0At the end of self-supervised training, the features extracted from the pre-trained frozen encoder are fed to a classifier for evaluation. For word-level lip-reading, we use a MS-TCN, followed by a linear layer with an output size of 500 for classification (Fig.\u00a0[fig:variationsd]2b). For the sentence-level lip-reading, the LiRA features are first fed to 12 conformer blocks, and then the encoded representations are used for CTC/attention joint training (Fig.\u00a0[fig:variationsd]2e).\n"]}
{"id": "2111.14819", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["dVAE Setups. We use a four-layer DGCNN[57] to learn the inter-patch relationships, modeling the internal structures of input point clouds. During dVAE training, we set the vocabulary size \\(N\\)  to 8192. Our decoder is also a DGCNN architecture followed by a FoldingNet\u00a0[62]. It is worth noting that the performance of dVAE is susceptible to hyper-parameters, which makes that the configurations of image-based dVAE\u00a0[41] cannot be directly used in our scenarios. The commonly used \\(\\ell _1\\) -style Chamfer Distance loss is employed during the reconstruction procedure. Since the value of this \\(\\ell _1\\)  loss is numerically small, the weight of KLD loss in Eq.REF  must be smaller than that in the image tasks. We set the weight of KLD loss to 0 in the first 10,000 steps and gradually increased to 0.1 in the following 100,000 steps. The learning rate is set to 0.0005 with a cosine learning schedule with 60,000 steps warming up. We decay the temperature in Gumble-softmax function from 1 to 0.0625 in 100,000 steps following\u00a0[41]. We train dVAE for a total of 150,000 steps with a batch size of 64.\n", "MPM Setups. In our experiments, we set the depth for the Transformer to 12, the feature dimension to 384, and the number of heads to 6. The stochastic depth\u00a0[15] with a 0.1 rate is applied in our transformer encoder. During MPM pre-training, we fix the weights of Tokenizer learned by dVAE. 25% \\(\\sim \\)  45% input point embeddings are randomly masked out. The model is then trained to infer the expected point tokens at those masked locations. In terms of MoCo, we set the memory bank size to 16,384, temperature to 0.07, and weight momentum to 0.999. We employ an AdamW[29] optimizer, using an initial learning rate of 0.0005 and a weight decay of 0.05. The model is trained for 300 epochs with a batch size of 128.\n"]}
{"id": "2108.12472", "categories": "cs.CL cs.LG", "paragraphs": ["All our experiments were run using NVIDIA V100 GPUs for training and validation, some trainings were done on A100. We distributed our training to 2-4 GPUs depending on availability. Each training epoch for CE ranged from 30 minutes to 1 hour depending on number of GPUs utilized.\n", "Validation and testing (1,779 and 2,155 samples for testA and testB of WebNLG+ 2020) lasted from 40 minutes to 1 hour depending on machines. Computation was dominated by beam search generation as we used beam search with beam size of 5 and a max sequence length of 192 (since linearized graph sequence can be quite long). We used the official scoring scripts released by WebNLG+ 2020 Challenge to score all our experiments. The evaluation of graph being the most computationally expensive as all possible matching combinations are tested in what looks like a factorial complexity, taking scoring of set of triples larger than 8 from impractical to not feasible.\n", "All our models were built using PyTorch. Total effective batch sizes were set to either 20 or 24 samples for our distributed training. We adjusted the batch size on each worker to ensure consistent global batch size of 20 or 24.\n"]}
{"id": "2107.02137", "categories": "cs.CL", "paragraphs": ["Both the universal representation module and the task-specific representation modules of ERNIE 3.0 uses the Transformer-XL[33] structure as the backbone. For the universal representation module, we adopt a structure with 48 layers, 4096 hidden units and 64 heads. For the task-specific representation modules, we adopt a structure with 12 layers, 768 hidden units and 12 heads. The total parameter of universal representation module and task-specific representation modules is 10 billion. The activation function used is GeLU[45].\nThe maximum sequence length of context and the memory length of language generation is set to 512 and 128, respectively.\nThe total batch size of all pre-training tasks is set to 6144.\nWe use Adam[46] with learning rate of 1e-4, \\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\) , L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps and linear decay of the learning rate.\nIn the first 10,000 steps, we also use the progressive learning to speedup convergence in the initial stage of pre-training. The model is trained for a total of 375 billion tokens with 384 NVDIA v100 GPU cards and is implemented on PaddlePaddle framework.\nBy virtue of parameter sharding used in [47], [48], we manage to reduce the memory usage of our model and address the problem of the total parameter of model exceeding the memory of a single GPU card.\n"]}
{"id": "2107.02195", "categories": "cs.LG cs.AI", "paragraphs": ["We test all audio encoders in the Music Recognition scenario, where training converges within \\(5\\times 10^8\\)  environment steps. We then choose the best-performing encoder for other experiments, where we train for \\(10^9\\)  steps in Sound Instruction scenarios and for \\(2\\times 10^9\\)  steps in Duel scenario. We fixed the image resolution to 128x72 and set the frameskip to 4 for all environments except Duel which was run with 2-frameskip.\n"]}
{"id": "2107.05599", "categories": "cs.LG cs.AI", "paragraphs": ["[20] presents an evolutionary approach for exploring and finding effective and customisable neural style transfer blends. Upwards of 1000 neural style transfer models trained on 1-10 style images each, can be blended through model interpolation, using an interface that is controlled by the user. MAP-Elites [55] in combination with a fitness function calculated using the output from a ResNet model [37] were used in evolutionary searches for optimal neural style transfer blends.\n"]}
{"id": "2107.08591", "categories": "cs.CV", "paragraphs": ["Our network is trained end-to-end with stochastic gradient descent (SGD) with batch size 16 (at most case), momentum 0.9, and weight decay 0.0001. The pre-trained models are trained on the ImageNet\u00a0 and the output stride is set to 1/8. Following previous works\u00a0, , , we use the poly strategy in which the current learning rate is multiplied by \\((1 - \\frac{iter}{max\\_iter})^{power}\\)  each iteration with power 0.9. The base learning rate is set to 0.01. Following the previous works\u00a0, , , the weight balance parameters \\(\\alpha \\)  and \\(\\beta \\)  are set to \\(10^3\\)  and 10 to maintain a balance with the cross-entropy loss function. The iteration number is set to 40K for Cityscapes, 10K for CamVid, 60K for Pascal VOC 2012, and 125K for ADE20K. The input size is set to \\(513 \\times 513\\)  for Cityscapes, \\(480 \\times 360\\)  for CamVid, and \\(473 \\times 473\\)  for both Pascal VOC 2012 and ADE20K during training and inference. For data augmentation, the random horizontal flip and random resize between 0.5 and 2 are adopted. Then, all images are resized to have the maximum extent of the long side of the input size and padded with mean to the input size. For inference, we verify the performance on a single scale and original inputs. Besides, only the conventional cross-entropy loss is applied in the experiments, the class probability weighting, hard sample mining strategy, and deep supervision\u00a0 are excluded unless otherwise specified. Our method is implemented by Pytorch\u00a0 framework. All networks are trained on a single NVIDIA Tesla V100 GPU with 32GB memory.\n"]}
{"id": "2107.10963", "categories": "cs.LG cs.CV", "paragraphs": ["In all our experiments, we used RMSProp optimizer.\nIn smaller scale experiments, the learning rate was chosen to be either \\(0.02\\) , or \\(0.04\\) .\nThe dropout keep probability and the weight decay were \\(80\\%\\)  and \\(10^{-5}\\)  correspondingly.\nIn all of our experiments, we used exponential moving averages of all trained variable for inference.\n"]}
{"id": "2108.06421", "categories": "cs.CV", "paragraphs": ["Other than the method for generating similar image pairs, identical parameters were used for GeoCLR and SimCLR to allow for comparison. Both method are trained on the all 86,772 images in the dataset. We also benchmark the performance of the proposed method against conventional supervised transfer learning using ResNet18 that was pre-trained on ImageNet.\n", "Though deeper CNN architectures, larger minibatch sizes and epoch are known to provide accuracy gains for SimCLR, these above parameters are set considering the computational power that can be reasonably deployed in the field, where access to high-performance computers networks is limited. The workstation used for experiments in this paper used a single NVIDIA TITAN RTX with 24 GB VRAM. The GeoCLR training and fine tuning with pseudo-labelling carried out in this work each took approximately a day (26 hours for GeoCLR training and a few minutes for fine-tuning) for the dataset of ~86k images gathered in 24 hours of bottom time over multiple AUV dives. This indicates that the results could be made available in timeframes relevant to assist planning and interpretation between dives during multi-day field expeditions.\n"]}
{"id": "2105.04019", "categories": "cs.LG cs.IR", "paragraphs": ["We use the Adam optimizer\u00a0[11] with a learning rate of \\(10^{-3.5}\\) , and up to \\(10^6\\)  iterations of training.\nFurthermore, we set \\(\\alpha =0.25\\)  and a use a steepness of two times the number of layers (\\(s=2n\\)  for odd-even and \\(s=(\\log _2n)(1+\\log _2n)\\)  for bitonic.)\nWe use a constant batch size of 100 as in previous works unless denoted otherwise.\nNote that, although \\(\\alpha \\)  is chosen as a constant value for all \\(n\\) , a higher accuracy is possible when optimizing \\(\\alpha \\)  for each \\(n\\)  separately.\n"]}
{"id": "2103.12235", "categories": "cs.CL", "paragraphs": ["Since documents can contain up to hundreds of sentences, for efficient training of our evidence retrieval model, we downsample the negative examples to 7 for IIRC and 3 for HotpotQA. But no downsampling is done during inference.\nFor IIRC, we take \\(m=4\\)  for the top-\\(m\\)  context marginalization and take \\(m=5\\)  for HotpotQA. For the weight for invalid context loss, we use \\(0.5\\)  for IIRCWe performed a simple binary search and found 0.5 to work better than 0 or 1. and 0 for HotpotQA since it does not have unanswerable questions.\nFor memory and storage efficiency, we tie the pretrained language model weights among all the components in our joint model.\nThe models are trained for 30 epochs for IIRC and 5 epochs for HotpotQA fullwiki.\nOur most expensive experiment takes about 1.5 days to run on two RTX 8000 (48GB) GPUs or one A100 (40GB) GPU, while a typical experiment takes about half of that computing power.Note that our models are jointly trained, thus this is the total training cost for both retrieval and reasoning.\n"]}
{"id": "2111.12525", "categories": "cs.CV", "paragraphs": ["We configured the segmentation network \\(f_{\\phi }(\\cdot )\\)  as a U-Net [0], the most commonly used network architecture for medical image segmentation, with an EfficientNet-b2 backbone [67]. For our proposed method, we trained the segmentation network using an Adam optimizer [68] with an initial learning rate of \\(3\\times 10^{-4}\\)  with learning rate decay. We evaluated our method at the 2k-th epoch where the learning rate decays to zero.\n"]}
{"id": "2103.14572", "categories": "cs.CV cs.LG", "paragraphs": ["Unless otherwise specified, Adam optimizer [30] with an initial learning rate of \\(0.0002\\) , weight decay \\(10^{-5}\\) , \\(\\beta _1=0.9\\)  and \\(\\beta _2=0.999\\)  was used for training. Learning rate was reduced by a factor of \\(0.2\\)  when the validation loss stopped improving after a dataset-dependent number of iterations. Training was stopped when the learning rate dropped below \\(10^{-6}\\)  or maximum number of iterations was reached.\nIn all our experiments we use 16-dimensional embedding space, i.e. the output from the U-Net after the last 1\\(\\times \\) 1 convolution has 16 channels. Input images were globally normalized to zero mean and a standard deviation of one unless stated otherwise.\n{FIGURE}"]}
{"id": "2104.07398", "categories": "cs.CL", "paragraphs": ["For target terminology extraction phase, we adopt the commonly used Transformer encoder with 1024 embedding/hidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer as the basic. During training, the batch size is set to 128 and the sentence length is limited to 100 BPE tokens. We employ the Adam [10] optimizer with \\(lr\\)  = 0.0001, \\(t_{warm\\_up}\\)  = 4000 and \\(dropout\\)  = 0.1.\n"]}
{"id": "2109.05168", "categories": "cs.CL", "paragraphs": ["The hyperparameters are selected based on the best performing model on the dev set.\nWe use grid search to fine-tune the model, and select select the learning rate from \\(\\lbrace 1e-5, 2e-5\\rbrace \\) , batch size from \\(\\lbrace 4,8\\rbrace \\)  and gradient accumulation from \\(\\lbrace 4,8,16\\rbrace \\) . The model is trained up to 4 epochs.\u00a0Details about the experiment setting is in Appendix.\n"]}
{"id": "2109.05281", "categories": "cs.CL cs.AI cs.CV cs.LG", "paragraphs": ["We implement COSMic\u2014as described in Section\u00a0\u2014with PyTorch [35] and train on a GTX1080 GPU.\nWe pre-compute BERThttps://github.com/google-research/bert and ResNethttps://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50V2 features using their TensorFlow [0] implementations. We use the public ViLBERThttps://github.com/facebookresearch/vilbert-multi-task implementation. We use a batch size of 4, and a learning rate of \\(2\\times 10^{-6}\\)  for fine-tuning ViLBERT and use RAdam optimizer and stop the training when the validation score does not change for 3 epochs.\nFor COSMic Vanilla, we train with a batch-size of 10, Adam optimizer [20] with a base learning rate of \\(10^{-3}\\)  that decays by a factor of \\(10^{-2}\\)  every 10 epochs.\nWe observe that the Vanilla converges in approximately 100 epochs and ViLBERT converges in 9 epochs. ViLBERT has 250 million parameters. COSMic Vanilla includes 3,062,913 trainable parameters. Pre-trained BERT-Large and ResNet50V2 have an additional 350 million parameters.\nThe setup for coherence-aware captioning models to obtain machine-generated captions for our study is the same as [2].\n"]}
{"id": "2110.14577", "categories": "cs.CV cs.LG", "paragraphs": ["We train all our models using stochastic gradient descent for 200 epochs and a batch size of 128 on RTX 2080 GPUs. We use a starting learning rate of 0.1 and a weight decay of \\(5.0e-4\\) . For ResNet18 experiments, we use a cosine scheduler for learning rate. For Wide ResNet-20-10 experiments, we use a step scheduler which multiplies the learning rate at epoch 60, 120 and 160 by 0.2.\n"]}
{"id": "2112.04228", "categories": "cs.CV", "paragraphs": ["We train our model on a single Nvidia 2080ti GPU with a total batch size of 32. We use the Adam optimizer with a learning rate of \\(5 \\times 10^{-4}\\)  (\\(\\beta _1 \\) =0.9, \\(\\beta _2\\) =0.998), and the weight decays to \\(10^{-3}\\) . We utilize the plateau learning rate schedule to update the learning rate, which tracks the BLEU [41] score on the validation set, the patience and factor are set to 9 and 0.5, respectively. The validation set is evaluated every 100 steps. During validation, we use a beam search algorithm with a beam size of 3 and a length penalty value -1 to decode the text sequence. Training ends when the learning rate is less than \\(10^{-7}\\) . During training, the weights of the loss function \\(\\lambda _1 - \\lambda _4\\)  are set to 10, 1, 0.6, and 0.4 respectively.\n"]}
{"id": "2108.04938", "categories": "cs.CV cs.AI cs.CL", "paragraphs": ["We first resize all images of OpenI to \\(206\\times 206\\)  and apply the unsupervised feature learner, PixelHop++. We use a three-level PixelHop++ with the following hyper-parameters: \\(w = 3\\) , \\(d = 1\\) , \\(s = 1\\) , and \\(E = 0.00005\\) . Then, we apply PCA to its output channels and concatenate the generated vectors to form a set of \\(Q\\)  visual features of dimension D, i.e., \\(V = [v_1, v_2, ..., v_Q], v_i\\in \\mathbb {R}^D\\) . In BERTHop, \\(D\\)  is set to be 2048. In our experiments setup, \\(Q\\)  is equal to 15 but may vary depending on the size of the output channels of the PixelHop++ model and also the number of PCA components.\n", "As for the transformer backbone, we use BlueBERT-Base (Uncased, PubMed+MIMIC-III) from Huggingface [40], a transformer library. Having the visual features from the visual encoder and text embedding, we train the transformer on the training set of OpenI with 2,912 image-text pairs. We use batch size = 18, learning rate = \\(1e-5\\) , max-seq-length = 128, and Stochastic Gradient Descent (SGD) as the optimizer with momentum = 0.9 and train it for 240 epochs.\n"]}
{"id": "2110.05999", "categories": "cs.CL", "paragraphs": ["We collect 322K contiguous texts from BookCorpus\u00a0[44] and keep the first 512 bpe subwords of each example for training. Since the warm-start training aims at initializing the latent embeddings for reconstructing the target text, we do not feed any input to the encoder. We use a fixed Gumbel temperature of 0.9 and a fixed learning rate of 1e-4. We use a batch size of 4 and a gradient accumulation step of 4 and train on the collected data for one epoch which takes about 7 hours on 1 GeForce RTX 2080 (11G).\n", "For fine-tuning the generator and the posterior network for text reconstruction, we anneal the Gumbel temperature from \\(\\tau _{max}=0.9\\)  to \\(\\tau _{min}=0.1\\)  using exponential decay schedule where the Gumbel temperature \\(\\tau \\)  at step \\(T\\)  is: \\(\\max [\\tau _{min}, \\tau _{max}\\times \\exp (-10^{-4}\\times T)]\\) . We linearly decrease the learning rate from 1e-4 to 0 throughout the fine-tuning. We use a batch size of 4 and a gradient accumulation step of 4. We fine-tune for five epochs on Wikiplots and one epoch on WritingPrompts, which takes about 12 hours and 6 hours on 1 GeForce RTX 2080 (11G), respectively.\n", "For fine-tuning the prior network, we initialize the encoder with the pre-trained parameters of the encoder of \\(\\text{BART}_{\\text{base}}\\) . We linearly decrease the learning rate from 1e-4 to 0 during training. The maximum target sequence length is set to \\(\\text{MaxLength}=64\\) . We use a batch size of 128 and a gradient accumulation step of 8. We fine-tune the model for 100 epochs which takes about 13 hours on 1 GeForce RTX 2080 (11G). During inference, we randomly sample a sequence of latent codes from the prior network autoregressively and set the minimum sequence length to 38 and 44 for Wikiplots and WritingPrompts, respectively.\n"]}
{"id": "2110.13953", "categories": "cs.LG", "paragraphs": ["To adversarially train the models as described in Section REF , we initialize with models trained in the standard fashion. We then train these models in an adversarial manner for 60 epochs of 1000 episodes each. We use the same hyperparameters and experimental setup as we did for the standard training, except that we reduce the learning rate by a factor of 10. Thus, the learning rate is initially set to 0.1 for the first 20 epochs, then modified to 0.006 for epochs 20 to 40, 0.0012 for epochs 40 to 50, and 0.0024 thereafter. To find the adversarial examples, we find the worst-case examples using algorithm  run for 3 iterations. We then use these worst-case examples as the query data to update the model parameters.\n"]}
{"id": "2110.10261", "categories": "cs.CL", "paragraphs": ["All our RNN LMs use a single layer of Gated Recurrent Units (GRUs). Input and output word embedding matrices are shared and tied during training. We set the dimensional of word embeddings to be 64. Similarly, the hidden states of the GRU are set to 64. Training uses a batch size of 32. We employ the Adam optimizer with an initial learning rate of 0.001. The learning rate is scheduled to reduce by a factor of 0.1 if the loss does not decrease for 10 epochs. We leverage an early stopping strategy, on the dev set perplexity, with a patience set to 15 epochs. The maximum number of arcs in a confusion bin is restricted to 5 and the confusion bin scores are re-normalized to sum to 1.0.\n"]}
{"id": "2110.10318", "categories": "cs.CL cs.LG", "paragraphs": ["Our default parameters are as follows: We use Adam optimizer with 500 warmup steps=500 and weight decay of 0.01. Our batch size is 16. Models were trained using 2 NVIDIA v100 GPUs. Training time for TPP ranged from 30 mins to 32hrs for different settings. NER training time was around 1.5 hrs. Sentiment training time was 1 minute, and UD POS training time was also 1 minute.\n"]}
{"id": "2109.10257", "categories": "cs.CV cs.RO", "paragraphs": ["For all of our experiments, we use SGD optimizer. The initial learning rate for GTA-IM is 0.01 and 0.03 for the PROX dataset. The number of training epochs is 450 and we decrease the learning rate by a factor of 0.2 every 200 epochs. We use a batch size of 128 and 1 second of observation and 2 seconds for predictions following the settings of\u00a0[5]. We used GTX-1080Ti for training on a 128 GB RAM machine. The need for a large RAM comes when the models are trained using the visual signal. Training each model took between 8 hours and 24 hours depending on the used dataset.\n"]}
{"id": "2102.05526", "categories": "cs.LG", "paragraphs": ["The dynamic \\(\\beta \\) -VAE has several tunable model parameters, as seen in Eq.\u00a0(REF ) to Eq.\u00a0(). These model parameters were tuned on an independent dataset, collected with the identical instrumentation but at a different location. The data had a similar distribution as the data used in this work and we obtained: \\(a=0.2\\)  and \\(b=0.05\\) , \\(w_1=w_2=1.2\\) , \\(w_3=0.9\\)  and \\(w_4=1.1\\) . These parameters were found to be sufficiently robust on the dataset used in this work without any fine-tuning.\n"]}
{"id": "2105.08665", "categories": "cs.LG cs.CV", "paragraphs": ["We train the 2D CNN + LSTM model and the 3D CNN model on the same dataset to benchmark the performance against each other. For training, we take a subset of UCF50 with 38 classes which have almost equal distribution of videos i.e. more than 120 videos per class. The dataset consists of 4557 videos and we use 90% of it for training and 10% of it for validation. For ensuring the best performance, we augment the data using random sampling and central cropping of videos.\n"]}
{"id": "2105.08649", "categories": "cs.LG cs.AI cs.IR cs.MM", "paragraphs": ["We implement our approaches using PyTorchhttps://pytorch.org/. We apply Adam [31] with a learning rate of 0.001 and a weight decay of \\(1e^{-6}\\)  to prevent overfitting, and a mini-batch size of 4096 across all tasks. For the fair competition, we set the default architecture of the dense neural network with two hidden layers and 100 neurons per layer for all models that involve DNN. To avoid overfitting, we perform the early-stopping strategy based on AUC on the validation set. A dropout method is also applied across all models with a rate of 0.5 for the MovieLens-1M dataset and 0.2 for the other two datasets to prevent overfitting. The dimension of feature embeddings is set to 16 for all the models across all tasks consistently. More specifically, the number of layers in DCN set to 2. The maximum order in HOFM is set to 3. The attention embedding size of model AFM and AutoInt is 64. Additionally, the number of heads in AutoInt is set to 4. The default number of logarithmic neurons in AFN is set to 1500, 1200, 800 for Criteo, Avazu, and Movielens datasets. For our model DCAP, we set the maximum depth of network to 2 as a bounded order of feature interactions.\n"]}
{"id": "2111.11703", "categories": "cs.LG cs.AI cs.SD eess.AS stat.ML", "paragraphs": ["For all models, teacher forcing was used, the batch size was set to 64, and training was conducted for 2 epochs, when the losses converged. The Adam optimizer [22] was used for all models, with the parameters \\((\\alpha , \\beta _{1},\\beta _{2})=(0.0005,0.9,0.999)\\) . For CLSM or VAE, we conducted KL-annealing linearly from \\(\\beta =0\\)  or \\(\\gamma =0\\)  for 2 epochs [0].\n"]}
{"id": "2111.00400", "categories": "cs.CL cs.SD eess.AS", "paragraphs": ["The three versions of FAN are as follows: 1- FAN-a: an LSTM based encoder, two LSTM based decoders, and an additive attention attender; 2- FAN-b: an LSTM based encoder, two self-attention based decoders, a cross-attention attender; 3- FAN-c: a self-attention based encoder and two self-attention based decoders, and a cross-attention attender. In FAN-a, we used a \\(3\\times 612\\)  LSTM in the encoder and two decoders. In FAN-b, we used a \\(4\\times 768\\)  LSTM in the encoder and 3 layers of self-attention layers in each decoder. In FAN-c, we used 12 self-attention layers in the encoder, 5 self-attention layers in the slot value decoder, and 3 self-attention layers in the slot tag decoder. The input dimension (d-model) was set to 256. For all self-attention layers, we used 4 heads and a 2,048-hidden unit feedforward net is used. For the multi-task and direct A2I in [11], the size of the encoder and decoder were set to \\(3\\times 612\\)  and \\(4\\times 612\\) , respectively. We trained these models using 500 hours of data in-house data.\n", "For the FSC dataset, we built smaller models to prevent overfitting as the FSC dataset is very small and not complex. We observed models of size \\(\\sim \\)  3 millions deliver good results and don't suffer from overfitting. For FSC, the FANS models were built as follows: In FAN-a, we used a \\(1\\times 256\\)  LSTM in the encoder and two decoders. In FAN-b, we used a \\(2\\times 232\\)  LSTM in the encode and 2 layers of self-attention layers in each decoder with 1 head and a 800-hidden unit feedforward net. In FAN-c, we used 2 self-attention layers in the encoder, 1 self-attention layer in the slot value decoder, and 1 self-attention layer in the slot tag decoder with 4 heads and a 1024-hidden unit feedforward net. The input dimension (d-model) for all these models was set to 128. For the multi-task and direct A2I in [11], the size of the encoder and decoder were set to \\(1\\times 256\\)  and \\(1\\times 350\\)  LSTM, respectively.\n"]}
{"id": "2103.16806", "categories": "cs.CV eess.IV", "paragraphs": ["Following previous works, we use isotropic Gaussian blur kernels to simulate the PSF. The range of kernel width is fixed to 1, and the kernel size is fixed to \\(8\\times 8\\) . For CAVE and Pavia, we directly take the whole image for training. As for the WV2, the LR-HSI are cropped to \\(128\\times 128\\)  image patches for training, and no data augmentations were used. The parameter \\(\\beta \\)  and \\(\\gamma \\)  are empirically set to 0.01 and 30, respectively. Adam optimizer is used with default setting. We train SRFN for 55000 iterations on CAVE, and the learning rate is initialized to \\(2\\times 10^{-4}\\) . The SRFN is trained for 20000 iteration on WV2 and 10000 iteration on Pavias, while the initial learning rate is set to \\(4\\times 10^{-4}\\) . Through all the experiments, the training batchsize is set to 1. All the models are trained on a server equipped with NVIDIA RTX 2080 Ti GPU.\n"]}
{"id": "2103.11263", "categories": "cs.CL cs.LG", "paragraphs": ["For all test-time learning variants, we limit the maximum number of questions generated per context to 4000 and the maximum number of training steps to 1500.\nThe number of training steps is linearly dependent on the selected batch size \\(\\in [16, 64]\\) .\nFor our \\(K\\) -neighbor TTL setup that uses Context Expansion, we limit the number of retrieved contexts to 500.\nIn Curriculum Test-Time RC, we ensure that all variants have an equal number (1000) of generated QA-pairs per-context.\nWe evaluate multiple learning rates within the range 1e-5 to 5e-5.\nWe use the Adam\u00a0[24] optimizer and truncate the paragraphs to a maximum sequence length of 384.\nThe number 384 was chosen by evaluating the 99th percentile of the combined length of question and the contexts, to reduce training overhead and GPU memory size.\nLong documents are split into multiple windows with a stride of 128.\nAll experiments were conducted on two Nvidia RTX-8000 GPUs. We use ten percent of the training data to perform three hyper-parameter trials for each variant.\nWe train models with three random seeds, and report the mean F1 and EM scores.\n"]}
{"id": "2104.07182", "categories": "cs.CV", "paragraphs": ["Each training sequential example comprises 10 past and current sweeps (\\(-0.9\\) s, \\(-0.8\\) s, ..., 0s), and 41 current and future timestamps for ground-truth supervision (0s, \\(0.1\\) s, ..., \\(4.0\\) s).\nThe frame at current timestamp is referred to as the key frame.\nEach scene on the in-house data set is 25s long, producing at most 200 complete sequential examples.\nWe trained all of the models with decimated key frames in the training split once (i.e., every sequential example whose key frame is at \\(t\\) , \\(t+0.2\\) s, \\(t+0.4\\) s, \\(\\ldots \\) , is used once during model training).\n"]}
{"id": "2106.03027", "categories": "cs.LG", "paragraphs": ["Optimization.\nAll models are trained in mixed-precision (32-bit weights, 16-bit gradients) using Stochastic Gradient Descent (SGD) with Nesterov's acceleration with momentum coefficient set to 0.9 and cosine annealing of the learning rate schedule for 200 epochs. Training of the Multi-Head model involves mini-batches that contain samples from all tasks.\n", "Hyper-parameter optimization.\nWe used Ray Tune\u00a0[30] for hyper-parameter optimization.\nThe Async Successive Halving Algorithm\n(ASHA) scheduler\u00a0[87] was used to prune hyper-parameter choices\nwith the search space determined by Nevergrad\u00a0[88]. The mini-batch size\nwas varied over [8, 16, 32, 64]; logarithm (base 10) of the learning rate was sampled from a uniform\ndistribution on \\([-4, -2]\\) ; dropout probability was sampled from a uniform\ndistribution on \\([0.1,0.5]\\) ; logarithm of the weight decay coefficient was sampled\nfrom \\([-6, -2]\\) . We used a set of experiments for multi-task learning\non the Coarse-CIFAR100 dataset with different\nsamples/class (100 and 500) to perform hyper-parameter tuning. The final values\nthat were chosen are, learning-rate of 0.01, mini-batch size of 16, dropout probability\nof 0.2 and weight-decay of \\(10^{-5}\\) .\n", "Data augmentation.\nMNIST and CIFAR10/100 datasets use padding (4 pixels) with\nrandom cropping to an image of size 28x28 or 32x32 respectively for data\naugmentation. CIFAR10/100 images additionally have random left/right flips for\ndata augmentation. Images are finally normalized to have mean 0.5 and standard\ndeviation 0.25.\n"]}
{"id": "2101.01444", "categories": "cs.CV", "paragraphs": ["Our discriminator models have three layers each, with 16 nodes per layer.\nAll layers use LeakyReLU activations with a leak of \\(0.2\\) , except for the last layer, which is Sigmoid-activated.\nThe discriminators are trained with soft labels (uniform distribution of \\(0.0..0.2\\)  and \\(0.8..1.0\\)  respectively) [14].\n", "The generators also have four layers each, with 16 nodes per layer.\nSimilar to the discriminators, the generator's layers use LeakyReLU activations, but with a leak of \\(0.01\\) .\nThe last layer uses linear activation.\n", "Generators and discriminators are optimized under the use of Adam optimizer [7] both with a learning rate of \\(0.0005\\) , which linearly decays to 0 after 100 epochs.\nWhereas the generators share a common optimizer, the discriminators are both trained by individual optimizers.\nDuring training, we use mini batches with a batch size of 16.\nThe whole training for each model in the ensemble lasts 200 epochs.\nThis training protocol is similar to that of the original CycleGAN implementation [16].\n"]}
{"id": "2109.12564", "categories": "cs.CV", "paragraphs": ["In the experiments, all the images are resized with \\(m = 224\\) . Two variants of the proposed Vision Transformer Hashing (VTS) are used for experiments, i.e., VTS32 and VTS16 having patch size \\(k = 32\\)  and 16, respectively. Thus, the number of patches \\(N = 49\\)  and 196 for VTS32 and VTS16, respectively. The hidden size (\\(de\\) ) is set as 768 in the experiments. The number of transformer blocks (\\(L\\) ) as well as the number of attention heads (\\(A_h\\) ) are 12 in the VTS models. The hash code is generated with 16, 32 and 64 bit length. All the models are trained for 150 epochs with a batch size of 32 using Adam optimizer with a learning rate of \\(1e^{-5}\\) . The testing is performed at every 30 epochs and the best results are reported.\n{FIGURE}"]}
{"id": "2103.01716", "categories": "cs.CV", "paragraphs": ["We trained four instances of the eum model. The first and second instances, model1 and model2, are trained with srt loss using feature embeddings obtained from ResNet-50 and MobileFaceNet, respectively. The third and fourth instances, model3 and model4, are trained with triplet loss using feature embeddings obtained from ResNet-50 and MobileFaceNet, respectively as an ablation study to our proposed srt.\nThe proposed EUM models in this paper are implemented by Pytorch and trained on Nvidia GeForce RTX 2080 GPU. All models are trained using SGD optimizer with initial learning rate of 1e-1 and batch size of 512. The learning rate is divided by 10 at \\(30k, 60k, ,90k\\)  training iterations.\nThe early-stopping patience parameter is set to 3 (around 30k training iteration) causing\nmodel1, model2, model3, and model4\nto stop after 80k, 70k, 60k, 10k training iterations, respectively.\n"]}
{"id": "2112.01723", "categories": "cs.CV eess.IV", "paragraphs": ["The training parameters used in [22] are provided here. We used an initial learning rate of \\(\\eta _{0} = 0.01\\)  and exponentially decayed that rate by computing:\n\\(\\eta _{k+1} = \\eta _{k} \\cdot \\exp (-0.6 \\cdot k),\\) \n", "at every \\(k\\)  epoch. The Binary-Cross Entropy loss function was modified by doubling the weight term for false positive error cases to trade-off false positives for false negatives:\n\\(L(y,\\hat{y}) = - y \\cdot \\log (\\hat{y}) - 2 \\cdot (1 - y) \\cdot \\log (1 - \\hat{y}),\\) \n"]}
{"id": "2105.00930", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["The FusionNet model contains a large number of trainable parameters. In order to prohibit overfitting, we have used dropout\\(=0.6\\)  along with Weight Regularization, Batch Normalization and Early Stopping. Both the pt-GAN model as well as the pose clustering module is trained independently. After training, the FusionNet model is trained for classification accuracy keeping the other two models frozen.\n"]}
{"id": "2112.13756", "categories": "cs.CL", "paragraphs": ["The skip-gram embeddings were trained using default parameters (vector dimensionality: 100; subword: between 3 and 6 characters; learning rate: 0.05; epochs: 5). The NN was trained for 25 epochs for all 100 classes using the corpus specific pre-trained skip-gram embeddings from the step before, applying default parameters (learning rate: 0.05).\n"]}
{"id": "2104.13913", "categories": "cs.CL", "paragraphs": ["For the fine-tuning of the BioBERT models, we use the learning rate of 2e-5, batch size of 16, training epoch of 10, and max sequence length of 128. During the fine-tuning of PubMedBERT models, the learning rate of 2e-5, batch size of 8, training epoch of 10 and max sequence length of 256 are utilized.\n", "In the contrastive pre-training step of the BERT models, we use the same learning rate with the fine-tuning, and the training epoch is selected from [2, 4, 6, 8, 10] based on the performance on the development set. If there is no development set (e.g., PPI task), we will use 6 as the default training epoch. Since contrastive learning benefits more from larger batch [4], we utilize the batch size of 256 and 128 for BioBERT and PubMedBERT respectively. In addition, the temperature parameter \\(\\tau \\)  is set to 0.1 during the training.\n"]}
{"id": "2106.04335", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["\nTraining tasks of FSAF.\nTo construct a diverse collection of tasks for the training of FSAF, we leverage GP functions with RBF, Mat\u00e8rn-3/2, and spectral mixture kernels to capture smooth functions, functions with abrupt local variations, and functions of periodic nature, respectively.\nFor both the RBF and the Mat\u00e8rn-3/2 kernels, we consider three possible ranges of lengthscales, including \\([0.07,0.13], [0.17,0.23], [0.27,0.33]\\) .\nFor the spectral mixture kernels, we consider mixtures of two Gaussian components of periods \\(0.3\\)  and \\(0.6\\)  and three possible ranges of the lengthscales, including \\([0.27,0.33], [0.47,0.53], [0.57,0.63]\\) .\nAs a result, there are nine candidate tasks in the task collection \\(\\operatorname{\\mathcal {T}}\\) .\nIn each training iteration \\(i\\) , three out of the nine tasks are selected uniformly at random from the above task collection (and hence \\(\\vert \\operatorname{\\mathcal {T}}_i\\vert =3\\)  in Algorithm ).\nThe input domain of these GP functions is configured to be \\([0,1]^{3}\\) .\nTo facilitate the training procedure, we discretize the continuous input domain by using the Sobol sequence to generate a grid on which the GP functions and the AFs are evaluated, as typically done in BO.\n", "Network architecture of each DQN particle.\nFor all the DQN particles used in the experiments, we adopt the standard dueling network architecture [53], where one value network and an advantage network are maintained to produce the estimated Q-values.\nFor a fair comparison between FSAF and MetaBO, both the value network and the advantage network of our FSAF are configured to have 4 fully-connected hidden layers with ReLU activation functions and 200 hidden units per layer.\nAs described in Section REF , the input of an advantage network consists of a four-tuple, namely the posterior mean \\(\\mu _t(x)\\) , the posterior standard deviation \\(\\sigma _t(x)\\) , the best observation so far \\(y_t^*\\) , and the ratio between the current timestamp and total sampling budget \\(\\frac{t}{T}\\) .\nAccordingly, the input of a value network consists of \\(y_t^*\\)  and \\(\\frac{t}{T}\\) .\n"]}
{"id": "2110.13414", "categories": "cs.CV cs.CR", "paragraphs": ["Unless otherwise specified, we use ResNet-34 [15]\nas the classifier. We train it using Stochastic Gradient Descent Optimizer\n[0] with the constant learning rate of 0.1,\nweight decay of 5e-4, and the momentum of 0.9. For CIFAR-10, we use\nthe batch size of 128 and train for 100 epochs. For GTSRB, the number\nof epochs is the same but the batch size is increased to 256.\n", "We compare our attack against the input-instance-key attack of Chen\net al. [1] since this attack is the closest to ours.\nWe follow the same settings (a key instance was chosen from one of\nthe classes and added with a noise generated from a uniform distribution\nin the range -5.0 to 5.0) they discussed in the paper for the poison\ninstance generation. We have generated poison instances for both CIFAR-10\nand GTSRB datasets. This is to compare the stealthiness of an attack\nbased on full-sized usual triggers vs full-size host-free triggers\nof our setting. For training the input-instance-key attack, in case\nof GTSRB we use keep right (class 38) as the Trojan class (key)\nand stop sign (class 14) as the target class and for CIFAR-10\nwe use cat (class 3) as the Trojan class (key) and dog\n(class 5) as the target class.\n"]}
{"id": "2108.09208", "categories": "cs.CV", "paragraphs": ["We finetuned our models on \\(S=3\\)  source domains and tested on the remaining target. We splitted our training sets in 90% train and 10% validation, and used the best performing model on the validation set for the final test, following the validation strategy described in Section dsam:sec:methodology. For preprocessing, we used random zooming with rescaling, horizontal flipping, brightness/contrast/saturation/hue perturbations and normalization using ImageNet's statistics. We used a batch size of 96 (32 images per source domain) and trained using SGD with momentum set at 0.9 and initial learning rate at 0.01 and 0.007 for ResNet's and AlexNet's experiments respectively. We considered an epoch as the minimum number of steps necessary to iterate over the largest source domain and we trained our models for 30 epochs, scaling the learning rate by a factor of 0.2 every 10 epochs. We used the same setup to train our ResNet-18 Deep All baselines. We repeated each experiment 5 times, averaging the results.\n", "For the classification model \\(C\\)  we use AlexNet and ResNet18 backbones. Specifically, Baseline, Rotation and Mixup are trained using SGD with \\(0.9\\)  momentum for \\(30k\\)  iterations. We set the batch size to 32 images per source domain: since in all the testbed there are three source domains each data batch contains 96 images. The learning rate and the weigh decay are respectively fixed to \\(0.001\\)  and \\(0.0001\\) . Regarding the hyperparameters of the individual algorithms, we empirically set the Rotation auxiliary weight to \\(\\sigma = 0.5\\)  and for Mixup \\(\\gamma = 0.4\\) .\n"]}
{"id": "2110.01269", "categories": "cs.CV", "paragraphs": ["We evaluate our method on real (indoor, outdoor) and synthetic datasets. The indoor dataset is 3DMatch [44]. We use the standard train/test splits and the procedure of [6], [9], [10], [7] to generate pairs of scans with at least 30% of overlap for training and testing. During training, as in [6], we apply data augmentation using random rotations in \\([0^{\\circ }, 360^{\\circ })\\)  around a random axis, and random scalings in \\([0.8, 1.2]\\) . For the experiments on outdoor data, we use the KITTI odometry dataset [13] and follow the same protocol as [6]: GPS-IMU is used to create pairs of scans that are at least 10m apart; the ground-truth transformation is computed using GPS followed by ICP. Unlike in [6], we do not use data augmentation during training on KITTI. For synthetic data, we use ModelNet40 [40] and follow the setup of [38] to simulate partial registration problems.\n", "All models are trained using AdamW [23], with a weight decay of \\(0.001\\) , a batch size of 1, and a learning rate of \\(0.001\\) . On 3Dmatch and KITTI, the models are trained for 100 epochs with a learning rate divided by 10 after 60 and 80 epochs. On ModelNet40, it is sufficient to train the models for 10 epochs (with a learning rate divided by 10 after 6 and 8 epochs) to observe convergence. All results are reported using the models obtained at the last epoch. The temperature \\(s\\)  in (REF ) is set to \\(s=0.03\\) .\n"]}
{"id": "2106.06770", "categories": "cs.LG cs.AI", "paragraphs": ["In terms of models, all our experiments use the same three models: A multilayer perceptron (MLP) with two hidden layers of 100 neurons each, the standard LeNet5 from [27], and the standard ResNet18\u00a0[28]. We used a single V100 GPU to train all models, resulting in training times which oscillated between 5 minutes for the MLP, to around 40 minutes for the ResNet18.\n"]}
{"id": "2109.01316", "categories": "cs.CV", "paragraphs": ["Multi scale: 3 random variables(marked as \\(\\alpha \\) ,\\(\\beta _1\\) ,\\(\\beta _2\\) ) are used to determine the resize scale. \\(\\alpha \\)  is first uniformly sampled from range (1.0,2.0), and with a 50% probability to take its reciprocal.\\(\\beta _1\\)  and \\(\\beta _2\\)  are both uniformly sampled from range (-0.2,0.2). An image of size (h,w) is then resized to (h*\\(\\alpha \\) *(1+\\(\\beta _1\\) ),w*\\(\\alpha \\) *(1+\\(\\beta _2\\) )).\n", "Dataset used, training schedule and loss function are different among 2 parts. In the first part, we only use VSPW training set. An AdamW optimizer is used, with betas=(0.9,0.999).Weight decay of AdamW is set to 0.02. The model is trained for 160k iterations in this part. The learning rate follows a linear schedule and warm-up is applied. To be specific, the learning rate of backbone grow from 0 to 6e-6 in the first 1500 iterations, and gradually reduce to 0 in the rest iterations, learning rate is always changing in a linear way.The learning rate of decoder(OCRNet) is always 10 times of the backbone's learning rate. A pixel-distribution-based loss function(described in Sec REF ) is used.\n", "In the second part, we aim to boost the model's performance on some classes with low IoU, so we add COCO dataset for training. Still with AdamW optimizer, The model is trained for 40k iterations in this part. Without warm-up, both backbone and decoder take the learning rate that reduce from 1e-5 to 0 in a linear way. Further more, we deepen the model, with the depth of 4 stages added to (4,6,20,4). A confusion-matrix-based loss function(described in Sec REF ) is used.\n", "    We get our baseline model VOLO-D5 architecture and pretrained models from https://github.com/sail-sg/volo. VOLO-D5 Model is trained with AdamW optimizer with default betas (0.95, 0.999) and uses imagenet pretrained model.The initial learning rate is set to 6e-5 and decreased by polynomial decay.\n"]}
{"id": "2107.09840", "categories": "cs.CL", "paragraphs": ["Our model is based on the multilingual BERT (mBERT)\u00a0[5] implemented in GluonNLP\u00a0[9]. As in previous work\u00a0[5], [39], we tokenize the input sentences using WordPiece, concatenate them, feed the sequence to BERT, and use the hidden representation of the first token\u00a0(\\([CLS]\\) ) for classification. The final output is computed by applying a linear projection and a softmax layer to the hidden representation. We use a dropout rate of\u00a0\\(0.1\\)  on the final encoder layer and fix the embedding layer during fine-tuning. Following [21], we fine-tune mBERT by\n[1)]\n"]}
{"id": "2110.10864", "categories": "cs.CV", "paragraphs": ["We use the SGD optimizer with Nesterov momentum\u00a0[44] to fine-tune VGG-16 and ResNet-38/56/164, where the momentum is set as 0.9.\nThe fine-tuning process takes 200 epochs with a batch size of 128, and the weight decay is set to be 1e-3.\nWe augment the training samples with a standard data augmentation scheme\u00a0[14].\nThe learning rates of VGG-16 and ResNet-38/56/164 are started as 0.006 and 0.05,\nand multiplied by 0.28 and 0.14 at 40% and 80% of the total number of epochs.\nOur training codes are implemented in PyTorch\u00a0.\n"]}
{"id": "2112.03126", "categories": "cs.CV cs.LG", "paragraphs": ["The ensemble of MLPs consists of 10 independent models. Each MLP is trained for \\({\\sim }4\\)  epochs using the Adam optimizer\u00a0[14] with \\(0.001\\)  learning rate. The batch size is 64. This setting is used for all methods and datasets.\n", "MLP architecture. We adopt the MLP architecture from [35]. Specifically, we use MLPs with two hidden layers with ReLU nonlinearity and batch normalization. The sizes of hidden layers are 128 and 32 for datasets with a number of classes less than 30, and 256 and 128 for others.\n"]}
{"id": "2112.02990", "categories": "cs.CV", "paragraphs": ["The 3D and 4D sparse U-Nets are implemented with MinkowskiEngine\u00a0[4] using 2cm voxel size for 3D and 5cm voxel size for 4D.\nFor pre-training we consider only geometry information from the scene-object sequence augmentations.\nWe use an SGD optimizer with initial learning rate 0.25 and a batch-size of 12. The learning rate is decreased by a factor of 0.99 every 1000 steps.\nWe train for 50K steps until convergence.\n"]}
{"id": "2104.11178", "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "paragraphs": ["We pre-train VATT from scratch using Adam\u00a0[45] with an initial learning rate of 1\\(e\\) -4, 10k warmup steps, 500k steps in total, a batch size of 2048, and a quarter-period cosine schedule to anneal the learning rate from 1\\(e\\) -4 to 5\\(e\\) -5. In the exploration experiments, we use a batch size of 512 while keeping the rest of the training parameters the same. Our pipeline is implemented in Tensorflow (v2.4), and our models are trained for 3 days using 256 TPUs (v3).\n"]}
{"id": "2104.01148", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["We report the hyperparameters used for ObSuRF in tab:hyperparams. In addition to the\narchitectural parameters introduced above, we note the importance of the standard deviation of the\ncolor distribution \\(\\sigma _C\\)  (eq:colordist) for tuning the relative importance of\ncolor compared to depth. We also report how we ramp up the overlap loss \\(\\mathcal {L}_O\\) :\nFrom the beginning of training to the iteration at which we start the ramp up, we set \\(k_O = 0\\) .\nDuring the ramp up period, we linearly increase \\(k_O\\)  until it reaches the maximum value\n\\(\\hat{k}_O\\) .\nWe train using the Adam optimizer with default parameters, and an initial learning rate of\n\\(4e-4\\) . We reduce the learning rate by a factor of \\(0.5\\)  every 100k iterations.\nFinally, we note that for the 3D models, we used gradient norm clipping during training, i.e.,\nat each iteration, we determine the L2 norm of the gradients of our model as if they would form a single vector.\nIf this norm exceeds 1, we divide all gradients by that norm. When training on MultiShapeNet, we find\nthat very rare, extremely strong gradients can derail training. We therefore skip all training steps with\nnorm greater than 1000 entirely.\n"]}
{"id": "2104.01037", "categories": "cs.CL cs.LG", "paragraphs": ["We initialize the Transformer with CamemBERT [18] weights for DEFT and BioBERT [14] for GENIA unless mentioned otherwise, and the remaining parameters using the method of [5]. Dropout [27] is applied with a probability of 0.25 everywhere. We optimize the parameters by backpropagation with Adam [11] without weight decay, over 40 epochs for DEFT and 10 epochs for GENIA. We use two learning steps: one for the Transformer weights, initialized at \\(4\\times 10^{-5}\\) , and one for the rest of the model, initialized at \\(9\\times 10^{-3}\\) . The learning rate follows a linear decay schedule with a warmup for 10% of the steps. We insert the tag embeddings in the Transformers at layer \\(L_{tag} = 6\\)  for BERT with 12 layers and 19 for BERT with 24 layers. On an Nvidia K80 GPU graphics card, learning on 100\u00a0documents takes about 20 minutes.\n"]}
{"id": "2109.04014", "categories": "cs.CL", "paragraphs": ["Our neural retrievers were trained on eight Nvidia RTX8000 GPUs, where we set the training epoch to be 30, learning rate (lr) be 1e-5, batch size (bs) be 64, gradient accumulation step (gas) be 4.\nAll the readers were performed at four GTX1080 and V100 NVIDIA GPUs. For both Image-DPR and Caption-DPR, In CReader, we set the training epoch as 3, lr as 2e-5, and batch-size as 16. In EReader, we set the training epoch as 3, lr as 1e-5, batch-size as 4, and gradient accumulation as 4.\n"]}
{"id": "2109.04002", "categories": "cs.CL", "paragraphs": ["We use the Adam optimizer [13] with \\(\\beta _1 = \\text{0.9}\\) , \\(\\beta _2 = \\text{0.98}\\)  to optimize the model.\nFurther, the same learning rate schedule as [31] is used, i.e., linearly increase the learning rate for 4000 steps to 2e-4 and decay proportionally to the inverse square root of the step number.\nWe accumulate the batch size to 9,600 and adopt half-precision training implemented by apexhttps://github.com/NVIDIA/apex for faster convergence [23].\nFor regularization, we also use a dropout [28] \\(p = \\text{0.3}\\)  and a label smoothing [30] \\(\\epsilon _{ls} = \\text{0.1}\\) .\nAs for our approach, we sample 256 candidates from each languages' development corpora every 100 steps to calculate the Self-evaluated Competence \\(c\\)  for each language and HRLs-evaluated Competence \\(\\hat{c}\\)  for each LRL.\n"]}
{"id": "2102.11055", "categories": "cs.LG", "paragraphs": ["Exploration. The training process starts after some number of time steps (1000 steps for Reacher-v2 and 10000 steps for the other environments), and we use a purely exploratory policy in this initial phase to collect samples for the replay buffer for all the algorithms. During training, Gaussian noise is added to each action for exploration for the neural cases. In the tabular case, we use the \\(\\epsilon \\) -greedy policy as the behavior policy instead.\n"]}
{"id": "2107.06916", "categories": "cs.CV cs.AI", "paragraphs": ["We train our compact CNN models from scratch using the SGD optimizer with a momentum of 0.9 and the batch size is set to 256. On CIFAR-10, we train the compact CNNs for a total of 300 epochs and the weight decay is set to 5\\(\\times \\) 10\\(^{\\text{-4}}\\) . The learning rate is initially set to 0.1, and then divided by 10 at the training points of 150 and 225 epochs. On ILSVRC-2012, 90 epochs are given to train compact ResNet-50 with the weight decay set to 1\\(\\times \\) 10\\(^\\text{-4}\\) , and the initial learning rate is set to 0.1, which is then multiplied by 0.1 at the points of 30 and 60 training epochs. Besides, following\u00a0[38], [36], [29],\nwe also consider the cosine scheduler\u00a0[60] to adjust the learning rate for ResNet-50 with the weight decay set to 1\\(\\times \\) 10\\(^\\text{-4}\\) . The initial learning rate is set to 1\\(\\times \\) 10\\(^\\text{-2}\\)  for ResNet-50.\n"]}
{"id": "2110.04291", "categories": "cs.CL cs.AI cs.IR cs.LG cs.LO", "paragraphs": ["All the models are implemented in Pytorch [33]. We use the\nHuggingFace library [44] to train BERT and ALBERT\nmodels. Both models are initialized with the weights of their \"base\" versions\nfor fair comparison. The transformer network in GlobalPair model is made up of 2\ntransformer blocks, with hidden size of 768, feed-forward intermediate layer\nsize 3072 and 12 attention heads. We use Adam optimizer and a batch size of 2\nfor all three datasets. For SIND and ROCStories, learning rate used is\n\\(5*10^{-6}\\)  and decay factor is \\(0.2\\)  per epoch. For ACL, we found the training\nto be quite stochastic but the final results reported are for an initial\nlearning rate of \\(1*10^{-5}\\) , decayed to \\(1*10^{-6}\\)  after 1st epoch and\nconstant afterwards. We use a beam of size 64.\n"]}
{"id": "2109.04843", "categories": "cs.CV", "paragraphs": ["We implemented the model using PyTorch 1.2 and trained it on four NVIDIA Tesla P100 GPUs. Our batches contained eight clips, each comprising six frames at 520\u00d7520 resolution. We used the Adam optimizer with a learning rate of 0.001, decreasing the rate by a factor of 0.7 after each epoch. Our test models generally converged after seven epochs, or about 9.5 hours of training.\n"]}
{"id": "2109.05872", "categories": "cs.LG cs.CR cs.DC", "paragraphs": ["By default, we assume there are \\(n = 50\\)  clients in total for each task, 20% of which are Byzantine nodes with fixed attack method, and the training data are IID among clients. To verify the resilience and robustness, we will also evaluate the impact of different fractions of malicious clients for different attacks and defenses. Furthermore, our approach will also be evaluated in non-IID settings. In all experiments, we set the lower and upper bounds of gradient norm as \\(L = 0.1\\)  and \\(R = 3.0\\) , and randomly select 10% of coordinates to compute sign statistics in our SignGuard-based algorithms. Each training procedure is run for 60 epochs for MNIST/Fashion-MNIST/AG-News and 160 epochs for CIFAR-10, and local iteration is always set to 1. We employ momentum in PS side and the momentum parameter is set to 0.9, and weight decay is set to 0.0005. More details on some key hyper-parameters are described in Appendix\u00a0 REF\n"]}
{"id": "2106.02637", "categories": "cs.CV", "paragraphs": ["Architecture. Through the introduction of object proposals, the architectural discrepancy is reduced between pretraining and downstream detection finetuning. Mask R-CNN\u00a0[16] is a commonly adopted framework to evaluate transfer performance. To demonstrate the extensibility and flexibility of SoCo, we provide details of SoCo alignment for the detection architectures R50-FPN and R50-C4. SoCo-R50-FPN: ResNet-50\u00a0[31] with FPN\u00a0[14] is used as the image-level feature encoder. RoIAlign\u00a0[16] is then used to extract RoI features on feature maps \\(\\lbrace P_2, P_3, P_4, P_5\\rbrace \\)  with a stride of \\(\\lbrace 4, 8, 16, 32\\rbrace \\) . According to the image areas of object proposals, each RoI feature is then transformed to an object-level representation by the head network as in Mask R-CNN. SoCo-R50-C4: on the standard ResNet-50 architecture, we insert the RoI operation on the output of the 4-th residual block. The entire 5-th residual block is treated as the head network to encode object-level features. Both the projection network and prediction network are 2-layer MLPs which consist of a linear layer with output size 4096 followed by batch normalization\u00a0[32], rectified linear units (ReLU)\u00a0[33], and a final linear layer with output dimension 256.\n", "Dataset. We adopt the widely used ImageNet\u00a0[0] which consists of \\(\\sim \\) 1.28 million images for self-supervised pretraining.\n", "Optimization. We use a 100-epoch training schedule in all the ablation studies and report the results of 100-epochs and 400-epochs in the comparisons with state-of-the-art methods. We use the LARS optimizer\u00a0[34] with a cosine decay learning rate schedule\u00a0[35] and a warm-up period of 10 epochs. The base learning rate \\(lr_{\\text{base}}\\)  is set to \\(1.0\\)  and is scaled linearly\u00a0[36] with the batch size (\\(lr\\)  = \\(lr_{\\text{base}} \\times \\)  BatchSize\\(/256\\) ). The weight decay is set to \\(1.0 \\times \\text{e}^{-5}\\) . The total batch size is set to 2048 over 16 Nvidia V100 GPUs. For the update of the target network, following\u00a0[2], the momentum coefficient \\(\\tau \\)  starts from \\(0.99\\)  and is increased to 1 during training. Synchronized batch normalization is enabled.\n"]}
{"id": "2108.07887", "categories": "cs.LG cs.RO", "paragraphs": ["We base our training setup on CHER\u00a0[7]. We train all agents on minibatches of size \\(k = 64\\)  for 50 epochs using MPI for parallelisation over 16 CPU cores; each epoch consists of 1600 (\\(16 \\times 100\\) ) episodes, with evaluation over 160 (\\(16 \\times 10\\) ) episodes at the end of each epoch. Remaining hyperparameters for the baselines are taken from the original work\u00a0[0], [7], [30]. Our method, DTGSH, uses partial trajectories of length \\(b = 2\\)  and \\(m = 100\\)  as the number of candidate goals.\n"]}
{"id": "2108.07421", "categories": "cs.LG", "paragraphs": ["It is straightforward to the optimal solution for (REF ), which is \\(\\mathbf {w}^b = \\text{sign}({\\mathbf {\\widetilde{w}}})\\) . To obtain the optimal solution for the scaling parameter \\(\\alpha \\) , we take the derivative of (REF ) with respective to \\(\\alpha \\) . It is \\( -2 \\mathbf {{\\widetilde{w}}}^T\\mathbf {w}^b + 2\\alpha p\\) . By setting it to 0, we will get \\(\\alpha = \\frac{\\mathbf {{\\widetilde{w}}}^T\\mathbf {w}^b}{p}\\) . Since \\(\\mathbf {w}^b = \\text{sign}({\\mathbf {\\widetilde{w}}})\\) , the scaling parameter \\(\\alpha \\)  will be\n\\(\\alpha = \\frac{\\sum _j |\\widetilde{w}_j|}{p}.\\) \n", "The optimal binary \\(\\mathbf {w}^b\\)  is obtained by \\(\\mathbf {w}^b = \\text{sign}({\\mathbf {\\widetilde{w}}})\\) , where sign() is the element-wise sign function which return 1 if the element is larger or equal than zero and return \\(-1\\)  otherwise. Similarly, \\(\\mathbf {V}^b\\)  can be obtained by \\(\\mathbf {V}^b = \\text{sign}({\\mathbf {\\widetilde{V}}})\\) .\nSince the sign() function are not differentiable, STE estimates the gradients with respect to \\(\\mathbf {\\widetilde{w}}\\)  and \\(\\mathbf {\\widetilde{V}}\\)  as if the non-differentiable function sign() is not present. In other word, STE will simply estimate \\(\\frac{\\partial \\text{sign}(\\tilde{w}_j)}{\\partial \\tilde{w}_j} \\)  as \\(\\frac{\\partial \\tilde{w}_j}{\\partial \\tilde{w}_j} = 1\\) . In practice, we also employ the gradient clipping as in\u00a0[9]. Then, the gradient for the non-differentiable sign function is\n\\(\\frac{\\partial \\text{sign}(\\tilde{w}_j)}{\\partial \\tilde{w}_j} = 1_{|\\tilde{w}_j|\\le 1}.\\) \n"]}
{"id": "2112.12252", "categories": "cs.CV", "paragraphs": ["Yolov5 [61] is a state of the art implementation of the Yolo object detection model implemented with multiple improvements to the Yolo framework that have been found in recent years. In this work, we used the unmodified YOLOv5m6 implementation of Yolov5 in release v5.0 [65] with an image size of 1280x1280px and a batchsize of 48. Unless otherwise specified, we used the provided weights pre-trained on COCO [62].\n", "Furthermore, as a two-stage detector we take the best performing single-model (no ensemble) on VisDrone from the workshop report [4] (DE-FPN), i.e. a Faster R-CNN (F.R.) with a ResNeXt-101 64-4d [66] backbone (removing P6), which is trained using color jitter and random image cropping. The anchor sizes and strides are decreased to (16, 32, 64, 128, 256) and (4, 8, 16, 32, 64).\n"]}
{"id": "2106.14851", "categories": "cs.LG cs.CR", "paragraphs": ["In each of our experiments, we randomly choose one user from the 530 FaceScrub identities to be the attacker. We perturb 100% of the training pictures of that user (70% of all pictures) with the chosen attack (Fawkes v0.3, Fawkes v1.0, or LowKey). The training set for the model trainer contains these perturbed pictures, as well as the training pictures of all other 529 FaceScrub users.\n", "For linear fine-tuning and end-to-end fine-tuning, we add a 530-class linear layer on top of a pre-trained feature extractor, and train either only the linear layer, or the entire model. For linear fine-tuning, we train a logistic regression model using sklearn. To fine-tune the entire model, we minimize the cross-entropy loss for 500 steps with a batch size of 32 using AdaDelta with learning rate \\(\\eta =1\\) . We perform no data augmentation during training.\n"]}
{"id": "2105.06977", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["We follow the Transformer base [42] configuration in all our experiments, with \\(N = 6\\)  encoder and decoder layers, \\(h = 8\\)  attention heads, hidden size \\(d_\\text{model} = 512\\)  and feedforward size \\(d_\\text{ff} = 2048\\) . We use the learning rate schedule and regularization described in [42]. We train using the Adam optimizer [15] with \\(\\beta _1 = 0.9, \\beta _2 = 0.98\\) .\n"]}
{"id": "2108.01208", "categories": "cs.CL cs.SD eess.AS", "paragraphs": ["The baseline pointer network has 1 encoder and decoder BLSTM layer with 128 hidden size. We train it using learning rate 0.0003 with Adam and batch_size 32. For our proposed model we employ 2 encoders (one for each turn) and 1 decoder with the same dimension, but modified attention head. We train it with batch size 128 and learning rate 0.0001 with Adam. We use pretrained ANEs that are fixed during training.\n"]}
{"id": "2109.09233", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["We train the models by applying 5-Fold Cross ValidationWe experiment also 10-Fold, but the models show worse performance in the test set., with the epochs of 5, learning rate as 1e-5, batch size as 2. We use the GPU of the Google Colabhttps://colab.research.google.com/ as an environment for training the models. We use a fixed random seed of 1234 to ensure reproducible results. The official results are obtained by a TIRA machine\u00a0[25].\n"]}
{"id": "2109.11375", "categories": "cs.LG math.PR", "paragraphs": ["We will approximate the posterior distribution \\(P_{X|Y=y}\\)  for arbitrary observations \\(y\\)  using a conditional SNF \\((X_0,...,X_T)\\)  with \\(T=6\\)  layers, where the layers themselves are defined as follows:\n", "For \\(t=1,4\\) , we use a deterministic layer \\(\\mathcal {K}_t(y,x,A)=\\delta _{\\mathcal {T}_t(y,x)}(A)\\) .\nHere, \\(\\mathcal {T}_t\\)  is a conditional INN with \\(L=4\\)  layers, where each subnetwork has two hidden layers with 128 neurons.\n", "For \\(t=2,5\\) , the layer \\(\\mathcal {K}_t\\)  consists of 3 Langevin steps, i.e., \\(\\mathcal {K}_t\\)  is defined by\n\\(\\mathcal {K}_t(y,\\cdot ,\\cdot )=\\underbrace{\\tilde{\\mathcal {K}_t}(y,\\cdot ,\\cdot )\\circ \\cdots \\circ \\tilde{\\mathcal {K}_t}(y,\\cdot ,\\cdot )}_{3\\text{ times}},\\) \n", "where \\(\\tilde{\\mathcal {K}_t}\\)  is the kernel from (REF ). In the Langevin steps we set \\(a_1 = 1e-6\\)  and \\(a_2^2 = 2e-6\\) .\n", "For layer \\(t = 3,6\\)  we take layers \\(\\mathcal {K}_t\\)  with 3 MCMC steps where the Markov kernel \\(Q_t\\)  is given by (REF ).\n", "Here we choose the interpolating densities \\(p_t^y(x)\\)  for \\(t=2,3\\)  as\n\\(p_t^y(x)=c_y\\,(p_Z(x)p_{X|Y=y}(x))^{1/2},\\) \n", "We compare the results of the conditional SNF with a conditional INN with \\(L=8\\)  layers, where each subnetwork has two hidden layers with 128 neurons.\nNote that the conditional INN and the conditional SNF have the same number of parameters.\nWe train both networks with the Adam optimizer [26] with a batch size of 6400 for 4000 steps and a learning rate of \\(10^{-3}\\)  for the loss function (REF ) with \\(\\lambda =0\\) .\n", "We train a conditional SNF with \\(T=8\\)  layers similarly to the previous example.\nThe layers \\(\\mathcal {K}_t\\)  for \\(t=1,3,5,7\\)  are deterministic layers with\nconditional INNs with \\(L=1\\)  layers, where the subnetworks has two hidden\nlayers with 64 neurons in each hidden layer.\nThe layers \\(\\mathcal {K}_t\\)  for \\(t=2,4,6,8\\)  consist of 10 MCMC steps using the kernel\n\\(Q_t\\)  as defined in (REF ).\nHere, we set \\(\\sigma =0.4\\) .\n", "As a comparison, we also implement a conditional INN with \\(L=4\\)  layers, where each subnetwork has two hidden layers with 64 neurons in each hidden layer.\nNote that this conditional INN has exactly the same number of parameters\nas the conditional SNF.\nWe train both networks using the Adam optimizer [26] with a batch size of 1600 and a learning rate of \\(10^{-3}\\)  for the loss function (REF ) with \\(\\lambda =0\\) .\nFor the SNF, we run 40 epochs, which takes approximately 50 seconds.\nSince for the conditional INN it takes longer until the loss saturates, we train the conditional INN for 5000 epochs, which takes approximately 8 minutes.\nEach epoch consists of 8 steps of the Adam optimizer.\n"]}
{"id": "2109.11442", "categories": "cs.CL", "paragraphs": ["In order to get training samples\nwhose structure mimics that of data observed in the real world, we ensured that our data is segmented by sentence (finishing by a PUNfrt) or by line (in manuscripts without punctuation). To do so, we transform the dataset using Protogenie [11] which handles some form of normalisation: we normalise Roman numerals into Arabic numerals to reduce the complexity of the data, as well as the number of allowed numbers and we split the complex morphology into several simple categories (Case, Tense, etc.). The full corpus is then split into 3 different parts, for training, development and testing, with a 80/10/10% ratio.\n", "All models shared most of the same parameters: they used a single linear layer, LSTM cells for the hidden network, a character embedding using RNN, a dropout of 0.32, learning rate of 0.0049, patience for the learning rate evolution of 2, a patience for early stopping of 5. We provide the configuration on our repository. We used the Ranger optimizer that has shown less variation in training with better scoresOn this topic, see the discussion between TC and Enrique Manjavacas at https://web.archive.org/web/20210914113014/https://github.com/emanjavacas/pie/issues/76..\n", "\nThe Character Embedding size, cemb_size, could be of 100, 150, 200 or 300\n\nThe number of layers for the Character Embeddings Encoder, cemb_layers, could be 1 or 2.\n\nThe size of the hidden layer, that encodes most of the context, was a value in the set \\(150, 200, 250, 300, 350\\) . We added a variation only for the lemma task at 170.\n\n"]}
{"id": "2107.03069", "categories": "cs.CL cs.SD eess.AS", "paragraphs": ["To ensure a reliable comparison, we perform all ASR and ST experiments under the same conditions and parameters. Specifically, we try to use the same parameters as in the implementation by [19], when possible.\nIn ASR trainings we use 4 CPUs and 2 workers to load the data. We fixed a maximum of 20000 tokens per batch. We used Adam optimizer and a learning rate of \\(1 \\cdot 10^{-3}\\)  with an inverse square root scheduler. We applied a warm-up for the first 10000 updates. We clipped the gradient to 10 to avoid exploding gradients. We used label smoothed Cross-entropy as a loss function, with a smoothing factor of 0.1. We used an update frequency of 16, simulating the use of 16 GPUs. We fix a maximum of 100000 updates for every training.\nIn ST trainings we use the same parameters as for ASR, but for the learning rate, that is \\(2 \\cdot 10^{-3}\\) , as done in [19].\nWe conducted the training of all our experiments in an NVIDIA GeForce RTX 2080 Ti GPU.\n"]}
{"id": "2112.04803", "categories": "cs.CL cs.LG", "paragraphs": ["Training Process: Each configuration of the model architecture is trained using Adam optimizer\u00a0[15] with a learning rate of 0.001, a batch size of 64 for maximum of 20 iterations. We use the 90:10 splits for train and validation splits to find the optimal hyperparameters.\n"]}
{"id": "2101.08122", "categories": "cs.CV eess.IV", "paragraphs": ["The parameters are optimized using the Adam optimization algorithm [25] with the suggested defaults for the hyperparameters (\\(\\beta \\) 1 = 0.9, \\(\\beta \\) 2= 0.999). The training is stopped when the validation loss does not decrease by 1% in between epochs. We use a fixed learning rate of \\(0.001\\)  and weight decay (\\(0.0001\\) ). The \\(\\gamma \\)  parameter in Eq.\u00a0(REF ) is set to 1 experimentally. At each iteration, we sample 5 patch pairs (or triplets for pretext Task 2) from each image to generate 6350 patch pairs per epoch. Data augmentation (90 degrees rotation and horizontal/vertical flips) are applied.\n", "To assess the performance on the pretext tasks, we use the blind test set extracted from \\(U\\) . For pretext Task 1, we assess the success rate in the task itself in percentage, while for Task 2, we consider the value of the loss. We also run the pretext tasks on the 12 images composing OSCD test set to assess domain shifts. Note that no OSCD labels are used at this stage.\n"]}
{"id": "2104.01371", "categories": "cs.CL", "paragraphs": ["Optimization: We used Adam optimizer\u00a0[21] with a linear scheduler that has warm-up steps. The initial learning rate was set to be \\(10^{-3}\\)  for BiMeanVAE and \\(10^{-5}\\)  for Optimus.\n"]}
{"id": "2104.01436", "categories": "cs.CL cs.LG", "paragraphs": ["Hyperparameters such as learning rate and the number of epochs were tuned with the help of validation data with patience parameter set to 4. As we assumed no labeled data was available from target domain, we use the source domain validation set for tuning. labeled data from target domain was used during testing only. Learning rate search space was limited to \\(\\lbrace 10^{-3}\\) , \\(10^{-4}\\) , \\(10^{-5}\\rbrace \\)  values only.\nWe trained BERT on a system with a `Nvidia Tesla P100' GPU with 12GB GPU-RAM, 96GB system RAM and 56 cores. Our model GLEN was trained on CPU only with 32GB RAM and 20 cores. We used 100 as hidden dimension size for all experiments and 300 dimensional GloVe embeddings were used as initial input token-level features. The number of parameters for GLEN was \\(\\approx 664\\) k which is \\(\\approx 165\\)  times smaller than BERT in terms of number of parameters. All results reported for GLEN and BERT are averaged across 3-runs, each corresponding to a different random seed for the experiment while training. Below we present the architecture details of our model GLEN:\n\n"]}
{"id": "2104.01437", "categories": "cs.LG cs.NA math.NA q-fin.CP", "paragraphs": ["where \\(\\bar{c}, \\xi \\)  and \\(\\delta \\)  are related to the SDE parameters as shown in equations ()-(), cf. .\n\n(St,t) = 4St e-t2( 1 - e-t ),\n", "Default training/testing parameters For GBM, the default parameters were \\(\\mu =0.05\\)  and \\(\\sigma =0.2\\) . The CIR parameters were \\(\\bar{S}=0.1,S_t=0.1,\\kappa =0.1, \\gamma \\in \\lbrace 0.1,0.3\\rbrace \\) , where \\(\\gamma =0.1\\)  corresponds to the case where the Feller condition is satisfied (\\(\\delta = 4\\) ) and \\(\\gamma =0.3\\)  to the case where the Feller condition is violated \\((\\delta = 0.44)\\) .\n"]}
{"id": "2102.02274", "categories": "cs.LG cs.AI cs.MA", "paragraphs": ["The training process for both environments uses the distributed actor-critic setup of the Importance Weighted Actor-Learner Architecture [15] (but other RL algorithms are also compatible). Each actor generates trajectories, which are sent to the learner in chunks of 100 environment steps. The learner then optimizes the RL and belief losses jointly. Further details on hyper-parameters used are given in Appendix .\n", "We experiment with the two versions of the Tiger game to evaluate the ability to learn agents' beliefs and use these beliefs to learn the policies via RL. We run these experiments following the same training method and architecture as detailed in Appendix .\nIn the first experiment, we match P2 with an \u201coptimal\u201d P1 in the version of the game with two players. The input to the policy and value functions for P2 is thus composed of samples \\(b_t^1(P2)\\)  from its own belief model (thus, if the model is correct, the policy should be able to learn how to map these samples to the right actions).\nIn the second experiment, we evaluate the second version of Tiger with three players. We match P3 against an optimal P2, and learn the policy and value conditioned on \\(b_t^2(P3)\\) . We observe in Fig. REF  how in both cases, only the agents that learn from \\(K=10\\)  samples can solve the problem, while agents that learn from \\(K=1\\)  samples (i.e. theoretically unable to learn the correct higher order beliefs) catastrophically fail.\n"]}
{"id": "2101.09060", "categories": "cs.CV cs.LG", "paragraphs": ["For the classification model \\(C\\)  we use AlexNet and ResNet18 backbones. Specifically, Baseline, Rotation and Mixup are trained using SGD with \\(0.9\\)  momentum for \\(30k\\)  iterations. We set the batch size to 32 images per source domain: since in all the testbed there are three source domains each data batch contains 96 images. The learning rate and the weigh decay are respectively fixed to \\(0.001\\)  and \\(0.0001\\) . Regarding the hyperparameters of the individual algorithms, we empirically set the Rotation auxiliary weight to \\(\\eta = 0.5\\)  and for Mixup \\(\\gamma = 0.4\\) .\n"]}
{"id": "2110.04544", "categories": "cs.CV cs.CL", "paragraphs": ["The first variant of CLIP-Adapter is adopted by default if not specified, which finetunes the image feature while freezes the classifier weight. In other words, it only implements CLIP-Adapter for the visual adapter.\u00a0The results of other variants that activate text adapter are presented in Section\u00a0REF . We use the same training hyperparameters as CoOp, including a batch size of 32 and a learning rate of \\(1\\times 10^{-5}\\)  for all datasets except for the residual ratio\n\\(\\alpha \\) .\u00a0We perform hyperparameter searching over different value selections of \\(\\alpha \\)  for each dataset and report the best performance among all searching spaces. We use ResNet-50\u00a0[16] as the visual backbone (visual encoder) and BERT\u00a0[22] as classifier weight generator (textual encoder). The hidden embedding dimensionality of both visual and text bottleneck layers is set to 256, which is a quarter of the original embedding dimensionality. In contrast to the learnable continuous prompts in CoOp, simple hand-crafted hard prompts are utilized as the text inputs of CLIP-Adapter, which is the same as CLIP. For generic-category image datasets, such as ImageNet, we adopt \u201ca photo of a {class}\u201d as the hard prompt template. For fine-grained classification datasets, we specify its corresponding domain keyword in the template for a better performance, for instance, \u201ca centered satellite photo of {class}\" for EuroSAT, and similarly for other fine-grained datasets.\n"]}
{"id": "2112.04674", "categories": "cs.CV cs.AI", "paragraphs": ["When pretraining our DualFormer on ImageNet-1K, we mostly follow the settings of DeiT [46] and Swin [34]. To be more specific, we employ an AdamW optimizer [27] for 300 epochs together with a cosine decay learning rate scheduler and 20 epochs of linear warm-up. The batch size is set to 1024, and the initial learning rate is 0.001. To avoid overfitting, a weight decay rate of 0.05 is used in our method. We include most of the augmentation and regularization strategies of [46] in training, except for repeated augmentation and exponential moving average (EMA), which has been verified ineffective in Swin [34].\n", "For DualFormer-B, we also pretrain it on the larger version of ImageNet, i.e., the ImageNet-21K dataset which contains 14.2 million images and 22 thousand classes. Following Swin Transformer [34], we utilize an AdamW optimizer for 100 epochs using a linear decay learning rate scheduler with a 5-epoch linear warm-up strategy. A batch size of 1024, an initial learning rate of 5e-4, and a weight decay of 0.01 are used. We also employ a stochastic depth drop rate 0.2 to improve its generalization ability.\n"]}
{"id": "2111.11113", "categories": "cs.LG", "paragraphs": ["The neural networks were fitted to the training data using similar architectures. For ProNet, we used an FNN-based encoder with two layers (each of size 64) and ReLu as activation function. For ProSeNet, we used an RNN-based encoder with two layers (each of size 64) and tanh as activation function. The FNN baseline had two layers (each of size 64) and ReLu as activation function. The RNN baseline had 2 layers (each of size 64) and tanh as activation function. A linear layer was added to the baseline models to obtain predictions of the right shape.\n", "We trained all neural networks over 400 epochs, using a batch size of 64 for RNN and ProSeNet, and 1024 for FNN and ProNet. For optimization, the Adam algorithm was used with default parameters, learning rate 0.001 and weight decay 0.001. The NLL was used as loss function. For ProNet and ProSeNet, we selected parameters of the diversity regularization \\((d_{\\textrm {min}}, \\lambda _{d})\\)  by performing 3-fold cross-validation over a grid of points in the parameter space \\(\\lbrace 1, 2, 3, 4, 5\\rbrace \\times \\lbrace 0.00001, 0.0001, 0.001, 0.01, 0.1\\rbrace \\) . Note that these parameters were optimized for each combination of prototypes \\(n\\)  and prediction prototypes \\(q\\)  in our experiments. The parameters \\(\\lambda _{c}\\)  and \\(\\lambda _{e}\\)  were set to 0.001, and we performed the projection step, see (REF ), every fifth epoch.\n", "Since we used the full state representation, we could safely make the Markov assumption and estimate the behavior policy using FNN-based models. For ProNet, we used an FNN-based encoder with two layers (each of size 64) and ReLu as activation function. The FNN baseline had two layers (each of size 64) and ReLu as activation function. A linear layer was added to obtain predictions of the right shape. We trained all models over 30 epochs, using a batch size of 128. For optimization, the Adam algorithm was used with default parameters, learning rate 0.001 and weight decay 0.001. Again, we selected parameters of the diversity regularization \\((d_{\\textrm {min}}, \\lambda _{d})\\)  by performing 3-fold cross-validation over a grid of points in the parameter space \\(\\lbrace 1, 2, 3, 4, 5\\rbrace \\times \\lbrace 0.00001, 0.0001, 0.001, 0.01, 0.1\\rbrace \\) . The parameters \\(\\lambda _{c}\\)  and \\(\\lambda _{e}\\)  were set to 0.001, and we performed the projection step every fifth epoch.\n"]}
{"id": "2111.00974", "categories": "cs.LG cs.CL cs.IR", "paragraphs": ["We set hyperparameters following the previous work [20],\neither with or without augmented data.\nFor data augmentation, relation path rules of length 1 to 6 are mined and applied; see Section\u00a0REF ,\\((1)\\)  and Appendix\u00a0.\nThe threshold values for data augmentation are set by grid search over \\(\\textit {topN}\\in \\lbrace 5,50\\rbrace \\)  and \\(\\textit {confTh}\\in \\lbrace 0, 0.6\\rbrace \\) .\nThese threshold values and early stopping of the training are determined by the MRR on the validation data.\nSee Appendix\u00a0 for the tuned values of hyperparameters and the selected thresholds.\n", "where \\(E\\)  is the set of all entities in the KG, \\(f\\)  represents the score function of the model, and \\(\\sigma \\)  is the sigmoid function.\nThe label \\(y_{e^{\\prime }} = 1\\)  if \\((e_h,r,e^{\\prime })\\)  is originally in the KG.\nIf \\((e_h,r,e^{\\prime })\\)  is an augmented triplet, then \\(y_{e^{\\prime }}\\)  is set to its weight (see Section\u00a0REF ,\\((2)\\) ).\nIf the triplet is neither in the KG nor an augmented triplet, then \\(y_{e^{\\prime }} = 0\\) .\n"]}
{"id": "2106.07091", "categories": "cs.CV cs.AI cs.LG cs.NE", "paragraphs": ["For each of the experiments, we used a CNN as the base network, with different numbers of layers depending the dataset image sizes. Figure\u00a0REF  shows the architectures of the base networks. We construct the other models from the base networks as discussed in the main paper. For the On and Off Center convolutions in OOCS-CNNs, we used kernels of size \\(5\\times 5\\)  for Imagenet and Norb datasets. We used smaller kernels of size \\(3\\times 3\\)  for the MNIST dataset, since the images are of smaller size. We calculated the On and Off resposes from the inputs and directly fed their summation to the network.\n{FIGURE}", "We had batch sizes of 64 in all experiments. We used Adam optimiser\u00a0[9] for experiments on Imagenet and Norb, with a learning rate of \\(10^{-4}\\) . In the experiment on Imagenet, we decreased the learning rate to half after 10 epochs which was mainly in favour of the baselines. In the Imagenet experiments with ResNet-34 we use SGD optimiser and start with a learning rate of 0.1, which we decay by a factor of 0.1 every 20 epochs and we trained the networks for 60 epochs. For scaling the gradient descent steps, we use a Nesterov-momentum of 0.9.\n{FIGURE}{FIGURE}"]}
{"id": "2106.07020", "categories": "cs.CV eess.IV", "paragraphs": ["The training of all the neural network models was performed at a PC with GTX-1080Ti GPUs, using Keras\u00a0[4] with a Tensorflow\u00a0[5] backend. For a simple regression model, the following training parameters were set. An optimizer RMSprop was chosen with a learning rate of \\(0.001\\) , which was reduced with patience 5. There were 20 epochs with 100 steps per epoch. The batch size was specified to be 30 with an image size of \\(256*256\\)  pixels. A model based on GAN training parameters was as follows. Loss functions were chosen binary cross-entropy and MAE. The optimizer was Adam. The batch size and image size were the same as for the simple model. The models were trained for 600 epochs, 100 steps per epoch, and the batch size of 30.\n"]}
{"id": "2105.02788", "categories": "cs.CV cs.GR cs.LG", "paragraphs": ["We first center and normalize the input mesh coordinates to fall within \\([-1, 1]\\)  along the widest dimension (preserving the aspect ratio).\nFor Convolutional Occupancy Networks, we uniformly sample 5 million surface points for the input point cloud and apply Gaussian noise with standard deviation of 0.05. For the ground truth occupancy points, we sample 4.5 million surface points and apply Gaussian noise with a standard deviation of 0.01 to help the model learn better surface details.\nWe augment this ground truth dataset with an additional 500,000 points sampled within the bounding box of the mesh.\nThe ground truth occupancy is calculated for each point in the dataset and used to train the model.\nFor SIREN we only need the ground truth occupancy points, but we sample 20 million near surface points, and 20 million points from the bounding box of the mesh.\n"]}
{"id": "2104.10093", "categories": "cs.LG cs.AI cs.CV stat.ML", "paragraphs": ["Neural network training is always done using the Adam-optimizer\u00a0[23] with default settings (\u00a0\\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\) ). Depending on the benchmark, the learning rate is 0.001 (MNIST and CIFAR-10) or 0.0001 (CIFAR-100 and CORe50).\n"]}
{"id": "2103.02691", "categories": "cs.CL cs.LG", "paragraphs": ["For the intent classification, we employ the Bert-Base modelhttps://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip with 12 Transformer layers, 768 hidden states, and 12 self-attention heads. The size of the hidden units in uni-direction LSTM is 512, inner-attention hidden layer \\(d_w\\)  is set to 600, and the number of attention head \\(r\\)  is 5. Furthermore, we use Adam optimizer with default values of \\(\\beta _1 = 0.9\\)  and \\(\\beta _2 = 0.99 \\) , and a learning rate of \\(1e-4\\)  and \\(2e-5\\)  for training the BiLSTM and fine-tuning the whole model respectively.\nEach update is computed through a batch size of 8 or 16 training examples and the number of epochs per batch are 32, 25, 16, and 8 epochs for 10-shot, 20-shot, 30-shot, and full-data settings, respectively. We apply the dropout as a regularization technique for our model to avoid over-fitting. We apply dropout after output of each BiLSTM layer and output of each sub-layer BERT encoder layers. We set the dropout rate as \\(0.1\\)  for all dropout layers.\n", "For training argument similarity, we used Adam optimizer with a learning rate of \\(2e-5\\)  and a batch size of 16 training samples. Furthermore, the model uses the pre-computed 300-dimensional word embeddings ConceptNet Numberbatch. The number of hidden units in uni-direction LSTM is 512 and the number of attention heads is 5. The model is trained for 8 epochs.\n"]}
{"id": "2102.06697", "categories": "cs.CV cs.LG", "paragraphs": ["There are not much hyper-parameters need to be tuned in the training process of qKC. Empirically we have noticed that the design of learning rate and batch size will affect the training process of qKC. For the Adam optimiser, we use the default configuration: 1) the coefficients used for computing running mean of gradient and its square are set as 0.9 and 0.999, respectively, 2) \\(eps\\)  is set as 1e-8 for numerical stability and 3) no L2 penalty term.\n"]}
{"id": "2103.16337", "categories": "cs.LG", "paragraphs": ["Using the dataset and the architecture described above, two GNNs are trained using each of the techniques described in \u00a7\u00a0REF  and \u00a7\u00a0REF , with \\(k=4\\) , \\(p=2\\) , \\(q=1\\) , \\(\\lambda = 0.05\\)  and \\(\\varepsilon =10^{-8}\\) .\nTraining is performed using Adam algorithm with 60 epochs and a fixed learning rate of \\(0.01\\) .\nRegarding the model distillation, rather than re-evaluating the MPN \\(F\\)  in (REF ) for every sample \\(X\\)  at each epoch, we precompute \\(F(X,\\omega (X))\\)  to save the computation time.\n"]}
{"id": "2103.16364", "categories": "cs.CV", "paragraphs": ["To conduct a fair comparison with state-of-the-art methods, we use an ImageNet [22] pre-trained ResNet50 [11] as our backbone network. We report results of IBN-ResNet50 [20] in Appendix\u00a0.\nAn Adam optimizer with a weight decay rate of 0.0005 is used to optimize our networks. The learning rate is set to 0.00035 with a warm-up scheme in the first 10 epochs. No learning rate decay is used in the training. The momentum encoder is updated with a momentum coefficient \\(\\alpha =0.999\\) . We renew pseudo labels every 400 iterations and repeat this process for 40 epochs. We use a batchsize of 32 where \\(N_P=8\\)  and \\(N_K=4\\) . We set \\(\\tau _{a}=0.5\\) , \\(\\tau _{c}=0.07\\)  and \\(N_{neg}=50\\)  in the proxy contrastive baseline. Our network is trained on 4 Nvidia 1080 GPUs under Pytorch framework. The total training time is around 2 hours on Market-1501. After training, only the momentum encoder is used for the inference.\n"]}
{"id": "2106.04493", "categories": "cs.LG cs.AI", "paragraphs": ["To train the CVNet used in the experiments, we employ 3 cerebellar quantization functions and use a memory size \\(A\\)  of 20000. The embedding dimension \\(m\\)  is chosen to be 50.\nFollowing the cerebellar embedding layer are fully connected layers having [32, 128, 32] hidden units with ReLU activations.\nWe maintain a target network which is updated every 100K steps. We use a batch size of 32 and run training for 20 epochs, with each epoch being one pass through the whole dataset. We apply Adam optimizer with a constant step size \\(3e^{-4}\\) . The Lipschitz regularization parameter \\(\\lambda \\)  is chosen to be \\(1e^{-4}\\)  since we find that a small \\(\\lambda \\)  is already quite effective at bounding the Lipschitz, as demonstrated in Figure\u00a0REF . For context randomization we use a range \\(rg\\)  of 30 minutes.\n", "Finally, we illustrate the training progress under different discount factors \\(\\gamma \\)  in Figure\u00a0REF .\nDuring training we record the average \\(V\\)  for each input batch and we plot its change against the training steps. The average value the function \\(V\\)  converges to depends on the \\(\\gamma \\)  being used. The value is smaller with a smaller \\(\\gamma \\) , in which case the convergence also happens faster.\nNote that a smaller \\(\\gamma \\)  implies a more aggressive discounting of future values, hence a shorter lookahead horizon beyond which any rewards are close to zero once brought to the present. The training becomes easier in this case since it does not need to look far into the future.\nIn general \\(\\gamma \\)  represents a trade-off between foresight and variance.\nThe \\(\\gamma \\)  used in the experiments is chosen to be 0.92 which is determined by a randomized search based on out-of-sample simulations.\n{FIGURE}"]}
{"id": "2105.09128", "categories": "cs.CV cs.LG cs.SD eess.AS", "paragraphs": ["The experiments are implemented in Pytorch 1.3.1 and performed on an NVIDIA TITAN XP. Ninety percent of the training images were selected, with a total of 4,067 image pairs. Data augmentation is performed on the training images with random rotation of \\(90^{\\circ {}}\\) , horizontal and vertical flip. A single configuration was used for all experiments and all scale factors. The Adam optimizer and L1 loss were adopted [40] using default parameter values of zero weight decay, and a learning rate initialized to \\(10^{\u22124}\\)  with step decay of \\(\\gamma = 0.5\\)  after 500 epochs. The output SR image size for all experiments is \\(192 \\times 192\\)  with a minibatch size of 8 batches.\n"]}
{"id": "2112.04421", "categories": "cs.CV", "paragraphs": ["We train our model using prediction methods, Global Rotation (\\(r_y\\) ), Local Rotation (\\(\\alpha \\) ), Single Bin, Tricosine, Voting Bin, and Confidence Bins to predict one of two targets: global rotation (\\(r_y\\) ) and location rotation (\\(\\alpha \\) ). Our models were first trained on a Nvidia V100 GPU for 1-D scalar based representation experiments and later on a Nvidia RTX3090 GPU for all other experiments. All models were trained with TensorFlow's default Adam optimizer  (lr= 0.001, \\(\\beta 1\\) = 0.9, \\(\\beta 2\\) = 0.99, \\(\\epsilon \\) = 1e-7), a batch size of 25, and 100 total epochs.\n"]}
{"id": "2104.04087", "categories": "cs.CL cs.SE", "paragraphs": ["The training goal of our system is to minimize the categorical\ncross-entropy. As training algorithm we use Adam with a learning\nrate of 0.0001. We used sequence-to-sequence models in our work with\nRNN encoder and decoder. The encoder and the decoder use GRU [19] units.\nOur models use Bahdanau attention[7] for the attention mechanism.\nThe size of the hidden units is 1024 and the size of the batches\nis 32. The models are trained between 5,000 - 30,000 steps\ndepending on the number of layers of each specific model and the size of the dataset.\nWe use the same input sequence length (100) and output sequence\nlength (30) as Jiang et al. [1]\nas 99% of the examples in the datasets have both the diffs and commit messages length less than these thresholds.\n"]}
{"id": "2102.00690", "categories": "cs.CV cs.RO", "paragraphs": ["We first evaluate the proposed monocular 3D detection network on the KITTI benchmark [26]. The dataset consists of 7,481 training frames and 7,518 test frames.\nChen et al. [34] further splits the training set into 3,712 training frames and 3,769 validation frames.\n"]}
{"id": "2107.05328", "categories": "cs.LG stat.ML", "paragraphs": ["The base VGG model and method for visualizing are implemented following [38], [6], which does not have batch normalization. For both SDP and sDprun we use the similar learning rate schedule adopted by [6]: fix the learning rate equals to \\(0.1\\)  at first 50% epochs, then reduce the learning rate to 0.1% of the base learning rate between 50% and 90% epochs, and keep reducing it to 0.1% for the last 10% epochs. The minibatch size is 128 for all experiments in Section\u00a04.3.\n"]}
{"id": "2112.06660", "categories": "cs.LG", "paragraphs": ["In our numerical experiments, all training and testing data are sampled uniformly in \\(\\Omega \\)  (or \\(\\partial \\Omega \\) ), and all networks are trained by Adam optimizer. The initial learning rate is set as \\(2\\times 10^{-4}\\)  with a decay rate \\(5 \\times 10^{-5}\\)  for each training epoch. For visualization of the training process, we test our model every 1000 epochs in the training process. The penalty parameter \\(\\beta \\)  for the orthogonality constraint (REF ) is set as 20, \\(\\gamma \\)  for the boundary constraint (REF ) and (REF ) is set as\n\\(\\gamma =\\left\\lbrace \\begin{aligned}\\gamma _0, \\quad &\\textup {if}~~i_{\\textup {epoch}}<0.1T_{\\max }\\\\10\\gamma _0,\\quad &\\textup {if}~~0.1T_{\\max }<=i_{\\textup {epoch}}<0.2T_{\\max }\\\\50\\gamma _0, \\quad &\\textup {if}~~ 0.2T_{\\max }<=i_{\\textup {epoch}}<0.25T_{\\max }\\\\100\\gamma _0, \\quad &\\textup {if}~~ 0.25T_{\\max }<=i_{\\textup {epoch}}<0.5T_{\\max }\\\\200\\gamma _0, \\quad &\\textup {if}~~ 0.5T_{\\max }<=i_{\\textup {epoch}}<0.75T_{\\max }\\\\500\\gamma _0, \\quad &\\textup {otherwise}\\end{aligned}\\right.\\) \n", "where \\(\\gamma _0=100\\)  in all our tests and \\(T_{\\max }\\)  represents the total epoch number. We implement our code in TensorFlow (version 1.14.0) on a work station (256-GB RAM, single NVIDIA GeForce GTX 2080Ti 12-GB).\n"]}
{"id": "2105.11210", "categories": "cs.CL", "paragraphs": ["Pre-training Dataset. Following LayoutLM, we pre-train StructuralLM on the IIT-CDIP Test Collection 1.0 [9]. It is a large-scale scanned document image dataset, which contains more than 6 million documents, with more than 11 million scanned document images. The pre-training dataset (IIT-CDIP Test Collection) only contains pure texts while missing their corresponding bounding boxes. Therefore, we need to re-process the scanned document images to obtain the layout information of cells. Like the pre-processing method of LayoutLM, we similarly process the dataset by using Tesseract\u00a0https://github.com/tesseract-ocr/tesseract, which is an open-source OCR engine. We normalize the actual coordinates to integers in the range from 0 to 1,000, and an empty bounding box \\((0; 0; 0; 0)\\)  is attached to special tokens [CLS], [SEP] and [PAD].\n", "StructuralLM is pre-trained on 16 NVIDIA Tesla V100 32GB GPUs for 480K steps, with each mini-batch containing 128 sequences of maximum length 512 tokens. The Adam optimizer is used with an initial learning rate of 1e-5 and a linear decay learning rate schedule.\n", "Hyperparameter N. For the cell position classification task, we test the performances of StructuralLM using different hyperparameter \\(N\\)  during pre-training. Considering that the complete pre-training takes too long, we pre-train StructuralLM for 100k steps with a single GPU card to compare the performance of different \\(N\\) . As shown in Figure REF , when the \\(N\\)  is set as 16, StructuralLM obtains the highest F1-score on the FUNSD dataset. Therefore, we set \\(N\\)  as 16 during the pre-training.\n{FIGURE}"]}
{"id": "2109.07383", "categories": "cs.CL", "paragraphs": ["For machine translation, we experiment on the IWSLT\u201914 De-En and WMT\u201914 En-De tasks using the identical settings as [31]. For language modeling, we experiment on the WikiText-103 dataset [21] with the same settings as [0]. We set the maximum number of tokens per sample to 1,843 to fit the memory constraints and apply gradient accumulation to keep the same batch size as [0] \u2019s work. All models are trained with mixed precision on 8 NVIDIA RTX 2080 Ti GPUs except for IWSLT ones, which only take one GPU for training.\n"]}
{"id": "2106.07719", "categories": "cs.CL cs.LG", "paragraphs": ["We use the following settings for training. The maximum sentence length for document was set to 75 tokens while that was set to 16 tokens for the query part. After doing some experiments, we finalized the embedding dimensions to \\(M=32\\) . We also employ 6-layers BERT for document embedding and 3-layers BERT for query embedding. The 3-layers BERT is called student model as it was trained from a 6-layerss teacher model.\n"]}
{"id": "2104.02555", "categories": "cs.CV cs.LG eess.IV", "paragraphs": ["We use a \\(F=256\\)  dimensional FDE, with the positional encoding being based on polar coordinates, i.e. Fourier coefficients of same frequency have the same radius.\nThe FDE is passed to a causal-linear transformer\u00a0[6] with 8 layers, 8 self-attention heads, a query and value dimensionality of 32, dropout of \\(0.1\\) , attention dropout of \\(0.1\\) , and a dimensionality of the feed-forward network of 1024.\n", "This setup is trained auto-regressively, i.e. with a triangular attention mask.\nWe use the rectified Adam optimizer (RAdam)\u00a0[26] with an initial learning rate of \\(0.0001\\)  and weight decay of \\(0.01\\)  for 100 epochs.\nThe batch size is 32.\nWe half the learning rate on plateauing validation loss.\n", "Like before, we consistently use \\(F=256\\)  dimensional FDEs, and employ the linear encoder and decoder method by Katharopoulos\u00a0et\u00a0al. \u00a0[6].\nMore specifically, we use 4 transformer layers, 8 self-attention heads per layer, a query and value dimensionality of 32, dropout of \\(0.1\\) , attention dropout of \\(0.1\\) , and a dimensionality of the feed-forward network of 1024.\nThe residual conv-block has \\(d_{\\text{conv}}=8\\)  intermediate feature channels.\n", "All networks are optimized using RAdam\u00a0[26], with an initial learning rate of \\(0.0001\\)  and weight decay of \\(0.01\\)  for 300 (MNIST), 120 (Kanji), and 350 (LoDoPaB) epochs.\nThe batch size is 32.\nWe half the learning rate on plateauing validation loss.\n{FIGURE}"]}
{"id": "2109.08336", "categories": "cs.CV cs.RO", "paragraphs": ["The proposed network is implemented using the PyTorch framework and trained on 12 Nvidia Tesla P100-16GB GPUs using \\( \\mathtt {DistributedDataParallel}\\) . The TorchSparse library\u00a0[19] is used for sparse convolutions.\nDuring training, the ground plane is first removed using RANSAC plane fitting followed by down-sampling using a voxel grid filter of \\(10cm\\) . Finally, input point clouds are limited to a maximum of \\(35K\\)  points.\nTo reduce overfitting, we apply the following data augmentations for training. Random point jitter is introduced using Gaussian noise sampled from \\(\\mathcal {N}(\\mu =0, \\sigma =0.01)\\)  clipped at \\(0.03m\\) . Each point cloud is also randomly rotated about the \\(z\\) -axis by an angle between \\( \\pm 180^{\\circ }\\) . Note that ground plane removal is not used during evaluation to speed up inference time as it does not affect evaluation performance of our proposed method.\n", "The dimensionality of the local features is set to 16 which results in a 256 dimension global descriptor for fair comparison with PointNetVLAD. In the local consistency loss \\( \\mathcal {L}_{lc}\\) , the margins \\(m_p\\)  and \\(m_n\\)  are set to 0.1 and 2.0, respectively. The local consistency loss is only applied to randomly sampled 5192 positive pairs similar to FCGF\u00a0[22]. The quadruplet loss margins are set to \\(\\alpha =0.5\\)  and \\(\\beta =0.3\\) . The distances for sampling positive and negative point cloud pairs are set to \\(\\tau _p=3 m\\)  and \\(\\tau _n=20 m\\) .\nFor \\( \\mathcal {L}_g\\)  we use 2 positives, 9 negatives and 1 other negative.\nWe train our model using Adam optimizer with an initial learning rate of 0.001 and a multi-step scheduler to drop the learning rate by a factor of 10 after 10 epochs and train until convergence for a maximum of 24 hours.\n"]}
{"id": "2109.08218", "categories": "cs.LG", "paragraphs": ["\nwhere the matrix \\(B\\)  is shared between all tasks, while \\(\\sigma _i\\)  and \\(\\epsilon _i\\)  are\nspecific to each task. We refer to this collection of tasks as MTRegression. The task is\ndefined so that loss functions of various tasks have significantly different scale,\nposing a testbed for multi-task optimization. The input and output dimensions are 250\nand 100, respectively, and the elements of \\(B\\)  and \\(\\epsilon _i\\)  are sampled\nindependently from \\(\\mathcal {N}(0, 10)\\)  and \\(\\mathcal {N}(0, 3.5)\\) , respectively. In our\nexperiments, we set \\(\\sigma _i = i\\)  and \\(n = 10\\) . The dataset has 9000 training samples\nand 1000 testing samples, where each sample has an input value and a label for each of\nthe \\(n\\)  tasks.\n", "Following [4], we train a fully connected network made of a shared trunk\nwith 4 fully-connected layers, ReLU activations, and a hidden size of 100, followed by\n\\(n\\)  one-layer task-specific output heads that produce the prediction for each task. Each\ntask's loss function is a standard squared error between model output and ground-truth.\nIn each experiment the network is trained to minimize the weighted sum of the \\(n\\)  task\nlosses: \\(\\sum _{i=1}^n w_i L_i\\) , where \\(w_i\\)  is determined differently by each method\nevaluated.\n", "The PubChem BioAssay (PCBA) dataset [36] contains data for 128 virtual\nscreening tasks. Virtual screening [32] is the process of predicting\nwhether a candidate molecule will bind to a biological target in order to identify\npromising compounds for new drugs. The 128 tasks are binary classification of candidate\nmolecules as either active or inactive for 128 different biological targets. In total,\nabout 440K candidate molecules are labeled, though each molecule is labeled for an\naverage of 78 tasks, yielding over 34 million labeled candidate/target pairs. The\nclasses are not balanced; on average, only about 2% of input molecules will bind to a\ngiven target. We use the PCBA data hosted in the DeepChem [29]\nrepository.\n", "Each input molecule is featurized as a 2048-length binary vector using RDKit\n[19] to compute ECFP features [31] (radius 4). We use the\nPyramidal architecture of [30]: a fully-connected network made of a\n2-layer shared feature extractor and 1-layer task-specific output heads. The feature\nextractor layers have 2000 and 100 units, respectively. Our loss function is the\nweighted sum of the cross-entropy classification loss for each task. To account for\nclass imbalance, we scale the loss for each class by a factor inversely proportional to\nthe number of samples of that class.\n"]}
{"id": "2106.03917", "categories": "cs.LG", "paragraphs": ["For rotation-based self-supervised training, we follow the objective formulated in [21].\nThe setup is mostly the same as for the standard training.\nThe only difference is that we reduce the batch size to 16 so that the extra rotated images used during the training can still fit into the GPU memory.\n", "For OE-based training, we by default fine-tune the standard models with OE/EnergyOE/MixupOE objective for 10 epochs so that only minimal extra computation is introduced.\nThe fine-tuning also adopts cosine learning rate schedule with the learning rate being \\(0.001\\) .\nWhen training from scratch, we follow the setup for standard training.\nRegardless of the specific objective, we use 32 as the batch size of outliers (i.e., the samples drawn from \\(\\mathcal {D}_{\\text{out}}^{\\text{OE}}\\) ) which fits within our GPU's memory.\n"]}
{"id": "2106.03847", "categories": "cs.LG cs.CV cs.GR cs.NE", "paragraphs": ["We used the same hyperparameter configurations as in the Pytorch [26] implementation of StyleGAN2-ada [10], while we did not use the adaptive augmentation capabilities. We used a fixed mapping depth of 8 layers during all our experiments. The hyperparameters were chosen by a random search and are presented in the attached code.\n"]}
{"id": "2109.02753", "categories": "cs.CL", "paragraphs": ["Our system is developed based on Huggingface Transformershttps://github.com/huggingface/transformers.\nThe mention extraction model is trained for one epoch with a learning rate of \\(1e-5\\)  and a batch size of 32. The IS assignment model is trained for three epochs with a learning rate of \\(3e-5\\)  and a batch size of 32. Both models are initialized using pre-trained RoBERTa\\(_{LARGE}\\)  contextual embeddings. They have 24 transformer blocks, 1024 hidden units, 16 self-attention heads, and around 355M parameters.\n"]}
{"id": "2112.07522", "categories": "cs.CL", "paragraphs": ["[7] show that\nlarge model size is necessary\nfor strong few-shot performance.\nWe use ALBERT-XXLarge-v2\n[33] \u2013 of size 223M parameters \u2013\nas our large PLM, which is\nadapted to be an LMTurker \\(A\\)  of\n\\(\\mathcal {T}\\)  with \\(\\mathcal {G}\\) .\nWith parameter reuse, ALBERT-XXLarge-v2\noutperforms\nlarger models\nlike the 334M\nBERT-large [13].\nIn contrast,\n\\(\\mathcal {S}\\) \nmust be small\nto be deployable\nin practical scenarios.\nWe use TinyBERT-General-4L-312D\n[29],\nwhich has 14.5M parameters, but performs comparably to\nBERT-base (110M).\n"]}
{"id": "2110.05098", "categories": "cs.CV", "paragraphs": ["Except ASF module (REF ), all weights of our SurroundNet are initialized randomly. Adam optimizer is adopted to optimize the whole network. The learning rate is set to 0.001, and the momentum is 0.9. In every mini-batch, we randomly crop 32 dark-light patch pairs, each of which has two paired 128*128 images. We set epoch to 100 on synthetic dataset, and fine-tune epoch is set to 3500 on LOL dataset. For ablation experiments, all the training epochs are set to 100 unless otherwise stated. All models are trained on the platform with Nvidia GTX 2080Ti GPU.\n"]}
{"id": "2110.11191", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["Training configurations. We train the networks using Adam [21] optimizer with \\(\\alpha = 2 \\times 10^{-4}\\) , \\(\\beta _1 = 0.5\\) , \\(\\beta _2 = 0.999\\)  and \\(\\epsilon = 10^{-8}\\)  for all datasets with a minibatch size of 32. Since we rely on the WGAN-GP loss [12], we set \\(n_{critic} = 5\\) , which sets the number of iterations of the discriminator per generator iteration.\n", "Noise injection details. The noise injector described in Section  3.4.1  samples a random noise \\(\\mathbf {r}_l\\)  using \\(\\mathcal {N}(0, 1)\\) . Each joint at resolution level \\(l\\)  has a respective weight to each channel and receives a different noise added channel-wise. This operation is applied to every generator's layer.\n"]}
{"id": "2103.06450", "categories": "cs.CV cs.AI cs.CL cs.LG", "paragraphs": ["\n\\(N\\)  (number of layers) = 6\n\n\\(d_{model}\\)  = 260\n\n\\(h\\)  (number of heads) = 4\n\n\\(d_{ff}\\)  (inner-layer of positionwise feed-forward network) = 1024\n\nActivation function inside feed-forward layer = GELU [11]\n\ndropout = 0.5\n\n", "The model was implemented in PyTorch [20], and training was carried out using 8 NVIDIA 2080Ti GPUs.\nFor full page datasets a mini-batch size of 56 combined with a gradient accumulation factor of 2 was used, yielding an effective batch-size of 112.\nSingle-line datasets had batch sizes as high as 200, but were adjusted downwards when using higher angles of image rotation.\nADAM optimizer [13] was employed with a fixed learning rate (\\(\\alpha \\) ) of 0.0002, \\(\\beta _{1}\\)  = 0.9 and \\(\\beta _{2}\\)  = 0.999.\n", "While all images in a batch must have the same size; we also set all batches to have the same image size, padding smaller images as needed.\nThis helps during training because any impending GPU OOM errors surface quickly at the beginning of the run.\nIt also makes the validation / test results agnostic of the batching scheme since the images will always be the same size regardless of how they are grouped.\nSmaller images within a batch are centered during validation and testing.\nPadding color can be either the max of 4 corner pixels or simply 0 (black), the choice having no impact on model accuracy.\n"]}
{"id": "2108.11626", "categories": "cs.CL cs.AI", "paragraphs": ["In CoMPM, CoM uses a pre-trained GPT2-medium as the initial state and PM uses a pre-trained distilRoBERTa as the initial state. We use the pre-trained model from the huggingface library\u00a0https://github.com/huggingface/transformers. The optimizer is AdamW and the learning rate is 1e-5 as an initial value. The learning rate scheduler used for training is get_linear_schedule_with_warmup, and the maximum value of 10 is used for the gradient clipping. We select the model with the best performance on the validation set. All experiments are conducted on one V100 GPU with 32GB memory.\n"]}
{"id": "2102.00086", "categories": "cs.CL", "paragraphs": ["For all the experiments, we fine-tune RoBERTa-large [26] over the corresponding corpus with one GTX2080 Ti.\nWe use the default hyperparameters as provided in the HuggingFace Transformers library [46], with two major changes: we use a learning rate of \\(10^{-5}\\)  and 8 batch size in all experiments.\n"]}
{"id": "2106.15535", "categories": "cs.LG cs.AI", "paragraphs": ["We use the default setting in Deep Graph Library\u00a0[37]Apache License 2.0. for model hyper-parameters. We use the Adam optimizer with initial learning rate of 0.01 and weight decay of 5e-4 to train all models for 400 epoch by minimizing the cross entropy loss, with early stopping on the validation set.\n"]}
{"id": "2109.15222", "categories": "cs.CV", "paragraphs": ["We use an encoder-decoder architecture with ResNet-18  without the classification layers as the encoder, two 1x1 convolutions in the bottleneck to reduce the number of channels and a simpler ResNet-based decoder. The final activation is sigmoid and we use binary-crossentropy loss for all models besides NSA (continuous) for which we use ReLU activation and mean squared error loss as the labels are unbounded. The models are trained on batches of size 64 using Adam  with a cosine-annealing learning rate  that decays from \\(10^{-3}\\)  to \\(10^{-6}\\)  over 320 epochs. For non-aligned objects, the loss takes longer to converge, so we use 560 epochs for the hazelnut, metal nut, and screw classes in the MVTec AD dataset. For rCXR, we use 240 epochs. The same training hyperparameters are used for all variants of the self-supervised task. Hyperparameters for the self-supervised task are given in the supplementary material. Note that in our implementation of FPI and CutPaste we also use object masks and the patch sizes are sampled from a truncated Gamma distribution rather than a uniform distribution as described in  and  to allow for a more fair comparison with NSA.\n", "The rCXR images also have a high resolution of \\(1024\\times 1024\\)  pixels but are grayscale. We resize them to \\(256 \\times 256\\)  pixels for training and apply a random rotation of up to 3 degrees, center-crop to \\(230\\times 230\\)  pixels and take a random crop of \\(224\\times 224\\)  pixels. For testing we use \\(224\\times 224\\)  center-crops of \\(256 \\times 256\\)  resampled images.\n"]}
{"id": "2110.09424", "categories": "cs.CV cs.CL cs.HC cs.LG", "paragraphs": ["Two diagnostic classifiers are used to retrieve the protected variables from the network latent representation, Logistic Regression (LR) and XGBoost. During our initial experiments, we conducted a grid search on the hyperparameters of the two algorithms, but observed little variation on the performance of XGBoost classifier. Because the hyperparameter search was very extensive for this classifier, we decided to select a set of average values for the parameters: loss reduction split\u00a0\\(\\gamma \\)  is fixed to\u00a0\\(0.1\\) , regularization parameter\u00a0\\(\\alpha \\in \\lbrace 0.3, 0.5\\rbrace \\)  and number of estimators is\u00a0500. For LR, we observed more variation in the output depending on the regularization parameter. We thus run a hyperparameter search each time, selecting the best value in the range\u00a0\\(10^{-4}...10^4\\) , depending on the performance on the validation set, and both\u00a0\\(\\ell _1\\)  and\u00a0\\(\\ell _2\\)  norms were tested.\n"]}
{"id": "2108.10274", "categories": "cs.CL stat.ML", "paragraphs": ["We use BiLSTMs with one hidden layer of 100 dimensions, 100-dimensional randomly initialised word embeddings, a label embedding size of 100. We train our models with RMSProp, a learning rate of \\(0.001\\) , a batch size of 128, and early stopping on the validation set of the main task with a patience of 3.\n"]}
{"id": "2112.01527", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["Semantic segmentation. We follow the same settings as\u00a0[13] to train our models, except: 1) a learning rate multiplier of 0.1 is applied to both CNN and Transformer backbones instead of only applying it to CNN backbones in\u00a0[13],\n2) both ResNet and Swin backbones use an initial learning rate of \\(0.0001\\)  and a weight decay of \\(0.05\\) , instead of using different learning rates in\u00a0[13].\n"]}
{"id": "2107.11262", "categories": "cs.CV eess.IV", "paragraphs": ["For our experiments, we fixed the LR image resolution to \\(8\\times 8\\)  and experimented with \\(128 \\times 128\\)  and \\(256 \\times 256\\)  for the HR image resolution\u2014we ablate the effect of LR image resolution in sec.\u00a0REF .\nWe train our networks with Adam\u00a0[20] and TTUR\u00a0[13], with a learning rate of \\(10^{-3}\\)  for the generator and \\(4 \\times 10^{-3}\\)  for the discriminator. We also used \\(R^1\\)  regularization\u00a0[27] with \\(\\gamma =0.5\\) , with a batch size of 8. Spectral normalization\u00a0[28] was used in all the layers of both \\(G\\)  and \\(D\\) . In eq.\u00a0REF , we use \\(\\epsilon =0\\)  to push the downscaled version of the generated image to be as close as possible to the LR target. We set \\(\\lambda _{cyc}=1\\)  when trained on \\(128 \\times 128\\) , and to \\(\\lambda _{cyc}=0.1\\)  for \\(256\\times 256\\) .\n{FIGURE}{FIGURE}{FIGURE}"]}
{"id": "2112.00133", "categories": "cs.LG cs.CV", "paragraphs": ["We conduct experiments on 64 TPU-v3 chips with a batch size of 8192. We use Adam optimizer\u00a0[27] (\\(\\beta _1 = 0.9, \\beta _2 = 0.99\\) ) with a linear learning rate decay. The initial learning rate is 6.4e-4. The weight decay is set to 5e-5 throughout the training. BatchNorm momentum is set to 0.9. To estimate the clipping bound \\(B\\)  for activation of non-binary quantized layers, we follow the method in [0] and use exponentially moving average (\\(\\alpha =0.9\\) ) of maximum value in a batch.\n"]}
{"id": "2107.04225", "categories": "cs.CV cs.HC", "paragraphs": ["A model was trained with our training split dataset only. We used the pretrained weight from TSAV to initialize the backbone for audio and video branch. The model was optimized using Adam optimizer and a learning rate of 0.0005. Random brightness augmentation was applied for each input clip. The mini-batch size was set to 32. The training and validating processes were performed using two GPU to allocate each of the teacher and student networks to one GPU.\n"]}
{"id": "2112.12731", "categories": "cs.CL", "paragraphs": ["Following the pre-training setting of ERNIE 3.0, ERNIE 3.0 Titan includes the universal representation module and the task-specific representation modules, which both use the Transformer-XL structure.\nWe adopt a structure with 12 layers, 768 hidden units, and 12 heads for the task-specific representation modules.\nWe adopt a structure with 48 layers, 12288 hidden units, and 192 heads for the universal representation modules.\nWe found that continually increasing the hidden size would make it difficult to load the parameter of output embedding in a single machine with eight cards with 32GB memory. In order to further increase the model capacity, we choose to scale the parameters of the point-wise feed-forward network alternatively. The inner layer of the universal representation modules has a dimensional of 196608, which is 16 times the hidden size of the model.\nThe total number of parameters of ERNIE 3.0 Titan is over 260 billion.\n", "We use Adam\u00a0[30] with learning rate of 1e-4, \\(\\beta _1=0.9\\) , \\(\\beta _2=0.95\\) , L2 weight decay of 0.1, We also clip the global norm of the gradient at 1.0 to improve the stability of pre-training. The maximum sequence length of context and the memory length of language generation is 512 and 128, respectively. We use progressive learning to speed up convergence in the first 4000 steps and linear decay of the learning rate. ERNIE 3.0 Titan is implemented on the PaddlePaddle framework and uses parallelism techniques that facilitate the efficient training of large models that do not fit in the memory of a single GPU. We will give the detail of these in the next section.\n"]}
{"id": "2103.16110", "categories": "cs.CV", "paragraphs": ["Dataset. For a fair comparison, we follow the same settings as the Top-1 FashionBERT\u00a0[20] and pre-train the proposed Kaleido-BERT\u00a0on the Fashion-Genhttps://fashion-gen.com/ dataset.\nIt contains 67,666 fashion products accompanied with text descriptions.\nEach product includes one to six images from different angles.\nAmong all the image-text pairs, like\u00a0[20], we use 260,480 for training, and 35,528 for testing.\n", "Implementation Details. Our Kaleido-BERT\u00a0 has: L=12, H=768, A=12. L is number of stacked Transformer blocks. H denotes the hidden activation, and A means the number of attention heads.\nWe implement our model with Tensorflow and use 8*Tesla V100 for pre-training.\nThe Adam optimizer is applied with a learning rate of \\(2e-5\\)  and weight decay \\(1e-4\\) . We adopt a warming-up strategy for the first 5K steps.\n"]}
{"id": "2103.16102", "categories": "cs.CL cs.LG", "paragraphs": ["All of our codes are written based on PyTorchhttps://pytorch.org/.\nTo extract the word definition of candidate answers, we use NLTK toolkit [0].\nThe transformer encoder we used is pretrained ALBERT-xxlarge-v2 modelhttps://github.com/huggingface/transformers.\nSince the code of DUMA is not open-source, we reimplement it by only using one co-attention layer where the attention heads are 64 and the dimension of Query, Key and Value are all 64, because it is pointed that more co-attention layers do not improve the performance [17].\nThe setting is also applied to our WN-DUMA for fair comparison.\n"]}
{"id": "2102.10739", "categories": "cs.LG", "paragraphs": ["Training hyper-parameters. For both fully and semi-supervised node classification tasks on the citation networks, Cora, Citeseer and Pubmed, we train our DGC following the hyper-parameters in SGC [23]. Specifically, we train DGC for 100 epochs using Adam [7] with learning rate 0.2. For weight decay, as in SGC, we tune this hyperparameter on each dataset using hyperopt [0] for 10,000 trails. For the large-scale inductive learning task on the Reddit network, we also follow the protocols of SGC [23], where we use L-BFGS [12] optimizer for 2 epochs with no weight decay.\n"]}
{"id": "2101.11212", "categories": "cs.CL", "paragraphs": ["For stage-II, we construct graphs with 5.4M and 0.6M edges for BBN and OntoNotes respectively. Curvature constant of the hyperbolic space is set to \\(K=1\\) . All the models are trained using Adam optimizer\u00a0[9] with learning rate = 0.001.\n"]}
{"id": "2107.11635", "categories": "cs.CV cs.AI", "paragraphs": ["Following previous works [18], [19], [39], [52],\nwe adopt ResNet34 and ResNet50 [16] as the backbone\nnetwork when working on the 5 standard datasets and on the 3 big ImageNet\nsubsets, respectively. The \u201crepresentation learning\u201d head (RL-head)\nand the \u201cclustering\u201d head (C-head) are two-layer neural networks\nwith ReLU activations. The length of the output vector of the RL-head\nis 128. The temperature \\(\\tau \\)  (Eq.\u00a0REF )\nis fixed at 0.1. To reduce variance in learning, we train our model\nwith 10 C-subheadsThe final \\(\\mathcal {L}_{\\text{cluster}}\\)  in Eq.\u00a0\nis the average of \\(\\mathcal {L}_{\\text{cluster}}\\)  of these C-subheads. similar to [19]. This only adds little extra computation\nto our model. However, unlike [18], [19], [52],\nwe do not use an auxiliary \u201cover-clustering\u201d head to exploit\nadditional information from data since we think our RL-head can do\nthat effectively.\n", "To train CRLC-semi, we use a SGD optimizer with an initial learning\nrate = 0.1, momentum = 0.9, Nesterov = False, and weight decay = 5e-4.\nSimilar to [35], we adjust the learning rate at\neach epoch using a cosine decay schedule [27]\ncomputed as follows:\n\\(\\text{lr}_{t}=\\text{\\text{lr}}_{\\text{min}}+(\\text{lr}_{\\text{init}}-\\text{lr}_{\\text{min}})\\times \\frac{1+\\cos \\left(\\frac{t}{T}\\pi \\right)}{2}\\) \n", "where \\(\\text{lr}_{\\text{init}}=0.1\\) , \\(\\text{lr}_{\\text{min}}=0.001\\) ,\n\\(\\text{lr}_{t}\\)  is the learning rate at epoch \\(t\\)  over \\(T\\)  epochs\nin total. \\(T\\)  is 2000 and 1000 for CIFAR10 and CIFAR100, respectively.\nThe number of labeled and unlabeled samples in each batch is 64 and\n512, respectively. In \\(\\mathcal {L}_{\\text{CRLC-semi}}\\)  (Eq.\u00a012 in the\nmain text), \\(\\lambda _{1}=1\\) , \\(\\lambda _{2}=5\\) , and \\(\\lambda _{3}=1\\) .\n"]}
{"id": "2108.06084", "categories": "cs.LG cs.DC", "paragraphs": ["We evaluate two sets of training parameters. The first set follows the Megatron-LM work: batch size 512, 300K total training steps (157B tokens), and learning rate \\(1.5\\times 10^{-4}\\)  with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 297K steps (with minimum learning rate \\(1\\times 10^{-5}\\) ). The second parameter set tests a more aggressive training strategy: batch size 4K (\\(8\\times \\)  larger), 37.5K total training steps (157B tokens), and learning rate \\(6\\times 10^{-4}\\)  (\\(4\\times \\)  larger) with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 34.5K steps (same minimum learning rate). For sequence length/context size, we mainly use 1K which is the default for GPT-2. But we also test 2K (on the smaller 117M model with batch size 512 and 157B tokens) which is the default for GPT-3\u00a0[4]. All experiments are performed with mixed precision/FP16 training, Adam optimizer (\\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.999\\) , \\(\\epsilon = 1\\times 10^{-8}\\) )\u00a0[10], weight decay of 0.01, checkpoint activation, same random seed (for Python, NumPy, PyTorch, and CUDA), and gradient clipping.\n"]}
{"id": "2103.04513", "categories": "cs.CV cs.LG", "paragraphs": ["On MNIST we use Adam optimizer, learning rate of 0.001, batch size of 128, and epochs of 50 for both the generator and the discriminator. The loss weights are set as \\(\\alpha =1\\) , \\(\\beta =1\\) , and \\(\\gamma =10\\) . On SVHN we use batch size of 64 and epochs of 40 for both the generator and the discriminator. The Adam optimizer parameterized with learning rate of 0.0002, \\(\\beta _1\\)  of 0.5, and \\(\\beta _2\\)  of 0.999 is used for the generator, and the Momentum optimizer parameterized with initial learning rate of 0.1 and momentum of 0.9 is used for the discriminator, the learning rate of the discriminator decays by a factor of 10 at epoch 20 and 30. The loss weights are set as \\(\\alpha =1\\) , \\(\\beta =1\\) , and \\(\\gamma =10\\) . On CIFAR-10 we use batch size of 128 and steps of 80000 for both the generator and the discriminator. The Adam optimizer parameterized with learning rate of 0.0002, \\(\\beta _1\\)  of 0.5, and \\(\\beta _2\\)  of 0.999 is used for the generator, and the Momentum optimizer parameterized with initial learning rate of 0.1 and momentum of 0.9 is used for the discriminator, the learning rate of the discriminator decays by a factor of 10 at epoch 100 and 150. The loss weights are set as \\(\\alpha =5\\) , \\(\\beta =4\\) , and \\(\\gamma =10\\) .\n"]}
{"id": "2109.11797", "categories": "cs.CV cs.CL", "paragraphs": ["We report experiments results of different training settings, including (1) zero-shot setting, where no training data is available, (2) few-shot setting, where \\(K\\)  training instances are available (\\(K=1,2,4,8,16\\) ), and (3) fully supervised setting, where full training set is available.\n"]}
{"id": "2110.05960", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["All models are trained for \\(T=10^5\\)  iterations (for GeoMNIST)\nor \\(T=3\\times 10^5\\)  iterations (for CIFAR)\nwith a learning rate of \\(0.005\\)  and\na batch size of 1 under the softmax cross-entropy loss.\nModels on GeoMNIST converged with training and validation losses\nto zero, and those on CIFAR to validation accuracies greater than \\(90\\%\\) .\n"]}
{"id": "2106.13802", "categories": "cs.CV", "paragraphs": ["We use Hedwighttps://github.com/castorini/hedwig,\nan open-source deep learning\ntoolkit with a number of implemented of document classification models.\nWe use a Tesla K80 GPU for all models requiring GPU for train, and use amazon EC2-t2.micro and EC2-c type machine when only CPU is needed . We use PyTorch 1.5 as the backend framework, and gensim [17] package for computing the node feature vectors using word2vec.\n"]}
{"id": "2107.12930", "categories": "cs.CL", "paragraphs": ["We use the original BERT implementation of devlin-etal-2019-bert.\nFor the development experiments,\nwe train our BERT model for 500,000 steps\nwith a sequence length of 128.\nWe use whole word masking\nand\nthe default hyperparameters and model architecture of BERTBASE [8] except a lower batch size of 32 in order to train on NVIDIA RTX 6000 GPUs with 24 GB RAM.\nThis corresponds to a bidirectional transformer [35] with 12 layers,\n12 attention heads,\na hidden layer size of 768,\nand GELU activations\n[15].\n"]}
{"id": "2104.06317", "categories": "cs.LG", "paragraphs": ["Our task is to first train the node embeddings and then directly evaluate its node classification ability. We set the same experimental settings as the SOTA [29], [9] and report the mean classi\ufb01cation results on the testing set after 50 runs of training followed by a linear model.\nWe initialize the parameters using Xavier initialization [5] and train the model using Adam optimizer [12] with an initial learning rate of 0.001. We follow the same settings as DGI does and set the number of epochs to 2000. We vary the the batch size from 50 to 2000, and the early stopping with a patience of 20 is adopted. The embedding dimension is set to 512. Unlike DGI, we use two layers of GCN. We set the step of random walk as 25, soft-margin \\(\\alpha \\)  as 0.9, dropout rate as 0.7.\n"]}
{"id": "2104.06411", "categories": "cs.LG cs.AI", "paragraphs": ["Seeing from this figure, our method was significantly sensitive to the hyper parameter \\(\\eta \\) . A wrong \\(\\eta \\)  such as \\(\\eta =1\\)  in the four-rooms domain could deteriorate the performance of the method. This may incur a cost for tuning hyper parameters. To solve this problem, we must fit \\(\\eta \\)  to optimal value. The environmental rewards of goals were 1 and 10,000 for four-rooms and pinball, respectively. The best \\(\\eta \\)  was 0.01 for four-rooms and 100 for pinball. Both \\(\\eta \\)  were one-hundredth of the environmental rewards.\n"]}
{"id": "2101.11302", "categories": "cs.CL", "paragraphs": ["We use the Ranger optimizer, an adapted version of Adam [14] with improved stability at the beginning of training \u2013 by accounting for the variance in adaptive learning rates [21] \u2013 and improved robustness and convergence speed [40], [39].\nWe use a batch size of 16 and a learning rate of 3e-5 to which we apply cosine annealing.\nFor meta-training, we perform 100 epochs of 100 episodes and perform evaluation with 5 different seeds on the meta-validation set after each epoch. One epoch consists of 100 update steps where each update step consists of a batch of 4 episodes. Early-stopping with a patience of 3 epochs is performed to avoid overfitting.\nFor the non-episodic baselines, we train for 10 epochs on the auxiliary languages while validating after each epoch. All models are created using the PyTorch library [29] and trained on a single 24Gb NVIDIA Titan RTX GPU.\n"]}
{"id": "2104.11747", "categories": "cs.CV cs.RO", "paragraphs": ["We train all models with the Adam\u00a0[14] optimizer for four epochs with a batch size of 16 and a learning rate of \\(0.0005\\) . Focal loss\u00a0[17] with \\(\\beta = 1\\)  is used for classification of edges and nodes, weight decay is set to 0.01 and weights are initialized randomly. In all experiments, graphs with \\(T=3\\)  timesteps are considered.\n"]}
{"id": "2110.15358", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["As in [80], [15], we use a pre-trained Faster R-CNN model\u00a0[29] that is trained on 4,000 video frames randomly sampled from the training set with object masks and attribute annotations to generate object proposals for each frame. We train the language program parser with 1,000 programs for all question types.\nAll deep modules (concept learner and program executor) are trained using Adam optimizer for 40 epochs on 8 Nvidia 1080Ti GPUs and the learning rate is set to \\(10^{-4}\\) . The camera matrix is optimized from 20 training videos. We set \\(\\Delta t=0.004\\text{s}, D=256, C=64, K=10, S=10\\) , and \\(T=128\\)  for CLEVRER\u00a0[80] and \\(T=20\\)  for Real-Billiard\u00a0[64]. In addition to our standard model that grounds object properties from question-answer pairs, we also train a variant (VRDP\u00a0\\(\\dag \\) ) on CLEVRER with an explicit rule-based program executor\u00a0[80] and object attribute supervisions (attribute annotation in 4000 frames learned by the Faster R-CNN model).\n", "For the physical model, we use the L-BFGS optimizer\u00a0[57] with an adaptive learning rate to optimize all physical parameters.\nThe optimization terminates when it reaches a certain number of steps or the loss is less than a certain value. In all experiments, the number of the optimization step is set to 20. The loss threshold is set to 0.0005 for the learning of collision-independent parameters (i.e., initial velocity, initial location, and initial angle), and 0.0002, 0.001, 0.01 for the optimization of collision-dependent parameters (mass and restitution) on [0, 40], [0, 80], and [0, 128] frames, respectively.\n"]}
{"id": "2105.14779", "categories": "cs.CL cs.HC cs.SD eess.AS", "paragraphs": ["We trained the end-to-end ASR using a Noam [30] optimizer for 50 epochs with a learning rate of 5 with \\(20,000\\)  warmup steps and dropout-rate of 0.1. The trade-off weights, \\(\\alpha \\) , for \\(\\mathcal {L}\\) , we use a value of \\(\\alpha =\\) 0.3. The number of encoder, decoder layers and attention-heads differed based on the choice of large/small architecture.\nAs for the text tokenization, we used word-piece byte-pair-encoding (BPE)\u00a0[31] for the multilingual ASR.\n", "Large ASR Architecture: For the large ASR (with \\(\\approx 1,000\\)  hours of speech), we used 12 encoder layers and 6 decoder layers each with 2,048 encoder/decoder units from FFN and 8 attention heads with 512 transformation dimensions. For the architecture, we used 31 CNN kernals. For multilingual dialectal ASR, we opt for an BPE of \\(10K\\) .\n", "Small ASR Architecture: For exploring the influence of different character space representation, we designed small-scale ASR using 8/4 encoder/decoder layers each with 2048 FFN units. As for the attention modules, we used 4 attention head with a dimension of 256. For the task, we opt for 15 CNN module kernals.\n"]}
{"id": "2105.14761", "categories": "cs.CL cs.LG", "paragraphs": ["There is relatively little existing work about document-level MT using pre-training. Although Flat-Transformer+BERT gives a state-of-the-art scores on TED and Europarl, the score on News is worse than previous non-pretraining model HAN [28]. G-Transformer+BERT improves the scores by margin of 0.20, 1.62, and 0.47 s-BLEU points on TED, News, and Europarl, respectively. It shows that with a better contextual representation, we can further improve document-level MT on pretraining settings.\n", "We generate the corresponding group tag sequence dynamically in the model according to the special sentence-mark tokens <s> and </s>. Taking a document \u201c<s> there is no public transport . </s> <s> local people struggle to commute . </s>\u201d as an example, a group-tag sequence \\(G=\\lbrace 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2\\rbrace \\)  is generated according to Eq REF , where 1 starts on the first <s> and ends on the first </s>, 2 the second, and so on. The model can be trained either randomly initialized or fine-tuned.\n", "Randomly Initialized.\nWe use the same settings as Transformer to train G-Transformer, using label-smoothing of \\(0.1\\) , dropout of \\(0.3\\) , Adam optimizer, and a learning rate of \\(5e-4\\)  with 4000 warmup steps. To encourage inferencing the translation from the context, we apply a word-dropout [4] with a probability of \\(0.3\\)  on both the source and the target inputs.\n", "Fine-tuned on Sentence-Level Transformer.\nWe use the parameters of an existing sentence-level Transformer to initialize G-Transformer. We copy the parameters of the multi-head attention in Transformer to the group multi-head attention in G-Transformer, leaving the global multi-head attention and the gates randomly initialized. For the global multi-head attention and the gates, we use a learning rate of \\(5e-4\\) , while for other components, we use a smaller learning rate of \\(1e-4\\) . All the parameters are jointly trained using Adam optimizer with 4000 warmup steps. We apply a word-dropout with a probability of \\(0.1\\)  on both the source and the target inputs.\n", "Fine-tuned on mBART25.\nSimilar as the fine-tuning on sentence-level Transformer, we also copy parameters from mBART25 [23] to G-Transformer, leaving the global multi-head attention and the gates randomly initialized. We following the settings [23] to train the model, using Adam optimizer with a learning rate of \\(3e-5\\)  and 2500 warmup steps. Here, we do not apply word-dropout, which empirically shows a damage to the performance.\n"]}
{"id": "2110.13223", "categories": "cs.LG cs.CV", "paragraphs": ["For the experiments which refer to all 171 tasks (rather than just the 12 we focus on in NOOCh , we use an Adam optimizer with 1e-3 learning rate and batch size 16.\nAll other training parameters are the same as noted elsewhere.\n"]}
{"id": "2110.13285", "categories": "cs.CV cs.LG", "paragraphs": ["We train our generative flow and solve inverse problems using Adam [8]. CelebA images are resized to \\(32\\times 32\\)  resolution and we use the test set to evaluate each method. Additionally, our generative flow uses \\(K=2\\)  and \\(L=5\\) , and duplicate \\(K\\)  for scales \\(2\\times 2\\)  and \\(1\\times 1\\) \u2014we use 14 coupling layers in total. We train the generative flow with a batch size of 32 and stop the training process after 100 epochs. Then, we solve each inverse problem with 1500 iterations and a learning rate of \\(0.005\\) . All experiments were made in a GeForce GTX 750 Ti.\n{FIGURE}"]}
{"id": "2110.05208", "categories": "cs.CV", "paragraphs": ["For a fair comparison with CLIP, we train our DeCLIP-ResNet50 and DeCLIP-ViT-B/32 from scratch for 32 epochs. Unless otherwise specified, we use full data, i.e., 88M image-text pairs, to obtain the best performance. The input resolution of the image encoder is \\(224\\times 224\\) , and the maximum context length of the text encoder is 76.\nThe learnable temperature parameter \\(\\tau \\)  is initialized to 0.07. The loss weights of additional supervision \\({\\alpha }\\) , \\({\\beta }\\)  and \\({\\gamma }\\)  are all set to 0.2.\nMore details can be found in Appendix\u00a0.\n"]}
{"id": "2112.01021", "categories": "cs.LG", "paragraphs": ["For training, we use batch sizes of 256, 64 for {Colored MNIST, Corrupted CIFAR-10} and {BFFHQ, BAR}, respectively. We use learning rates of 0.001, 0.0001 for {Colored MNIST, Corrupted CIFAR-10, BFFHQ} and BAR, respectively. Also, we use cosine annealing from initial learning rates \\(lr\\)  to \\(lr * 0.1^3\\)  for learning rate scheduling\u00a0[26] for all datasets. Note that, we do not use random horizontal flipping for ColoredMNIST. Additionally, the original image size of BFFHQ is 128 but we resize them to 224 by following the previous work [21].\n", "In contrastive learning, we use Normalized Temperature-scaled Cross Entropy NT-Xent\u00a0[4] with a temperature parameter 0.01. For projection head \\(H\\) , 2-layer MLP and a linear layer with the dimensions from the input to the output as [512, 512, 128] and [100, 100] for {Corrupted CIFAR-10, BFFHQ, BAR} and Colored MNIST respectively, following [4] for the most of the settings.\n"]}
{"id": "2102.06589", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["In order to maintain a guarantee, the PAC-Bayes upper bound in Theorem REF  requires a loss function bounded between 0 and 1. However, the losses we use are not bounded in general. Let \\(N_\\theta \\)  be an arbitrary network parameterized by \\(\\theta \\)  and \\(N_\\theta (z)\\)  be the output of the network given sample \\(z \\in \\) . Consider arbitrary loss \\(f\\) , which maps the network's output to a real number. If \\(\\Vert N_\\theta (z)\\Vert  \\le r, \\forall \\ \\theta \\in , \\forall \\ z \\in \\) , then we can perform a linear scaling of \\(f\\)  to map it onto the interval \\([0,1]\\) . We define the minimum and maximum value achievable by loss function \\(f\\)  as follows\n\nMf  := z,  , N(z) rf(, z)\n"]}
{"id": "2102.06529", "categories": "cs.CV cs.AI", "paragraphs": ["The model used in both experiments is a frrcnn network with a rn152 backbone pre-trained on ImageNet.\nModels were fine-tuned using the StyleCOCO training and validation datasets \u2014 or a subset thereof in the case of expntrain.\nDefault training parameters from PyTorch were used except where modifications were shown to improve performance and those non-default parameters are listed below.\n2 layers of the backbone were made trainable and the remaining 3 were frozen to retain the pre-trained ImageNet weights.\nModels were trained for 15 epochs, though early stopping (patience=3.0) was employed in order to avoid overfitting.\n", "The model was optimized using sgd, using an initial learning rate of 0.005, a momentum of 0.9, and weight decay of 0.0005.\nThe learning rate was adjusted over the course of the training using a stepped learning rate scheduler, which multiplied the rate by 0.2 every 5 epochs.\nA warm-up period of 5,000 iterations was also implemented to ease into the initial learning rate.\nThe resulting learning rate curve is shown in fig:learningrate.\n{FIGURE}"]}
{"id": "2106.03279", "categories": "cs.LG", "paragraphs": ["Across all three examples, we consider the discounted setting where the discount factor is \\(\\gamma = 0.95\\) .\nThe learning rate is set to be \\(\\alpha = 0.01\\) .\nThe number of demonstrated trajectories is set to be 100 in both the random and near-optimal settings.\n"]}
{"id": "2108.11430", "categories": "cs.LG cs.ET", "paragraphs": ["We train all models for 200 epochs using RAdam\u00a0 optimizer with an initial learning rate of 0.002, an exponential decay rate of 0.98 per epoch, and a weight decay of 5e-4.\nOn CIFAR-10/100, images are augmented by random horizontal flips and random crops with 4 paddings.\nOn TinyImageNet, StanfordDogs-120, and StanfordCars-196, additional color jitter is added.\nMini-batch sizes are 64, 128, 64, and 64 for our 3-layer CNN, ResNet-18, DenseNet-121, and MobileNetV2, respectively.\n"]}
{"id": "2105.07636", "categories": "cs.LG cs.AI cs.CV stat.ML", "paragraphs": ["\nUnlike previous works like (Ruff et al., 2018; Goyal et al., 2020), we uniformly use an SGD optimizer with batch_size = 256. Although, training for each class represent a completely different problem, we adopt this to maintain consistency and isolate out the effect of optimizers for DOC vs. DOC\\(^3\\)  performances.\n\nFor DOC we fix the total number of iterations for gradient updates to 300. Except for class `DOG' and `Truck' we use 400 and 50 respectively. For DOC\\(^3\\)  we fix it to 350. This is in the same range as (Ruff et al., 2018), and hence incurs similar computation complexity as the baseline DOCC and DROCC algorithms.\n\nFinally for DOC\\(^3\\)  we fix \\(\\Delta = 0\\) .\n\n"]}
{"id": "2110.12072", "categories": "cs.LG", "paragraphs": ["For knowledge distillation on the ImageNet dataset, we run all distillation for 50 epochs with a batch size of 128, an initial learning rate of \\(0.1\\)  for training from scratch and \\(0.00001\\)  for fine-tuning, with milestones at \\([20, 30, 40]\\)  of a decreasing rate of \\(0.1\\) . The SGD optimizer with \\(0.9\\)  momentum is used to update the model parameters, and a weight decay of \\(0.0001\\)  is applied. For basic knowledge distillation, we set the temperature to 1 and the coefficients of the cross-entropy loss and KL-divergence loss both to \\(0.5\\) . For KDIGA, we keep the same setting as that of the basic KD, and set the coefficient of the input gradient alignment term to \\(\\frac{10^3}{B}\\) , where \\(B\\)  is the batch size of the inputs.\nFor experiments on the CIFAR-10 dataset, we run distillation for 200 epochs with a batch size of 125, an initial learning rate of \\(0.1\\)  with milestones at \\([100, 150]\\)  of a decreasing rate of \\(0.1\\) . The SGD optimizer with a momentum of \\(0.9\\)  and a weight decay of \\(0.0002\\)  is used to update the parameters. We set the coefficients of the cross-entropy loss and the KL-divergence loss both to \\(0.5\\) , and the coefficient for the input gradient alignment to \\(\\frac{10}{B}\\) , where \\(B\\)  is the batch size.\n"]}
{"id": "2111.09808", "categories": "cs.LG cs.CV", "paragraphs": ["The convolutional architecture uses convolution with 64 \\(3 \\times 3\\)  filters, followed by \\(2 \\times 2\\)  Max-Pooling, then 128 \\(3 \\times 3\\)  filters with \\(2 \\times 2\\)  Max-Pooling, and finally 128 \\(3 \\times 3\\)  filters with \\(2 \\times 2\\)  Max-Pooling. The network is complete with two fully connected layers, one with 256 units, and the output layer with \\(C\\)  units equal to the number of classes, and a softmax activation. All layers except the output use a ReLU activation, and we insert Batch Normalization layers between Convolutional and Max-Pooling layers.\n"]}
{"id": "2111.09733", "categories": "cs.CV", "paragraphs": ["We augment the training dataset with randomly rotated by 90,180,270 degrees and horizontal flip. The training image patches with the size \\(256 \\times 256\\)  are extracted as input \\(I_{in}\\)  of our network.\nThe network is trained for \\(7.5 \\times 10^5\\) , \\(1.5 \\times 10^6\\)  steps on Haze4k [19] and RESIDE [15] respectively.We use Adam optimizer with initial learning rate of \\(2 \\times 10^{-4}\\) , and adopt the CyclicLR to adjust the learning rate, where on the triangular mode, the value of gamma is 1.0,base momentum is 0.8, max momentum is 0.9, base learning rate is initial learning rate and max learning rate is \\(3 \\times 10^{-4}\\) . PyTorch [20] was used to implement our models with 4 RTX 3080 GPU with total batchsize of 40.\n"]}
{"id": "2106.08556", "categories": "cs.CL cs.AI", "paragraphs": ["The proposed models were implemented in PyTorch [23], and Hugging Face Transformers [32]. The Deep Graph Library (DGL) [31] was used for implementing the Coref-GNN. The trainable parameters were optimized by Adam [13]. The learning rate of the GCN component was 1e-3, and that of BART was set at 2e-5. We trained each model for 20 epochs and selected the best checkpoints on the validation set with ROUGE-2 score. All experiments were run on a single Tesla V100 GPU with 16GB memory.\n"]}
{"id": "2112.05646", "categories": "cs.CV", "paragraphs": ["Following recent trends [11], [2], [30], [23], the model is trained on the MS1MV2 dataset [11], which is the same data used to train the teacher model. The MS1MV2 is a refined version of MS-Celeb-1M [19] and contains 5.8M images of 85k identities. For the teacher network, the images are used unmodified, while for the student network, with a probability of 0.5, synthetic masks with random colors and random small deviations in shape are added. The synthetic masked images were created by mapping a template mask image on the extracted landmarks used for pre-processing with small variations in the mapped key points. All the images are aligned and cropped to 112x112x3 using MTCNN [42] and then normalized to have pixel values between -1 and 1. The simulated mask approach will be publicly provided to ensure comparability and reproducibility.\n{FIGURE}"]}
{"id": "2102.05399", "categories": "cs.CV", "paragraphs": ["We train the method for 11 epochs at a batch size of 1 and a learning rate of 0.0031. The momentum and weight decay values are set to 0.9 and 0.0001 respectively. The learning rate is decayed by 10 times at epochs 4, 8 and 10. We adopt multi-scaled training approach where the shorter edge is randomly sampled from six different scales of 1200,1000,800,600 and 400. The network is trained on an NVIDIA Tesla V100 GPU, 32 GB memory. Following are the training trajectories for the (1) complete network (Fig\u00a0REF ), and (2) the light enhancement module (Fig\u00a0REF ).\n{FIGURE}{FIGURE}"]}
{"id": "2103.12095", "categories": "cs.LG cs.AI", "paragraphs": ["All the models are optimized with Adam using the \\(\\ell _1\\)  loss as the cost function associated with the HR predictions. In each epoch, we compute the validation loss. After training is complete, we load the model weights that yielded the lowest validation loss. Training was done using a batch size of 64 over 100 epochs for PCE-LSTM and DeepConvLSTM. FFNN showed slower convergence and hence was trained for 200 epochs. Subject 9 of the PAMAP2 dataset was not included in the analysis because the corresponding time series is shorter than the length of the training segment (102s).\n"]}
{"id": "2111.13445", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["Here we discuss the general hyperparameters and experimental setup used for the full and linear finetuning experiments.\nRegarding data loading image augmentation settings, we are careful to match them to the ones used in the original upstream training protocol. Specifically, this affects the choice of whether to use Bicubic or Bilinear image interpolation for image resizing; for example, RigL models were trained using Bicubic interpolation, whereas the other pruning methods considered used the Biliniar interpolation. All ResNet and MobileNet models considered were trained using standard ImageNet-specific values for the normalization mean and standard deviation. In the case of full finetuning, we used dataset-specific normalization values for the downstream tasks; these were obtained by loading the dataset once with standard data augmentations and computing the means and variances of the resulting data. For linear finetuning, we use standard ImageNet normalization values.\nFor both full and linear finetuning, we use the same training hyperparameters as [51]; specifically, we train for 150 epochs, decreasing the initial learning rate by a factor of 10 every 50 epochs. We use 0.01 as the initial learning rate for all linear finetuning experiments; for full finetuning, we empirically found 0.001 to be the initial learning rate which gives comparable results for most datasets except Aircraft and Cars, for which we use 0.01.\nOur experiments were conducted using PyTorch 1.8.1 and NVIDIA GPUs.\nAll full finetuning experiments on the ResNet50 backbone were repeated three times and all linear finetuning experiments five times.\n"]}
{"id": "2105.10430", "categories": "cs.LG cs.NE q-fin.TR", "paragraphs": ["As described above, we adapt DeepLOB [34] as our encoder and further details of the architecture and hyperparameters can be found in their work. In terms of the decoder, we use a single LSTM with 64 units for both Seq2Seq and Attention, denoted as DeepLOB-Seq2Seq and DeepLOB-Attention respectively. We include a wide variety of benchmark algorithms in the experiment, including a support vector machine (SVM [27]), a multi-layer perceptron (MLP [27]), a convolutional network (CNN-I [26]), a LSTM ([27]), a variant convolutional network (CNN-II [28]), as well as an Attention-augmented-Bilinear-Network with one hidden layer (B(TABL) [25]) and two hidden layers (C(TABL) [25]). Note that these benchmark algorithms produce a single-point estimation and the authors did not test on all prediction horizons available for the FI-2010 dataset.\n"]}
{"id": "2106.01112", "categories": "cs.CL", "paragraphs": ["For all the experiments involving training, we run the experiments five times with different random seeds for model weights initialization to reduce the risk of randomness. The experiments are performed on a single Tesla V100 32GB GPU with a batch size of 512. The model is trained for 20 epochs and its parameters are optimized using the Adam optimizer. The average run time for each epoch is around 8 hours and 15 minutes. The initial learning rate is set to 0.002 and decays by a factor of 0.5 per epoch. A dropout of 0.5 is also applied.\n", "For Empathetic Dialogue and DailyDialog, the context window length, \\(M\\)  is set to 4, because these two datasets contain relatively short conversations (4.31 and 7.90 average number of utterances per dialogue respectively). A context window size of 4 ensures each utterance is connected to all the remaining utterances in most of the dialogues. The utterances may provide important contextual information to each other within a dialogue. For ConvAI2, \\(M\\)  is set to 2 to avoid introducing too much irrelavant context information. This is because most of the conversations in ConvAI2 are about two people getting to know each other and there are frequent topic changes in the conversations. \\(M\\)  serves as an important hyperparameter to control the influence of an utterance on the rest in a dialogue.\n"]}
{"id": "2108.02319", "categories": "cs.LG cs.AI", "paragraphs": ["We evaluate our model using the PyTorch deep learning library\u00a0[20] on a machine equipped with 32 CPU cores (Intel Xeon Silver 4110 CPU@2.10GHz), 188GB RAM, and eight GPU cores (GeForce GTX 1080TI) with 11GB VRAM.\n", "We use ADAM\u00a0[21] for optimization, where we reduce the learning rate by a factor of 0.1 starting from 0.001 if the validation performance does not improve for five epochs.\n"]}
{"id": "2109.12846", "categories": "cs.LG cs.AI cs.CY", "paragraphs": ["We evaluated HAGEN on two real-world benchmarks in Chicago and Los Angeles by CrimeForecaster\u00a0[19]. In our experiments, we use the same \u201ctrain-validation-test\u201d setting as the previous work \u00a0[19], [9]. We chronologically split the dataset as 6.5 months for training, 0.5 months for the validation, and 1 month for testing. For the vital hyperparameters in HAGEN, we use two stacked layers of RNNs. Within each RNN layer, we set 64 as the size of the hidden dimension. Moreover, we set the subgraph size of the sparsity operation as 50 and the saturation rate as 3. For the learning objective, we fix the trade-off parameter \\(\\lambda \\)  as \\(0.01\\) , similar to the common practice of other regularizers.\n"]}
{"id": "2103.01644", "categories": "cs.CV cs.AI", "paragraphs": ["The proposed network is trained for 70 epochs with Adam optimizer [15] and set the initial learning rate to \\(5e-4\\) . We then reduce the learning rate at epoch 5 and 20 by \\(\\gamma =0.1\\) . We optimise the network by minimising:\n\\(\\mathcal {J} = \\alpha \\mathcal {J}_1 + \\beta \\mathcal {J}_2\\) \n"]}
{"id": "2110.02497", "categories": "cs.LG cs.AI", "paragraphs": ["In contrast, inputs for the ImageNet supervised learning task are of shape \\(3 \\times H \\times W\\) . This presents a challenge for pretraining using the ImageNet task, since input shapes for the ImageNet task and our RL tasks are different. We work around this issue by slightly modifying the ImageNet task and altering the pretraining network architecture. Given a single input image and label pair \\((\\textbf {I}, y)\\) , the standard loss used for ImageNet training is the cross-entropy loss between the predicted and true class label. In our pretraining setup, a single datapoint consists of \\(F\\)  random samples from the ImageNet dataset: \\(\\lbrace  (\\textbf {I}_1, y_1), (\\textbf {I}_2, y_2) \\ldots (\\textbf {I}_F, y_F) \\rbrace \\) . We then feed all \\(F\\)  inputs \\(\\lbrace  \\textbf {I}_1, \\textbf {I}_2, \\ldots \\textbf {I}_f \\rbrace \\)  into our network at once, resulting in an input shape of \\(3F \\times H \\times W\\) . Notably, this input shape is identical to that which will be used during RL training. The network is tasked with classifying all \\(F\\)  inputs at once, so our pretraining setup uses the following loss for a single datapoint:\n\\( \\mathcal {L}(\\theta ) = \\frac{1}{F} \\sum _{i=0}^F \\mathcal {L}_{CE}(y_i, f_{i, \\theta }(\\hat{y} | \\textbf {I}_i)) \\) \n", "In order to solve this issue of \u201csignal mixing\", our final network architecture consists of grouped convolutional layers. A grouped convolution with \\(N\\)  groups will take a set of \\(kN\\)  channels as input, and apply \\(N\\)  independent convolutions on channels \\(\\lbrace 1, \\ldots k\\rbrace , \\lbrace k + 1, \\ldots 2k\\rbrace , \\ldots \\lbrace (N-1)k + 1, \\ldots , N k\\rbrace \\) . Given an input of size \\(3F \\times H \\times W\\) , we utilize a grouped convolution with \\(N = F\\)  groups in order to apply a different convolutional filter to each \\(3 \\times H \\times W\\)  input image in parallel.\n", "In our experiments we select \\(F = 3\\) , following previous work on DeepMind Control tasks [5]. Figure\u00a0REF  and Figure\u00a0REF  outline our network architecture in detail. Using this setup, we observe faster convergence and higher accuracy during ImageNet pretraining. For all RL experiments with ImageNet pretraining, we pretrain our fully parallel network for 400 epochs on 100 ImageNet classes. We select 100 random classes from those used in the ImageNet-R dataset [7], which consists of visually dissimilar classes. Since the networks used in RL training are not usually as powerful as those used to solve ImageNet, we do not want to spend inordinate amounts of time learning to separate closely related classes, such as \u201cNorwich terrier\" and \u201cNorfolk terrier\".\n"]}
{"id": "2112.02221", "categories": "cs.CV", "paragraphs": ["Training has been done on GPU and we used NVIDIA Ge Force 1080 TI GPU for training. We used a system that has 64GB RAM, 1TB hard disk along with 250GB SSD. The total number of epochs during training are 200 and one epoch length is 1000. RPN overlap is 0.7 and IOU is set to 0.5 during non-maximum suppression. The learning rate is set to 1e-5.\n"]}
{"id": "2112.02215", "categories": "cs.LG cs.AI math.OC", "paragraphs": ["Here we report the fixed set of hyper parameters used by all methods. These were determined based on two factors: (1) the commonly used settings across the RL literature (for example 64x64 architecture and batch size 64 is most commonly used across many different problems and methods), and by sampling random combinations from a large grid of hyper parameters and comparing results trends to narrow down the set of hyper parameters to consider to consistently well-performing values and reasonable ranges.\n", "For PARL, a common set of hyper-parameters were used across all five settings. The discount factor was set to 0.75, learning-rate was set to 0.001, and the sample-averaging approach used was quantile sampling with 3 demand-samples per step.\n"]}
{"id": "2112.02214", "categories": "cs.CV cs.GR", "paragraphs": ["All the experiments are implemented using PyTorch. In the training stage, the optimization function is Adam, with a constant learning rate of 1e-4 and a batch size of 1. We train our model for 100 epochs and apply it to the testing data directly.\n"]}
{"id": "2105.04983", "categories": "cs.LG", "paragraphs": ["We employed the TT-RNN model in the recurrent layer to compress the weight matrix \\(\\mathbf {W}^{xh} \\in \\mathbb {R}^{M \\times P}\\)  by reshaping and expressing it in the TT-format, in accordance to the dimensions of the input tensors \\(\\ten {X}_t\\) , such that \\(P=2 \\times 2 \\times 5 \\times 6 \\times 4\\)  and \\(M= 4\\times 4 \\times 4\\times 4 \\times 4\\) . An illustration of the interaction between \\(\\ten {X}_t\\)  and the tensorized \\(\\mathbf {W}^{xh}\\)  is shown in Fig. REF  in Tensor Network form.\n", "The so established TT-RNN model was trained using stochastic gradient descent with a learning rate of \\(10^{-5}\\)  over 20 epochs, with a batch size of 66, and using the categorical cross-entropy loss function. The TT-ranks \\((R_1, R_2, R_3, R_4)\\)  for the TT-RNN model were empirically found to be optimal for rank \\(R_i = 6\\) , \\(i=1, \\dots , 4\\) . It was also found that higher values of \\(R_i\\)  would result in over-fitting, and lower values in under-fitting. In turn, this suggests that the TT-ranks may be used for regularization in addition to compression.\n{FIGURE}"]}
{"id": "2106.12423", "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "paragraphs": ["We used 8 GPUs for all our training runs and continued the training until the discriminator had seen a total of 25M real images when training from scratch, or 5M images when using transfer learning.\nFigure\u00a0REF  shows the hyperparameters used in each experiment.\nWe performed the baseline runs (configs\u00a0a\u2013c) using the corresponding standard configurations by Karras\u00a0et\u00a0al.\u00a0[32], [30]:\nStyleGAN2 config\u00a0F (\u201cstylegan2\u201d in the official implementation) for the high-resolution datasets in Figure\u00a0REF , left;\nand ADA 256\\(\\times \\) 256 baseline (\u201cpaper256\u201d) for the low-resolution ablations in Figure\u00a0REF  and Figure\u00a0REF , right.\n", "Many of our hyperparameters, including discriminator capacity and learning rate, batch size, and generator moving average decay, are inherited directly from the baseline configurations, and kept unchanged in all experiments.\nIn configs\u00a0c and\u00a0d, we disable noise inputs\u00a0[31], path length regularization\u00a0[32], and mixing regularization\u00a0[31].\nIn config\u00a0d, we also decrease the mapping network depth to 2 and set the minibatch standard deviation group size to 4 as recommended in the StyleGAN2-ADA documentation (\u201cauto8\u201d).\nThe introduction of explicit normalization in config\u00a0d allows us to use a higher learning rate, 0.0030, for the generator in all subsequent configurations.\nIn Figure\u00a0REF , right, we show results for path length regularization with weight 0.5 and mixing regularization with probability 0.5.\n"]}
{"id": "2109.08569", "categories": "cs.CL", "paragraphs": ["We train the model for 200K steps using a batch size of 140. We use 20K steps for BERT warmup, 10K steps for decoder warmup, and a max position of 512. We use 4 Nvidia Quadro RTX 5000 GPUs. We use the checkpoint with highest ROUGE score on validation set for testing.\n"]}
{"id": "2101.03958", "categories": "cs.LG cs.AI cs.NE", "paragraphs": ["Meta-Training details:\nWe search over programs with maximum 20 nodes, not including inputs or parameter nodes. A full list of node types is provided in Appendix . We use a population size of 300, tournament size of 25, and choose these parameters based on the ones used in [30]. Mutations occur with probability \\(0.95\\) . Otherwise a new random program is sampled. The search is done over 300 CPUs and run for roughly 72 hours, at which point around \\(20,000\\)  programs have been evaluated. The search is distributed such that any free CPU is allocated to a proposed individual such that there are no idle CPUs. Further meta-training details are in Appendix .\n", "Training environments:\nThe choice of training environments greatly affects the learned algorithms and their generalization performance. At the same time, our training environments should be not too computationally expensive to run as we will be evaluating thousands of RL algorithms. We use a range of 4 classical control tasks (CartPole, Acrobat, MountainCar, LunarLander) and a set of 12 multitask gridworld style environments from MiniGrid [7]. These environments are computationally cheap to run but also chosen to cover a diverse set of situations. This includes dense and sparse reward, long time horizon, and tasks requiring solving a sequence of subgoals such as picking up a key and unlocking a door. More details are in Appendix .\n", "The training environments always include CartPole as an initial hurdle. If an algorithm succeeds on CartPole (normalized training performance greater than \\(0.6\\) ), it then proceeds to a harder set of training environments. For our experiments, we choose these training environments by sampling a set of 3 environments and leave the rest as test environments. For learning from scratch we also compare the effect of number of training environments on the learned algorithm by comparing training on just CartPole versus training on CartPole and LunarLander.\n", "RL Training details:\nFor training the RL agent, we use the same hyperparameters across all training and test environments except as noted. All neural networks are MLPs of size (256, 256) with ReLU activations. We use the Adam optimizer with a learning rate of \\(0.0001\\) . \\(\\epsilon \\)  is decayed linearly from 1 to \\(0.05\\)  over \\(1\\mathrm {e}{3}\\)  steps for the classical control tasks and over \\(1\\mathrm {e}{5}\\)  steps for the MiniGrid tasks.\n"]}
{"id": "2111.10794", "categories": "cs.CV", "paragraphs": ["To validate the performance of our method on various datasets, we conduct experiments on not only COCO and ImageNet, which are mainly used in the other methods, but also Tiny ImageNet, which is a relatively small dataset.\nCOCO\u00a0[15] consists of about 118K training images which containing common objects in complex everyday scenes.\nImageNet\u00a0[7] consists of about 1.28M training images in 1K image classes.\nTiny ImageNet\u00a0[14] is a miniature of ImageNet.\nIt consists of 100K training images of size 64\\(\\times \\) 64 in 200 image classes.\n", "The pre-training setup mostly follows DenseCL\u00a0[22].\nA ResNet-50\u00a0[11] is adopted as a backbone.\nSGD optimizer is utilized and its weight decay and momentum are set to 0.0001 and 0.9, respectively.\nThe initial learning rates are set to 0.5, 0.3, and 0.03 in Tiny ImageNet, COCO, and ImageNet, respectively and cosine annealing schedule is used.\nThe batch size is set to 256, using 8 V100 GPUs.\nThe number of training epochs are set to 200, 800, and 200 in Tiny ImageNet, COCO, and ImageNet, respectively.\n"]}
{"id": "2110.05626", "categories": "cs.LG cs.CV", "paragraphs": ["We train models using SGD with initial learning rate of 0.1 and use cosine annealing learning rate scheduling [20]. We train all models for 100 epochs and perform evaluation on the model saved at the epoch which has the highest accuracy on the test set. For ResNet-18 models on CIFAR-10, we run 3 trials. For each trial, all models are seeded to the same seed. For all other models we run a single trial.\n"]}
{"id": "2107.04082", "categories": "cs.CL cs.SD eess.AS", "paragraphs": ["[itemsep=0em]\nTime-stride layer: reduce input sequence length by \\(R=4\\)  times (80 input dimension, 320 output dimension).\n\nLinear layer: 320 input dimension, 512 output dimension.\n\n", "[itemsep=0em]\nLinear layer: 512 input dimension, 1024 output dimension.\n\n1D Convolution layer: 1024 input dimension, 1024 output dimension, kernel size 48, filter groups 16.\n\nTransformers: 24 layers, 1024 input dimension, 16 attention head, 4096 feedforward dimension, GELU activation function, pre-layer norm [30].\n\nLinear layer: 1024 input dimension, 768 output dimension.\n\n", "For masking over the latent speech representation \\(Z\\) , we sample \\(p=0.065\\)  as the starting indices and we mask the next \\(M=5\\)  frames.\nOverall, this model has 300 million parameters.\n", "\nwav2vec 2.0 En is trained on English only,\n\nXLSR-7 is trained with 7 languages (en, es, fr, de, ru, my, ja). We resample the data with \\(\\alpha =0.5\\) .\n\nXLSR-25 is trained with all 25 languages. We resample the data with \\(\\alpha =0.5\\) .\n\n", "We set the diversity loss hyperparameter \\(\\lambda = 0.1\\)  for all experiments.\nAll models are trained using the Adam optimizer [31] with learning rate \\(lr=1e-3\\)  for wav2vec 2.0 En and \\(lr=5e-3\\)  for XLSR-{7,25} up to 300000 updates. We also add weight decay \\(1e-2\\)  with \\(\\ell ^2\\)  weight penalty. We anneal the learning rate by using a linear decay learning schedule, with warm-up step up to 32000 updates and linearly decay to 0 after that. We crop the input sequence length up to 2000 samples (equals to 20 seconds). For each update step, wav2vec 2.0 En calculates the gradient from 18 hours of data, and XLSR-{7,25} calculates the gradient from 36 hours of data.\n"]}
{"id": "2108.04556", "categories": "cs.CL cs.AI cs.PL", "paragraphs": ["We train CLSEBERT using Transformer with 12 layers, 768 hidden sizes and 12 attention heads. CLSEBERT is trained on 8 NVIDIA Tesla V100 with 32GB memory. The lengths of sequences containing special tokens in NL, PL and AST are set to 96, 160 and 256, respectively. The batch size is set to 128. The learning rate is set to 1e-4. We use an Adam optimizer to optimize parameters of the model. Finally, the model is trained with 110K steps. All experiments are implemented by PyTorchhttps://pytorch.org/.\n"]}
{"id": "2105.06423", "categories": "cs.LG cs.AI", "paragraphs": ["Training Setting on ImageNet.\nAll networks are trained using 8 GPUs with a mini-batch of 32 per GPU. We train all the architectures from scratch for 100 epochs using stochastic gradient descent (SGD) with momentum \\(0.9\\)  and weight decay 1e-4. The base learning rate is set to \\(0.1\\)  and is multiplied by \\(0.1\\)  after \\(30,60\\)  and 90 epochs. The fine-tuning procedure uses the same configuration except that the initial learning rate is set to \\(0.01\\) . The coefficient of sparse regularization \\(\\lambda _1\\)  and \\(\\lambda _2\\)  are set to 7e-5 and 3.5e-5. Besides, the covariance matrix in the proposed BW technique is calculated within each GPU. Like [33], we also use group-wise decorrelation with group size 16 across the network to improve the efficiency of BW.\n", "Training setting on CIFAR-10 and CIFAR-100. We train all models on CIFAR-10 and CIFAR-100 with a batch size of 64 on a single GPU for 160 epochs with momentum \\(0.9\\)  and weight decay 1e-4. The initial learning rate is 0.1 and is decreased by 10 times at 80 and 120 epoch. The coefficient of sparse regularization \\(\\lambda _1\\)  and \\(\\lambda _2\\)  are set to 4e-5 and 8e-5 for CIFAR-10 dataset and 7e-6 and 1.4e-5 for CIFAR-100 dataset.\n"]}
{"id": "2111.09999", "categories": "cs.CV cs.CR", "paragraphs": ["\nLarge Scale Visual Recognition (ImageNet\u00a0[15]). ImageNet is a highly popular real-world dataset with a million high-resolution images of a large variety of objects used for training state-of-the-art deep perception models. The goal is to recognize visual scenes among 1,000 different classes. This is one of the most popular dataset in computer vision for benchmarks state-of-the-art models. In this task, we utilized state-of-the-art pre-trained models available from Pytorch Deep Learning library\u00a0[42]; notably, these models are used as base models by many Machine Learning practitioners for transfer learning to build systems for different visual tasks.\n\n", "[h!]\nInputs: a batch of images \\(\\lbrace \\mathbf {x}^\\text{(i)}\\rbrace _{i=1}^m\\)  with batch size \\(m\\) , source label \\(\\lbrace y^\\text{(i)}_{\\text{source}}\\rbrace _{i=1}^m\\) ,\ntarget label \\(\\lbrace y^\\text{(i)}_{\\text{target}}\\rbrace _{i=1}^m\\) ,\nmodel \\(p_M\\) , latent vector \\(\\textbf {z}\\) , the hyper-parameter \\(\\lambda \\)  to balance the loss, natural generator \\(G_{\\theta }\\) , and the thresholds to detect TnT \\(\\tau _\\text{batch}\\) , \\(\\tau _\\text{val}\\)  for batch and validation set respectively.\nInitialization:\n\\(ASR < \\tau _\\text{val}\\) \nSample a batch of images \\(\\lbrace \\mathbf {x}^\\text{(i)}\\rbrace _{i=1}^m\\) \nSample a latent variable \\(\\textbf {z}\\sim p(\\textbf {z})\\) \n\\({\\delta }= G_{\\theta }(\\textbf {z})\\)  a flower patch\nGenerate the mask \\(\\textbf {m}\\)  based on \\({\\delta }\\) \n\\({\\delta }^{\\prime } \\leftarrow \\text{bgremoval}({\\delta }, \\textbf {m})\\) Background removal\n\\(i=1,...,m\\) \n\\({\\mathbf {x}^{\\prime }}^\\text{(i)} = (1-\\textbf {m})\\odot \\mathbf {x}^\\text{(i)}+\\textbf {m}\\odot {\\delta }^{\\prime }\\) \n\\(y^\\text{(i)}_{\\text{argmax}} = \\arg \\max _y p_M(y | {\\mathbf {x}^{\\prime }}^\\text{(i)})\\) \n\\(y^\\text{(i)}_{\\text{argmax}} = y_\\text{target}^\\text{(i)}\\) \n\\(fool = fool + 1\\) \n\n\n\\(L=\\ell (\\lbrace {\\mathbf {x}^{\\prime }}^\\text{(i)}\\rbrace _{i=1}^m,\\lbrace y_{\\text{target}}^\\text{(i)}\\rbrace _{i=1}^m) - \\lambda ~ \\ell (\\lbrace {\\mathbf {x}^{\\prime }}^\\text{(i)}\\rbrace _{i=1}^m,\\lbrace y_{\\text{source}}^\\text{(i)}\\rbrace _{i=1}^m)\\)\n", "\\({\\theta }\\)  has not converged\n\\(t=1,...,n_\\text{critic}\\) \n\\(i=1,...,m\\) \nSample real data \\(\\mathbf {x}\\sim \\mathbb {P}_r\\) , latent variable \\(\\textbf {z}\\sim p(\\textbf {z})\\) , a random number \\(\\epsilon \\sim U[0,1]\\) \nGet the fake sample \\(\\tilde{\\mathbf {x}} \\leftarrow G_{\\theta }(\\textbf {z})\\) \n\\(\\hat{\\mathbf {x}} \\leftarrow \\epsilon \\mathbf {x}+ (1-\\epsilon )\\tilde{\\mathbf {x}}\\) \n\\(L^\\text{(i)} \\leftarrow D_{\\omega }(\\tilde{\\mathbf {x}}) - D_{\\omega }(\\mathbf {x}) + \\lambda (\\Vert {\\nabla _{\\hat{\\mathbf {x}}}D(\\hat{\\mathbf {x}})}\\Vert _2 - 1)^2 \\) \n\n\\({\\omega }\\leftarrow \\text{Adam}(\\nabla _{\\omega }\\frac{1}{m}\\sum _{i=1}^mL^\\text{(i)},{\\omega },\\alpha ,\\beta _1,\\beta _2)\\) \n\nSample a batch of latent variable \\(\\lbrace \\textbf {z}^\\text{(i)}\\rbrace _{i=1}^m \\sim p(\\textbf {z}).\\) \n\\({\\theta }\\leftarrow \\text{Adam}(\\nabla _{\\omega }\\frac{1}{m}\\sum _{i=1}^m-D_{\\omega }(G_{\\theta }(\\textbf {z})),{\\theta },\\alpha ,\\beta _1,\\beta _2)\\) \n\nTraining procedure for natural generation network (with generator parameters \\({\\theta }\\) ).\n"]}
{"id": "2102.07954", "categories": "cs.CV cs.AI stat.ML", "paragraphs": ["We exactly follow the training settings in\u00a0[44]\u00a0https://github.com/facebookresearch/AttentiveNAS.\nSpecifically,\nwe train our supernets for 360 epochs with cosine learning rate decay.\nWe adopt SGD training on 64 GPUs. The mini-batch size is 32 per GPU.\nWe use momeutm of 0.9, weight decay of \\(10^{-5}\\) , dropout of \\(0.2\\) , stochastic layer dropout of \\(0.2\\) .\nThe base learning rate is set as 0.1 and is linearly scaled up for every 256 training samples.\nWe use AutoAugment [7] for data augmentation and set label smoothing coefficient to \\(0.1\\) .\n"]}
{"id": "2103.02225", "categories": "cs.LG", "paragraphs": ["For VD-DDPG, we set the KL weight \\(\\beta \\)  as 1000 and the clip value \\(c\\)  as 0.2.\nThe latent variable dimension (\\(z\\)  dim) is set to 50 and the representation dimension is set to 100.\nWe collect trajectories experiences in the first 5000 time steps and then pre-train the trajectory model (along with the representation model) and the conditional VAE for 15000 time steps, after which we start the training of the actor.\nAll the time steps above are counted into the total time steps for a fair comparison.\nThe trajectory return model (along with the representation model) is trained every 10 time steps for the pre-train process and is trained every 50 time steps in the rest of training, which already ensures a good performance in our experiments.\nThe actor network and the conditional VAE are trained every 1 time step.\n"]}
{"id": "2110.03921", "categories": "cs.CV cs.LG", "paragraphs": ["We train ViDT for 50 epochs using AdamW\u00a0[12] with the same initial learning rate of \\(10^{-4}\\)  for its body, neck and head. The learning rate is decayed by cosine annealing with batch size of 16, weight decay of \\(1\\times 10^{-4}\\) , and gradient clipping of \\(0.1\\) . In contrast, ViDT\u00a0(w.o. Neck) is trained for 150 epochs using AdamW with the initial learning rate of \\(5\\times 10^{-5}\\)  by cosine annealing. The remaining configuration is the same as for ViDT.\n"]}
{"id": "2111.14556", "categories": "cs.CV", "paragraphs": ["ImageNet. ImageNet 2012 [12] comprises 1.28 million training images and 50,000 validation images from 1000 different classes. For ResNet-based models, we follow the training schedule in [53] and train all the models for 100 epochs. We use SGD with batchsize 256 on 8 GPUs. Cosine learning rate is adopted with the base learning rate set to 0.1. We apply standard data augmentation, including random cropping, random horizontal flipping and normalization. We use label smoothing with coefficient 0.1. For experiments on Transformer-based models, including PVT and Swin-Transformer, we follow training configurations in the original paper.\n", "COCO. COCO dataset [30] is a standard object detection benchmark and we use a subset of 80k samples as training set and 35k for validation. For ResNet and SAN models, we train the network by SGD and 8 GPU are used with a batchsize of 16. For PVT and Swin-Transformer models, we train the network by adamw. Backbone networks are respectively pretrained on ImageNet dataset following the same training configurations in the original paper. We follow the \"1x\" learning schedule to train the whole network for 12 epochs and divide the learning rate by 10 at the 8th and 11th epoch respectively. For several transformer-based models, we follow the configurations in the original paper, and additionally experiment \"3x\" schedule with 36 epochs.\nWe apply standard data augmentation, that is resize, random flip and normalize. Learning rate is set at 0.01 and linear warmup is used in the first 500 iterations. We follow the \"1x\" learning schedule training the whole network for 12 epochs and divide the learning rate by 10 at the 8th and 11th epoch respectively. For several transformer-based models, we follow the configurations in the original paper, and test with \"3x\" schedule. All mAP results in the main paper are tested with input image size (3, 1333, 800).\n", "ADE20K. ADE20K [55] is a widely-used semantic segmentation dataset, containing 150 categories. ADE20K has 25K images, with 20K for training, 2K for validation, and another 3K for testing. For two baseline models, PVT and Swin-Transformer, we follow the training configurations in their original paper respectively. For PVT, we implement the backbone models on the basis of Semantic FPN [25]. We optimize the models using AdamW with an initial learning rate of 1e-4 for 80k iterations. For Swin-Transformer, we implement the backbone models on the basis of UperNet [48]. We use the AdamW optimizer with an initial learning rate of 6e-5 and a linear warmup of 1,500 iterations. Models are trained for a total of 160K iterations. We randomly resize and crop the image to 512 \u00d7 512 for training, and rescale to have a shorter side of 512 pixels during testing.\n"]}
{"id": "2111.02394", "categories": "cs.CV", "paragraphs": ["Following previous methods [44], [39], [6], [43], we pre-train our models on IC17-MLT for 300 epochs, in which images are cropped and resized to 640 \\(\\times \\)  640 pixels.\nWe then finetune the models for 600 epochs.\nThe dilation size \\(s\\)  is set to 9 in our experiments unless explicitly stated.\nAll models are optimized by Adam with batch size 16 on 4 GPUs.\nWe adopt a \u201cpoly\" learning rate schedule with an initial learning rate of \\(1 \\times 10^{-3}\\) .\nTraining data augmentations include random scale, random flip, random rotation, random crop, and random blur.\n"]}
{"id": "2111.02358", "categories": "cs.CV cs.CL cs.LG", "paragraphs": ["Our models adopt the same network configuration as ViT\u00a0[11] and BEiT\u00a0[1].\nVLMo-Base consists of 12-layer Transformer blocks with 768 hidden size and 12 attention heads.\nVLMo-Large is a 24-layer Transformer network with 1024 hidden size and 16 attention heads.\nThe intermediate size of feed-forward networks is 3072 and 4096 for base-size and large-size models, respectively.\nFor images, the input resolution is \\(224 \\times 224\\)  and the patch size is \\(16 \\times 16\\)  during pre-training.\nWe apply RandAugment\u00a0[8] to the input images.\nThe tokenizer of the uncased version of BERT is employed to tokenize the text.\nThe maximum text sequence length is set to 40.\n", "We pretrain the models for 200k steps with 1024 batch size.\nWe utilize AdamW\u00a0[28] optimizer with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.98\\) .\nThe peak learning is 2e-4 for the base-size model, 5e-5 for the large-size model.\nWeight decay is set to \\(0.01\\) .\nWe use linear warmup over the first \\(2.5\\) k steps and linear decay.\n"]}
{"id": "2106.04149", "categories": "cs.LG", "paragraphs": ["We adopted [22] a two-layer ReLU Multi-Layer Perceptron (MLP) for classification tasks on 4 UCI datasets, trained for 1000 episodes with batch-size 64 and Adam [15] optimizer. We report the best performance for each smooth rate under a set of learning rate settings, \\([0.0007, 0.001, 0.005, 0.01, 0.05]\\) .\n", "We adopted [22] a two-layer ReLU Multi-Layer Perceptron (MLP) for classification tasks on 4 UCI datasets, trained for 1000 episodes with batch-size 64 and Adam [15] optimizer. We report the best performance for each smooth rate under a set of learning rate settings, \\([0.0007, 0.001, 0.005, 0.01, 0.05]\\) .\n"]}
{"id": "2109.07953", "categories": "cs.CL", "paragraphs": ["Our model is implemented in Python 3, and mainly uses the\nfollowing dependencies: torch\nas the machine learning library, nltk\nfor text preprocessing, transformers\nfor\ntheir BERT implementation, and numpy\nfor high-level mathematical operations in CPU. During our experiments, we used machines with a\nsingle GeForce GTX 1080Ti GPU, 4 CPUs and 16GB of\nRAMs. The training times for all datasets are less than a day.\nThe total number of parameters depends on the number of attributes, each of which has their own attribute-specific adapters.\nIn our experiments, excluding the embedding matrices and classifiers that vary a lot across datasets and tasks,\nBERT-base with Injectors can have a total of 105M parameters with 19M (18%) trained for tasks with two attributes, or a total of 121M parameters with 36M (29%) trained for tasks with four attributes.\nUsing the accuracy of the model on the development set, we tuned the learning rate (from \\(1e-6\\) , \\(3e-5\\) , \\(1e-5\\) , and \\(3e-4\\) ), the adapter size (from 32, 48, 64, and 128), and the hypercomplex dimensions (from 2, 4, 6, and 8).\n"]}
{"id": "2102.07266", "categories": "cs.LG cs.AI cs.RO stat.ML", "paragraphs": ["Sparse Dynamic. The only difference with the original dynamic model is the additional application of the confusion-contribution loss \\((L_{CC})\\)  during the training process. A hyperparameter selection of \\(\\lbrace k_1=0.1, k_2=1\\rbrace \\)  was seen to give competitive results for most ProcGen environments.\n", "All the three configurations are trained using the Proximal policy optimization (PPO) [30] algorithm, which is ran with 4 parallel workers for gradient computations as this is seen to enhance performance. Each worker is trained for 50M steps, thus equating to a total of 200M steps across all the 4 workers. All results are reported as the average across 4 runs using 500 levels for training.\n"]}
{"id": "2102.07358", "categories": "cs.LG cs.AI cs.CV", "paragraphs": ["Generally, the scaling factor \\(\\alpha \\)  in our algorithm is set to \\(1e-4\\) .\nThe learning rate is selected from [1e-1, 1e-2, 1e-3, 1e-4, 1e-5], for the value with the best performance in experiments. The training epochs are empirically set as multiples of 10 and are selected for each experiment. We pre-run each experiment to determine the epoch value and stop training when the performance does not increase in the next 20 epochs to prevent over-fitting.\n", "In the VisDA-C experiments, the training epochs in each training step is chosen as: \\({ep}_1 = 90\\) , \\({ep}_2 = 90\\) , \\({ep}_3 = 40\\) , \\({ep}_4 = 180\\) . The learning rate for experiment is set to \\(1e-5\\) . Training batch size is set to 128. For the baseline \\(B_t\\) , it is trained for 90 epochs, and learning rate is \\(1e-5\\) . For \\(B_{f_1}\\) , it is trained on the source data with weak labels for 90 epochs and on the target data for 90 epochs, and the learning rate is \\(1e-5\\) . For \\(B_{f_2}\\) , it is trained on the source data with weak labels for 90 epochs and on the target data for 90 epochs, and the learning rate is \\(1e-5\\) . The image augmentation techniques are also applied for baselines \\(B_{t}\\) , \\(B_{f_1}\\) , \\(B_{t_2}\\) , and our approach. Other baselines use their original augmentation setting. We similarly use the function in the Pytorch vision package for the implementation,\nand the images may be rotated from \\(-3\\)  to 3 degree, or changed to gray-scale with a probability of 0.1, or horizontally flipped with a probability of 0.5.\n", "In the CIFAR-10 experiments, the training epochs in each training step is chosen as: \\({ep}_1 = 40\\) , \\({ep}_2 = 30\\) , \\({ep}_3 = 70\\) , \\({ep}_4 = 70\\) . The learning rate is set to \\(1e-3\\) . Training batch size is set to 128. For the baseline \\(B_t\\) , it is trained for 70 epochs, and the learning rate is \\(1e-3\\) . For \\(B_{f_1}\\) , it is trained on the source data with weak labels for 30 epochs and on the target data for 40 epochs, and the learning rate is \\(1e-3\\) . For \\(B_{f_2}\\) , it is trained on the source data with weak labels for 30 epochs and on the target data for 40 epochs, and the learning rate is \\(1e-3\\) . The image augmentation techniques are still applied to baselines \\(B_{t}\\) , \\(B_{f_1}\\) , \\(B_{t_2}\\) , and our approach. We use the function in the Pytorch vision package for the implementation,\nand the images are horizontally flipped with a probability of 0.5.\n"]}
{"id": "2103.04290", "categories": "cs.CL", "paragraphs": ["MTLHealth makes use of a technique called dropout, where a percentage of the output is randomly set to zero, which helps to prevent over-fitting by removing noise in the representations [20]. Dropout probability is set at \\(p = 0.5\\) .\n"]}
{"id": "2108.04135", "categories": "cs.CV cs.LG", "paragraphs": ["We trained the networks using an Adam optimizer [31] with a learning rate of \\(10^{-4}\\)  and beta1, beta2 values of 0.5 and 0.999. Hyper-parameters \\(\\lambda _{\\mathrm {prior}_{X}}\\) , \\(\\lambda _{\\mathrm {prior}_{Y}}\\) , \\(\\lambda _{\\mathrm {cyc}_{X}}\\)  and \\(\\lambda _{\\mathrm {cyc}_{Y}}\\)  were experimentally set to 10, 0.5, 5 and 0.25. Furthermore, we used a reduce on plateau learning rate scheduler with a patience of 10 epochs and a factor of 10. Batches of 8 patches were used and the models were trained for 35 epochs (\\(\\sim 220\\) k steps) on an NVIDIA TITAN XP GPU with 12 GB of VRAM. All experiments were repeated three times with a different initialization seed.\n"]}
{"id": "2109.10760", "categories": "cs.CV", "paragraphs": ["Training images and masks are obtained from FFHQ dataset\u00a0[9] by following the procedure as described in Section REF . We choose FFHQ dataset as it contains considerable variation in terms of age, ethnicity, poses and illumination conditions, and it has good coverage of accessories such as eyeglasses, sunglasses and hats. We obtain about 35,000 images and 10,000 masks for training, and 4000 images and 1000 masks for validation. Our model can do inference on images from VoxCeleb [14], CelebA-HQ [8], unseen faces from FFHQ or any other unconstrained faces. The network is trained using 256 \\(\\times \\)  256 images with a batch size of 8. The model is optimized using Adam optimizer [10] with \\(\\beta _1\\)  = 0.0 and \\(\\beta _2\\)  = 0.9. All generators are trained with learning rate \\(10^{-4}\\) , and discriminators are trained with learning rate of \\(10^{-5}\\) .\n"]}
{"id": "2110.08176", "categories": "cs.LG cs.HC cs.MA", "paragraphs": ["Agents are trained using a distributed set of environments running in parallel. Each agent is trained using one GPU on \\(N \\times 200\\)  environments, where \\(N\\)  is the number of agents being trained in the population. Agents are trained for \\(1 \\times 10^9\\)  environment steps which takes between three and eight days depending on the size of the training population. As the environment involves two players, each one samples with replacement from the training population of agents every episode.\n"]}
{"id": "2106.02866", "categories": "cs.LG", "paragraphs": ["For unsupervised pretraining, we select a multi-layer convolutional network as the encoder \\(\\phi _{\\theta }\\) , and we select a two-layer transformer with hidden dimension 256 as the sequential model \\(\\psi _{\\rho }\\) . Here, the positive pair is \\((h_{t+k}, c_t)\\)  where \\(k\\)  is the number of time steps ahead, and the negative pairs are \\((h_i, c_t)\\) , where \\(h_i\\)  hidden representations of a batch of random hidden representations assumed to be unrelated to \\(c_t\\) . The scoring function \\(f\\)  based on Equation (1) in the main text at step \\(t\\)  with \\(k\\)  steps ahead is \\(f_k = f_k(h, c_t) = \\exp ((h)^\\top W_k c_t)\\) , where \\(W_k\\)  is a learnable linear transformation defined separately for each \\(k\\in \\lbrace 1,...,K\\rbrace \\)  and \\(K\\)  is predetermined as 12 time steps. The loss will then be formulated as:\n\\( \\ell ^{\\mathrm {InfoNCE}}_{t} = - \\frac{1}{K} \\sum _{k=1}^K \\big [{\\rm log}\\frac{{\\rm exp}(f_k(h_{t+k}, c_t))}{\\sum _{h_i\\in \\mathcal {N}}{\\rm exp}(f_k(h_i, c_t)))}]\\) \n", "Classifier-based method use \\(\\int _{\\mathcal {Z}}D_{\\mathrm {KL} }(P_{X,Y,Z}\\Vert P_{X,Z} P_{Y|Z})\\,{\\rm d}P_{Z}\\)  to estimate conditional mutual information. To be specific, given \\(n\\)  i.i.d samples \\(\\left\\lbrace x_{i}, y_{i}, z_{i}\\right\\rbrace _{i=1}^{n}, (x_{i}, y_i, z_i) \\sim P_{X, Y, Z}\\) , \u00a0[28] use the generative model GAN\u00a0[13]\nto model the conditional distribution \\(P(Y|Z)\\) . For notation simplicity, we refer the GAN model as \\(\\hat{P}^{\\rm GAN}(Y|Z)\\) . Given samples from the joint distribution, \\(P_{X, Y, Z}\\) , and samples from \\(P_{X,Z} \\hat{P}^{\\rm GAN}_{Y|Z}\\) , classifier-based method labels the points drawn from \\(P_{(X, Y, Z)}\\)  as \\(label=1\\)  and the points from \\(\\hat{P}_{X,Z} P^{\\rm GAN}_{Y|Z}\\)  as \\(label=0\\) . Then, it trains a binary classifier for predicting the assigned binary label. Then the point-wise likelihood ratio \\(\\frac{p(x,y,z)}{p(x,z) p(y|z)}\\approx \\frac{p(x,y,z)}{p(x,z) p^{\\rm GAN}(y|z)}\\)  of each data point \\((x_i,y_i,z_i)\\)  can be calculated by \\(\\frac{Pr(label=1|(x_i,y_i,z_i))}{1 - Pr(label=1|(x_i,y_i,z_i))}\\) , where \\(Pr(label=1|(x_i,y_i,z_i)\\)  is the predicted probability of data point has \\(label=1\\)  from the classifier. Using the point-wise likelihood, we can obtain \\(\\int _{\\mathcal {Z}}D_{\\mathrm {KL} }(P_{X,Y,Z}\\Vert P_{X,Z} P_{Y|Z}) {\\rm d}P_Z\\)  by plugging the point-wise likelihood into a lower bound of KL-divergence. Further discussions of this classifier-based estimation method is out of the scope of our discussion, and readers could refer to\u00a0[28] for more details.\n"]}
{"id": "2110.13705", "categories": "cs.LG", "paragraphs": ["IHDP: This data is constructed from the Infant Health and Development Program (IHDP). There are 100 files in which each file contains 747 subjectsThe IHDP data set is available at https://github.com/Osier-Yi/SITE/tree/master/data. Both factual and counterfactual are provided for each subject, which provides a ground truth for evaluating causal inference algorithms.\n", "Twins: This data is a benchmark task that utilizes data from twin births in the USA between 1989-1991. There are 11399 subjects in this dataThe Twins data set is available at https://github.com/jsyoon0823/GANITE/tree/master/data or https://github.com/AMLab-Amsterdam/CEVAE/tree/master/ datasets/TWINS.. The samples in the data set are all twins, \\(t = 1\\)  represents the heavier baby, and the outcome \\(Y\\)  corresponds to the mortality of each of the twins in their first year of life.\n", "ACIC: This data is a collection of semi-synthetic datasets derived from the linked birth and infant death data (LBIDD)\u00a0[18]. It was developed for the 2018 Atlantic Causal Inference Conference competition (ACIC)We use the scaling folder in ACIC to evaluate our methods. The ACIC data set is available at https://github.com/IBM-HRL-MLHLS/IBM-Causal-Inference-Benchmarking-Framework/tree/master/data/LBIDD., which include 30 different data generating process settings with subject sample sizes from 1,000 to 50,000.\n", "To train CEVIB, we use the architecture in Figure REF . For both IHDP and ACIC experiments, we randomly split each file data into test/validation/train with proportion 63/27/10 and report the In SampleIn Sample uses all observational data for both training and prediction. and Out of SampleOut of Sample uses the training set to train and the test set for prediction. errors, and repeat the procedure for 25 times. For Twins experiments, we randomly split the data into test/validation/train with proportion 56/24/20 and report the In Sample and Out of Sample errors, and repeat the procedure for 50 times.\n"]}
{"id": "2110.13864", "categories": "cs.LG cs.AI cs.CV cs.DC", "paragraphs": ["where \\({\\mathbf {W}}_{t,I}^{k}(\\alpha =1)\\)  indicates that \\({\\mathbf {W}}_{t,I}^{k}\\)  is trained using Equation\u00a0REF  with setting \\(\\alpha =1\\)  (i.e., the \\(k\\) -th device is benign). A special case is \\({\\mathbf {W}}_t({\\mathbb {S}}\\setminus {\\mathbb {M}})\\) , which means the global model is optimized when all the malicious devices do not conduct attacks before the \\(t\\) -th round. To quantify the attack effect on the global model, we define the Attack Effect on Parameter (AEP) as follows:\n"]}
{"id": "2110.04866", "categories": "cs.LG", "paragraphs": ["For all experiments, we train CoRGi with Adam\u00a0[17] and a learning rate of 0.001.\nWe employ early stopping on validation loss, with train, test, and validation sets split in 8:1:1 ratio.\nWe use binary cross entropy loss (BCE) for binary values and mean squared error (MSE) for ordinal values.\nWe apply dropout\u00a0[41] on the message passing layers, the prediction MLPs, as well as on edges, with rates chosen from \\(\\lbrace \\) 0.1, 0.3, 0.5, 0.7\\(\\rbrace \\)  with respect to the validation set performance.\nFor the baselines, the parameter settings are done in the following manner:\n1) When the settings of the comparison models overlap with CoRGi's, e.g., the number of message passing layers or the learning rate, we used the same configurations as CoRGi.\n2) For the parameter settings that are unique to the comparison model, we followed the setting that is disclosed in the original paper.\n3) When the setting disclosed in the original paper is not applicable to the datasets used or our training environment, we select those that yield the best validation performance.\n{FIGURE}"]}
{"id": "2103.15396", "categories": "cs.CV", "paragraphs": ["After solving the challenge of data, we train a model based on PCN [43]. Concretely, we use Adam [13] optimizer with a starting learning rate at 0.0001, which decays by 0.7 in every 50,000 iterations. We train the model on one NVIDIA TITAN XP GPU with batch size 32 for 300,000 iterations. Furthermore, due to the limitation of GPU memory, we remove the detailed output branch after the model is trained. Finally, we initialize our spatial shape prediction network with the saved weights.\n"]}
{"id": "2101.04109", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["All experiments are conducted on an Nvidia 32GB V100 using the PyTorch and Tensorflow framework.\nWe consider \\(\\textsc {Bert}_\\textrm {Base}{}\\)  as the shared encoder model with \\(\\textrm {MAX\\_SEQ\\_LEN}=512\\)  and the warm-up proportion \\(0.1\\) .\nBoth the explanation generation and task prediction models are trained using Adam optimizer\u00a0[14] with a batch size of 16, and \\(\\textrm {learning\\_rate}=1e-5\\) .\nModels are trained for 10 epochs with early-stopping criteria on the validation set and \\(\\textrm {patience}=3\\) .\nThe MLP for the task classification consists of a dropout layer with a 10% chance of masking, followed by a 256 dimensional hidden dense layer, again followed by a Sigmoid output layer.\nThe explanation decoder consists of a 128-dimensional GRU with a uniform random kernel analyzer.\nNote that the final outputs of the explanation generator correspond to the sub-token representations of\u00a0Bert.\nAdjacent sub-tokens are merged to their corresponding original words through max-pooling.\nThe best \\(\\lambda \\)  is chosen over a validation set that provides the best trade-off between task performance and token-F1. The best \\(\\lambda \\)  values for Movie Reviews, MultiRC, FEVER are \\(5.0, 20.0, 2.0\\)  respectively.\nAfter training the explanation generation network in ExPred, we remove instances that the auxiliary output predicts wrongly, and use the rest to train the prediction model. This is to avoid distraction from the wrong predictions from the explanation prediction phase. Note that this is only done during training, while the predictions on the validation and test sets are regardless of the task prediction from the explanation phase.\n"]}
{"id": "2101.04279", "categories": "cs.CV", "paragraphs": ["During training, we random crop patches of size [224, 224] from the images and the batchsize is set to 16. The parameters of the IFNet are initialized by Xavier\u00a0[9]. Adam\u00a0[12] optimizer with the learning rate of \\(5\\times 10^{-5}\\)  is used to train our model and the learning rate is halved every 1000 epochs. We random flip images horizontally and scale brightness and saturation for data augmentation. When inference, image will be fed into our model directly.\n"]}
{"id": "2112.03615", "categories": "cs.CV cs.AI", "paragraphs": ["We run our training algorithm for 50 epochs on MNIST and F-MNIST and 200 epochs on CIFAR-10, using the Adam optimizer [19], a learning rate of 0.001, weight decay of 0.0001, and batch-sizes of 128. We use no data augmentation on MNIST and F-MNIST and use normalization, random cropping, and flipping on CIFAR-10. In all of our experiments, we use 86% of the data for training and 14% for testing.In the implemented regularizers from prior work, we used the \\(\\lambda \\)  that was suggested by the respective authors. While we found out that the strength of the SMD regularizer (also \\(\\lambda \\) ) in the range \\([0.5, 2]\\)  gives good results. Thus in all of our experiments, we take \\(\\lambda =1\\) . We report all the results as an average over 5 independent trials (we include the standard deviations in the Appendix A).\nWe report results for the ensembles of 3 members in the main paper, and for 5 and 8 in the Appendix C.\n", "In the setting of adversarial training, we follow the EAT approach [41] by creating adversarial examples on 3 holdout pre-trained ensembles with the same size and architecture as the baseline ensemble. The examples are created via PGD-\\(L_\\infty \\)  attack with 10 steps and \\(\\epsilon =0.1\\) .\n"]}
{"id": "2112.03555", "categories": "cs.LG stat.ML", "paragraphs": ["The CGL part in each local model is parameterized by a \\(d\\times d\\)  matrix named \\(\\mathbf {U}\\)  and the Gumbel-Sigmoid approach is leveraged for approximating the binary form. Each entry in \\(\\mathbf {U}\\)  is initialized as 0. The temperature \\(\\tau \\)  is set to \\(0.2\\)  for all settings. Then, for the causal mechanism approximation part, we use 4 dense layers with 16 variables in each hidden layer. All weights in the Network are initialized using the Xavier uniform initialization.\n"]}
{"id": "2103.05930", "categories": "cs.CV", "paragraphs": ["We train the network using standard SGD [13]. The mini-batch size is set to 16 and 32 for Cityscapes and ADE20K respectively. And we use the momentum of 0.9 and a weight decay of \\(5e^{(-4)}\\) . Similar to other works[1], [31], we apply the `poly' learning rate policy in which the initial learning rate is set to \\(1e^{(-2)}\\)  and decayed by \\((1-\\frac{iter}{max_iter})^{power}\\)  with power=0.9. The training images are augmented by employing random color jittering, random horizontal flipping, random cropping, and random scaling with 5 scales {0.75, 1.0, 1.5, 1.75, 2.0}. For Cityscapes, images are cropped into size of \\(1024\\times 1024\\) , and the network is trained with 200k iterations. For ADE20K, crop size of \\(512\\times 512\\)  and 250K training iterations are used for training.\n"]}
{"id": "2111.03184", "categories": "cs.LG cs.AR cs.DC", "paragraphs": ["During each SDMM step, the dense input is stored in on-chip LUT RAM, where multiple rows would be stored on the same slice of memory in order to fully utilize it. The limitation where only a single row can be read from each LUT RAM slice at a time induces data collision when multiple reads are needed for a same RAM slice and at a same time. As explained in Section\u00a0REF , both data replication and row grouping can effectively reduce data collision. The less data collision will in return results in smaller latency of computing SDMM. On the other hand, due to the irregular nature of graph adjacency matrices, individual rows have very different sparsity which results in PE imbalance, we statistically minimize this effect by utilizing larger tiles. As GCN has the hidden size of 16, we set each PE to have 16 multiply-accumulators and have the fixed relationship between tile size \\(T\\)  and row grouping \\(g\\)  that \\(T=16g\\) . Therefore, we evaluate the impact of latency from dense data replication \\(r\\)  and tile size \\(T\\) , as shown in Fig.\u00a0REF . We can see that the latency of computing is decreased by more dense data replications as well as larger tile sizes. At 8 replicas, LW-GCN's SDMM latency is reduced by up to 44.23% (on PubMed) compared to 1 replica under the same 512-row tile setup. At 4096-row tiles, SDMM latency is reduced by up to 61.83% (on PubMed) with the same replication setup. The ideal cases in Fig.\u00a0REF  is estimated by summing up the total amount of workload, and assuming every PE is fully utilized.\n{FIGURE}"]}
{"id": "2106.06130", "categories": "cs.LG physics.chem-ph q-bio.MN", "paragraphs": ["Datasets.\nWe use 20 million unlabelled molecules sampled from Zinc15 [45], a public access database that contains purchasable \u201cdrug-like\u201d compounds, to pre-train GeoGNN. We randomly sample 90% of the molecules for training and the remaining for evaluation.\n"]}
{"id": "2103.07098", "categories": "cs.CL cs.SI", "paragraphs": ["As the dataset in not balanced, F1-Macro is used to compare the models as in many prior stance studies [19], [15] . For co-training, we have five hyper parameters namely: 1) \\(\\theta _u\\) , 2) \\(\\theta ^T\\) , 3) K (mixing parameter), 4) k (most used hashtags), and, 5) p (popular retweets count). The values of these parameters are determined by trials on one of the datasets (Student Marches). By a uniform grid search on SM dataset using F1-macro as criterion, we find the following values that work well: \\(k=250\\) , \\(p=1000\\) , \\(\\theta _u = 0.7, \\theta ^T = 0.7,K = 0.2\\) . For \\(k\\)  and \\(p\\) , the parameter search range was 100 to 10,000. For \\(\\theta _u\\) , \\(\\theta ^T\\)  and \\(K\\) , the search range was 0 to 1. Five iterations of co-training was used as the classifiers appear to converge after four iterations.\n", "For training classifier with weak labels, a standard desktop was used for SVM, but for the neural-network based models, a machine with Nvidia GT-1080Ti GPU was used. Only weak labeled examples (excluding any test data) were used for training. The training of neural-networks took between half-an-hour to two hours but training SVM took less than half-an-hour. BERT, which has 110 million parameters, is much larger than LSTM which is in order of 200,000 parameters. However, for BERT we only tune the model whereas for LSTM we fully train the model (except the embeddings).\n"]}
{"id": "2107.00152", "categories": "cs.CL", "paragraphs": ["We use Adam\u00a0[16] for the training of all our models. Our question type classifiers and template exemplar classifiers are trained with a maximum learning rate of \\(1 \\times 10^{-5}\\)  and a batch size of 32. For training generation models, the maximum learning rate is \\(3 \\times 10^{-5}\\)  and each batch contains at most \\(32{,}768\\)  tokens. Mixed-precision training is adopted for all models except for models with GATs.\n"]}
{"id": "2103.08317", "categories": "cs.LG", "paragraphs": ["In our GBDT and XGBT regression models, we considered  max_depth, learning_rate, n_estimators, subsample  as the main parameters to be hyper-tuned, where:  max_depth represents the maximum depth of the individual regression estimators (each estimator is a decision tree (DT)), learning_rate is the contribution of each tree to the overall outcome, n_estimators is the number of boosting stages to perform, and subsample is the fraction of samples to be used for fitting the individual base learners (if smaller than 1.0 this results in Stochastic Gradient Boosting). subsample parameter interacts with the n_estimators parameter. Choosing \\(\\textit {subsample} < 1.0\\)  leads to a reduction of variance and an increase in bias.\n"]}
{"id": "2106.11582", "categories": "cs.CV", "paragraphs": ["This experiment uses the Adam optimizer with a 0.0002 learning rate\nand sets the batch size to 32 in our training process. In Fig.\u00a0\nREF  and Fig.\u00a0REF \nshow the accuracy and loss curves of different deep learning models\nin this experiment.\nWe find that the loss and accuracy curves of\nthe training set are converging after training for 40 layers.\nTherefore, considering the computational\nperformance of the workstation, we finally set 50 epochs for training.\n{FIGURE}{FIGURE}"]}
{"id": "2108.07058", "categories": "cs.CV", "paragraphs": ["For all experiments shown in the main paper, we use SGD optimizer with 0.9 momentum and 0.0001 weight decay.\nThe standard data augmentation of horizontal flipping and scaling are also applied.\nIn addition, the weights of the batch normalization\u00a0[16] layers derived from the ImageNet pre-trained models are kept frozen.\nTo be consistent with prior works, we have not incorporated any testing time augmentation tricks.\nFor semantic segmentation, the model is trained for 65K iterations starting with a learning rate of 0.01 that is reduced by a factor of 10 at 40K and 55K.\nFor the other three dense prediction tasks, the model is trained for 90K or 270K iterations with the initial learning rate of 0.02 that is reduced to 0.002 at 60K/210K and 0.0002 at 80K/250K.\nOur implementation is based on the Detectron2\u00a0[45] with the default configurations, to maintain a fair comparison with prior works, neither have we tuned any training hyperparameters nor used advanced data augmentations.\n"]}
{"id": "2108.00882", "categories": "cs.CV", "paragraphs": ["To evaluate the performance of the proposed SANet, five polyp segmentation datasets are adopted, including Kvasir\u00a0[9], CVC-ClinicDB\u00a0[1], CVC-ColonDB\u00a0[2], EndoScene\u00a0[17] and ETIS\u00a0[15]. To keep the fairness of the experiments, we follow\u00a0[4] advice and take exactly the same training and testing dataset division. Besides, six state-of-the-art methods are used for comparison, namely U-Net\u00a0[14], U-Net++\u00a0[24], ResUNet\u00a0[22], ResUNet++\u00a0[10], SFA\u00a0[5] and PraNet\u00a0[4]. We use Pytorch to implement our model. All input images are uniformly resized to 352\u00d7352. For data augmentation, we adopt the random flip, random rotation and multi-scale training. The whole network is trained in an end-to-end way, using stochastic gradient descent (SGD). Initial learning rate and batch size are set to 0.04 and 64, respectively. We train the entire model for 128 epoches.\n"]}
{"id": "2109.02631", "categories": "cs.LG", "paragraphs": ["Training is performed on a single DGX machine with 8 Tesla V100 GPUs. 7 GPUs are filled with actor instances (2 or 3 per GPU depending on design size) running our DREAMPlace based RL environment. The final GPU is used for running the training algorithm for the policy and value networks. Training is performed for 1.5e6 training steps (between 10,000 and 25,000 placement episodes depending on placement episode length) and the best solution found during training is reported. This training process takes between 24 and 48 hours depending on the size of the partition.\n"]}
{"id": "2107.03852", "categories": "cs.LG", "paragraphs": ["\nRuntime Environment: Nvidia GP107CL Quadro P620\n\nArchitecture: Autoencoder\n\nEncoder:\n\nContains 4 VGG Blocks\n\nVGG Block has 2 Convolution Layers followed by a maxpooling layer\n\nBatch normalization layer was used at the end of each layer before the activation function\n\nActivation Function: ReLU\n\n\nDecoder:\n\nDecoder part of the model consists of convolution transpose layers with batch normalization layer at the end of each layer before the activation function\n\nActivation Function: ReLU, Sigmoid (Output Layer)\n\n\n\nBatch Size: 16\n\nLearning Rate: 0.001\n\nNumber of Epochs: 500 (CAE), 2000 (DEC and IDEC)\n\nOptimizer: Adam\n\n"]}
{"id": "2110.14295", "categories": "cs.LG cs.GT cs.SY eess.SY math.OC", "paragraphs": ["For both experiments, we set the total training episodes \\(L = 5000\\) , trajectory generation size \\(B = 5\\) , exploratory policy parameter \\(\\lambda = 1.5\\) , and resample constant \\(\\kappa = 1\\) . We note that such setup of \\(B\\)  and \\(\\kappa \\)  then implies a mini-batch size \\(|\\tilde{\\mathcal {D}}_{\\cdot , \\cdot }| = 1000\\)  after appending past experiences and including samples from all time periods \\(t < T-1\\)  as specified in Section REF . We initialize our critic parameters \\(\\operatornamewithlimits{\\text{\\textbf {w}}}\\)  near the true analytical parameters and actor parameters \\(\\theta \\)  to 0. We fix the learning rate for actor parameter update \\(\\alpha _{\\theta } = 2\\)  and use EMA learning rate \\(\\alpha ^{(l)}_{\\text{w}} = 2/(l + 1)\\)  for our critic parameter update.\n"]}
{"id": "2104.05353", "categories": "cs.LG stat.ML", "paragraphs": ["Our defense: We evaluate our defense on the CIFAR-10 dataset (\\(N=32\\) ), for which there are well-established benchmarks in adversarial ML. In our defense, we use \\(4 \\times 4\\)  patches (\\(n=4\\) ) and an overcomplete dictionary with \\(L=500\\)  atoms. The stride \\(S=2\\) , so the encoder output is a \\(15 \\times 15 \\times 500\\)  tensor (\\(m=15\\) , \\(L=500\\) ). The regularization parameter in equation\u00a0REF  is set to \\(\\lambda = 1\\) , in the upper range of values resulting in convergence. The number of iterations in dictionary learning is chosen as 1000 to ensure convergence. The number of dictionary atoms \\(L\\)  is chosen to be 10 times the ambient dimension of patches.\n", "We test our defense for \\(T=1,2,5,10,15,\\)  and 20 with hyperparameter \\(\\beta = 3\\)  for the threshold in equation\u00a0REF . We train the CNN-based decoder in supervised fashion in tandem with the classifier, using the standard cross-entropy loss. We use a cyclic learning rate scheduler [16] with a maximum learning rate of \\(\\eta _{max}=0.05\\)  for \\(T=1,2\\)  and \\(\\eta _{max}=0.02\\)  for \\(T=5,10,15,20\\) . In order to provide a consistent evaluation, we employ the ResNet-32 classifier used in [13] and train it for 70 epochs.\n", "Benchmarks: For a fair comparison, we use the same ResNet-32 classifier architecture for the benchmarks. We train the PGD adversarially trained model from [13] with the same cyclic learning rate with \\(\\eta _{max}=0.05\\)  for 100 epochs. We train the model for TRADES defense with learning rate \\(\\eta =0.01\\)  for the first 50 epochs and then with \\(\\eta =10^{-3}\\)  for the next 50 epochs. For both PGD adversarially trained model and TRADES, training hyperparameters are \\(\\epsilon =8/255\\) , \\(\\delta =1/255\\) , \\(N_S=10\\) , \\(N_R=1\\) . Additionally for TRADES \\(\\lambda _{\\text{TRADES}}=1/6\\) . We also report on naturally trained network (i.e., no defense). This network is also trained for 70 epochs with the same cyclic learning rate with \\(\\eta _{max}=0.05\\) .\n"]}
{"id": "2109.04604", "categories": "cs.CL", "paragraphs": ["We tuned the only hyper parameter for data augmentation, the percentage of augmented data points, \\(p\\) , for MNLI. On this task we augmented 5, 10, and 15% of sentence pairs from training data, and found 5 and 10% of training data as the best thresholds for ACCESS and NTS respectively.\nFor TACRED, we did not use this hyper parameter. Instead, we used all simplifications that preserve critical information for data augmentation. That is, we added all simplified sentences that preserve the subject and object entities necessary for the underlying relation. We found that 66% of training data sentences could be simplified while preserving this information by ACCESS, and 72% by NTS.\n"]}
{"id": "2106.08254", "categories": "cs.CV cs.LG", "paragraphs": ["The network architecture of BEiT follows that of ViT-Base\u00a0[17] for a fair comparison.\nWe use a 12-layer Transformer with 768 hidden size, and 12 attention heads. The intermediate size of feed-forward networks is 3072.\nWe employ the default \\(16 \\times 16\\)  input patch size.\nWe directly borrow the image tokenizer trained by\u00a0[37]. The vocabulary size of visual tokens is 8192.\n", "We pretrain BEiT on the training set of ImageNet-1K\u00a0[39], which contains about \\(1.2\\) M images.\nOur augmentation policy includes random resized cropping, horizontal flipping, color jittering\u00a0[45].\nNotice that we do not use the labels for self-supervised learning.\nWe use the \\(224 \\times 224\\)  resolution in our experiments.\nSo the input is split to \\(14 \\times 14\\)  image patches, and the same amount of visual tokens.\nWe randomly mask at most 75 patches (i.e., roughly \\(40\\%\\)  of total image patches).\n", "The pre-training runs for about 500k steps (i.e., 800 epochs) with 2k batch size.\nAdam\u00a0[25] with \\(\\beta _1=0.9, \\beta _2=0.999\\)  is employed for optimization. The learning rate is set to 1.5e-3, with a warmup of 10 epochs, and cosine learning rate decay.\nThe weight decay is \\(0.05\\) .\nWe employ stochastic depth\u00a0[22] with a \\(0.1\\)  rate, and disable dropout.\nThe 500k training steps take about five days using 16 Nvidia Telsa V100 32GB GPU cards.\n"]}
{"id": "2107.09437", "categories": "cs.LG cs.AI nlin.CD physics.data-an", "paragraphs": ["With the analytical framework that can map the weight distribution of the hidden layer to the order-chaos phase diagram, we can then study the back-propagation process during the model training process in the phase diagram.\nIn our experiment, we use the standard Fashion-MNIST image dataset\u00a0[32] for training and testing. This dataset contains 70,000 greyscale images of 10 types of clothing and accessories, of which 60,000 images are used for training and 10,000 images are for testing. Since the network architecture of our model requires a vector as input, we flatten the 2D images into a 1D vector.\nAs each sample image in the Fashion-MNIST dataset has \\(28 \\times 28\\)  = 784 pixels, we design the hidden layer with 784 neurons, such that there are \\(784\\times 784\\)  weights in the hidden layer. The activation function of the hidden layer is \\(\\tanh \\)  in line with the theoretical framework. We use the basic stochastic gradient descent (SGD) with momentum as the optimization algorithm. After every epoch during the training process, we calculate the mean and variance of the weights in the hidden layer to identify the ordered/chaotic phase of the hidden layer.\n{FIGURE}", "In the training process of a neural network with back-propagation, the training data are repeated fed into the model to optimize the weights iteratively. Each time the whole set of training sample is used for one round of back-propagation, the training time is defined as incremented by one epoch. Hence, at the beginning of each epoch, we calculate \\(J_0\\)  and \\(J\\)  from the mean and variance of the hidden layer's weights, and map its coordinates to the order-chaos phase diagram for the first 500 epochs.\nThe trajectory in Fig.\u00a0REF (a) represents the model's evolution path during training. We initialize the hidden layer weight matrix \\(W\\)  with \\(J_0=0\\)  and \\(J=0.5\\) , such that the hidden layer starts from the ordered phase, i.e., the white region in the figure. As the training proceeds, the model evolves from the ordered phase towards the chaotic phase. When the model is around the edge of chaos \\((J=1)\\) , the test loss is the lowest as Fig.\u00a0REF (a)-(b) shows, indicating optimal model performance around this point. This demonstrates that the model optimality near the edge of chaos also holds for single hidden layer neural networks, even though this network is a feedforward process rather than an iterative recurrent computation.\n{FIGURE}"]}
{"id": "2102.02335", "categories": "cs.CL cs.AI cs.IR", "paragraphs": ["We train our Multihead Attention model for Claim Identification, MA-CIN, on datasets mentioned in Section . The CDC dataset contains total of 522 articles. Amongst these, there are 47 articles with 8 or more annotated claim sentences which are considered as evaluation set (CDC_Eval) for this dataset. Next, for DNF-300 and DNF-700, we asked two annotators to manually tag at least 5 sentences as \u201cclaim-worthy\u201d in each of the 50 articles. Sentences which were consented by both the annotators as \u201cclaim-worthy\u201d were finalized as ground truth claims for these 50 articles, and used as testing set for evaluating the model performance on DNF datasets. The remaining 475 articles from CDC, 250 articles from DNF-300, and 650 articles from DNF-700 were split into 5 folds to train the model using a 5-Fold cross validation [14], where we use 4 folds for training and 1 fold for validation. Each of the three settings, described in Section- : MA-CIN(HV), MA-CIN(OHWV) and MA-CIN(Combined), was trained with each of the three datasets, and evaluated on DNF_Eval and CDC_Eval. Total number of parameters for these three settings are 15,012,916 (10,240 non-trainable), 40,975,656 (10,240 non-trainable) and 41,812,564 (12,288 non-trainable) respectively. All other network parameters are displayed in supplemental material.\n", "In each setting, we use batch normalization, ReLU non-linearity as an activation function, and a dropout of 0.5 for every convolution operation. We trained all the models for 2000 epochs, where, for every training we used Adam optimizer with a learning rate \\(lr=0.0001\\) , \\(\\beta _{1}=0.99\\)  and \\(\\beta _{2}=0.0\\) . There was no weight decay set as the model was trained in a self-supervised setting with finite epochs and an already small learning rate. Glove 300D word embedding was used for all our experiments and the number of input sentences was set to 500. The models were trained on three 11GiB Nvidia 1080Ti GPUs in parallel.\n"]}
{"id": "2109.13122", "categories": "cs.LG", "paragraphs": ["Our code is a re-implementation of the conjugate-gradient Newton procedure of\nrecent versions of Liblinear . The hyperparameters\nrelated to stopping condition for conjugate-gradient iterations (\\(\\epsilon =0.5\\) ) preconditioner (\\(\\alpha =0.01\\) ), back-tracking line search (\\(\\alpha =0.5\\) ,\n\\(\\eta =0.01\\) , \\(\\text{max\\_steps}=20\\) ), and stopping condition for the\noptimization (\\(\\epsilon =0.01\\) ) have been taken from their code. To reduce\nmodel size, all weights below a threshold of \\(0.01\\)  have been clipped to zero.\n"]}
{"id": "2106.07229", "categories": "cs.LG cs.CR", "paragraphs": ["We simulate the proposed model by the SEAL library  released by Microsoft. Our simulation environment is a dual Intel Xeon Platinum 8280 CPU (112 cores) with 512GB memory. We allocate one thread per one channel of each layer by using the OpenMP library to improve the execution speed of the ResNet-20.\n", "The model parameters are prepared by the following training method. We use 32 x 32 color images, subtract the mean of the pixels in the training dataset, and adopt a data argumentation method such as shifting and mirroring horizontally for training. We adopt the He initialization  as the weight initialization and no dropout. We train the model with 32 \\(\\times \\)  32 mini-batches and cross-entropy loss function. The learning rate starts with a 0.001 learning rate divided by 10 after 80 epochs and 100 after 120 epochs during training. The classification accuracy with the trained model parameters is 91.89%, which is tested with 10,000 images.\n"]}
{"id": "2103.00488", "categories": "cs.CL", "paragraphs": ["The batch size used in our experiments is 32. We train each model for 15 epochs. The initial learning rate for the text encoder is \\(1.0 \\times 10^{-5}\\) , and for other parameters, the initial learning rate is set to \\(5.0 \\times 10^{-4}\\) . We evaluate our model on the validation set at each epoch. If the macro F1 score doesn't increase, we then decay the learning rate by a factor of 0.1. The minimum learning rate is \\(5.0 \\times 10^{-7}\\) . We use Adam optimizer [11] in all our experiments.\n"]}
{"id": "2110.13675", "categories": "cs.CV", "paragraphs": ["We conduct all experiments on two popular benchmarks, i.e., PASCAL VOC [8] and MS COCO [21]. On the PASCAL VOC benchmark, we train all models on the trainval set 2007+2012 (containing \\(16,551\\)  images from 20 categories) and evaluate them on the test set 2007 (containing \\(4,952\\)  images) [8]. On the MS COCO benchmark, we train all models on the training set 2017 (containing 118K images from 80 categories) and evaluate them on the val set 2017 (containing 5K images) [21].\nWe train all state-of-the-art models with the original implementation released by the authors. Specifically, we follow the original implementation's training protocol with default parameters and the number of training epochs with different losses [30], [31], [42], [3]. Implementation details of all models are given in Appendix REF . All experiments are run with NVIDIA V100 GPUs. Code is available at https://github.com/Jacobi93/Alpha-IoU.\n", "YOLOv5. We train YOLOv5s and YOLOv5x with different losses following the original code's training protocol at https://github.com/ultralytics/yolov5 with the released version being v4.0.\nWe train both models from scratch using the same hyperparaemter in the file named \"hyp.scratch.yaml\".\nThe configuration is set following the file \"yolov5s.yaml\" for YOLOv5s and \"yolov5x.yaml\" for YOLOv5x, respectively.\nThe batch size is 64, the initial learning rate is \\(0.01\\) , and the number of training epochs is 300 in all experiments.\nThe file \"voc.yaml\" is set for models trained on PASCAL VOC while the file \"coco.yaml\" is set for those trained on MS COCO.\n", "Faster R-CNN. We train Faster R-CNN with different losses following the original code's training protocol at https://github.com/open-mmlab/mmdetection/tree/master/configs/faster_rcnn.\nThe configuration is set following the file \"faster_rcnn_r50_fpn.py\" for Faster R-CNN with the backbone being ResNet-50-FPN.\nThe file \"schedule_1x.py\" is set for models trained with 1x schedule and single scale.\nThe checkpoint and logging configuration is set in \"default_runtime.py\".\nThe batch size is 16, the initial learning rate is \\(0.02\\) , and the number of training epochs is 12 in all experiments.\nThe file \"coco_detection.py\" is set for models trained on MS COCO. We do not train Faster R-CNN on PASCAL VOC.\n", "DETR. We train DETR with different losses following the original code's training protocol at https://github.com/open-mmlab/mmdetection/tree/master/configs/detr.\nThe configuration is set following the file \"detr_r50_8x2_150e_coco.py\" for DETR with the backbone being ResNet-50.\n\"8x2\" stands for using 8 GPUs (we use NVIDIA V100 GPUs) in parallel with 2 images trained on every GPU (i.e., batch size is 16). The number of training epochs is 150. The initial learning rate is \\(1\\mathrm {e}{-4}\\)  for the first 100 epochs and \\(1\\mathrm {e}{-5}\\)  for the rest 50 epochs.\nThe checkpoint and logging configuration is set in \"default_runtime.py\".\nThe file \"coco_detection.py\" is set for models trained on MS COCO. We also modify it for models trained on PASCAL VOC.\n"]}
{"id": "2104.14090", "categories": "cs.LG", "paragraphs": ["For the ellipse dataset, we train the unrolled network using a batch size of 15 for 60 epochs. The F-FPN network training used a batch-size of 15 for 50 epochs. The unrolled network architecture contains 20 total layers (i.e. update steps) \u2013 the number of layers was chosen on the memory capacity of the GPU.\n", "For the LoDoPab dataset, we train the unrolled and F-FPN networks using a batch-size of 50 for 50 epochs total. The unrolled network architecture contains 14 total layers (i.e. update steps) \u2013 the number of layers was chosen on the memory capacity of the GPU.\n"]}
{"id": "2101.02697", "categories": "cs.CV", "paragraphs": ["Our capture setup consists of 53 cameras positioned around the subject. For each subject, we record a set of 30 expressions with a hair-cap. And a neutral expression with no hair-cap. Each frame is fit with a 3D face model including rigid head pose which we use to center the volume between different identities and expressions. We do not use any of the mesh information during training.\nWe train our network on 50 subjects using 40 viewpoints and test on held out viewpoints. Additionally, for the expression-based model we train our network on 25 expressions and test on the remaining expressions.\nDuring training, we divide each target image into a \\(16 \\times 16\\)  grid, and randomly sample a ray from each grid location for a total of 256 rays per training image. Further, we sample \\(n_s=128\\)  points along the ray while clamping the sample points to lie in a unit volume cube. We train our model with a batch-size of 4. Our model takes around 24 hours to converge on 4 Nvidia Tesla V100s.\n"]}
{"id": "2109.02903", "categories": "cs.CL cs.AI", "paragraphs": ["For fine-tuning, we use a single GPU for bilingual models and 8 GPUs for multilingual models. We use dropouts of 0.1, label smoothing of 0.1, warmup of 16,000 steps, 2048 tokens per batch per GPU, learning rate of 0.001 and weight decay of 0.00001 with the ADAM optimizer for training. For mBART50, we use dropouts of 0.3, warmup of 16,000 steps, 512 tokens per batch per GPU, and learning rate of 0.00003.A small learning rate is needed since we can train on very small batches given the large model size. For unidirectional and multilingual models trained from scratch on PMI and PIB data, we use smaller hidden and filter sizes of 512 and 2048, respectively, keeping all other hyperparameters the same as for IndicBART. This is because the corpora sizes are rather small and a small model should be sufficient. For consistency of training we use the same IndicBART vocabularyNaturally, we use unified script vocabularies when we use the unified script IndicBART model. across all experimentsExperiments on mBart50 will use the vocabularies specific to these models..\n", "We train our models till convergence on the development set BLEU scores [33] which are computed via greedy decoding every 1,000 batches. For multilingual models we use the global development set BLEU score, an average of BLEU scores for each language pair. Different from most previous works, instead of decoding a single final model, we choose a particular model checkpoint for a language pair with the highest development set BLEU score for that pair. Therefore, we treat multilingualism as a way to get a (potentially) different model per language pair leading to the best BLEU scores for that pair and not as a way to get a single model that gives the best performance for each language pair. During decoding the test sets, we use beam search with a beam of size 4 and a length penalty of 0.8. We report the BLEU scores on the decoded results computed using sacreBLEUBLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.5.1 [35].\n"]}
{"id": "2110.15709", "categories": "cs.CL cs.LG", "paragraphs": ["The datasets used for training the model are Data 2, 3, 4 presented in Section . Moreover, the text preprocessing phase, carried out before the training of the models, was composed of a specific text cleaning for BERtikal, performed using the clean_bert function presented in Section REF . That function makes few changes to the texts and keeps the uppercased letters.\n", "Our model was trained from the checkpoint made available in Neuralmind's Github repositoryhttps://github.com/neuralmind-ai/portuguese-bert - Accessed on 07/30/2021. by the authors of a recent research [12]. In the training phase, we (i) kept the configuration of the model and vocabulary used by the authors, (ii) used the Masked Language Model (MLM) objective with masking probability 0.15, (iii) used one epoch, (iv) batch size equals to 4 texts, and (v) made use of a Tesla T4 GPU. The optimizer settings have been set as the default for the Transformers package from the company Hugging FaceSee https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py - Accessed on 10/22/2020. and the full training took approximately one week to be completed.\n"]}
{"id": "2103.06627", "categories": "cs.CV", "paragraphs": ["We adopt ResNet50 as the backbone network. Models are trained on MS1Mv2 [13], [8] for 20 epochs with batch size 512 and initial learning rate 0.1, dropped by 0.1 every 5 epochs. 512 samples of the last iteration are used for visualization.\n{FIGURE}"]}
{"id": "2111.03930", "categories": "cs.CV cs.CL", "paragraphs": ["As mentioned in the previous section, our Tip-Adapter has two versions. The first version is training-free, which directly sets the adapter's MLP weights following Eq.\u00a0(REF ). The second version allows further fine-tuning of the adapter initialized by the properly set weights. The two version are denoted as Tip-Adapter and Tip-Adapter-F in this section, respectively.\nEach model is trained with 1, 2, 4, 8, and 16 few-shot training sets, and tested on the full test sets. For the CLIP backbone, we utilize ResNet-50 [23] as the visual encoder and a transformer [13] as the textual encoder. In terms of prompt design, we adopt prompt ensembling in \u00a0[50], which inputs 7 templates into the CLIP textual encoder and then averages them as the final prompt. The 7 templates are: \u201citap of a [CLASS].\u201d, \u201ca bad photo of the [CLASS].\u201d, \u201ca origami [CLASS].\u201d, \u201ca photo of the large [CLASS].\u201d, \u201ca [CLASS] in a video game.\u201d, \u201cart of the [CLASS].\u201d and \u201ca photo of the small [CLASS].\u201d.\nTo fine-tune Tip-Adapter-F, we train it with a batch size of 256, and use Stochastic Gradient Descent (SGD)\u00a0[33], [41] with a learning rate \\(0.001\\)  and a cosine scheduler. In contrast to the 200-epoch training in CoOp and CLIP-Adapter, Tip-Adapter-F only requires 20 epochs for fine-tuning and has super-fast convergence speed, saving much computational cost for training.\n"]}
{"id": "2110.00842", "categories": "cs.CL", "paragraphs": ["The task description is tokenized into words. We do not remove stop words or lemmatize words. Embeddings for words are obtained by using pretrained word2vec [17] vectors (300 dimensional). Next, these embeddings are passed into an encoder RNN made of LSTM cells. The LSTM network is unidirectional with hidden dimension of 100. The decoder RNN is also unidirectional with hidden dimension as 100. The last hidden state of the encoder RNN is used to initialise the hidden state of the decoder.\nWe train each model for 400 epochs using the Adam optimizer [12]. We choose the hyperparameters and best epoch for each model by obtaining results on the validation set using beam size of 3 and not enforcing executability. Since we adapt the model from puig2018virtualhome the size of network is still same with around 3M parameters.\n"]}
{"id": "2108.10600", "categories": "cs.LG", "paragraphs": ["Dropout. Commonly used as regularizer in convolutional neural networks, it prevents overfitting and co-adaptation of the feature maps [30]. During the training procedure a certain number of neurons are randomly removed, dropping units with a probability \\(p\\) . We fix the probability of dropping a connection equal to \\(50\\%\\) , i.e. \\(\\textit {p} = 0.5\\) .\n", "All the training parameters are fixed as in [17]. The Adam optimizer's parameters \\(beta1\\)  and \\(beta2\\)  have been set to \\(0.9\\)  and \\(0.999\\)  respectively. The mini-batch size has been set to 100. During the batch normalization procedure, the \\(\\epsilon \\)  value of \\(10\\textsuperscript {-5}\\)  has been added to the mini-batch variance. In order to compute the mean and variance of the training samples, the moving average has been implemented using a fixed decay rate value of \\(0.999\\) . The learning rates parameter \\(lr\\)  has been fixed to \\(10\\textsuperscript {-4}\\) . The maximum number of iterations has been set to 100, with the early stopping patience parameter equal to 50.\n"]}
{"id": "2104.02947", "categories": "cs.LG", "paragraphs": ["We use the bert-base-uncasedhttps://huggingface.co/bert-base-uncased\nas the base transformer for our English models\n(for CA and IN locale), camembert-base\u00a0[17]\nhttps://huggingface.co/camembert-base as the base transformer for FR\nlocale,\nand bert-base-multilingual-uncased\nhttps://huggingface.co/bert-base-multilingual-uncased\nas the base transformer for DE locale. We\ntrain our models upto 10 epochs, with a batch size of 16, Adam\noptimizer with learning rate\nof \\(2e-5\\)  with a schedule of linear warmup of first 10000 steps and then\nlinear decay. We set \\(\\epsilon =1\\)  in the loss equation\u00a0REF , and \\(\\alpha =0.4\\) \nin the inference equation\u00a0REF . For the joint training (CQA\ntriplets and user query triplets), we have two training runs (data mixing and multi-task\nas described in section\u00a0REF ) per locale and picked\nthe best models (data mixing for CA, FR and multi-task for DE, IN).\nWe use the Pytorchhttps://pytorch.org,\nHuggingface\u00a0[29] and\nSentence-Transformers\u00a0[22] libraries to develop our\nmodels on an Nvidia\nV100 GPU and hence our training time per batch and inference time per sample\nare same as that of Sentence-Transformers with BERT (base-model, 110M\nparameters).\n"]}
{"id": "2103.01287", "categories": "cs.CL", "paragraphs": ["Dialogue agent training In this work, we use DQN\u00a0[22], which is an off-policy RL algorithm, to train the dialogue agent in both Step 1 and Step 4.\nWe implemented the DQN algorithm by utilizing the RL training modules in ConvLab.\n", "User 1 We assume that \\(\\textit {User}_1\\)  has no specific requirements about the interactions, and he only cares if his task can be accomplished successfully with fewer turns. According to this user profile, we handcraft the corresponding user reward function \\(f_1(\\cdot )\\) . Assuming the interaction between the agent and \\(\\textit {User}_1\\)  terminates at time step \\(m\\) , and the task status is Successful, a large positive value will be returned to the dialogue agent as the intermediate reward for the final system action:\n\\(f_1(s_m, a_m)=\\left|r\\right|.\\) \n", "User 3 We design a new user, \\(\\textit {User}_3\\) , who is forward-looking as described in Section\u00a0REF . After each interaction turn, \\(\\textit {User}_3\\)  will estimate the potential cost to finish remaining tasks. This feature is incorporated to the rule-based user simulator. We use \\(goal\\)  and \\(goal^{\\prime }\\)  to denote the initial tasks and the remaining tasks in the same dialogue respectively. We define \\(goal-goal^{\\prime }\\)  as tasks that have been completed already. The function \\(h^{\\prime }(goal^{\\prime })\\)  is defined as:\n\\(h^{\\prime }(goal^{\\prime }) = \\frac{\\textit {cost}_\\textit {{spent}}}{h(goal-goal^{\\prime })} * h(goal^{\\prime }) * \\gamma ,\\) \n"]}
{"id": "2107.13117", "categories": "cs.CV", "paragraphs": ["We initialized the weights of the conv layers using He's initialization [161]. The training process is performed for 165,000 iterations using the Adam optimizer [206], with a decay rate of gradient moving average \\(\\beta _1=0.9\\)  and a decay rate of squared gradient moving average \\(\\beta _2=0.999\\) . We used a learning rate of \\(10^{-4}\\)  and reduced it by 0.5 every 25 epochs. The mini-batch size was 32 training patches per iteration.\n"]}
{"id": "2112.02379", "categories": "cs.CV", "paragraphs": ["Synthesized Testing Benchmark.\nFor the reference-based evaluation, we synthesize turbulence-clean image pairs using TurbulenceSim_P2S\u00a0[26] which is the current state-of-the-art turbulence simulation works.\nThe synthesis is conducted on the selected first 100 images of CelebAHQ\u00a0[16], named CelebAHQ100, and the parameters of TurbulenceSim_P2S are carefully selected to match the real-world turbulence images.\nSpecifically, we set D, r0, and corr as \\(\\lbrace 5, 1.25, -0.01\\rbrace \\)  for the CelebAHQ100 simulation.\nThe synthesized image pairs are provided in the supplementary document in 8-bit sRGB format with a resolution of \\(512\\times 512\\) .\n", "Real-world Testing Benchmark.\nFor evaluating the performance of different methods on real-world turbulence degraded images without pixel-wise corresponding ground truths, we use face recognition accuracy based on indoor reference clear images.\nThe authors of\u00a0[47] provided us high-quality raw real-world turbulence degraded faces which are taken at 300 meters from the camera in a hot day. In addition to those images, we received the corresponding indoor face images without turbulence for reference.\nWe crop and wrap faces with the pre-trained RetinaFace\u00a0[11] network.\nThe final dataset contains images from 89 separate individuals each having 3 turbulence degraded images in different poses. We call this data TubFace89 and show its sampled images in Figure\u00a0REF .\n{FIGURE}{FIGURE}{FIGURE}"]}
{"id": "2112.02373", "categories": "cs.CV", "paragraphs": ["Our model implementation is based on the Pytorch framework and Faiss library [4]. And using 8 NVIDIA Tesla V100 for training the Transformer model. For training settings. We use Adamw [8] as the optimizer, the initial learning rate of model finetune is set to 0.0001, and cosine scheduler [7] is used to adjust the learning rate. For most models, we train 50 epochs on the training dataset, and train 200 epochs on the small number of labeled data in Phase 1. See our code for some details and differences. Besides, our local retrieval is accelerated by GPU and need a certain amount of memory to build the index database.\n"]}
{"id": "2104.07149", "categories": "cs.CL cs.AI", "paragraphs": ["We fine-tune BERT for up to 40 epochs with a batch size of 32. To prevent over fitting, we use early stopping on the validation loss. We optimize BERT parameters using gluonnlp's bertadam optimizer with a learning rate of 5e-5 and no weight decay. These are the default hyper-parameters provided in the gluon tutorial for intent classification and slot labeling (https://nlp.gluon.ai/model_zoo/intent_cls_slot_labeling/index.html).\n"]}
{"id": "2105.04241", "categories": "cs.CL cs.LG", "paragraphs": ["All ReadTwice models are initialized with the public RoBERTa (base) checkpointhttps://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz adapted to Tensorflow by [15]. Further, models are pre-trained for 1M steps on 64 TPU cores using the LAMB optimizer [23].\n"]}
{"id": "2102.09812", "categories": "cs.LG cs.AI cs.RO", "paragraphs": ["Most of the training parameters correspond to the implementation of [37], a state-of-the-art model-based RL algorithm for learning to plan in latent-space from image observations.\nDLC trains every 1000 environment steps for 200 iterations with the Adam optimizer\u00a0[46]. The batch size is set to 50. The representation, value and actor model are respectively trained with learning rates 6e-4, 6e-4, and 8e-5. Gradients over the magnitude of 100 are clipped for all models. The prior \\(\\sigma _{\\tau ,s}^{prior}\\)  and posterior \\(\\sigma _{\\tau ,s}^{post}\\)  variance in the transition model are bounded from below to a minimum value of 0.1.\nThe model loss on true observations \\(J_{M,s_t^1, s_t^2}\\)  is weighted twice as much as the model losses on predicted opponent observations \\(J_{M,s_t^1, \\tilde{s}_t^2}\\)  and \\(J_{M,\\tilde{s}_t^1, s_t^2}\\) . Throughout, we use \\(\\gamma =0.99\\)  and \\(\\lambda =0.95\\) .\nThe model learning horizon is \\(L=50\\)  whereas the imagination horizon is \\(H=15\\) . Value and action models are trained on the same trajectory rollouts.\n"]}
{"id": "2101.10460", "categories": "cs.LG cs.AI cs.NE", "paragraphs": ["Next, for fixed batch size \\(b = 2L\\) , we vary \\(\\lambda \\)  = 1e-6, 1e-5, 1e-4, 1e-3, 5e-3, 5e-2, 5e-1, and 5 and train the model with 500 epochs. Figure REF  plots the prediction accuracy with respect to different choices of \\(\\lambda \\) . As the regularization on the latent space is ignored by assigning very small parameter \\(\\lambda \\) , the overall prediction performance is poor. The performance is quickly improved when higher \\(\\lambda \\)  is selected - the latent constraint starts to dominate the reconstruction term with larger \\(\\lambda \\) . The best range of \\(\\lambda \\)  is between [1e-4, 1e-2].\n"]}
{"id": "2104.11228", "categories": "cs.CV cs.GR", "paragraphs": ["We train both the attribute prediction and the latent manipulation networks using Adam optimizer for 60 epochs with the batch size of 128. The initial learning rate is set to \\(0.01\\)  and decayed by \\(0.1\\)  every 10 epochs. For the latent manipulation network, the weights of each loss term are set as \\(\\lambda _r=1\\) , \\(\\lambda _l=0.001\\) , and \\(\\lambda _c=0.01\\)  for all our experiments, respectively. To train the domain-specific StyleGAN2 generator \\(\\text{G}^*\\)  for each out-of-domain domain, we finetune the pretrained StyleGAN2 for \\(32,000\\)  iterations with the batch size of 16 on each dataset using the same learning rate scheduler but a smaller learning rate of \\(0.002\\) . Besides, Pytorch3D is used as the differential renderer.\n"]}
{"id": "2101.09688", "categories": "cs.CL cs.AI cs.LG cs.NE", "paragraphs": ["For fine-tuning BERT on the OntoNotes data, the following settings were used. Standard hyperparameter choices of \\(\\beta _1=0.9, \\beta _2=0.999, \\epsilon =10^{-8}\\) , and a dropout probability of 0.1 were chosen. Model training and validating with a 80/20 train/test split of the training data, across training epochs \\(\\in \\lbrace 1,\\dots , 10\\rbrace \\) . The selected (epoch) model was that with the highest pronoun prediction accuracy on the validation set.\n"]}
{"id": "2107.04357", "categories": "cs.CV cs.LG", "paragraphs": ["Once the document has been processed and visibility graph has been generated, we feed them to our Graph Recurrent Neural Network (GRNN) model framework with the 7-dimensional node input space to get projected to a higher order space encoding with individual node features preserving the structural content information of the document. The graph-level RNN used in our work uses 4 layered GRU with 128 dimensional hidden state. To output the adjacency vector prediction, the edge-level RNN uses 4 layered GRU cells with 16 hidden dimensional state. To get the predicted adjacency vector in the output, the edge-level RNN maps the 16 dimensional hidden state to a 8 dimensional vector through a MLP and ReLU activation, then another MLP maps the vector to a scalar with sigmoid activation. We initialize the the edge-level RNN by the output of the graph-level RNN when generating the start of sequences \\(S_{i-1}^{\\pi }\\) . We use the highest layer hidden state of the graph-level RNN to initialize with a linear layer to match the dimensionality. During the training time, ground truth has been used rather than the model's own predictions. During the inference time, the model is allowed to use its own predicted graph samples at each time step to generate a graph. The Adam Optimizer has been used for minibatch size of 32. We set the learning rate to be 0.001 which is decayed by 0.2 at every 100th epoch in all experiments.\n"]}
{"id": "2109.05074", "categories": "cs.CL cs.AI cs.LG cs.SI", "paragraphs": ["We trained the resulting fBERT for 25 epochs using the MLM objective with \\(0.15\\)  probability to randomly mask tokens in the input. The language model is trained with a batch size of 32 and a 512 maximum token length using the Adam optimizer with a learning rate of \\(5e-5\\) . The training time took 5 days on a single Nvidia V100 GPU.\n{FIGURE}"]}
{"id": "2104.12642", "categories": "cs.CV cs.LG", "paragraphs": ["We train the full-sized CompOFA network (\\(D=[4], W=[6]\\) ) on ImageNet [5] using the same base architecture of MobileNetV3 as OFA. We use a batch size of 1536 and a learning rate of 1.95 to train on 6 NVIDIA V100 GPUs. All other training hyperparameters are kept the same as OFA for accurate comparison.\n"]}
{"id": "2106.06916", "categories": "cs.LG stat.ML", "paragraphs": ["Training parameters.\nFor the optimization of NTL, we utilize Adam as the optimizer, with learning \\(\\gamma =0.0001\\)  and batch size of 32. For all datasets, we randomly select 8,000 samples from their own training sets as the source data, and 1,000 samples from their own testing sets as the test data (if a dataset does not have test set, we select its test data from the training set without overlapping with the chosen 8,000 source samples). And the sample quantities of the source and auxiliary domain are always the same. In the training of adversarial augmentation, the optimizer is also Adam, and we set the learning rate to \\(\\gamma =0.0002\\)  with two decay momentums \\(0.5\\)  and \\(0.999\\) . The batch size is 64, and the dimension of the latent space fed to the generator is 256.\n"]}
{"id": "2106.06911", "categories": "cs.CV", "paragraphs": ["Stride Level. The level of stride is how many rows or columns that get skipped over. This tuning parameter allows the algorithm to move faster but it makes sacrifice by skipping some variables. For example, we investigate stride level of 1 starting from row \\(i\\)  and column \\(j\\) . Assume we use a \\(2 \\times 2\\)  window and let us start from \\((i,j)\\) . We can visualize this action by using the following diagram\n\\(\\begin{array}{lll}\\text{Original matrix: } &\\begin{bmatrix}(i, j) & (i, j+1) \\\\(i+1, j) & (i+1, j+1) \\\\\\end{bmatrix} \\\\\\\\\\stackrel{\\text{stride}=1}{\\longrightarrow } &\\begin{bmatrix}(i, j+1) & (i, j+2) \\\\(i+1, j+1) & (i+1, j+2) \\\\\\end{bmatrix} \\\\\\end{array}\\) \n", "If we are at the end of the column for a row, we move down by moving to the first column of the next row. For example, in a grid structure of size \\(6 \\times 6\\) , assume we are in the last position in a certain row \\(i\\) . The action of stride level 1 can be taken using the following diagram\n\\(\\begin{array}{lll}\\text{Original matrix } \\\\\\text{in the end of a row: } \\\\\\begin{bmatrix}(i, 5) & (i, 6) \\\\(i+1, 5) & (i+1, 6) \\\\\\end{bmatrix}\\stackrel{\\text{stride}=1}{\\longrightarrow }\\begin{bmatrix}(i+1, 1) & (i+1, 2) \\\\(i+2, 1) & (i+2, 2) \\\\\\end{bmatrix} \\\\\\end{array}\\) \n", "Again assume we are at row \\(i\\)  and column \\(j\\) . Suppose we set stride level to be 2 and we want to move down. This means we increase increment of 2 on the number of rows and the action is the following\n\\(\\begin{array}{lll}\\text{Original matrix: } \\\\\\begin{bmatrix}(i, j) & (i, j+1) \\\\(i+1, j) & (i+1, j+1) \\\\\\end{bmatrix} &\\stackrel{\\text{stride}=2}{\\longrightarrow } &\\begin{bmatrix}(i+2, j) & (i+2, j) \\\\(i+3, j) & (i+3, j) \\\\\\end{bmatrix} \\\\\\end{array}\\) \n", "If this window happens to be in the final position of a row, then we move down by skipping one row and we start with the first column. If we have a grid structure of size \\(6 \\times 6\\) , this action is shown in the following diagram\n\\(\\begin{array}{lll}\\text{Original matrix } \\\\\\text{in the end of a row: } \\\\\\begin{bmatrix}(i, 5) & (i, 6) \\\\(i+1, 5) & (i+1, 6) \\\\\\end{bmatrix}\\stackrel{\\text{stride}=2}{\\longrightarrow }\\begin{bmatrix}(i+2, 1) & (i+2, 2) \\\\(i+3, 1) & (i+3, 2) \\\\\\end{bmatrix} \\\\\\end{array}\\) \n", "Alternatively, we can initiate the starting point to be at a higher level such as two or three. This allows algorithms to run more efficiently in large-scale data sets. For a simple example, in a matrix that is sized \\(6 \\times 6\\)  (see \u00a72.3 for the first artificial example), we have the first row of variables to be \\(\\lbrace X_1, X_2, ..., X_6\\rbrace \\)  and the second row of variables to be \\(\\lbrace X_7, X_8, ... X_{12}\\rbrace \\) . At a starting point of 2, we start with \\(X_8\\)  to pass over the rolling window, because this variable sits at the position with the second row and the second column. This can be illustrated in the following matrix\n\\(\\text{Starting point} = 2:\\begin{bmatrix}X_1 & X_2 & \\dots \\\\X_7 & \\color {red}{X_8} & \\dots \\\\X_{13} & \\vdots & \\ddots \\\\\\end{bmatrix}_{6 \\times 6} \\\\\\) \n"]}
{"id": "2108.05635", "categories": "cs.CV cs.RO", "paragraphs": ["Following \u00a0[31], a poly learning rate policy is adopted. The initial learning rate is set as 0.01 and the learning rate at each iteration is the initial learning rate multiplied by \\(({1-\\frac{iter}{total\\_iter}})^{0.9}\\) . The momentum and weight decay rates are set to 0.9 and 0.0001, respectively. The networks are trained with 8 mini-batch sizes per GPU using stochastic gradient descent (SGD). We set 150 epochs for training. As in existing methods, parameters in the encoder are initialized from the weights pretrained from the ImageNet\u00a0[48] while those in the decoder and the memory module are randomly initialized.\nTo avoid overfitting, data augmentation is exploited during training including horizontal flipping, scaling (from 0.5 to 2.0), and rotation (from -10\\(^{\\circ }\\)  to 10\\(^{\\circ }\\) ).\n"]}
{"id": "2109.09519", "categories": "cs.CL", "paragraphs": ["For the pre-training corpora, the English conversation samples are extracted from Reddit comments, which are collected by a third party and made publicly available at pushshift.io [5]. To guarantee the data quality, we follow the elaborate cleaning process as PLATO-2 [4]. After filtering, the data is split into training and validation sets in chronological order. The training set contains 811M (context, response) samples, ranging from December 2005 to December 2019. For the validation set, 0.2M samples are selected from the rest data after December 2019. The English vocabulary contains 8K BPE tokens [30], constructed with the SentencePiece library. The Chinese pre-training data is collected from public domain social medias. After filtering, there are 1.2B (context, response) samples in the training set. As for the Chinese vocabulary, it contains 30K BPE tokens.\n"]}
{"id": "2112.05328", "categories": "cs.CL cs.AI", "paragraphs": ["We use a pretrained unimodal model using the huggingface library\u00a0https://huggingface.co/. The optimizer used for training is AdamW and the learning rate scheduler is \\(get\\_linear\\_schedule\\_with\\_warmup\\) . We use training epochs from 5 to 10 depending on the subtask. Also, the learning rate is 1e-5 in subtask #1 and 1e-6 in other subtasks. We use multi-GPU training for two (or four) A100 (or V100) GPUs through apex\u00a0https://github.com/NVIDIA/apex.\n"]}
{"id": "2103.02405", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["We use hidden size \\(task_H=8\\) , layers \\(task_L=2\\) , heads \\(K=2\\) , structure learner's hidden size \\(func_H=8\\)  and \\(d_{pos}=4\\) . We compare to an MLP with 2 hidden layers and hidden size 16. We use \\(\\ell _{sparse}=0.0\\)  and \\(\\ell _{struct}=10.0\\) . We train the models for 250 epochs. To optimize the model, we use Adam optimizer with learning rate \\(0.001\\) . We report the average performance for 3 random seeds.\n{FIGURE}"]}
{"id": "2102.01223", "categories": "cs.CL cs.LG", "paragraphs": ["We use a standard Transformer architecture [46] with model dimension 256. The encoder consists of 2 layers with 4 self-attention heads and the decoder consists of 1 layer with 1 self-attention head and 1 attention head over the slots. We use the same positional encodings as in [46].\nWe feed in the sentences with less than 128 characters to our model and consider the number of slots as 64 (half of the maximum input length). In addition, we take the dimension of slots as 128, and run the algorithm for \\(T{=}1\\)  iterations.We choose \\(T{=}1\\)  iterations for simplicity and efficiency, and because preliminary experiments showed no improvements with more iterations. We leave the investigation of how to get improvements from more iterations to future work. We initialized the slots according to equation (REF ) in Sections REF  and REF .\n", "We scheduled the \\(\\lambda \\)  parameter in the training loss to start with a low value of \\(2 \\times 10^{-5}\\)  and exponentially increase it every 10 epochs until it reaches a certain limit. We control this parameter in a way that the final number of open gates roughly equals the average number of BPE tokens in a sequence.\nWe used Adam optimizer [26] for training our models with learning rate \\(10^{-4}\\)  and train our models for 200 epochs. More details of the settings are available in the Appendix.\n{FIGURE}"]}
{"id": "2102.01208", "categories": "cs.LG stat.ML", "paragraphs": ["We directly use the code provided for IBP\u00a0[12] and use the same CNN architectures (small, medium, large and wide) on MNIST and CIFAR-10 datasets. We use adaptive hyperparameter selection for \\(\\lambda \\)  or a piecewise linear schedule for \\(\\lambda \\)  for all robust training methods. The MNIST networks are trained for 100 epochs each with a batch size of 100 while the CIFAR networks are trained for 350 epochs each with a batch size of 50. We use the standard values of \\(\\epsilon = 0.3\\)  for MNIST and \\(\\epsilon = 8/255\\)  as the training target perturbation size \\(\\epsilon _{train}\\) . Following\u00a0[12], the schedule of \\(\\epsilon \\)  starts at 0 for a warmup period (2000 training steps on MNIST, 5000 training steps on CIFAR), followed by a linear increase to the desired target perturbation size (10000 training steps on MNIST, 50000 training steps on CIFAR), after which \\(\\epsilon \\)  is fixed at the target level. Additional details are reported in Appendix\u00a0.\n"]}
{"id": "2104.08116", "categories": "cs.CL", "paragraphs": ["For both MLM and PSP, we used cross-entropy loss.\nAs an optimiser, we used AdamW [24] with a 5e-5 learning rate and a 0.01 weight decay.\nFor regularisation, we set a 10% dropout probability.\nMaximum input sequence length is 128 tokens.\nFor adapting to unlabelled data, we trained for one epoch, i.e. one pass over all additional data, which matches [11].\nTraining batch size was 128.\nFor finetuning on labelled data, we trained for three epochs with a batch size of 32, which corresponds to default settings recommended by [9].\n"]}
{"id": "2104.08006", "categories": "cs.CL", "paragraphs": ["We carry out pre-training with 12-layer encoder, 12-layer decoder ProphetNet models. The hidden size is 1,024, feed forward size is 4,096, future tokens' prediction length is 2. Both the max sequence lengths of the input and output are set to 512.\n", "For ProphetNet-En, ProphetNet-Zh, ProphetNet-Multi, ProphetNet-Dialog-En, and ProphetNet-Code, we carry out un-supervised pre-training with masked span prediction task. Spans of continuous tokens are masked out from the encoder input sentences and predicted from the decoder side. We masked continuous 9 tokens in every 64 tokens from the encoder side, and predict the 9 tokens on the decoder side. In other words, for maximum 512 encoder sequence length, totally \\(8(spans) \\times 9(tokens\\ per\\ span) = 72\\)  tokens are masked and predicted. If the last part does not reach a maximum length of 64, 15% continuous tokens are masked. ProphetNet-Dialog-En has special tokens [X_SEP] to separate turns in a session and [SEP] to separate different sessions. For ProphetNet-Dialog-Zh, we conduct supervised pre-training. Previous turns of dialogs are fed into the encoder, and the response is predicted from the decoder. It means that for a multi-turn session with \\(n\\)  sentences, \\(n-1\\)  samples are created for pre-training. The pre-trained ProphetNet-Dialog-Zh can be used to directly generate dialogs without finetuning.\n"]}
{"id": "2106.09637", "categories": "cs.CV", "paragraphs": ["The AttDLNet was implemented on PyTorch [24] and run on a hardware setup with an NVIDIA GFORCE GTx1070Ti GPU and an AMD Ryzen 5 CPU with 32 GB of RAM.\nThe hyperparameters are trained using the Adam optimizer [25] with a learning rate of 0.001 and the cosine embedding loss function from the PyTorch framework.\n", "Since place matching is computed in the cosine space, AttDLNet is trained in the same similarity space, using the cosine loss function, with a margin parameter that has to be set. To assess the best margin value, a margin study was conducted, for which AttDLNet was trained and evaluated several times, using each time a different margin value. The discrete margin values used in the study are: 0.0, 0.3, 0.5, 0.7, 0.8, 0.85, 0.9, 0.95. Training and evaluation were performed using the conditions described in previous sections.\n"]}
{"id": "2102.09582", "categories": "cs.CV eess.IV", "paragraphs": ["The tumor types or organ labels were evenly separated in the three training, validation, and testing groups and the data were sampled with a batch size of 8. The FiLMed U-Nets of depth 4 for the spinal cord tumor and 5 for the chest CT were trained with a Dice loss function until the validation loss plateaued for 50 epochs (early stopping with \\(\\epsilon = 0.001\\) ). The depth was chosen according to the size of the input images. The initial learning rate was 0.001 and was modulated according to a cosine annealing learning rate.\n"]}
{"id": "2102.11582", "categories": "cs.LG stat.ML", "paragraphs": ["We train the softmax baselines on CIFAR-10/100 for 350 epochs using SGD as the optimiser with a momentum of 0.9, and an initial learning rate of 0.1. The learning rate drops by a factor of 10 at epochs 150 and 250. We train the 5-Ensemble baseline using this same training setup. The SNGP and DUQ models were trained using the setup of SNGP and hyper-parameters mentioned in their respective papers [41], [57]. For models trained on ImageNet, we train for 90 epochs with SGD optimizer, an initial learning rate of 0.1 and a weight decay of 1e-4. We use a learning rate warmup decay of 0.01 along with a step scheduler with step size of 30 and a step factor of 0.1.\n"]}
{"id": "2102.11603", "categories": "cs.CV cs.AI cs.IR cs.LG cs.RO", "paragraphs": ["We used a margin value of \\(\\alpha =0.3\\)  for computing the loss which was then minimized using SGD optimizer, with weight decay rate of \\(0.001\\)  and momentum \\(0.9\\) . The initial learning rate was set to \\(0.0001\\)  which was reduced by a factor of \\(0.5\\)  every 50 epochs. For the Oxford Robotcar dataset, we ran training for only 60 epochs and for other larger datasets, Brisbane City Loop, Nordland and MSLS, training was done for 200 epochs (this is due to the increased number of negatives in proportion to the size of the database as we only consider 10 negatives for each query\u00a0[0]). For generating positives/negatives for the triplet loss, we used a maximum/minimum distance of \\(5/20\\)  meters for the city datasets and \\(10/40\\)  frames for the Nordland dataset. For city datasets, we used \\(L_d\\)  as 5 and \\(w\\)  as 3 for training. For the Nordland dataset, these values were set to 10 and 5 respectively. During testing, we used a sequence length of 5 for all datasets and all methods.\n"]}
{"id": "2104.15104", "categories": "cs.CL cs.IR", "paragraphs": ["For both the models, we select 100 as the dimension of the word embeddings, and 50 as the dimension of all the other embeddings, i.e., POS-tag embedding, entity-type embedding, and positional embedding. Following prior work, we restrict the length of each sentence to be 50 (truncating long sentences if necessary). We select the hidden units of the BiLSTM network as 100. We choose a batch size of 10, and Adam with initial learning rate of \\(0.0002\\) . We select the dimension of the graph representation to be 150. When using GTNs, the number of edge-types (\\(L\\) ) is 35, which is determined by the number of unique types of dependency relations, e.g., nsubj, case, etc., as obtained from the dependency parser.\n"]}
{"id": "2104.15135", "categories": "cs.CL cs.AI cs.HC cs.LG", "paragraphs": ["When the model is transparent and the explanation displays the model parameters in an intelligible way, humans can directly adjust the parameters based on their judgements. This idea was adopted by [53], [51] where humans can adjust a bar chart showing word importance scores, corresponding to the parameters of the underlying Naive Bayes model.\nIn this special case, steps 2 and 3 in Figure\u00a0REF  are combined into a single step.\nBesides, human feedback can be used to modify the model parameters indirectly.\nFor example, [91] increased a word weight in the Naive Bayes model by 20% for the class that the word supported, according to human feedback, and reduced the weight by 20% for the opposite class (binary classification).\nThis choice gives good results, however,\nit is not clear why and whether 20% is the best choice here.\n"]}
{"id": "2104.08710", "categories": "cs.CL", "paragraphs": ["When finetuning for QA, REALM performs an approximate MIPS search for retrieving relevant documents. Additionally, the system is trained on a single machine with a 12GB GPU with \\(k=5\\)  and batch size 1. While this is modest use of resources, we show that this results in suboptimal training of the system. We begin by scaling the realm system during training. We perform exact MIPS search by leveraging the efficiency of large matrix multiplications of TPUs [25] to compute the retrieval score, \\(S_{retr}\\) , for \\(\\sim \\) 13M passages of corpus and extract \\(c\\)  passages having the highest scores. We further increase the training batch size from 1 to 16 by leveraging 8 TPUv3 cores on Google Cloud for distributed training. Finally, we increase the number of documents passed to the reader to \\(k=10\\)  during training.\n"]}
{"id": "2112.05827", "categories": "cs.CV cs.LG", "paragraphs": ["To provide real-world scenarios for our training setup, we augment the described datasets with corruptions that reduce image quality. The face images are corrupted using motion blur, JPEG compression, additive Gaussian noise, scaling (width to height ratio \\(\\sim \\! 0.9\\!-\\!1.1\\) ), down-sampling and smoothing. To corrupt the iris images, blurring matrix, \\(B\\) , warping, \\(W\\) , downsampling, \\(D\\) , and additive noise \\(\\bar{n}\\)  are considered: \\(\\bar{X}=DBWX+\\bar{n}\\)  as described in\u00a0[75].\n", "Similarly, as presented in Fig.\u00a0REF , the fingerprint images are degraded using two corruptions\u00a0[57], [56]. The first corruption consists of warping the clean fingerprints\u00a0[56] by randomly sampling the first two principal warp components extracted from the Tsinghua Distorted Fingerprint Database\u00a0[76], [56]. The other corruption considers fading the fingerprint ridges at random points\u00a0[57]. Data augmentation is also performed on the fingerprint images, where 20 samples are generated for each fingerprint image by translating the core point both vertically and horizontally using distances coming from Gaussian distributions\u00a0[38]. Here, ten translated images are generated using a Gaussian distribution with parameters \\(\\mu =0\\)  and \\(\\sigma =2.5\\) . The remaining ten augmented images are generated with \\(\\mu =0\\)  and \\(\\sigma =5\\) .\n", "Training: We initially train each \\(\\mathrm {qNet}_k^{\\mathrm {a}}\\)  for the classification setup with a varying number of modality samples per multimodal sample set, where a feature vector of size 512 is trained using uniform angular loss and network compactness loss as defined in Equations\u00a0REF  and\u00a0REF , respectively. Iris and fingerprint unimodal networks are trained on their respective BioCOP modalities, while the face network is trained on the combination of BioCOP and VGGFace2 datasets. The estimated normalized quality scores for degraded samples in the BioCOP dataset can be found in Fig.\u00a0REF . Each row in this figure presents eight samples of the same subject to construct the unimodal multi-sample set. The number of samples from a modality in a multimodal sample set is chosen to represent the test datasets. Therefore, up to 30 samples are considered for the face modality, while for the other two modalities up to five samples are considered.\n", "The main branch of \\(\\mathrm {qNet}_k^{\\mathrm {a}}\\)  networks are initialized with weights pre-trained on Imagenet\u00a0[79]. The other parameters are initialized using Kaiming initialization\u00a0[80]. The preprocessing algorithm consists of the channel-wise mean subtraction. The five-fold cross-validation method is considered to estimate the best hyperparameters during the training phase. The training algorithm is deployed using mini-batch stochastic gradient descent with momentum of \\(0.9\\) . The training is regularized by weight decay of \\(5\\times 10^{-4}\\)  and \\(50\\%\\)  dropout for the fully-connected layers, except for the last layer of each network where the representations are considered for recognition. The moving average decay is set to \\(0.99\\)  for all the networks except the iris modality, for which it is set to \\(0.9\\) . Batch size is set to 32 and 16 for unimodal and multimodal frameworks, respectively. The initial learning rate is set to \\(0.1\\) . The learning rate decreases exponentially by a factor of \\(0.1\\)  after \\(10^5\\)  iterations, and then every \\(5\\times 10^4\\)  iterations, with the final learning rate of \\(10^{-6}\\) .\n"]}
{"id": "2106.05596", "categories": "cs.CV eess.IV", "paragraphs": ["As we use a Siamese network based approach for training our feature extractor, we create pairs of images for training. Each pair corresponds to an unmasked reference and a masked probe image. The network outputs a similarity in [0,1] with 0 indicating imposter and 1 indicating authentic match. Since absolute difference is taken between embeddings from a shared weight siamese network, the ordering of masked/unmasked images as reference and probe has no effect on the final similarity scores. Figure\u00a0REF  contains a high level overview of the architecture.\n"]}
{"id": "2105.04642", "categories": "cs.CV", "paragraphs": ["In the experiment, the videos are re-sampled at 1 fps and input to the model. The number of the MoN samples is set to 10. During model training, the generator encoder is pre-trained with the surgical phase recognition task for 20 epochs. The pre-training is accomplished with the same dataset; therefore, no additional data are used. During GAN training, we use small epochs to train the generator and the discriminator in an iterative fashion, where the epoch size is 64 and the number of epoch is 2000.\n{FIGURE}"]}
{"id": "2112.13593", "categories": "cs.LG cs.CL q-fin.TR", "paragraphs": ["We restrict 14 days for a textual sample and set the duration of the historical trending description as 64-day. The prediction interval is five days, and there are 64 shuffled samples in a batch. The maximal length of the text is 64 in words. And the maximal length of a textual sequence \\(s\\)  is restricted to 96, with excess clipped. To improve the expression of word vectors, we set the latent dimension as 512. All weight matrices in the model are initialized with the fan-in trick, while we set biases as zero in the beginning. We train the model as an Adam optimizer with an initial learning rate of \\(0.001\\)  and follow a linear decay strategy. We use the input dropout rate of \\(0.2\\)  and the weight decay rate of \\(0.001\\)  to regularize.\n"]}
{"id": "2102.12877", "categories": "cs.LG cs.SE", "paragraphs": ["Most specifications related to the training of TELESTO are listed in tbl:evalspecs. We employ leave one group out (LOGO) cross-validation for data splitting, i.e. the five injections of each anomaly and each service component are split as 3/1/1 as a training/validation/test split.\nFor TELESTO itself, we choose a graph node feature dimensionality of 64 and set the number of graph transformation levels to 5.\nFor the TAGCN layers, we choose \\(k=3\\)  fixed-size learnable filters as recommended in\u00a0[4].\nFor the GAT layers, we choose \\(K=8\\)  parallel attention mechanisms to produce rich node features with multi-head attention.\nLastly, the JK LSTM-aggregator is equipped with seven layers in order to learn a reasonable node weighting based on node features.\nWe choose ELU\u00a0[2] as activation function for the FFF block.\nThe final softmax calculates a distribution over anomaly classes, whereof the highest is used as the prediction target.\n"]}
{"id": "2109.05729", "categories": "cs.CL", "paragraphs": ["We train our models on the open source large-scale raw text, Chinese Wikipedia and a part of WuDaoCorpus. The training data contains 200GB cleaned text ranges from different domains. We use Jieba to segment Chinese words for Whole Word Masking and use WordPiece tokenizer inherited from BERT to split input text into tokens. We use Adam to train the models for 500k steps, with the batch size of 2048, the learning rate of 1e-4, \\(\\beta _1 = 0.9 \\) , \\(\\beta _2 = 0.98\\) , weight decay of 0.01. We warmup the learning rate for first 10,000 steps then do linear decay. In addition, a Chinese BART is pre-trained with the same corpora, tokenization and hyper-parameters as a baseline.\n"]}
{"id": "2104.09903", "categories": "cs.CV cs.AI", "paragraphs": ["Adam optimizer and MSE loss are used in both models, with a learning rate of \\(3\\times 10^{-4}\\)  in the 3D ResNet case, and \\(10^{-4}\\)  in the CNN-GRU. Another difference between both models is in the batch size, and in the number of epochs, being 5 and 100 in the 3D model, and 3 and 150 in the RNN, respectively. Also, in the 3D-CNN model, early stopping is used, with a patience of 7, so the training end in epoch 25/100. To perform some regularization, the output targets are normalized between -1 (\\(30 km/h\\) ) and 1 (\\(100 km/h\\) ).\n"]}
{"id": "2106.09997", "categories": "cs.CL", "paragraphs": ["In the pre-training step, we denote the number of Transformer encoder layers as L, the size of hidden vectors as H, and the number of self-attention heads as A. We followed the setting of BERTBASE (L=12, H=768, A=12, total parameters=110M) and continued to train 200K steps from cased BERTBASE checkpoint. The maximum sequence length was fixed to 512, and the batch size was set to 128. We used Adam with a learning rate of 2e-5 and epsilon of 1e-8 and employed cased BERTBASE vocabulary with 30K tokens.\n"]}
{"id": "2111.10513", "categories": "cs.CL", "paragraphs": ["Our models are trained using the Adam [5] optimizer. Following [15], we also use the \u201cNoam\u201c learning rate scheduler, linearly increasing the learning rate from 0 for the first 8000 steps, then decaying afterward. We also set Adam's \\(\\beta _2 = 0.998\\)  and use a label smoothing factor of \\(0.1\\) .\n", "For batching, we accumulate tokens until we reach a maximum size of approximately 32,000 tokens per batch, an increase over the 25,000 tokens used in [15]. We then train the base model and the large model for 100,000 steps and 300,000 steps, respectively. All our models are trained on 8 NVIDIA Tesla P100 GPUs in parallel using the OpenNMT-py [6] toolkit.\n"]}
{"id": "2106.03631", "categories": "cs.CL", "paragraphs": ["We adopt the VAE architecture from [6], using a LSTM encoder-decoder. Unless stated otherwise, (word embedding, LSTM, representation embedding) dimensionalities for YNOC and POS datasets are (4D, 32D, 4D) and (4D, 64D, 8D), respectively, and we use the latent code to initialize the hidden state of the LSTM decoder. We use greedy decoding. All models are trained from multiple random starts using Adam\u00a0[24] with learning rate 0.001 for 10 epochs. We set batch size to 256 and 512 for YNOC and POS, respectively.\n"]}
{"id": "2108.12988", "categories": "cs.LG cs.MA", "paragraphs": ["We adopt the same set of hyperparameter for experiments. 12 rollouts are executed in parallel when training. The maximum length of the replay buffer is \\(1e6\\) . Episode length is set to 20. The dimension of the latent code \\(z\\)  is 6. The critic also adopts a self-attention network in a similar way with MAAC [9]. And the number of gradient steps of policy and critic parameters in each update, i.e., \\(K\\) , is set as 10. And \\(\\alpha =1\\)  works well in experiments. Batch size is set to 1024 and Adam is used as the optimizer. The initial learning rate is set to \\(0.0003\\) . In all experiments, we use one NVIDIA Tesla P40 GPU.\n"]}
{"id": "2103.07829", "categories": "cs.CL", "paragraphs": ["Pre-training Data \u00a0\u00a0\u00a0We use the same in-domain data as in LXMERT\u00a0 for pre-training. It consists of the image caption data from MS COCO\u00a0, Visual Genome\u00a0, and image question answering data from VQA v2.0\u00a0, GQA balanced version\u00a0 and VG-QA\u00a0. The total amount of the dataset is 9.18M image-and-sentence pairs on 180K distinct images. Besides, we also use additional out-of-domain data from Conceptual Captions\u00a0 and SBU Captions\u00a0 for model pre-training, which consists of about 4M image-text pairs on 4M images.\n"]}
{"id": "2109.03334", "categories": "cs.CL cs.AI", "paragraphs": ["All experiments were performed with the 3-billion parameter version of T5-UQA. We made use of DeepSpeed ZeRo optimizations [31] to fit the 3B model into the largest GPUs available to us (A100-40GB). Models were trained to 30 epochs, where generation performance (ROUGE-1) plateaued. We use the default hyperparameters for training provided in the Huggingface Transformers library [36]. To improve inference quality, at inference time we use a batch size of 1, a beam search over 64 beams, and (given the diversity of generations, and the preference for shorter generations even after considerable training) combine all facts generated in the top 10 beams (after splicing on the fact delimiter) into a candidate list of generated facts.\n"]}
{"id": "2105.10146", "categories": "cs.CL", "paragraphs": ["As mentioned previously, we use the sentence-transformers [26] library to conduct our experiments. For every run of training, including pre-training and fine-tuning steps wherever applicable, we train for 2 epochs and checkpoint at intervals of 10000 steps. We override the previous checkpoint only if results improve on our dev set.\n", "A constant batch size of 32 is maintained across all our experiments. The AdamW [15] optimizer is used with a default learning rate of \\(2e^{-5}\\) . A linear warmup scheduler is applied to slowly increase the learning rate to a constant after 10% of training iterations (warm-up ratio = 0.1) to reduce volatility in the early iterations of training. A single Tesla V100 GPU (32 GB) is used for all training iterations.\n"]}
{"id": "2110.07206", "categories": "cs.CV cs.RO", "paragraphs": ["The used datasets are resized to 1024 \\(\\times \\)  512 including both training and testing.\nAll the networks are trained from scratch using Adam optimizer for 100 epochs with a total batch size of 8.\nThe learning rate is first initialized to 0.0001 and divided by 5 following milestones at the 30th, 50th, and 80th epochs.\nThe output channel width \\(k\\)  is [14,16,20,20,40] for the 71-layer and [14,16,40] for the 33-layer.\nAdditionally, by empirical finding, the coefficient \\(\\alpha \\)  for high-level tasks is set to 0.01, 0.05, and 0.002 for lane detection, depth estimation, and object detection, respectively.\nAll the experiments are performed by using one NVIDIA TITAN X GPU and one Intel Core i7-6700K CPU based on the PyTorch framework.\n"]}
{"id": "2105.05641", "categories": "cs.CL cs.LG", "paragraphs": ["We pre-train 5 BERT base and large uncased English models, each with the same configurations as in [10] using Tensorflowhttps://github.com/tensorflow/models/tree/master/official/nlp/bert. However, each model differs in its random seed, resulting in different parameter initializations and training data permutations. Hence, it is expected that the checkpoints will each end up at a different local minima. It should be noted that BERT uses static masking instead of dynamic masking, so the set of pre-training examples remains the same.\n"]}
{"id": "2104.05752", "categories": "cs.CL cs.LG cs.SD eess.AS", "paragraphs": ["ASR-Text-Speech-2 Model\n Similar to the ASR-Text-Speech-1 model, ASR-Text-Speech-2 also improves upon Text-Speech by taking ASR transcripts as its input. However, ASR-Text-Speech-2 uses ASR transcripts in addition to ground truth transcripts to domain-adapt the BERT branch before joint training it with the acoustic branch. In this domain adaptation phase, we build a separate BERT model that has a structure identical to the text encoder in Text-Speech. We domain-adapt this BERT model on the ground truth and ASR transcripts from our target datasets with a learning rate of 2e-5. We then use this domain-adapted BERT model as our text branch without any further fine-tuning. By freezing the BERT text branch in the final training round, we let the domain-adapted text embeddings guide our audio embeddings. This is why, in this stage, we only use pairs of audio and ground truth transcripts, which provides better-quality text embeddings compared to ASR transcripts.\n"]}
{"id": "2108.13073", "categories": "cs.CL", "paragraphs": ["Pre-training was only done with GRU encoders, as discussed in Section\u00a0REF .\nDue to the large number of entities in the pre-training set, 1-N sampling is only performed with negative examples from the same batch, and batches of size 4096 were used, following\u00a0[3].\nThe learning rate was selected from {\\(1\\cdot 10^{-4}, 3\\cdot 10^{-4}\\) }, while the dropout rate was selected from {\\(0.2, 0.3\\) } for ConvE and {\\(0.3, 0.4\\) } for TuckER.\nFor 5\\(^\\star \\) E, the dropout rate was not used, but N3 regularization was\u00a0[13], its weight selected from {\\(0.1\\) ,\\(0.03\\) }.\nFor TuckER, models with embedding dimensions 100, 200, and 300 were trained.\nWe saved the best model of each dimension for fine-tuning.\nFor ConvE, models with embedding dimensions 300 and 500 were trained, and the best model for each dimension was saved for fine-tuning.\nFollowing\u00a0[10], we use a single 2d convolution layer with 32 channels and \\(3\\times 3\\)  kernel size.\nWhen the dimension of entities and relations is 300, they were reshaped into \\(15\\times 20\\)  inputs, while the \\(20\\times 25\\)  input shapes were used for the 500-dimensional embeddings.\nFor 5\\(^\\star \\) E, models with embedding dimensions 200 and 500 were trained, and the best model for each dimension was saved for fine-tuning.\nFollowing\u00a0[3], we trained each model for 100 epochs.\nTesting on the validation set is performed each 20 epochs, and the model with the best overall mean reciprocal rank (MRR) is selected.\n"]}
{"id": "2108.13051", "categories": "cs.LG cs.AI", "paragraphs": ["The embedding model requires hp tuning to be trained effectively on a specific dataset.\nThe two most important hp in a kge, according to [13], are the embedding dimension and the optimizer (with its learning rate).\nOther parameters, like the negative sampling, can affect the time to train a model, but they are less significant for the final accuracy.\nIn particular, in fig:negative, negative sampling is difficult to manage since it is hard to have a negative set (a set of false triples) available. Perturbing the triples randomly is challenging as there is no certainty that this is not a possible repurposed drug, and inserting it in the negative set would indicate to the model to penalize an actually correct representation of the triple. For this reason, we choose a low value for the negative sampling that reduces the probability of this event and saves computational resources.\nThe result shows that a low embedding dimension yields the worst accuracy. Instead, an exaggerated embedding dimension does not bring benefits but only faster overfitting and a higher computational cost. For the kg proposed in this work, the best embedding size is 128 (fig:embDim) since it is the best compromise between accuracy and model complexity. The best optimizer proved to be ADAM with learning rate \\(\\lambda = 10^{-4}\\) , coherently to [16] (fig:optimizer,fig:lr).\n"]}
{"id": "2108.13032", "categories": "cs.CL cs.LG", "paragraphs": ["To save pretraining time, we tokenize the corpus in advance and cache the results to files. The tokenization can be done with 20 parallel 4-core CPU machines in 2 hours. Following RoBERTa [22], we use Masked Language Modeling (MLM) as the pretraining objective, without Next Sentence Prediction.\n", "For optimization we use Adam with \\(0.01\\)  weight decay [2]. The learning rate is set to 1e-4, with 10k steps warmup then linear decay to 0.\n"]}
{"id": "2108.13865", "categories": "cs.CV cs.AI cs.LG cs.RO", "paragraphs": ["Our InSeGAN modules are implemented in PyTorch. As alluded to above, we generate \\(224\\times 224\\)  depth images using our simulator; however, we use \\(64\\times 64\\)  images in our InSeGAN pipeline. To this end, each \\(224\\times 224\\)  image is rescaled to \\(64\\times 64\\)  and normalized using mean subtraction and normalization by the variance. For training, we use horizontal and vertical image flips for data augmentations. We do not use any other augmentation scheme.\n"]}
{"id": "2104.06069", "categories": "cs.LG cs.DC", "paragraphs": ["For BERT pre-training, we set the parameters in (REF ) as \\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.999\\) , \\(c_{min} = 0.01\\)  and \\(c_{max} = 0.3\\)  for LAMB and 1-bit LAMB. For 1-bit LAMB, we set \\(\\beta _3 = 0.9\\) , \\(r_{min} = 0.5\\) , \\(r_{max} = 4.0\\) , and \\(r_{threshold} = 0.1\\)  in Algorithm\u00a0REF . For convergence analysis, we set total batch size as 64K for seqlen 128 and 32K for seqlen 512. For performance analysis, we test different batch sizes from 8K to 64K.\n", "For BERT pre-training seqlen 128, the learning rate starts from \\(1\\times 10^{-3}\\) , exponentially increases to \\(12\\times 10^{-3}\\)  as a warmup in the first 450 steps, then decays into 0.9 of the original after every 250 steps. The total number of steps is 5993. For 1-bit LAMB we use the first 1000 steps (16.7%) as the warmup stage. For BERT pre-training seqlen 512, the learning rate starts from 0, exponentially increases to \\(2\\times 10^{-3}\\)  as a warmup in the first 150 steps, then decays into 0.9 of the original after every 150 steps. The total number of steps is 555. For 1-bit LAMB we use the first 107 steps (19.3%) as the warmup stage.\n", "For GLUE benchmarks we use Adam optimizer and perform single-task training on the dev set. Following the setup in the BERT paper\u00a0[6], we use a batch size of 32 and fine-tune for 3 epochs for all GLUE tasks. For each task, we select the best learning rate among \\(\\lbrace 2\\times 10^{-5},3\\times 10^{-5},4\\times 10^{-5},5\\times 10^{-5}\\rbrace \\) . For SQuAD fine-tuning we use Adam optimizer and the same parameters as published by HuggingFace (batch size = 24, learning rate = \\(3\\times 10^{-5}\\) , dropout = \\(0.1\\) , 2 epochs).\n", "For experiments in Section\u00a0REF , for both LAMB and 1-bit Adam we use batch size = 16K, 28125/3125 steps for seqlen 128/512, weight decay = 0.01, linear LR warmup and decay. For LAMB, we use learning rate = \\(3.54\\times 10^{-3}\\) , 10% LR warmup, clipping configs (\\(c_{min}\\)  and \\(c_{max}\\)  in (REF )) as 0.1 and 1. For 1-bit Adam, we use learning rates \\(\\in \\lbrace 1\\times 10^{-4},2\\times 10^{-4},3\\times 10^{-4}\\rbrace \\) , LR warmup \\(\\in \\lbrace 5\\%,10\\%,20\\%\\rbrace \\) . All of these training parameters (except LAMB clipping configs) are from the LAMB paper. For 1-bit Adam, following the original work's strategy we set the number of warmup steps as 4000 (out of total 28125 steps) for seqlen 128 and 475 (out of 3125) for seqlen 512.\n"]}
{"id": "2104.05942", "categories": "cs.LG cs.SY eess.SY math.OC", "paragraphs": ["and then compute the model parameters of robust RENs based on the matrix partition of \\(H\\)  in (REF ). A special case with \\(Q=-\\frac{1}{\\gamma }I,R=\\gamma I, S=0\\)  and \\(D_{22}\\)  was reported in [56].\n"]}
{"id": "2110.04020", "categories": "cs.LG stat.ML", "paragraphs": ["Our models are trained using the Adam optimizer [34] with the triangular learning rate schedule from [57].\nOur weight distribution experiments are conducted using 900 transformers trained by likelihood maximization with SGD as done in [20].\n"]}
{"id": "2110.04077", "categories": "cs.CV cs.AI eess.IV", "paragraphs": ["We train the model using Adam with \\({\\beta }_1=0.5\\)  and \\({\\beta }_2 = 0.9\\) \u00a0[15] with a mini-batch size of 128.\nFor all the convolutions in the modules, we set the kernel size to [4, 4], stride to [2, 2] and padding to [1, 1].\nWe set the numbers of output channels of the block to [64, 128, 256, 64] for the convolution in the encoder, [256, 256, 128, 64] for the transposed convolution in the generator, and [64, 128, 256, 256] for the convolution in the discriminator.\nOther hyper-parameters are tuned with the successive halving pruner in a hyper-parameter optimization framework Optuna\u00a0[0]. Such hyper-parameters include the learning rates of the encoder and generator, the learning rate of the discriminator using two time-scale update rule (TTUR)\u00a0[10], the gradient penalty coefficient in WGAN-GP, the dimensions of latent vectors from the encoder, \\(\\lambda \\)  in Eq.\u00a0(REF ) and the slope parameters of leaky ReLU activation functions in the modules.\n"]}
{"id": "2110.07038", "categories": "cs.CL cs.AI", "paragraphs": ["Following BERT\u00a0[8], we train ElasticBERT in two different configurations: ElasticBERTBASE and ElasticBERTLARGE, which have the same model sizes with BERTBASE and BERTLARGE, respectively. The parameters of ElasticBERT are initialized with BERT, and therefore it has the same vocabulary and tokenizer as BERT. ElasticBERT is pre-trained on \\(\\sim \\) 160GB uncompressed English text corpora, which is comprised of English Wikipedia (12GB), BookCorpus (4GB)\u00a0[60], OpenWebText (38GB)\u00a0[12], and part of the C4 corpus (110GB)\u00a0[32]. We use Adam optimizer\u00a0[18] with \\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.999\\)  to pre-train ElasticBERTBASE and ElasticBERTLARGE for 125K steps with the batch size of 4096 and learning rate of 2e-4. Our implementation is based on Huggingface's Transformers\u00a0[50] and the Megatron-LM toolkit\u00a0[37]. ElasticBERT is trained on 64 32G NVIDIA Tesla V100 GPUs.\n"]}
{"id": "2110.07816", "categories": "cs.CL cs.AI", "paragraphs": ["All models are trained with Transformer architecture [32], implemented in the Fairseq framework [20].\nThe individual models are trained with the model hidden size of 256, feed-forward hidden size of 1024, and 2 layers. All multilingual models either cluster-based or universal MNMT models with or without knowledge distillation were trained with the model hidden size of 512, feed-forward hidden size of 1024, and 6 layers. We use the Adam optimizer [12] and an inverse square root schedule with warmup (maximum LR 0.0005). We apply dropout and label smoothing with a rate of 0.3 and 0.1 for bilingual and multilingual models respectively.\n"]}
{"id": "2109.05432", "categories": "cs.CV", "paragraphs": ["We use PyTorch as our training framework. The initial learning rate is set to 0.3 which is then adjusted by cosine annealing. SGD optimizer is adopted with a weight decay of 1e-4. We train our PSS-Net for a total of 250 epochs with a batch size of 1024.\n{FIGURE}"]}
{"id": "2109.05411", "categories": "cs.LG cs.DC cs.NI cs.SY eess.SY math.OC", "paragraphs": ["For all experiments, we initialize our model with \\(\\mathbf {w}_0=\\mathbf {0}\\)  and SGD batch size \\(b=64\\) . In each round, we uniformly sample \\(K\\)  devices at random, which run \\(E\\)  steps of SGD in parallel. For all experiments, we use an initial learning rate \\(\\eta _0=0.1\\)  with decay rate \\(\\frac{{\\eta }_0}{1+r}\\) , where \\(r\\)  is communication round index. We evaluate the aggregated model in each round on the global loss function. Each result is averaged over 50 experiments.\n"]}
{"id": "2105.13318", "categories": "cs.CL", "paragraphs": ["All our grammar correction models are standard Seq2Seq (not Seq2Edits) Transformers [37] trained with Adafactor [32] using the Tensor2Tensor [36] TensorFlow [0] library. Our corruption models are either standard Transformers or Seq2Edits models [34].The focus of our work was to examine techniques for synthetic data correction while keeping the correction model fixed. Hence, we do not use Seq2Edits models for correction. We use a Tensor2Tensor joint 32K subword vocabulary and the `Big' hyper-parameter set for all our models. For our tagged corruption models we extend the subword vocabulary by the 25 ERRANT error tags.\n"]}
{"id": "2108.07794", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["We perform the pre-training on ShapeNet\u00a0[0], a dataset composed of richly-annotated shapes represented by 3D CAD models of objects from 55 common categories. To generate the random room, we first need to randomly sample multiple objects from the the dataset. The number of objects we sample is a random integer from 12 to 18, which is similar to the average number of objects in ScanNetV2 scenes. Then for each sampled object, we perform the random room generation algorithm mentioned in Section\u00a0REF . The object-level contrastive learning loss is used to train the model in an unsupervised manner.\n", "For the downstream 3D object detection task, we use the backbone models proposed in\u00a0[34] and\u00a0[62], which take as input 40,000 points. Following the network configurations in these two works, we use the 1024-point feature as the output of the backbone models and perform contrastive learning on this feature. During pre-training, we use the Adam optimizer\u00a0[22] with initial learning 0.001. We train the model for 300 epochs and the learning rate is multiplied by 0.1 at the 100-th and 200-th epcoh. The batch size is set to 16 such that roughly 200\\(\\sim \\) 300 unique objects are involved in the contrastive learning at every iteration.\n"]}
{"id": "2102.08602", "categories": "cs.CV cs.LG", "paragraphs": ["We consider two training setups for the ImageNet classification task.\nThe 90 epochs training setup trains models for 90 epochs using standard preprocessing and allows for fair comparisons with classic works.\nThe 350 epochs training setup trains models for 350 epochs using improved data augmentation and regularization\nand is closer to training methodologies used in modern works with state-of-the-art accuracies.\n", "In the 90 epoch setup, we use the vanilla ResNet for fair comparison with prior works.\nWe used the default hyperparameters as found in official implementations without doing additional tuning.\nAll networks are trained end-to-end for 90 epochs via backpropagation using SGD with momentum 0.9.\nThe batch size \\(B\\)  is 4096 distributed across 32 TPUv3 cores\u00a0[33] and the weight decay is set to 1e-4.\nThe learning rate is scaled linearly from 0 to 0.1B/256 for 5 epochs and then decayed using the cosine schedule\u00a0[42].\nWe use batch normalization with decay 0.9999 and exponential moving average with weight 0.9999 over trainable parameters and a label smoothing of 0.1.\nThe input image size is set to 224x224.\nWe use standard training data augmentation (random crops and horizontal flip with 50% probability).\n", "All mobilenet architectures are trained for 350 epochs on Imagenet with standard preprocessing at 224x224 resolution.\nWe use the same hyperparameters as\u00a0[25].\nMore specifically, we use RMSProp with 0.9 momentum and a batch size of 4096 split across 32 TPUv3 cores.\nThe learning rate is warmed up linearly to 0.1 and then multiplied by 0.99 every 3 epochs.\nWe use a weight decay 1e-5 and dropout with drop probability of 0.2\n"]}
{"id": "2106.13638", "categories": "cs.LG cs.SY eess.SY", "paragraphs": ["We use the machine learning platform TensorFlow [13] for the implementation of the NNs and the training process utilises the Adam-Optimiser [14] with a decaying learning ratePlease refer to the published code for details. The initial learning rate is set to values between 0.01 and 0.001 and the decay leads to reduction between one and two orders of magnitude at the end of the training. For all variants of the NNs we use two hidden layers and 150 nodes per layer.\nThe training data are selected from a simulated database which segments the input domain [time and power disturbance] into a equally spaced grid with a 0.001s and 0.002 p.u. granularity. The exact training dataset used in the results section will be specified by the number of trajectories \\(N_{traj}\\) , i.e. each trajectory links to a power disturbance, and the number of data points along each trajectory \\(N_{TS}\\) , hence the total number of data points \\(N_{x}= N_{traj} \\cdot N_{TS}\\) . The collocation points for PINNs form an equally spaced grid with 25 trajectories and 41 instances along each trajectory. We test all types of NN on the entire simulated database, i.e \\(N_{traj} = 301\\) , \\(N_{TS} = 2001\\) . The reason for not using a validation dataset is that this work does not aim for comparing how well each NN type can be tuned with respect to its hyper-parameters. Instead we opted for choosing hyper-parameters, e.g. the loss term weights, that robustly yield a fair comparison so that we can explore the characteristic of each methodology. For a regular use case the partitioning into a validation and test dataset is strongly encouraged. The data creation and training is all performed on a regular machine (i5-7200U CPU @ 2.50GHz, 16GB RAM).\n"]}
{"id": "2106.13552", "categories": "cs.CV cs.AI", "paragraphs": ["1) \\(\\alpha \\)  determines the significance of the unpaired distance preserving loss. If \\(\\alpha = 0\\) , graph pattern loss only considers the pairwise distance between representations of the same objects from different modalities and the mutual distance preserving loss. However, only providing mutual distance does not give the correct guidance of the unpaired distance. From Figure.\u00a0REF -(a), we can observe that both higher or lower values of \\(\\alpha \\)  result in poor performance. Based on the above analysis, the unpaired distance preserving loss plays an important role in cross-modal retrieval.\n"]}
{"id": "2105.02103", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["We use face images of size \\(112 \\times 112\\) , detected and cropped by RetinaFace detector [40], and employ ResNet-34 and ResNet-100 [83] architectures for the encoder. Final \\(L_2\\) -normalized embeddings are of size \\(D=256\\)  for ResNet-34 and \\(D=512\\)  for ResNet-100 models. For experiments with Prototype Memory hyperparameters we used CosFace [5] loss with \\(s=64\\)  and \\(m=0.4\\) . The learning rate started from \\(0.1\\)  and divided by 10 at \\(100k\\) , \\(200k\\) , \\(250k\\) , \\(275k\\)  iterations finishing at \\(300K\\)  iterations. For the experiments with larger networks, we used PM-100: ResNet-100, pre-trained using Prototype Memory (\\(M=200,000\\) , \\(r=0.2\\) ) and CosFace with \\(s=64\\)  and \\(m=0.4\\) . The learning rate started from \\(0.1\\)  and divided by 10 at \\(200k\\) , \\(400k\\) , and \\(500k\\)  iterations, finishing at \\(540K\\) . Mini-batch size is 512 for all models, mini-batch sampling is group-based iterate-and-shuffle.\n"]}
{"id": "2102.00461", "categories": "cs.CL stat.AP stat.ML", "paragraphs": ["During training, XLM-RoBERTa's weights were kept frozen and only the BiLSTM and CRF layers were updated. We experimented BiLSTM with \\(16, 32, 64, 128, 256\\)  and 512\nhidden units and more layers,\nbut in the end, having a small segmentation module, with 64 hidden units and 1 layer, generically yielded the best performances in the validation splits.\nWe used a dropout layer of value 0.25 between the BiLSTM and the CRF, and the RMSprop\noptimizer with a fixed learning rate of 0.001.\n"]}
{"id": "2107.00708", "categories": "cs.CV", "paragraphs": ["Following [34], [37], [8], we train our network by using the training sets of DIV2K [0] and Flickr2K[33]. The evaluations (in PSNR) are performed over four standard datasets Set5 [2], Set14 [41], B100 [25] and Urban100 [13]. We apply the degradation in Eq.REF  to generate LR images in both training and testing. Specifically, we first train a model by applying degradations of isotropic Gaussian kernels and noises, where the kernel size is fixed at \\(21 \\times 21\\) , the kernel width is set to the range\\([0.2, 4.0]\\)  and the noise level is set at \\([0, 75]\\)  as in [37], [34]. We also train our model on degradations with anisotropic Gaussian kernels and noises, where the kernels have a Gaussian density function \\(N(0,\\Sigma )\\) , the covariance matrix \\(\\Sigma \\)  is determined by a random rotation angle \\(\\theta \\sim {U(0,\\pi )}\\)  and two random eigenvalues \\(\\lambda _1,\\lambda _2 \\sim {U(0.2,4)}\\) , and the noise is set to the range \\([0, 25]\\) , as in [34]\n"]}
{"id": "2112.08761", "categories": "cs.LG", "paragraphs": ["This section lists parameters used in the FL experiments that have not been included in the description of the experimental setup.\nThe local training on each device uses mini-batches of size 64.\nThe optimizer is the same as for the fitness evaluation (SGD with momentum set at 0.9 and weight decay of \\(10^{-4}\\) ).\nThe learning rates are set at 0.035 for the network with FEMNIST (as in [9]), and 0.05 for DenseNet-40/100.\nThe NN for FEMNIST is also similar to the one used in [9], i.e., CNN with two \\(5{\\times }5\\)  convolutional layers with 32 and 64 filters, respectively, each with ReLU activation and each followed by a \\(2{\\times }2\\)  max pooling.\nThe convolutional part is followed by two fully connected layers with 512 and 62 neurons (number of classes), respectively.\nThe implementation of this NN with structured dropout is depicted in fig:feminstnetwork.\n(The NN for CIFAR-10/100 are explained in the description of the experimental setup.\n"]}
{"id": "2104.09839", "categories": "cs.LG cs.AI", "paragraphs": ["The Adam algorithm [8] is used for gradient-based optimization. The number \\(n\\)  of iterations is chosen sufficiently large to reach a cost function plateau. The learning rate \\(\\lambda \\)  is adjusted by a rough trial and error. All static non-linearities are modeled as feed-forward Neural Networks with a single hidden layer containing 10 neurons and hyperbolic tangent activation function.\n"]}
{"id": "2111.05805", "categories": "cs.CL", "paragraphs": ["We use the cache of pretrained models of Transformers to initialize the model.\nFor MNLI finetuning, we train the model with 32 batch size and 128 max sequence length for 3 epochs. We use AdamW\u00a0[19] optimizer with 2e-5 learning rate.\nFor SQuAD finetuning, we train the model with 12 batch size, 384 max sequence length and 128 document stride for 2 epochs. We use AdamW optimizer with 3e-5 learning rate. For both datasets we only use the training data to finetune the model.\n", "For XLA-MAML, for both datasets we use the same data preprocessing parameters as the English model. We use batch size 8 for both inner-step update and meta-step update. We use learning rate of 1e-5 with SGD optimizer for the inner-step update, and use learning rate of 1e-5, weight decay of 0.01 with Adam optimizer\u00a0[12] and a linear learning rate scheduler for the meta-step update. NLI models with memory usage of around 10G use 1 minute for each 100 meta steps trained on GeForce GTX 1080Ti and QA models with memory usage of around 22G use 2 minutes for each 100 meta steps trained on TITAN RTX.\n"]}
{"id": "2110.07560", "categories": "cs.CL", "paragraphs": ["Training Setup and Hyper-parameters.\nFor both SFTs and adapters, we train for the lesser of 100 epochs or 100,000 steps of batch size 8 and maximum sequence length 256, subject to an absolute minimum of 30,000 steps since 100 epochs seemed insufficient for some languages with very small corpora. Model checkpoints are evaluated every 1,000 steps (5,000 for high-resource languages) on a held-out set of 5% of the corpus (1% for high-resource languages), and the one with the smallest loss is selected at the end of training. We use the AdamW optimizer [21] with an initial learning rate of 5\\(e\\) -5 which is linearly reduced to 0 over the course of training.\n", "Following [28], the adapter reduction factor (i.e., the ratio between model hidden size and adapter size) was set to 2 for a total of \\(\\sim \\) 7.6M trainable parameters. For comparability, we set the same number of trainable parameters \\(K\\)  for our language SFTs. This results in language SFTs with a sparsity of 4.3% for mBERT and 2.8% for XLM-R.\n", "Importantly, during language sparse fine-tuning, we decouple the input and output embedding matrices and fix the parameters of the output matrix; otherwise, we find that the vast majority of the \\(K\\)  most changed parameters during full fine-tuning belong to the embedding matrix, seemingly due to its proximity to the model output, and downstream performance is poor. We also fix the layer normalization parameters; all other parameters are trainable.\nFor language adaptation, we apply L1 regularization as described in \u00a7REF  with \\(\\lambda = 0.1\\) .\nNote that the specified training regime is applied in the same way during both phases of LT-SFT.\n", "For NLI, we employ the same fine-tuning hyperparameters as [11]: 5 epochs with batch size 32, with checkpoint evaluation on the validation set every 625 steps, and an initial learning rate of 2\\(e\\) -5. We apply a two-layer multi-class classification head atop the MMT output corresponding to the [CLS] token.\n"]}
{"id": "2108.02768", "categories": "cs.LG cs.AI", "paragraphs": ["We ran experiments with the Lookahead optimizer [31] with its default hyperparameters, and found that it significantly helps with training DeepSet models, while its effect on other architectures is minimal. We didn't use any form of regularization \u2014 at no point in our experiments we observed overfitting behaviour thanks to the infinite supply of synthetic data. We also clipped gradients whose L2 norm surpassed 1 for increase training stability. We trained all of the networks using the PyTorch framework [20], on NVIDIA T4 GPUs. Depending on the task and model, each training run took about 1 to 8 days to complete.\n"]}
{"id": "2105.01904", "categories": "cs.LG cs.AI", "paragraphs": ["Training epochs. In each iteration, the 155 training levels are randomly permuted and searched by the agent until a reward is received, with a cap of 50, 100 search nodes for the backward and forward agents, respectively.\nWe perform 100 training iterations starting with a \\(TD(0)\\)  learning rate \\(\\alpha \\) =0.01, and multiplying it by \\(0.98\\)  at each iteration. Training takes a few minutes on a single core.\n"]}
{"id": "2109.00373", "categories": "cs.CV", "paragraphs": ["fine-tuning stage. The initial learning rate is set as \\(0.00002\\)  and the weight decay is \\(0.05\\) .\nWe set the crop size of the input image as \\(512 \\times 512\\)  and batch size as 16 by default.\nBesides, the networks are fine-tuned for 240 epochs on the train set. For each iteration, we randomly select one frame from the videos to train our framework.\n"]}
{"id": "2103.13858", "categories": "cs.CV eess.IV", "paragraphs": ["Under the architecture of CGI, the training data comes from bucket signal after multiple sampling of target based on a set of fixed random speckles sequence.\nThe size of speckle and target is both 28*28.\nEach pixel of the target is sampled once, and 784 bucket signal values are formed.\nWe construct the bucket signal sequence into a bucket signal array as an input of CGAN.\nConsequently, the size of bucket signal array is 28*28.\nIn the following description, each input of CGAN is formed after a complete round of sampling, that is, 784.\nThe targets in our experiment are handwritten letters and numbers.\nLetter targets include 10 categories of \"A,B,C,D,E,F,G,H,I,J\", and number targets include 10 categories of \"0,1,2,3,4,5,6,7,8,9\".\nWe trained four networks using 5000, 10000, 20000 and 60000 samples, and each category of target contains 500, 1000, 2000 and 6000 in four networks, respectively.\nMeanwhile, 500, 1000, 2000 and 5000 epochs are performed in each network. Fig.\u00a0REF  shows the handwritten targets in our experiment.\n{FIGURE}"]}
{"id": "2111.07556", "categories": "cs.CV cs.AI", "paragraphs": ["However, the current distillation is mainly used in classification tasks. By increasing the temperature, teachers can output the soft knowledge of \"6 is not only like 6, 6 is also like 4\", so as to enhance the generalization ability of students' models. There are only two papers on the application of distillation to regression tasks.\n\\(L_{distill}=\\left\\lbrace \\begin{array}{rcl}||O_{student}-O_{teacher}||^2_2 & & {if \\ \\ ||y-O_{teacher}||_2 > \\mu }\\\\f(O_{student}, m) & & {if \\ \\ ||y-O_{teacher}||_2 < \\mu }\\\\\\end{array} \\right.\\) \n"]}
{"id": "2106.01606", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["Threat models. We further consider the \\(\\ell _2\\) -norm threat model, in which we set \\(\\epsilon =1.0\\)  and \\(\\alpha =0.25\\)  in the 10-step PGD adversary. The learning curves of PGD-AT and TRADES are shown in Fig.\u00a0REF , which also exhibit similar results.\n"]}
{"id": "2106.01583", "categories": "cs.LG", "paragraphs": ["where \\(\\alpha ^{[A]}\\)  and \\(\\alpha ^{[B]}\\)  are per-graph weight of the training procedure. When we have two graphs, \\(\\alpha ^{[A]} + \\alpha ^{[B]} = 1\\) .\n", "When the number of graphs is two, we can perform alternatives to save one linear transformation matrix. Alternative 1 is to use \\(Q^{[A \\rightarrow B]} \\in \\mathcal {R}^{m_A \\times m_B}\\)  to align \\(\\mathcal {G}^{[A]}\\) 's raw attribute space to \\(\\mathcal {G}^{[B]}\\) 's:\n\\(\\scalebox {0.8}{f(Q^{[A \\rightarrow B]}; W^{[B]}_{(1)}, W_{(2)}) = \\alpha ^{[A]} \\cdot \\mathcal {L} \\left( g(X^{[A]}Q^{[A \\rightarrow B]}, A^{[A]}, W^{[B]}_{(1)}, W_{(2)}),~A^{[A]} \\right)} \\nonumber \\\\\\scalebox {0.8}{+~\\alpha ^{[B]} \\cdot \\mathcal {L} \\left( g(X^{[B]}, A^{[B]}, W^{[B]}_{(1)}, W_{(2)}),~A^{[B]} \\right),}\\) \n", "And alternative 2 is to use \\(Q^{[B \\rightarrow A]} \\in \\mathcal {R}^{m_B \\times m_A}\\)  to align \\(\\mathcal {G}^{[B]}\\) 's raw attribute space to \\(\\mathcal {G}^{[A]}\\) 's.\n"]}
{"id": "2106.01598", "categories": "cs.CL", "paragraphs": ["We implemented the Logistic Regression and SVM with random_state equal to 0, C equal to 1, and max_feature equal to 13,000. In addition, we implemeneted the Text-CNN and GRU with 5 epochs, batch_size equal to 512, sequence_length equal to 300, conv_layer_size equal to 5, 128 units, dropout equal to 0.1, and using sigmoid activation function. Finally, we implement the Toxic-BERT with 5 epochs, train_batch_size equal to 16, and test_batch_size equal to 8. We used the Simple transformerhttps://simpletransformers.ai/ for implementing the Toxic-BERT model.\n"]}
{"id": "2106.01560", "categories": "cs.DL cs.CL", "paragraphs": ["\nFor PyTorch, we use seeds 133,133/1337/13370 is the default seed setting in AllenNLP. 11, and 22\n\nFor Numpy, we use seeds 1337, 111, and 222\n\nFor Python's random library, we use seeds 11370, 1111, and 2222\n\n"]}
{"id": "2101.06021", "categories": "cs.CV", "paragraphs": ["All our experiments are implemented in PyTorch and evaluated on a single NVIDIA RTX 1080Ti GPU. To train our network, we randomly crop input images to \\(256\\times 256\\)  pixel size. The batch size is set to 6 during training. The Adam solver is used to train our models for 3000 epochs. The initial learning rate is set to 0.0001, the decay rate set to 0.5 and step size is 500. We normalize image to range [0,1] and then subtract 0.5, so that our input's range is [-0.5,0.5].\n"]}
{"id": "2112.13408", "categories": "cs.LG cs.AI", "paragraphs": ["For low resolution dataset, we use batch size of 128 by default because larger batch size even makes models slower to converge. For high resolution dataset, we use batch size of 32 to pre, because of the memory limit of experiment equipment. We use NAdam optimizer [29] for all our experiments but adjust learning rate and use decay at different experiments.\n"]}
{"id": "2110.03215", "categories": "cs.CL cs.LG", "paragraphs": ["The input and output sequence length is fixed to 350. We use gradient accumulation for cases where the same number of training batches could not be loaded on the GPUs due to the varying memory consumption required for different methods and set the global batch size to 60. We use Adafactor optimizer with an initial learning rate of 1e-3. We show the effects of learning rate variation regarding the trade-off between maintaining previous knowledge and acquiring new knowledge in Appendix . We use learning rate warm-up for the first 10% of training and linearly decay the learning rate to half of the initial learning rate towards the end of training. For all of the experiments, we use 4 32GB V100 GPUs for training with each method except Mix-Review, where we use 16 32GB V100 GPUs. The details of the configurations used for evaluation on each individual CKL task are provided in Appendix REF .\n"]}
{"id": "2102.13326", "categories": "cs.CV cs.AI", "paragraphs": ["Different hyperparameters are set to evaluate their effect on the model. Fig.REF , REF  show the generalized accuracy curves on the two benchmark datasets with varying methods of splitting and different hyperparameters.\nAs shown in Fig.REF  and Fig.REF , the value of hyperparameter setting is represented with the horizontal axis, the generalization accuracy of the seen classes is represented with the vertical axis, and the corresponding accuracy of the seen class is represented with red value in the curve. The hyperparameters with the highest generalization accuracy will be selected as the parameter of the model. Therefore, the model needs different hyperparameters values for different settings. For CUB, the \\(k\\)  value of \\(top-k\\)  is set as 4 and 1 in SCS and SCE cases, respectively. While for NAB, the \\(k\\)  value of \\(top-k\\)  is 3 and 1 in the case of SCS and SCE, respectively. \\(top-k\\)  represents the sharing of k classes of text.\n"]}
{"id": "2109.03787", "categories": "cs.CV cs.RO", "paragraphs": ["For the data augmentation, we followed other papers to do the rotation and flipping along the y axis , . We set the batch size as 2 and adopted the Adam optimizer with a one-cycle learning rate policy. The maximum learning rate was set to 0.002, and the total training epoch was set to 30. For the loss function, we combined the weighted cross-entropy loss  and the Lov\u00e1sz-Softmax loss  together. Thanks to the parameter-free FID (fully interpolation decoding) module, all our experiments were conducted on a single RTX 2080 Ti with the mix-precision choice in PyTorch.\n"]}
{"id": "2102.06191", "categories": "cs.CV cs.LG", "paragraphs": ["We use a DeepLab-v3\u00a0[9] model with dilated\u00a0[84] ResNet-50 backbone\u00a0[24]. The backbone is initialized from MoCo v2\u00a0[12] pre-trained on ImageNet, unless defined otherwise. We train the model for 60 epochs using batches of size 64. The model weights are updated through SGD with momentum \\(0.9\\)  and weight decay \\(1e^{-4}\\) . The initial learning is set to \\(0.004\\)  and decayed with a poly learning rate scheme. We use the same set of augmentations as SimCLR\u00a0[10] to generate positive pairs \\((X, X^+)\\) , while making sure that each image contains at least a part of the salient object \\((\\text{area} > 10\\%)\\) . The features of negatives \\(\\left\\lbrace \\mathbf {z}_{\\mathcal {M}_{X^-_0}}, \\ldots , \\mathbf {z}_{\\mathcal {M}_{X^-_K}} \\right\\rbrace \\)  are saved in a memory bank, with \\(K\\)  set to 128. The negatives are encoded with a momentum-updated version of the network following\u00a0[23]. We use dimension \\(D=32\\)  and temperature \\(\\tau =0.5\\) .\n"]}
{"id": "2102.02888", "categories": "cs.LG cs.DC", "paragraphs": ["For GLUE benchmarks we use original Adam optimizer and perform single-task training on the dev set. We search over the hyperparameter space with batch sizes \\(\\in \\lbrace 8,16\\rbrace \\)  and learning rates \\(\\in \\lbrace 1\\times 10^{-5},3\\times 10^{-5},5\\times 10^{-5},8\\times 10^{-5}\\rbrace \\) . Other setting are the same as pre-training task.\n"]}
{"id": "2102.02723", "categories": "cs.CL", "paragraphs": ["We tuned the model hyperparameters on the development set. For\ntraining the macro planning and the text generation stages, we used\nthe Adagrad [9]\noptimizer. Furthermore, the text generation stage made use of\ntruncated BPTT [59] with truncation\nlength\u00a0100. We learn subword vocabulary\n[51] for paragraph plans in the macro\nplanning stage. We used 2.5K merge operations for RotoWire\nand 8K merge operations for MLB. In text generation, we learn a joint\nsubword vocabulary for the macro plan and game summaries. We used 6K\nmerge operations for RotoWire and 16K merge operations for\nMLB. All models were implemented on OpenNMT-py [23]. We\nadd to set\u00a0\\(\\mathcal {E}\\)  the paragraph plans corresponding to the\noutput summary paragraphs, to ensure full coverage during training of\nthe macro planner. During inference for predicting macro plans, we\nemploy length normalization\n[0] to avoid penalizing longer\noutputs; specifically, we divide the scores of beam search by the\nlength of the output. In addition, we adopt bigram blocking\n[41]. For MLB, we further block beams containing more\nthan two repetitions of a unigram. This helps improve the diversity of\nthe predicted macro plans.\n"]}
{"id": "2103.03457", "categories": "cs.CL cs.AI", "paragraphs": ["Optimization We adopt the default optimization setting in [45]. Adam [21] optimizer with \\(\\beta _1=0.9, \\beta _2=0.98\\)  and \\(\\epsilon =10^{-9}\\) . The learning rate scheduler is inverse_sqrt with warmup steps \\(4,000\\) , default learning rate is \\(0.0005\\) . Label smoothing [44] is used with value \\(0.1\\) . As introduced, to learn the predictors, we clamp the \\(\\operatorname{softmax}\\)  output with value \\(0.05\\) .\n"]}
{"id": "2106.05124", "categories": "cs.CV", "paragraphs": ["We set the scales for the phase congruency estimation as 4, with the scaling factor between successive filters being 2. For different scales of the learnable Gabor kernels, the size of learnable convolutional kernels are set as \\(7\\times 7\\) , \\(13\\times 13\\) , \\(19\\times 19\\) , and \\(25 \\times 25\\) . It is worth noting that although the above kernels seem to be overly large for the common CNN, they actually function well during our experiments under the regularization of the Gabor wavelets. As for the three trainable layers, we set the initial value of \\(\\alpha = 2\\)  as it is originally defined as the scaling factor, and we set \\(\\beta = 1\\) , \\(\\gamma = 1\\) .\n"]}
{"id": "2106.05142", "categories": "cs.LG", "paragraphs": ["We trained all unsupervised methods for 25k steps with a batch size of 2048. We used an Adam optimizer with a linear warm-up between 1e-5 and 1e-3 for 2.5k steps followed by cosine decay schedule as introduced by [5]. We selected the common contrastive parameters from performances on the validation set for \\(\\mathcal {L}^{\\textsc {CL}}\\)  objective. More details can be found in Appendix . We used a temperature of 0.1, a queue of size 65536, and an embedding size of 64 for all tasks. We set the momentum to 0.999 for MIMIC-III Benchmark tasks and 0.99 for Physionet 2019. Concerning parameters specific to our method, for \\(\\textsc {NCL}(n_w)\\)  we chose \\(\\alpha = 0.3\\)  and \\(w = 16 \\)  on MIMIC-III Benchmark and \\(\\alpha = 0.4\\)  and \\(w = 12 \\)  on Physionet 2019. For \\(\\textsc {NCL}(n_Y)\\) , we use \\(\\alpha = 0.9\\)  for all tasks. These parameters were selected using grid searches reported in Appendix\u00a0. For auto-encoding methods, we used a decoder with a mirrored architecture to the common encoder. However, we did not normalize the representations to the unit sphere.\n"]}
{"id": "2109.02248", "categories": "cs.LG", "paragraphs": ["We have used two different types of training in our experiments: resourceful and frugal. For the resourceful training, we trained our models in the conventional train/test approach. To do so, we have made 3-fold and 5-fold cross-validation strategies. In addition to the resourceful training approach based on the \\(k\\) - fold cross-validation, we also evaluated our experiments with a frugal training approach based on few-shot learning. Here, we only trained the model on 2 samples per class for each dataset. To ensure generalizability of the findings of the experiments, we made 100 runs with different randomizations so that the samples selected for the training will not be redundant. We also used 4 thresholds for the top biomarkers extraction which are 5, 10, 15 and 20. All the hyperparameters were selected using grid search. For all models, the learning rates ranged between 0.0001 and 0.001. For DiffPool, the hidden dimension, the output dimension, the assignment ratio and the number of convolution layers were equal to 256, 512, 0.1 and 3, respectively. For GAT, the numbers of hidden units and head attentions were equal to 8. For GCN, the number of hidden units is equal to 64. For g-U-Nets, the number of layers, hidden and convolution layer dimensions were equal to 3, 512 and 48, respectively. For SAGPool, the hidden dimension and the pooling ratio were equal to 256 and 0.5, respectively.\n"]}
{"id": "2109.02247", "categories": "cs.CL cs.AI", "paragraphs": ["Training is performed by optimizing the binary cross-entropy loss function for pairwise edge classification. We use the AdamW optimizer with a learning rate of 1e-6 for the parameters of the transformer models used in extracting node embeddings. For the parameters of the RGCN encoder and edge classifier, we use the Adam optimizer with a learning rate of 1e-4. We train our models for 10 epochs with a batch size of 8 documents. Test results are reported corresponding to the best validation \\(\\tau \\) .\n"]}
{"id": "2109.14879", "categories": "cs.CV cs.LG", "paragraphs": ["All models are trained using a mini-batch size of 2 using \\(180\\times 180\\times 4\\)  image patches that are padded (reflect mode) on each side with 92 voxels along \\(x\\)  and \\(y\\)  and 20 along \\(z\\)  spatial dimension to account for valid convolutions.\nOptimization is done using the Adam optimizer with \\(10^{-5}\\)  learning rate.\nThe model is applied to the validation data every 1000 iterations and the best model according to the Jaccard index is used for the final evaluation.\n", "Stratified patch sampling is employed to speed up the training by ensuring that at least one patch in a mini-batch contains liver pixels.\nWe use a weighted soft dice loss to enable training with partially annotated patches (required for slice sampling strategies)[16]:\n\\(L_{DSC} = 1 - \\frac{2 \\sum _{i}{w_i y_i p_i}}{\\sum _i w_i y_i + \\sum _i w_i p_i}\\) \n"]}
{"id": "2112.02906", "categories": "cs.CV", "paragraphs": ["The images were cropped and resized to \\(480\\times 480\\)  in the training. The network was trained using the ADAM optimizer [43], with the learning rate starting at zero and warming up to \\(3e^{-3}\\)  in 500 steps before remaining at \\(3e^{-3}\\) . We set the batch size to one, but accumulated the gradient over 16 batches. Under these settings, the proposed model converges on NVIDIA Titan RTX in about two days.\n"]}
{"id": "2106.05681", "categories": "cs.CV", "paragraphs": ["\\(\\bullet \\)  SGD method is adopted to optimize the model. The initial learning rate is set to be 0.005 in a single GTX 1080Ti with batchsize 4 and is decreased by 0.1 at the 8th and 11th epoch, respectively. WarmUp [9] is also employed in the first 500 iterations. Totally there are 12 training epochs.\n"]}
{"id": "2106.05656", "categories": "cs.CV", "paragraphs": ["Dataset and Models Our method is validated on the popular ImageNet 1k dataset [8]. This dataset contains 1.28M images in the training set and 5K images in the validation set from 1000 classes. We only use the training set during the process of self-supervised learning. As to models, we choose the classical DeiT-S [24] and popular Swin-T [19] as representatives of all transformer-based architectures. After the backbone, a 3-layer MLP with hidden dimension 2048 is added as the projection head. When evaluating our pretrained model, we both use the k-NN algorithm and train a linear classification for 100 epochs as former works. Top-1 accuracy is reported.\n", "Training Configurations Our model is optimized by AdamW [21] with learning rate \\(2\\times 10^{-3}\\)  and batch size 1024. Weight decay is set to be 0.04. We adopt learning rate warmup [11] in the first 10 epochs, and after warmup the learning rate follows a cosine decay schedule [20]. The model uses multi-crop similar to [0] and data augmentations similar to [12]. The setting of momentum, temperature coefficient, and weight decay follows [1]. The coefficient \\(\\lambda _1\\)  of basic instance discrimination task is set as 1.0 while the restoration task \\(\\lambda _2\\)  is set as 0.6.\n"]}
{"id": "2112.07225", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["For a fair comparison, our experiments are conducted under the most commonly used codebase of long-tailed studies: Open Long-Tailed Recognition (OLTR)\u00a0[19], using PyTorch\u00a0[21] framework. The model structures used for CIFAR, ImageNet-LT, Places-LT and iNaturalist18 datasets are ResNet32, ResNeXt50, ResNet152 and ResNet50, respectively. The model for Places-LT is pre-trained on the full ImageNet-2012 dataset while models for other datasets are trained from scratch. For ImageNet-LT, Places-LT, and iNaturalist18, we train 90, 30, and 200 epochs in the first standard training stage; and 10, 10, and 30 epochs in the second margin calibration stage, with the batch size of 256, 128, and 256, respectively. For CIFAR-10-LT and CIFAR-100-LT, the models are trained for 13,000 iterations with a batch size of 512. We use the SGD optimizer with momentum 0.9 and weight decay \\(5e-4\\)  for all datasets except for iNaturalist18 where the weight decay is \\(1e-4\\) . In the standard training stage, we use a cosine learning rate schedule with an initial value of 0.05 for CIFAR and 0.1 for other datasets, which gradually decays to 0. In the margin calibration stage, we use a cosine learning rate schedule with an initial learning rate starting from 0.05 to 0 for all datasets. \\(\\gamma \\)  is set to \\(1.2\\)  for all datasets. The hyper-parameters of compared methods follow their paper. For fairness, we use the same pre-trained model for decision boundary adjustment methods.\n"]}
{"id": "2106.03714", "categories": "cs.CV", "paragraphs": ["All the experiments are conducted upon PyTorch\u00a0[35] and the timm\u00a0[55] library. The models are trained on ImageNet-1k from scratch without auxiliary dataset.\nFor the ablation experiments, we follow the standard training schedule and train our models on the ImageNet dataset for 300 epochs. When compared to state-of-the-art (SOTA) models, we use the advanced training recipes as proposed in [47]. Detailed training hyper-parameters are listed in the appendix.\n"]}
{"id": "1401.6497", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["The variational Bayesian inference is guaranteed to converge only to a local minimum. To avoid getting stuck in poor local solutions, it is important to choose an initialization point. In our model, the top level hyperparameters including \\(\\mathbf {c}_0,\\mathbf {d}_0\\) , \\(a_0,b_0\\)  are set to \\(10^{-6}\\) , resulting in a noninformative prior. Thus, we have \\(\\mathbb {E}[\\Lambda ]=\\mathbf {I}\\)  and \\(\\mathbb {E}[\\tau ]=1\\) . For the factor matrices, \\(\\lbrace \\mathbb {E}[\\mathbf {A}^{(n)}]\\rbrace _{n=1}^N\\)  can be initialized by two different strategies, one is randomly drawn from \\(\\mathcal {N}(\\mathbf {0},\\mathbf {I})\\)  for \\(\\mathbf {a}^{(n)}_{i_n}\\) , \\( \\forall i_n\\in [1,I_n],\\forall n\\in [1,N]\\) . The other is set to \\(\\mathbf {A}^{(n)}= \\mathbf {U}^{(n)}\\Sigma ^{(n)^{\\frac{1}{2}}}\\) , where \\(\\mathbf {U}^{(n)}\\)  denotes the left singular vectors and \\(\\Sigma ^{(n)}\\)  denotes the diagonal singular values matrix, obtained by SVD of mode-\\(n\\) matricization of tensor \\(\\mathcal {Y}\\) . The covariance matrix \\(\\mathbf {V}^{(n)}\\)  is simply set to \\(\\mathbf {I}\\) . The tensor rank \\(R\\)  is usually initialized by the weak upper bound on its maximum rank, i.e., \\(R\\le \\min _n P_n\\) , where \\(P_n= \\prod _{i\\ne n} I_i\\) . In practice, we can also manually define the initialization value of \\(R\\)  for computational efficiency.\n", "[tb]\nFully Bayesian CP Factorization (FBCP)\n\n\nInput: an \\(N\\) th-order incomplete tensor \\(\\mathcal {Y}_\\Omega \\)  and an indicator tensor \\(\\mathcal {O}\\) .\nInitialization: \\(\\tilde{\\mathbf {A}}^{(n)},\\mathbf {V}_{i_n}^{(n)}, \\forall i_n\\in [1,I_n],\\forall n\\in [1,N]\\) , \\(a_0,b_0,\\mathbf {c}_0,\\mathbf {d}_0\\) , and \\(\\tau =a_0/b_0\\) , \\(\\lambda _r=c_0^r/d_0^r, \\forall r\\in [1,R]\\) .\n\\(n=1\\)  to \\(N\\) \nUpdate the posterior \\(q(\\mathbf {A}^{(n)})\\)  using (REF );\nUpdate the posterior \\(q(\\lambda )\\)  using (REF );\nUpdate the posterior \\(q(\\tau )\\)  using (REF );\nEvaluate the lower bound using (REF );\nReduce rank \\(R\\)  by eliminating zero-components of \\(\\left\\lbrace \\mathbf {A}^{(n)}\\right\\rbrace \\)  (an optional procedure);\nconvergence.\nComputation of predictive distributions using (REF ).\n"]}
{"id": "1410.2535", "categories": "cs.CV stat.ME", "paragraphs": ["The origin of the coordinate system is assumed to be aligned with the left camera position and orientation, so that only the right camera has to be calibrated in order to define the camera pair \\((C_{\\ell },C_{\\mathrm {r}})\\) . Let \\(\\mathbb {S}_{\\mathrm {r}} = \\mathbb {R}^d\\) , \\(d > 0\\) , be the space in which the state of the right camera is described. In general, the components of a given state vector \\(\\mathbf {s}\\)  in \\(\\mathbb {S}_{\\mathrm {r}}\\)  can be\n"]}
{"id": "1412.5567", "categories": "cs.CL cs.LG cs.NE", "paragraphs": ["The core of our system is a recurrent neural network (RNN) trained to ingest\nspeech spectrograms and generate English text transcriptions. Let a single\nutterance \\(x\\)  and label \\(y\\)  be sampled from a training set \\(\\mathcal {X} =\\lbrace (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\ldots \\rbrace \\) . Each utterance, \\(x^{(i)}\\) , is\na time-series of length \\(T^{(i)}\\)  where every time-slice is a vector of audio\nfeatures, \\(x_t^{(i)}, t=1,\\ldots ,T^{(i)}\\) . We use spectrograms as our\nfeatures, so \\(x^{(i)}_{t,p}\\)  denotes the power of the \\(p\\) 'th frequency bin in\nthe audio frame at time \\(t\\) . The goal of our RNN is to convert an input\nsequence \\(x\\)  into a sequence of character probabilities for the transcription\n\\(y\\) , with \\(\\hat{y_t} = \\mathbb {P}(c_t|x)\\) , where \\(c_t \\in \\lbrace \\textrm {a,b,c,}\\ldots ,\\textrm {z},\\textit {space},\\textit {apostrophe},\\textit {blank}\\rbrace \\) .\n", "Our RNN model is composed of 5 layers of hidden units. For an input \\(x\\) , the\nhidden units at layer \\(l\\)  are denoted \\(h^{(l)}\\)  with the convention that\n\\(h^{(0)}\\)  is the input. The first three layers are not recurrent. For the\nfirst layer, at each time \\(t\\) , the output depends on the spectrogram frame\n\\(x_t\\)  along with a context of \\(C\\)  frames on each side.We typically\nuse \\(C\\in \\lbrace 5, 7, 9\\rbrace \\)  for our experiments. The remaining non-recurrent layers\noperate on independent data for each time step.\nThus, for each time \\(t\\) , the first 3 layers are computed by:\n\\(h^{(l)}_t &= g(W^{(l)} h^{(l-1)}_t + b^{(l)})\\) \n", "\nwhere \\(g(z) = \\min \\lbrace \\max \\lbrace 0,z\\rbrace , 20\\rbrace \\)  is the clipped rectified-linear (ReLu)\nactivation function and \\(W^{(l)}, b^{(l)}\\)  are the weight matrix and bias\nparameters for layer \\(l\\) .The ReLu units are clipped in order to keep\nthe activations in the recurrent layer from exploding; in practice the units\nrarely saturate at the upper bound. The fourth layer is a bi-directional\nrecurrent layer\u00a0[37]. This layer includes two sets\nof hidden units: a set with forward recurrence, \\(h^{(f)}\\) , and a set with\nbackward recurrence \\(h^{(b)}\\) :\n\\(h^{(f)}_t &= g(W^{(4)} h^{(3)}_t + W_r^{(f)} h^{(f)}_{t-1} + b^{(4)}) \\\\h^{(b)}_t &= g(W^{(4)} h^{(3)}_t + W_r^{(b)} h^{(b)}_{t+1} + b^{(4)})\\) \n", "\nNote that \\(h^{(f)}\\)  must be computed sequentially from \\(t=1\\)  to \\(t=T^{(i)}\\)  for\nthe \\(i\\) 'th utterance, while the units \\(h^{(b)}\\)  must be computed sequentially\nin reverse from \\(t=T^{(i)}\\)  to \\(t=1\\) .\n"]}
{"id": "1810.00378", "categories": "cs.LG stat.ML", "paragraphs": ["In each experiment we train the GAN for 200,000 epochs over mini-batches of 2,048 samples, with the generator performing one gradient update per mini-batch and the adversary performing three. We set the learning rate of the networks to 0.02. The generator outputs floating-point numbers constrained to the range \\([0, 2^{16}-1]\\) , which are rounded to the nearest 16-bit integer for evaluation. The evaluation dataset consists of 400 mini-batches of 2,048 input vectors each, for a total of 819,200 input samples. The generator outputs 8 floating-point numbers for each input, each yielding 16 bits for the full output sequence. In total, each evaluation output thus consists of 104,857,600 bits, produced from a single random seed. Larger outputs were not produced due to disk quotas on the cluster used to run the models.\n"]}
{"id": "1804.00079", "categories": "cs.CL", "paragraphs": ["A set of \\(k\\)  tasks with a common source language, a shared encoder \\(\\mathbf {E}\\)  across all tasks and a set of \\(k\\)  task specific decoders \\(\\mathbf {D_1} \\ldots \\mathbf {D_k}\\) . Let \\(\\theta \\)  denote each model's parameters, \\(\\alpha \\)  a probability vector (\\(p_1 \\ldots p_k\\) ) denoting the probability of sampling a task such that \\(\\Sigma _{i}^{k} p_i = 1\\) , datasets for each task \\(\\rm I\\!P_1 \\ldots \\rm I\\!P_k\\)  and a loss function \\(L\\) .\n"]}
{"id": "1803.11284", "categories": "cs.CL cs.AI", "paragraphs": ["We obtained product titles from online catalogs containing a variety of products. Our experiments pertaining to this paper concentrate around the attribute `Brand'.\nFor `Brand', we collect \\(61,374\\)  product titles for the experiment. Training, validation, and test data are generated with a 60/20/20 split ratio respectively. Titles are further tokenized by whitespace and labeled according to the annotation scheme described in Section 1. For accurate labels to train and validate our model, we acquire `Brand' attributes for the set of product titles through crowdsourcing tasks.\n"]}
{"id": "1809.02838", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["We train two separate Graph Convolutional Networks (GCN) [11] for \\(\\mu _i\\)  and \\(\\mathbf {L}_i\\) , as the inference network \\(\\mathcal {M}\\) . A GCN computes hidden layers with the adjacency matrix of a graph and takes input from graph node values. Here we treat data points \\({i} \\cup \\alpha (i)\\)  as a small graph with \\({\\Sigma }_{(i, \\alpha (i)), (i, \\alpha (i))}\\)  as the adjacency matrix and \\(\\mathbf {y}_{(i, \\alpha (i))}\\)  as values at graph nodes. In this graph, \\(i\\)  is unique because we are computing parameters for \\(i\\) . To break the symmetry relation between \\(i\\)  and any other data point in \\(\\alpha (i)\\) , we set \\(\\mathbf {A}= [{\\Sigma }_{i, i}, 0; {\\Sigma }_{\\alpha (i), i}, {\\Sigma }_{\\alpha (i), \\alpha (i)}]\\)  as the adjacency matrix for the GCN. The input to the GCN is \\(\\mathbf {y}_{(i, \\alpha (i))}\\) . In the GCN, the hidden layer \\(\\mathbf {H}^{(l+1)}\\)  is computed from a previous layer \\(\\mathbf {H}^{(l)}\\)  as follows,\n\\(\\mathbf {H}^{(l+1)} = \\sigma (\\mathbf {D}^{-\\frac{1}{2}} \\mathbf {A}\\mathbf {D}^{-\\frac{1}{2}}\\mathbf {H}^{(l)}\\mathbf {W}^{(l)}).\\) \n"]}
{"id": "1804.06512", "categories": "cs.CL", "paragraphs": ["The size of the dialogue-level and utterance-level LSTM state is set as 200 and 150 respectively. Word embedding size is 300. Embedding size for system action and slot values is set as 32. Hidden layer size of the policy network is set as 100. We use Adam optimization method\u00a0[12] with initial learning rate of 1e-3. Dropout rate of 0.5 is applied during supervised training to prevent the model from over-fitting.\n", "In imitation learning, we perform mini-batch model update after collecting every 25 dialogues. System actions are sampled from the learned policy to encourage exploration. The system action is defined with the act and slot types from a dialogue act\u00a0[7]. For example, the dialogue act \u201c\\(confirm(date=monday)\\) \u201d is mapped to a system action \u201c\\(confirm\\_date\\) \u201d and a candidate value \u201c\\(monday\\) \u201d for slot type \u201c\\(date\\) \u201d. The slot types and values are from the dialogue state tracking output.\n", "In RL optimization, we update the model with every mini-batch of 25 samples. Dialogue is considered successful based on two conditions: (1) the goal slot values estimated from dialogue state tracking fully match to the user's true goal values, and (2) the system is able to confirm with the user the tracked goal values and offer an entity which is finally accepted by the user. Maximum allowed number of dialogue turn is set as 15. A positive reward of +15.0 is given at the end of a successful dialogue, and a zero reward is given to a failed case. We apply a step penalty of -1.0 for each turn to encourage shorter dialogue for task completion.\n"]}
{"id": "1804.06679", "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "paragraphs": ["MNIST: The data samples are 784-dimensional vectors, each entry assuming a grayscale value of a \\(28\\times 28\\)  image representing a handwritten digit. This dataset is divided into 60000 training samples and 10000 test samples. We further performed a \\(80\\%-20\\%\\)  split off the training samples as a labeled training set \\(\\mathcal {D}_t\\)  and a labeled validation set \\(\\mathcal {D}\\) .\n", "CIFAR-10: The dataset samples are 3072-dimensional vectors, each entry assuming a \\(32\\times 32 \\times 3\\)  colored image of one of the ten items in the dataset. This dataset is divided into 50000 training samples and 10000 test samples. We further performed a \\(80\\%-20\\%\\)  split off the training samples as a labeled training set \\(\\mathcal {D}_t\\)  and a labeled validation set \\(\\mathcal {D}\\)  .\n"]}
{"id": "1804.06202", "categories": "cs.CV", "paragraphs": ["Training settings.\nFor CIFAR, we adopt the same training settings as \u00a0[45].\nWe use SGD with Nesterov momentum to update network,\nstarting from learning rate 0.1 and\nmultiplying with a factor 0.1 at 200 epochs, 300 epochs and 350 epochs.\nWeight decay is set as 0.0001 and momentum as 0.9.\nWe train the network with batch size as 64 for 400 epochs and report the accuracy at the final iteration.\nThe implementation is based on Caffe\u00a0[15].\nFor Tiny ImageNet, we use the similar training settings as CIFAR,\nexcept that we train for totally 200 epochs and multiply the learning rate\nwith a factor 0.1 at 100 epochs, 150 epochs and 175 epochs.\nTo adapt Tiny ImageNet to the networks designed for CIFAR,\nwe set the stride of the first convolution layer as 2,\nwhich is adopted in \u00a0[9] as well.\n"]}
{"id": "1804.05805", "categories": "cs.LG cs.CR cs.CV stat.ML", "paragraphs": ["\nHardware: Notebook PC with I7-7700HQ, 16GB RAM, GTX 1050 GPU\n\nSoftware: Matlab 2018a, Neural Network Toolbox, Image Processing Toolbox, Parallel Computing Toolbox\n\nParameter Optimization Settings: SGDM, Max Epochs = 20, Mini-Batch Size = 128\n\nTraining Dataset: MNIST training dataset with 50,000 images\n\nTraining Accuracy: 99.5%\n\nTesting Dataset: MNIST testing dataset with 10,000 images\n\nTesting Accuracy: 98.73%\n\n", "\nHardware: Notebook PC with I7-7700HQ, 16GB RAM, GTX 1050 GPU\n\nSoftware: Matlab 2018a, Neural Network Toolbox, Image Processing Toolbox, Parallel Computing Toolbox\n\nParameter Optimization Settings: SGDM, Max Epochs = 30, Mini-Batch Size = 128\n\nTraining Dataset: MNIST training dataset with 50,000 images\n\nTraining Accuracy: 100%\n\nTesting Dataset: MNIST testing dataset with 10,000 images\n\nTesting Accuracy: 99.16%\n\n", "\nParameter Optimization Option: Batch Size = 128, Epochs = 50, Loss Function = tf.nn.softmax_cross_entropy_with_logits, Optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\nTraining Accuracy:\n\nMNIST (99.99% on 60,000 images)\n\nCIFAR-10 (99.83% on 50,000 images)\n\n\nTesting Accuracy:\n\nMNIST (99.36% on 10,000 images)\n\nCIFAR-10 (78.30% on 10,000 images)\n\n\n"]}
{"id": "1806.02070", "categories": "cs.CV", "paragraphs": ["We set the network parameters as follows:\nThe weights of each convolution layer of the stacked hourglass network are initialized with the method as described in\u00a0, the biases with\u00a00.\nThe networks do not employ any normalization layers or dropout, but use an L2 weight regularization factor of 0.00001.\nDue to the demanding training of recurrent neural networks, in terms of both memory and computational requirements, we set the mini-batch size to 1.\nWe train the recurrent networks for sequences of 10 consecutive frames.\nFor the non-recurrent neural networks, we use a mini-batch size of 10.\nWe train all networks with ADAM\u00a0 for total 40000 iterations and a learning rate of 0.0001, while the learning rate is reduced to 0.00001 after 20000 iterations.\nTraining of a recurrent networks took \\(\\approx 12\\)  hours, training of the non-recurrent networks took \\(\\approx 8\\)  hours on a single NVIDIA Titan Xp with 12 GB.\n"]}
{"id": "1805.03616", "categories": "cs.CL cs.LG stat.ML", "paragraphs": ["We employ six convolutional layers for both the encoder and decoder.\nAll embeddings, including the initialized embedding and the output produced by the decoder before the final linear layer, have a dimensionality of 256. We also adopt the same dimensionality for the size of linear layer mapping between hidden and embedding states.\nWe use a learning rate of 0.25 and reduce it by a decay rate of 0.1 once the validation ROUGE score stops increasing after each epoch until the learning rate falls below \\(10^{-5}\\) .\nWe first train the basic topic-aware convolutional model with respect to a standard maximum likelihood objective, and then switch to further minimize a mixed training objective [18], incorporating the reinforcement learning objective \\(L_\\text{rl}\\)  and the original maximum likelihood \\(L_\\text{ml}\\) , which is given as\n\\(L_\\text{mixed} = \\lambda L_\\text{rl} + (1 - \\lambda )L_\\text{ml},\\) \n"]}
{"id": "1805.06556", "categories": "cs.CL", "paragraphs": ["We preserve the settings used in [41] where possible. As a result, the size of the hidden dimensions of the LSTM and the feedforward network is 250. The dropout ratio for the LSTM is set to \\(0.4~\\) . Unlike the model it is based on, our model uses word embeddings of length 1124. These result from concatenating a 100 dimension learned word embedding, with a 1024 dimension learned linear combination of the internal states of a bidirectional language model run on the input sentence as described in [32]. We refer to them below as ELMo (Embeddings\nfrom Language Models). For the learned embeddings, words with \\(n\\)  occurrences in the training data are replaced by \\(\\langle \\text{UNK} \\rangle \\)  with probability \\(\\frac{1 + \\frac{n}{10}}{1 + n}\\) . This does not affect the ELMo component of the word embeddings. As a result, even common words are replaced with probability at least \\(\\frac{1}{10}\\) , making the model rely on the ELMo embeddings instead of the learned embeddings. To make the model self-contained, it does not take part-of-speech tags as input. Using a linear layer over the last hidden layer of the classification model, part-of-speech tags are predicted for spans containing single words.\n"]}
{"id": "1806.10866", "categories": "cs.CV", "paragraphs": ["For training the different cnn, we use hyper parameters as suggested by [16]:\nAll networks are trained using the bcel and adam\u00a0[25].\nAs labels, we use phoc vectors with levels 1, 2, 4, 8 and omit bi- or trigrams.\nThe momentum values \\(\\beta _1\\)  for the mean and \\(\\beta _2\\)  for the variance are set to the recommended \\(0.9\\)  and \\(0.999\\)  respectively while the variance flooring parameter \\(\\epsilon \\)  is set to \\(10^{-8}\\) .\nThe initial learning rate is set to \\(10^{-4}\\)  and divided by 10 after \\(70\\,000\\)  training iterations for the gw and Botany.\nTraining on these two data sets is carried out for a total of \\(80\\,000\\)  iterations\nFor the iam the step size is set to \\(100\\,000\\)  and training is run for \\(240\\,000\\)  iterations.\n", "Similar to [16], the images are not preprocessed but the pixel values are scaled such that black pixels have a pixel value of \\(1.0\\)  while white pixels \\(0.0\\) .\nWe also augment the training set using the algorithm presented in [16].\nThis way, training images are generated such that there exists an equal amount of images per class used during training and the total amount of images is \\(500\\,000\\) .\n"]}
{"id": "1804.07878", "categories": "cs.CL", "paragraphs": ["In all our experiments, we use a minibatch size of 64, dropout rate of 0.3,\n4 RNN layers of size 1000, a word vector size of 600, learning rate of 0.8\nacross all LSTM-based multilingual experiments. For single-source single-target translation, we use 2 RNN layers of\nsize 500, a word vector size of 500, and learning rate of 1.0. All learning\nrates are decaying at the rate of 0.7 if the validation score is not improving\nor it is past epoch 9. We use SGD as our learning algorithm.\nWe build our code based on OpenNMT [31].\nFor the ablation study, we train on BLEU scores directly\nuntil the Generalization Loss (GL) exceeds a threshold of \\(\\alpha = 0.1\\) \n[44]. GL at epoch \\(t\\)  is defined as\n\\(GL(t) = 100 ( 1- \\frac{E_{val}^t}{E_{opt}^t} )\\) ,\nmodified by us to suit our objective using BLEU scores [44].\n\\(E_{val}^t\\)  is the validation score at\nepoch \\(t\\)  and \\(E_{opt}^t\\)  is the optimal score up to epoch \\(t\\) .\nWe evaluate our models using both BLEU scores [41] and qualitative evaluation.\n"]}
{"id": "1802.09232", "categories": "cs.CV", "paragraphs": ["In order to merge different datasets, we convert the poses to a common\nlayout, with a fixed number of joints equal to the dataset with more joints.\nFor example, when merging the datasets Human3.6M and MPII, we use all the 17\njoints in the first dataset and include one joint on MPII.\nAll the included joints have an invalid value that is not taken into\naccount in the loss function.\nAdditionally, we use and alternated human pose layout, similar to the layout\nfrom the Penn Action dataset, which experimentally lead to better scores\non action recognition.\n", "We optimize the pose regression part using the RMSprop optimizer with initial\nlearning rate of 0.001, which is reduced by a factor of 0.2 when validation score\nplateaus, and batches of 24 images.\nFor the action recognition task, we train both pose and appearance models\nsimultaneously using a pre-trained pose estimation model with weights\ninitially frozen. In that case, we use a classical SGD optimizer with Nesterov\nmomentum of 0.98 and initial learning rate of 0.0002, reduced by a factor of 0.2 when\nvalidation plateaus, and batches of 2 video clips.\nWhen validation accuracy stagnates, we divide the final learning rate by 10 and\nfine tune the full network for more 5 epochs.\nWhen reporting only pose estimation scores, we use eight prediction blocks\n(\\(\\mathnormal {K}{}=8\\) ), and for action recognition, we use four prediction blocks (\\(\\mathnormal {K}{}=4\\) ).\nFor all experiments, we use cropped RGB images of size \\(256\\times 256\\) .\nWe augment the training data by performing random rotations from \\(-45^{\\circ }\\) \nto \\(+45^{\\circ }\\) , scaling from \\(0.7\\)  to \\(1.3\\) , vertical and horizontal\ntranslations respectively from \\(-40\\)  to \\(+40\\)  pixels, video subsampling by a factor from 1 to 3,\nand random horizontal flipping.\n"]}
{"id": "1802.09130", "categories": "cs.CL", "paragraphs": ["To train and evaluate all of the methods in a fair and consistent way,\nwe used the standard 10 fold Cross-Validation in FLU2013 dataset, and within each topic of PHM2017 dataset. The results reported in the next section are the averages over the test folds. To build the folds, we preserved the original distribution of the labels, and randomly assigned the tweets to each fold. Since the set of the positive tweets is small, we kept the folds fixed across all of the cross validation experiments, to ensure that all of the methods were trained and tested in identical train/validate/test folds and thus the results can be compared directly.\n"]}
{"id": "1802.02733", "categories": "cs.CV", "paragraphs": ["We implement our proposed method based on the Caffe framework, and the proposed alternating optimization algorithm is implemented using CUDA. All experiments are conducted on a GPU Server which has 8 Nvidia Titan Xp GPUs.\n", "During layer-wise optimization, we set maximum iterations of the proposed alternating optimization method to 20 which is enough for training according to Figure REF .\nWe adopt different fine-tuning settings for different network architecture.\n", "AlexNet We fine-tune AlexNet using a SGD solver with momentum=0.9, weight decay=0.0005. The learning rate starts at 0.001, and is divided by 10 after 100k, 150k, and 180k iterations. The network is fine-tuned for 200k iterations with batch-size equals to 256. Before training, images are resized to have 256 pixels at their smaller side. Random cropping and mirroring are adopted in the training stage and center cropping is used in the testing stage.\n", "ResNet-18 We fine-tune the ResNet-18 using a SGD solver with momentum=0.9, weight decay=0.0005. The learning rate starts at 0.0005, and is divided by 10 every 200k iterations. We run the training algorithm for 650k iterations with batch size equal to 128. We use random cropping and mirroring for data augmentation. Like AlexNet, images are resized to have 256 pixels at their smaller side.\n"]}
{"id": "1802.02745", "categories": "cs.CL cs.CV cs.LG", "paragraphs": ["For both the MLP and the CNN, we train the network to minimize negative log-likelihood loss, using stochastic gradient descent (SGD) with the RMSprop update rule and a typical batch size of 32. There are a few exceptions to this batch size: when the training set is very small, we adjust the batch size to ensure there are at least 5 training batches. Thus, for a training set with \\(N\\)  categories and \\(K\\)  examples per category (a total of \\(N*K\\)  training points), we use a batch size of min(32, \\(\\frac{N*K}{5}\\) ). The number of training epochs was chosen such that the network loss reaches an asymptote for each the MLP and CNN. Training loss is monitored and used to save the best model.\n"]}
{"id": "1803.05785", "categories": "cs.CV", "paragraphs": ["Our models are trained using Tensorflow [16] with the \\(L^1\\)  norm loss function, we set the learning rate as \\(10^{-4}\\)  and use Adam\u00a0[17] optimisation method to train the model, all weights to be trained in the model are initialised using Xavier\u00a0[18] initialisation method.\n"]}
{"id": "1811.07344", "categories": "cs.CV", "paragraphs": ["To compare the effects of changes in transfer learning techniques, all training parameters are kept consistent unless otherwise specified. For MORPH-II, all images are scaled down to 200x240. All input is standardized before being fed into the network. The batch size is set to 50, and models are trained for 60 epochs. The original dropout rate of 0.5 is retained, and the ReLU activation function is used in all weight layers. The Adadelta optimizer is used with its default values. Gender models use the binary cross entropy loss function, and age models use mean absolute error (MAE). Results for age estimation are reported as an MAE, which is defined as:\n\\({\\text{M}AE} = \\frac{1}{n}\\sum _{i=1}^{n}|y_i - \\hat{y_i}|.\\) \n", "\\(S_1\\)  is used to train the models. During the parameter training process, models are supplied with a validation set of 500 random samples from \\(S_3\\) . To show the performance of models as data are added, the training set is split into several sets that are trained upon serially. This also helps avoid the issues that arise from using too much computer memory. The model parameters with the lowest loss on the validation set sample is saved and then fully validated on \\(S_2\\cup S_3\\) , a set of 44,624 images.\n"]}
{"id": "1805.08688", "categories": "cs.CV cs.LG", "paragraphs": ["To train the pedestrian candidate generator, both the original images and the horizontally flipped images which contain at least one annotated bounding box are used, which results in around \\(68,000\\)  training images in total. Among all the annotated bounding boxes, there are about \\(109,000\\)  annotated bounding boxes in 'Person_full' class, \\(60,000\\)  annotated bounding boxes in 'Person_occluded' class, and \\(35,000\\)  bounding boxes in 'People' class. All the images are of size \\(480\\times 640\\) . The model is fine-tuned from the Microsoft COCO [13] pre-trained SSD model for \\(40,000\\)  iterations using the standard stochastic gradient descent (SGD) algorithm and the back-propagation algorithm at a learning rate of \\(10^{-5}\\) .\n", "To train the classification system, all the ground-truth annotations and the pedestrian candidates generated from the previous stage with height greater than 40 pixels and confidence score larger than \\(0.01\\)  are selected, and rescaled into a fixed size of \\(250\\times 250\\)  to represent the training samples. For data augmentation, a \\(224\\times 224\\)  patch is randomly cropped out of each training sample and horizontally flipped with probability \\(0.5\\) . To label the training samples, the soft-label method as described by Equations\u00a0(REF ) and\u00a0(REF )\nis implemented. The thresholds \\(th_a\\)  and \\(th_b\\)  are set to \\(0.4\\)  and \\(0.6\\) , respectively. To build the classification networks, one ResNet-50 [14] and one GoogleNet [15] are used as the classification networks. Both of the classifiers are fine-tuned from the ImageNet pre-trained models using the standard SGD algorithm and the back-propagation algorithm at a learning rate of \\(10^{-4}\\) .\n"]}
{"id": "1807.09388", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["We train and evaluate the proposed LAPRAN with three widely used benchmarking datasets. The first two are MNIST and CIFAR10. The third dataset is made following the rule used in prior SR work [21], [23], [32], which uses 91 images from Yang et al. [38] and 200 images from the Berkeley Segmentation Dataset (BSD) [0]. The 291 images are augmented (rotation and flip) and cut into \\(228,688\\)  patches as the training data. Set5 [3] and Set14 [41] are pre-processed using the same method and used for testing.\n", "We implemented a 4-stage LAPRAN for CS image reconstruction. We resize each training image to \\(64 \\times 64\\)  and train the LAPRAN with a batch size of 128 for 100 epochs with early stopping. We use Adam solver with a learning rate of \\(1 \\times 10^{-4}\\) . The training takes roughly two days on a single NVidia Titan X GPU.\n"]}
{"id": "1812.09926", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["In the searching stage, we train a small network stacked by 8 cells (parent graphs) using SNAS with three levels of resource constraint for 150 epochs. This network size is determined to fit into a single GPU. Single-level optimization is employed to optimize \\(\\mathbf {\\theta }\\)  and \\(\\mathbf {\\alpha }\\)  over the same dataset as opposed to bilevel optimization employed by DARTS. The rest of the setup follows DARTS (Appendix G.1). The search takes 32 hoursThe batch size of SNAS is 64 and that of ENAS is 160. on a single GPUAll the experiments were performed using NVIDIA TITAN Xp GPUs.\n", "We follow the training settings as in [13]. The neural operation parameters \\(\\mathbf {\\theta }\\)  are optimized using momentum SGD, with initial learning rate \\(\\eta _{\\mathbf {\\theta }} = 0.025\\)  (annealed down to zero following a cosine schedule), momentum 0.9, and weight decay \\(3 \\times 10^{-4}\\) . The architecture distribution parameters \\(\\mathbf {\\alpha }\\)  are optimized by Adam, with initial learning rate \\(\\eta _{\\mathbf {\\alpha }} = 3 \\times 10^{-4}\\) , momentum \\(\\beta = (0.5, 0.999)\\)  and weight decay \\(10^{-3}\\) . The batch size employed is 64 and the initial number of channels is 16.\n"]}
{"id": "1812.05276", "categories": "cs.CV", "paragraphs": ["During training, we use ADAM [17] optimizer with an initial learning rate of 0.001 for the first 90 epochs and then decay the learning rate by 0.1 in every 10 epochs. We train 120 epochs in total. Each batch consists of 8 point clouds evenly distributed on 4 GPU cards. For each input point cloud, we sample 64 proposals, with a ratio of 1:3 for positives and negatives. Our implementation is based on Tensorflow [1]. During training the car model, a proposal is considered positive if its PointsIoU with a certain ground-truth box is higher than 0.55 and negative if its PointsIoU is less than 0.55 with all ground-truth boxes. The positive and negative PointsIoU thresholds are 0.5 and 0.5 for the pedestrian and cyclist model.\n"]}
{"id": "1807.10675", "categories": "cs.CL cs.LG stat.ML", "paragraphs": ["To remain comparable with the baseline models on CoNLL [0] and GermEval [12], we train the word embeddings with dimension 100Lample et al. [0] use dimension 100 for English, but 64 for German. We increase this dimension to close the gap., window size of 8 and minimum word count threshold of 4, consequently, setting the LSTM dimension to 100 as wellFor word2vec, we performed an extensive search on numerous embeddings with dimension values \\( (50,100,150,200,300) \\)  along with minimum word count threshold and window size values in the range of \\( [4,200] \\)  and \\( [5,10] \\) , respectively. However, no major differences were observed in the final results.. We choose dimension 25 for character-based embeddings and the final CRF-layer, and train the network in 100 epochs with a batch-size of 1 and dropout rate of 0.5. As an optimization method, we use the stochastic gradient descent with a learning rate of 0.005.\nApart from fitting the LSTM dimension to 300 while using the 300-dimensional pretrained German fastText embeddings [24], the model is fixed throughout our experiments to these settings. Any further sophisticated hyperparameter tuning (e.g. Population Based Training) is left for future work.\n"]}
{"id": "1802.02601", "categories": "cs.CV", "paragraphs": ["In all our experiments, we set \\(N = 1\\)  and \\(k = 4\\) , and used SGD with Nesterov momentum\u00a0[0], [38], [42] and cross-entropy loss in training.\nThe initial learning rate was set at 0.1, weight decay to \\(5.0{\\times }10^{-4}\\) , momentum to 0.9 and minibatch size to 64.\nThe learning rate was dropped by a factor of 0.2 at 60, 120 and 160 epochs, and we trained for a total of 200 epochs, following the settings used in\u00a0[51].\n"]}
{"id": "1809.04730", "categories": "cs.CV", "paragraphs": ["As real-time processing is required for autonomous vehicle operation, we adopted the ENet\u00a0[11] architecture for semantic segmentation. The model was trained and tested on a GTX 1080 Ti GPU and also tested on a NVIDIA DRIVE PX2. The network can take arbitrary sized images for both training and testing, and can predict a segmented image with a resolution of \\(640 \\times 360\\) . The learning rate was set to be \\(5e-6\\)  at the beginning and decayed by \\(1e-1\\)  when the validation error stopped improving for 100 epochs.\n", "Models were firstly trained on the Cityscapes dataset\u00a0[1], then fine-tuned using our USYD_Cloudy_Set. The original Cityscapes dataset has more than 30 classes, of which a number are not relevant to our local environment. To optimize the network, we remapped these 30 classes into 12 categories to better represent the categories expected in the USYD datasets.\n{FIGURE}{FIGURE}"]}
{"id": "1805.05151", "categories": "cs.LG stat.ML", "paragraphs": ["We use 100, 150, and 200 filters each having the window size of 2, 3, and 4, respectively, and pooling length of 2, 3, and 4, respectively. We do not tune these hyperparameters in any experimental setting since the goal was to have an end-to-end comparison with the same hyperparameter setting and understand whether our approach can outperform the baselines or not. Furthermore, we do not filter out any vocabulary item in any settings.\n"]}
{"id": "1807.08447", "categories": "cs.LG cs.AI cs.CL stat.ML", "paragraphs": ["\u2013 Entity Embedding Size: 256, Relation Embedding Size=64, Attribute Embedding Size = 16, Type Embedding Size = 16, Attribute Value Embedding Size = 512. We tried multiple batch sizes with very minor difference in performance and finally used size of 2000. For hidden units per layer, we use size = 64. We used \\(C=50\\)  negative samples and \\(Z=20\\)  negative labels. The learning rate was initialized as 0.01 and then decayed over epochs. We ran our experiments for 5 epochs after which the training starts to convert as the dataset is very large. We use loss weights \\(b\\)  as 0.6 and margin as 1. Further, we use \\(K = 50\\)  random walks of length \\(l = 3\\)  for each entity\nWe used a train/test split of 60%/40% for both the triples set and labels set.\nFor baselines, we used the implementations provided by the respective authors and performed grid search for all methods according to their requirements.\n"]}
{"id": "1807.02609", "categories": "cs.LG stat.ML", "paragraphs": ["Datasets.\nWe use CIFAR-10/100 ,\nImageNet  and\nCaltech-UCSD Birds (CUB)  datasets.\nThe CIFAR datasets have \\(50{,}000\\)  training\nimages and \\(10{,}000\\)  test images. Each image has \\(32^2\\)  size and each pixel has RGB color. CIFAR-10/100\nhave 10 and 100 classes, respectively.\nFollowing , we use data-augmentation techniques to the training images: random horizontal flip,\nrandom-crop with 4 pixel zero-padding, normalizing pixel value by channel means and standard deviations.\nImageNet has\n\\(1.2\\)  million training images and \\(50{,}000\\)  validation images of \\(1,000\\)  classes.\nSimilarly, CUB has \\(5{,}994\\)  training images and \\(5{,}794\\)  test images of 200 fine-grained bird species.\nFor hierarchical anytime prediction, we obtain a hierarchical taxonomy of the CUB dataset from WordNet  by following .\nWe obtain 99 non-leaf nodes (i.e., coarse labels) from the taxonomy with maximum depth 8.\nBy taking ancestors (i.e., coarse labels) of the leaf nodes (i.e., original fine-grained labels) up to distance 3,\nwe build \\(D=4\\)  different levels of labels: 200, 183, 149 and 80 labels from fine-grained to coarse-grained.\nNote that CIFAR-100 has its own 20 coarse-grained labels.\nWe use the same augmentation techniques as  for training ImageNet and CUB images.\nNote that the image size after the augmentations is \\(224^2\\) .\n", "Optimization.\nAll models are trained by stochastic gradient descent (SGD) with\nNesterov momentum of momentum \\(0.9\\)  without dampening and MSRA initialization .\nWe use a weight decay of \\(10^{-4}\\)  and an initial learning rate of \\(0.1\\)  for all experiments.\nThe models for CIFAR and CUB are trained for 300 and 150 epochs, respectively, with a batch size of 64.\nThe learning rate is divided by 10 after \\(50\\%\\)  and \\(75\\%\\)  epochs.\nFor ImageNet, we use same hyperparameters as CIFAR except the learning schedule, where the total number of epochs is 90 and\nlearing rate is diveded at 30 and 60 epochs, and the batch size of 96.\nWe randomly select \\(5{,}000\\) , \\(50{,}000\\) , \\(1{,}000\\)  images of the training set for validation in CIFAR, ImageNet,\nand CUB datasets, respectively.\nNote that we use the original validation set as test set in ImageNet. All models are averaged on 5 trials.\n"]}
{"id": "1810.12836", "categories": "cs.CL", "paragraphs": ["Conversational Response Prediction. We model the conversational response prediction task in the same manner as [25]. We minimize the negative log-likelihood of \\(\\widetilde{P}(s_i^R \\mid s_i^I)\\) , where \\(s_i^I\\)  is a single comment and \\(s_i^R\\)  is its associated response comment. For the response side, we model \\(g^R(s_i^R)\\)  as \\(g(s_i^R)\\)  followed by two fully-connected feedforward layers of size 320 and 512 with \\(tanh\\)  activation. For the input representation, however, we simply let \\(g^I(s_i^I) = g(s_i^I)\\) .In early experiments, letting the optimization of the conversational response task more directly influence the parameters of the underlying sentence encoder \\(g\\)  led to better downstream task performance.\n", "Quick Thought. We use a modified version of the Quick Thought task detailed by [13]. We minimize the sum of the negative log-likelihoods of \\(\\widetilde{P}(s_i^R \\mid s_i^I)\\)  and \\(\\widetilde{P}(s_i^P \\mid s_i^I)\\) , where \\(s_i^I\\)  is a sentence taken from an article and \\(s_i^P\\)  and \\(s_i^R\\)  are its predecessor and successor sentences, respectively. For this task, we model all three of \\(g^P(s_i^P)\\) , \\(g^I(s_i^I)\\) , and \\(g^R(s_i^R)\\)  by \\(g\\)  followed by separate, fully-connected feedforward layers of size 320 and 512 and using \\(tanh\\)  activation.\n", "Natural Language Inference (NLI). We also include an English-only natural language inference task\u00a0[1]. For this task, we first encode an input sentence \\(s_i^I\\)  and its corresponding response hypothesis \\(s_i^R\\)  into vectors \\(u_1\\)  and \\(u_2\\)  using \\(g\\) . Following infersent17, the vectors \\(u_1\\) , \\(u_2\\)  are then used to construct a relation feature vector \\((u_1, u_2, |u_1-u_2|, u_1*u_2)\\) , where \\((\\cdot )\\)  represents concatenation and \\(*\\)  represents element-wise multiplication. The relation vector is then fed into a single feedforward layer of size 512 followed by a softmax output layer that is used to perform the 3-way NLI classification.\n", "Translation Ranking. Our translation task setup is identical to the one used by [9] for bi-text retrieval. We minimize the negative log-likelihood of \\(\\widetilde{P}(s_i \\mid t_i)\\) , where (\\(s_i\\) , \\(t_i\\) ) is a source-target translation pair. Since the translation task is intended to align the sentence representations of the source and target languages, we do not use any kind of task-specific feedforward layers and instead use \\(g\\)  as both \\(g^I\\)  and \\(g^R\\) . Following [9], we append 5 incorrect translations that are semantically similar to the correct translation for each training example as \u201chard-negatives\u201d. Similarity is determined via a version of our model trained only on the translation ranking task. We did not see additional gains from using more than 5 hard-negatives.\n"]}
{"id": "1806.03863", "categories": "cs.CV", "paragraphs": ["We used randomly extracted subsequences of 32 frames for pose and 64 frames for action in training; the evaluation was done on the full sequences, that have up to 250 frames \u2013 10 seconds of video. The spatial resolution of the input frames at both training and evaluation time is \\(224\\times 224\\) , obtained by random cropping at training time and central cropping for evaluation. We also randomly flipped the videos horizontally during training.\n", "All our models were trained using SGD with momentum 0.9. For both tasks, the Par-Inception models were trained with initial learning rate of 0.1, and batch size of 4. For keypoint localisation, the learning rate was decreased by a factor of 10 after 35k and 55k iterations of training, whereas for action classification, it was decreased after 50k and 60k iterations. For both tasks, we ran 70k iterations of training.\n", "The Par-DenseNet models were more memory intensive so we used a smaller batch size, 1 for keypoint localization, and 2 for classification. We trained the models with learning rate 1.0 for keypoints and 0.1 for actions, for a total of 150k iterations, lowering the learning rate by a factor of 10 at 100k iterations.\n"]}
{"id": "1808.01426", "categories": "cs.CL", "paragraphs": ["The corresponding parameters of controlled experimental models are described as follows. For all models, we have set the word embeddings and RNN hidden states to be 128-dimensional and 256-dimensional respectively for source encoders, extractive encoders and decoders. Contrary to [10], we learn the word embeddings from scratch during training, because our training dataset is large enough. We apply the optimization technique Adagrad with learning rate 0.15 and an initial accumulator value of 0.1, as well as employ the gradient clipping with a maximum gradient norm of 2.\n", "We train on a single GeForce GTX 1080 GPU with a memory of 8114 MiB, and the batch size is set to be 16, as well as the beam size is 4 for beam search in decoding mode. For the seq2seq dual attentional models without pointer-generator, we trained them for about two days. Models with pointer-generator expedite the training, the time cost is reduced to about one day. When we add coverage, the coverage loss weight \\(\\lambda \\)  is set to 1.0, and the model needs about one hour for training.\n"]}
{"id": "1810.12081", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["where \\(\\frac{\\partial \\omega _{t+1}}{\\partial \\theta }|_t\\)  represents the effect of \\(\\theta \\)  to the value of \\(\\omega _{t+1}\\)  happened only at timestep \\(t\\) , but not related with the effect to the value of \\(\\omega _t\\) . Therefore we equivalently have \\(\\frac{\\partial \\omega _{t}}{\\partial \\theta }=0\\)  in calculating \\(\\frac{\\partial \\omega _{t+1}}{\\partial \\theta }|_t\\) . The last equation in Eqn. (REF ) again leverages the symmetry of Hessian matrix.\n"]}
{"id": "1810.05221", "categories": "cs.LG stat.ML", "paragraphs": ["\nStopping criteria \u2013 all models were trained for 30 epochs.\nWe then chose the architecture configuration that was in place for the epoch with the highest score on validation set.\n\nLearning rate and optimizers \u2013 \\(D1\\)  was optimized using a stochastic gradient descent optimizer with a learning rate of 0.01.\n\\(D2\\)  and \\(G\\)  were optimized using the Adam optimizer with a learning rate of 0.001.\n\nDropout and batch normalization \u2013 \\(G\\)  contains a 10% rate dropout after each hidden layer.\n\\(D1\\)  contains batch normalization after each hidden layer.\n\nWarm up values \u2013 we evaluated \\(D2\\)  with warm up values of zero, one, three, and six epochs (see \"Training and initialization strategies\" in the previous section for more details).\n\nInitialization \u2013 each experiment was run 30 times, using different initialization parameters.\n\n"]}
{"id": "1810.05436", "categories": "cs.CL cs.IR cs.LG", "paragraphs": ["As noted above, the topic modeling approach used in our experiments with HiTR is LDA.\nFollowing\u00a0[0], [6], [30] we set the number of topics to 100.\nWe set the two hyperparameters to \\(\\alpha =1/T\\)  and \\(\\beta =0.01\\) , where \\(T\\)  is the number of topics, following\u00a0[15].\nIn the re-estimation process, at each step of the EM algorithm, we set the threshold for removing unnecessary components from the model to \\(0.01\\)  and remove terms with an estimated probability less than this threshold from the language models, as in [9].\n"]}
{"id": "1811.07157", "categories": "cs.CV cs.LG", "paragraphs": ["As explained above, we have GPU memory constrained,\nwe will report results of previous basic 3D models\u00a0[50], [6] re-implemented and trained by using the same amount of resources as our RCN.\nThe main hyperparameters involved in the training of a 3D network are learning rate, batch size, and the number of iterations.\nThese parameters are interdependent, and their optimal setting depends on the computational power at disposal.\nFor instance, Tran\u00a0\u00a0[50] would use 64 GPUs, with the training process distributed across multiple machines.\nIn such a case, when vast computational resources are available[50], [6], [5],\ntraining takes 10-15 hours\u00a0[50],\nallowing for time to identify the optimal parameters.\nThe availability of such computational power, however, is scarce.\n"]}
{"id": "1811.07017", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["All the models are implemented using PyTorch 0.4.1 [22]. Adam optimizer [12] is used with a learning rate of 0.001. We used one layer LSTM models with hidden dimensions of size 128 and 256. Net2Net is used to expand LSTM models of size 128 to 256. For the GEM model, we keep one minibatch (10 examples) of data per task for obtaining the projected gradients. We follow the guidelines and hyperparameter configurations as specified in the respective papers for both GEM and Net2Net models.\n{FIGURE}"]}
{"id": "1811.07056", "categories": "cs.CV cs.LG", "paragraphs": ["Hence, we created pre-training datasets by sampling examples from the source dataset using the importance weights. We start by choosing a desired pre-training dataset size, often large. We then sample examples from the source dataset at a rate proportional to the importance weights, repeating examples as needed. We report results that construct a pre-training dataset of 80 million examples for JFT, and 2 million examples for ImageNet. We used the same sampled pre-training dataset with both the Inception v3 and AmoebaNet-B experiments.\n"]}
{"id": "1812.06158", "categories": "cs.CL cs.LG stat.ML", "paragraphs": ["In all our experiments we set \\(N\\)  (number of instances of the target class in in-domain training data) to 20. This number of examples is small enough and can be easily labelled by hand. At the same time, it produces models of reasonable quality. Figure REF  compares the performance of models trained on 10 and 20 examples. We see the significant boost in performance for the latter case. Moreover, in the rightmost plot the learning curve for the smaller dataset goes down after the 40-th epoch, which does not happen when the larger dataset is used. This shows that \\(N=20\\)  is a reasonable trade-off between model performance and cost of labelling.\n", "In the Protonet model we set \\(p\\)  to 0.5. Therefore, the model is trained on the instances of the target class \\(C\\)  half of the steps, and another half of the times it is shown instances of some other randomly chosen class.\n", "We optimize all models with Adam optimizer in pytorch implementation. Base and WarmBase methods use batches of 10 sentences during in-domain training. We train out-of-domain RNN baseline (warm-up for WarmBase and WarmProto* models) using batch of size 32. All models based on prototypical network use batches of size 100 \u2014 40 in support set and 60 in query set.\nWe also use L2-regularization with a multiplier 0.1. All models are evaluated in terms of chunk-based \\(F_1\\) -score for the target class [14].\n"]}
{"id": "1808.03986", "categories": "cs.CL cs.AI cs.CV", "paragraphs": ["We have used RMSPROP optimizer to update the model parameter and configured hyper-parameter values to be as follows: \\(\\text{learning rate}=0.0004, \\text{batch size} = 200, \\alpha = 0.99, \\epsilon =1e-8\\)  to train the classification network . In order to train a triplet model, we have used RMSPROP to optimize the triplet model model parameter and configure hyper-parameter values to be: \\(\\text{learning rate}=0.001, \\text{batch size} = 200, \\alpha = 0.9, \\epsilon =1e-8\\) .\nWe also used learning rate decay to decrease the learning rate on every epoch by a factor given by:\n\\(Decay\\_factor=exp\\left(\\frac{log(0.1)}{a*b} \\right)\\) \n"]}
{"id": "1812.08781", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["Our model depends on two parameters: the number of eigen components \\(\\eta \\)  used for spectral clustering, and the temperature \\(\\sigma \\)  used for controlling the confidence. We used \\(\\eta =200\\)  and \\(\\sigma =40\\)  in our main submission. In Figure\u00a0REF , we show the effects of the two parameters respectively.\n"]}
{"id": "1802.10529", "categories": "cs.LG stat.CO stat.ML", "paragraphs": ["Unfortunately, this approach is of little use in the streaming data scenario we have been focussing on; it would entail storing the groupings in a way which would complicate the fitting algorithm. Luckily, however, having repeated observations is not the only sufficient condition for a mixture model to be identifiable (and note that it is not a necessary condition). A mixture of logistic regression models can also be identified, informally, when the \u00d2pattern\u00d3 within each individual logistic regression is clear enough. If enough unique values are available for the independent variables, then this can be used as a sufficient criteria for identifiability.  showed that for a single independent variable having \\(q\\)  unique values, a mixture with \\(k \\le \\sqrt{q+2}-1\\)  can be identified, at least if each of the logistic regressions is itself also identified (in the same way as a standard non-mixture logistic regression cannot be fit without additional constraints if the number of parameters is larger than the number of observations). Hence, when treating the observations as Bernoulli, sufficient criteria for identifying mixtures logistic regression models can also be obtained in the data stream. However, we should note that the number of components that can be identified as more unique values of an independent variable are observed grows slowly: for 100 unique values,  Theorem only guarantees sufficiency for identifying 9 mixture components. Since we are considering large, continuous, data streams here, we consider the situation of more than 100 unique values of an independent variable quite likely, but we must still caution for over-enthusiasm when choosing a high number of mixture components. A formal discussion of identifiability for mixture models in general can be found in  or , while  discuss specifically the mixture of logistic models.\n", "Formally choosing the \u201cbest\u201d number of components \\(K\\)  is more challenging: although we can easily fit multiple models with different choices of \\(K\\)  in parallel, formal tests to determine the value of \\(K\\)  in the face of streaming data are still actively being developed . The traditional, offline, methods for selecting the number of components mostly rely on comparing either the likelihood or the \\(AIC\\)  or \\(BIC\\)  values of the competing models , . The likelihood \\(L(\\hat{\\theta } | \\mathcal {D})\\)  (or log-likelihood \\(l(\\hat{\\theta } | \\mathcal {D})\\)  of the mixture of logistic regression is easily computed for a static dataset (see also Eq. REF ) and a set of parameter estimates. Subsequently computing the \\(AIC\\)  or \\(BIC\\)  to enable model comparisons is straightforward:\n\\(AIC &=& 2k - 2 \\ln (\\hat{L}) \\\\BIC &=& -2 \\ln (\\hat{L}) k \\ln (n)\\) \n"]}
{"id": "1807.05698", "categories": "cs.CV", "paragraphs": ["In the training process, we randomly generate 100 patch pairs with a size of \\(64\\times 64 \\)  from every training image pairs.\nThe entire network is trained on an Nvidia 1080Ti GPU based on Pytorch.\nWe use a batch size of 64 and set the depth of SCAN as \\(d=7\\)  with the receptive field size \\(35\\times 35\\) .\nFor the nonlinear operation, we use leaky ReLU\u00a0[33] with \\(\\alpha =0.2\\) .\nFor optimization, the ADAM algorithm\u00a0[34] is adopted with a start learning rate \\(5\\times 10^{-3}\\) .\nDuring training, the learning rate is divided by 10 at \\(15,000\\)  and \\(17,500\\)  iterations.\n"]}
{"id": "1809.01498", "categories": "cs.CL cs.AI cs.LG stat.ML", "paragraphs": ["Word embeddings are trained on a 2013 dump of Wikipedia that has been filtered to contain only pages with at least 20 page views.Available at https://storage.googleapis.com/lateral-datadumps/wikipedia_utf8_filtered_20pageviews.csv.gz The raw text has been preprocessed as outlined in appendix\u00a0REF . This results in a corpus of 463k documents with 498 Million words. For learning word embeddings in Euclidean space we use the skip-gram implementation of fastTexthttps://github.com/facebookresearch/fastText, whereas the hyperbolic model has been implemented in C++ based on the fastText code. For the hyperbolic model, the two layers of parameters were identified as this resulted in better performance in informal experiments. The detailed hyperparameters for both models are described in appendix\u00a0REF .\n"]}
{"id": "1805.02718", "categories": "cs.CV", "paragraphs": ["We used Adam to minimize the L2 loss w.r.t. a signed Euclidean distance transform (SEDT) of the binary labels. As the SEDT is not meaningful far away from synapses, we scaled it and applied a \\(\\tanh \\)  nonlinearity that saturates between [-1,1]: \\(STDT = \\tanh (SEDT/s)\\) . Our experiments indicated that the scaling factor has little effect on performance (data not shown). We chose \\(s=50\\)  as the default parameter. Simple thresholding converts the predicted \\(STDT\\)  into binary labels.\n{FIGURE}"]}
{"id": "1807.06583", "categories": "cs.CV cs.RO", "paragraphs": ["Across all experiments, for all three models, training is performed for a fixed number of 200 epochs using a batch size of 32. The Adam optimizer [32] is used through the learning process with the following values for its parameters\u2014(\\(learning rate=0.001, \\beta 1=0.9, \\beta 2=0.999, eps=1e-08, weight decay rate=0, amsgrad=False\\) )\n"]}
{"id": "1805.01089", "categories": "cs.CL cs.LG", "paragraphs": ["We limit the vocabulary to 50,000 most frequent words appearing in the training set. We set the word embedding and the hidden size to 256, 512, and 512 for Toys, Sports, and Movies datasets, respectively. The word embedding is random initialized and learned from scratch. The encoder is a single-layer bidirectional LSTM, the decoder is a single-layer unidirectional LSTM, and the classifier is a two layer feed-forward network with a 512 hidden dimension. The batch size is 64, and we use dropout with probability \\(p = 0.2,\\ 0.05,\\ 0.0\\)  for Toys, Sports, and Movies datasets, respectively.\n"]}
{"id": "1811.05688", "categories": "cs.LG cs.SD stat.ML", "paragraphs": ["Our implementation of all proposed architecture is based on Pytorch library [22]. Mini-batch stochastic gradient descent was used as optimization algorithm for all our architectures. Detailed settings like learning rates and loss functions can be found in the corresponding sub-sections. One special note, however, for the LSTM and CRF variants: due to the exploding and vanishing gradient problem [2], these models cannot learn if we simply input the whole songs. Hence, during training, we chop songs further into some sequences of notes that contain complete phrases only, i.e., no phrase will be broken into two piece and shared by two adjacent sequences. For LSTM variants, each sequence of notes contains 5 phrases precisely. For CRF variants, each sequence of notes contains at least 80 notes and at least two, but an unknown number of complete phrases. All sequences have less than 120 notes, the upper-bound to which we pad our sequence length. We input the whole song only at validation time.\n{FIGURE}"]}
{"id": "1811.05320", "categories": "cs.LG stat.ML", "paragraphs": ["The hyperparameters of the T-GCN model mainly include: learning rate, batch size, training epoch, and the number of hidden layers. In the experiment, we manually adjust and set the learning rate to 0.001, the batch size to 64, and the training epoch to 3000.\n"]}
{"id": "1803.02188", "categories": "cs.CV", "paragraphs": ["Training Databases for Faces. We train our system using the 3DDFA data of [95]. The 3DDFA data provides projection and 3DMM model parameters for the Basel [60] + FaceWarehouse [12] model for each image of the 300W database. We use the topology defined by this model to define our UV space and rasterize the images to obtain per-pixel ground truth UV coordinates. Our training set consists of the LFPW trainset, Helen trainset and AFW, thus 3148 images\nthat are captured under completely unconstrained conditions\nand exhibit large variations in pose, expression, illumination,\nage, etc.\nMany of these images contain multiple faces, some of which are not annotated. We deal with this issue by employing the out-of-the-box DPM face detector of Mathias et al.\u00a0[56] to obtain the regions that contain a face for all of the images. The detected regions that do not overlap with the ground truth landmarks do not contribute to the loss. For training and testing, we have rescaled the images such that their largest side is 800 pixels.\n", "We have used two different network architectures for our experiments. In particular, in order to be directly comparable to the DeepLab-v2 network in semantic segmentation experiments we first used a ResNet101\u00a0[33] architecture with dilated convolutions ( atrous )\u00a0[15], [53], such that the stride of the CNN is 8 and (b) an Hourglass-type network [57]. We use bilinear interpolation to upscale both the \\(\\hat{q}\\)  and \\(\\hat{r}\\)  branches before the losses. The losses are applied at the input image scale and back-propagated through interpolation. We apply a weight to the smooth \\(L1\\)  loss layers to balance their contribution. In our experiments, we have used a weight of 40 for quantized\u00a0(\\(d=0.1\\) ) and a weight of 70 for non-quantized regression, which are determined by a coarse cross validation.\n", "For the ResNet based network, we use an initialization with a network pre-trained for the MS COCO segmentation task\u00a0[49]. The new layers are initialized with random weights drawn from Gaussian distributions. Large weights of the regression losses can be problematic at initialization even with moderate learning rates. To cope with this, we use initial training with a lower learning rate for a warm start for a few iterations. We then use a base learning rate of \\(0.001\\)  with a polynomial decay policy for \\(20k\\)  iterations with a batch size of 10 images.\n", "For the hourglass architecture, we adopt [57] with inception-v2 module (a Figure describing the network can be found in the Appendix). Each deconvolution layer involved is using a dilated convolution following by a \\(3\\times 3\\)  convolution layer with stride 1 and same output channels as input channels.\n", "For the DenseReg cascade architecture (i.e., end-to-end trainable dense shape regression and articulated pose estimation by means of landmark localisation) we used a stack of two hourglasses. The first hourglass network is the one described above. The second hourglass network is regressing to a heatmap representation of facial landmarks/body joints (68-channel heatmap for the landmark localisation experiments and 16-channel heatmap of the body pose estimation experiments). We apply \\(L2\\)  loss to the heatmap regression. Weights are applied to balance losses of both first and second hourglasses to have equal contribution. During training, we are randomly scaling with ratio between 0.75 and 1.25, randomly rotating with angle -30 to 30 degree, and randomly cropping images of size \\(321\\times 321\\)  to \\(256\\times 256\\) .\n"]}
{"id": "1809.09767", "categories": "cs.CV", "paragraphs": ["The following apply to all three types of our models. Images, unless mentioned that left and right viewpoint images were used from the RobotCar dataset, were trained on rear views only. And unless stated otherwise, images in our trials were scaled to \\(286\\times 286\\)  size and randomly cropped to \\(256\\times 256\\)  for training. If a \\(512\\times 512\\)  resolution is used, training crops are of size \\(384\\times 384\\)  due to memory constraints. Memory also restricts training on resolutions higher than \\(512\\times 512\\) . Inference is always on the pre-crop size because our fully-convolutional architecture allows for arbitrary input sizes. Batches are not used, and random image flipping (left-right) is enabled. Training is run for 40 epochs. Learning rates begin at \\(2\\text{e-}4\\)  for generators and \\(1\\text{e-}4\\)  for discriminators, are constant for the first half of training and decreasing linearly to zero during the second half. The \\(\\lambda \\)  from equation\u00a0(REF ) is set to 10.0, as in [27].\n", "DenseVLAD uses the \\(k=128\\)  pretrained cluster centers provided by\u00a0[24]. The VLAD vectors are projected down to 4096 dimensions via PCA prior to comparisons. We also keep the default SIFT extraction scales used in DenseVLAD, at \\(n \\in \\lbrace 4, 6, 8, 10\\rbrace \\) .\n{FIGURE}"]}
{"id": "1802.04200", "categories": "cs.CL", "paragraphs": ["We train our models with Adam [15], with a learning rate of \\(0.001\\) , and a mini-batch size of 64 for BTEC, and 32 for LibriSpeech (because of memory constraints).\nWe use variational dropout [16], i.e., the same dropout mask is applied to all elements in a batch at all time steps, with a rate of \\(0.2\\)  for LibriSpeech and \\(0.4\\)  for BTEC. In the MT tasks, we also drop source and target symbols at random, with probability \\(0.2\\) . Dropout is not applied on recurrent connections [17].\n", "We train all our models on LibriSpeech train augmented with the Google Translate references, i.e., the source side of the corpus (speech) is duplicated, and the target side (translations) is a concatenation of the aligned references with the Google Translate references.\nBecause of GPU memory limits, we set the maximum length to 1400 frames for LibriSpeech input, and 300 characters for its output. This covers about \\(90\\%\\)  of the training corpus. Longer sequences are kept but truncated to the maximum size. We evaluate our models on the dev set every 1000 mini-batch updates using BLEU for AST and MT, and WER for ASR, and keep the best performing checkpoint for final evaluation on the test set.\n"]}
{"id": "1804.00874", "categories": "cs.CV cs.LG", "paragraphs": ["The network is trained on the SceneNet RGB-D dataset\u00a0[18] which is composed of photorealistic renderings of randomised indoor scenes.\nIt provides colour and depth images as well as semantic labeling and poses, out of which we only make use of the two former ones.\nWe make use of the ADAM optimiser\u00a0[14] with an initial learning rate of \\(10^{-4}\\) .\nWe train the network for 6 epochs while reducing the learning-rate to \\(10^{-6}\\) .\n"]}
{"id": "1804.08262", "categories": "cs.CL", "paragraphs": ["As described in sec:neural-transducers, we used encoder-decoder architectures with global attention. Specifically, both the encoder and decoder consisted of 2-layer LSTMs. The encoder was bidirectional and output from the forward and backward LSTMs was concatenated. Both encoder and decoder had 100 hidden units and all character embeddings were 300 hidden units. Networks were trained using Adadelta with a base learning rate of 1.0. Minibatches of size 20 were used. Dropout between layers was set at 0.3.\n"]}
{"id": "1804.00495", "categories": "cs.CV cs.LG", "paragraphs": ["The model parameters hat must be learned from the data are \\(\\theta =\\lbrace w,C_{\\phi },\\beta ,\\alpha ,\\eta \\rbrace \\) . All of these parameters have concrete interpretations, and, therefore, we must specify some constraints on the model values to ensure the result is reasonable. We specify the following constraints and explain their interpretations below:\n\\(\\begin{aligned}&(1) w_1=-2.5,\\quad (2) w_2\\le -0.5, \\quad (3) w_7=w_8\\ge 0,\\\\&(4) w_{11}=w_{12} \\ge 0, \\quad (5)w_{13}\\le 0, \\quad (6) w_{14}\\le 0,\\\\&(7) w_{17}\\le 0, \\quad (8) w_{18}\\le 0,\\quad (9) 2w_2+w_7+w_{11}\\le w_{14}+w_{18},\\\\&(10) C_{\\phi }\\ge 0, \\quad (11) \\beta \\ge 0,\\quad (12) \\alpha \\ge 0, \\quad (13)\\eta \\ge 0.\\end{aligned} \\) \n", "We select the discount factor \\(\\gamma =0.99\\)  for the MDP. This parameter determines the effective time horizon. A samll \\(\\gamma \\)  would result in a greedy policy, which is undesirable. We solve this MDP using Gaussian Process Dynamic Programming [13].\n"]}
{"id": "1802.05203", "categories": "cs.CV", "paragraphs": ["We selected the number of epochs for stopping training by contrasting training loss and validation loss over epochs. We split the public training dataset into a training set and a validation set by randomly picking 80% and the remaining 20% cases from each scanner respectively. Thus in total, the models were trained on 48 cases and validated on 12 cases. Figure REF  shows the curves of training and validation loss over 100 epochs.\nIt could be observed that the validation loss did not show a descending trend at around 50 epochs. The reason to choose 50 epochs rather than a higher one is 1) to avoid over fitting on the training data, and 2) keep low computational cost.\n{FIGURE}", "The size of batch and learning rate have a large influence on the stability of the training process.\nTo our empirical observation, if the learning rate was set to values bigger than 10\\(^{-3}\\) , the training loss would be suddenly reaching to nearly 0 (i.e., the worst performance) at some beginning epoch and would remain not updating the training loss. Both of the batch size and learning rate directly influence the magnitude of the gradient and sometimes will lead to a gradient exposure issue.\nTherefore the batch size was set to 30 and learning rate was set to 0.0002 throughout all of the experiments.\n"]}
{"id": "1802.06488", "categories": "cs.CV cs.AI cs.NE", "paragraphs": ["The proposed Tiny SSD network was trained for 220,000 iterations in the Caffe framework with training batch size of 24. RMSProp was utilized as the training policy with base learning rate set to 0.00001 and \\(\\gamma = 0.5\\) .\n{FIGURE}"]}
{"id": "1810.10254", "categories": "cs.CL", "paragraphs": ["In this section, we present the experimental settings for pointer-generator network and language model. Our experiment, our pointer-generator model has 500-dimensional hidden states and word embeddings. We use 50k words as our vocabulary for source and target. We evaluate our pointer-generator performance using BLEUBLEU is computed using multi_bleu.perl from MOSES package score. We take the best model as our generator and during the decoding stage, we generate 1-best and 3-best using beam search with a beam size of 5. For the input, we build a parallel monolingual corpus by translating the mixed language sequence using Google NMTGoogle NMT Translate API\u00a0to English (\\(w^{\\ell _1}\\) ) and Mandarin (\\(w^{\\ell _2}\\) ) sequences. Then, we concatenate the translated English and Mandarin sequences and assign code-switching sequences as the labels (\\(y^{cs}\\) ).\n"]}
{"id": "1809.00903", "categories": "cs.CV", "paragraphs": ["In our experiments, we use the FCN8s\u00a0[21] as the semantic segmentation model. The backbone is VGG16\u00a0[32] which is pretrained on the ImageNet dataset\u00a0[7]. We apply the PatchGAN [17] as the discriminator, in which the discriminator tries to classify whether overlapping image patches are real or fake. Similar to EBGAN [40], we add the Gaussian noise to the generator. During training, Adam [15] optimization is applied with \\(\\beta _1\\) =0.9 and \\(\\beta _2\\) =0.999. For the Conservative Loss, we apply \\(a = \\mathrm {e}\\)  and the balanced weight \\(\\lambda = 5\\) . The ablation study will give more detailed explanations. Due to the GPU memory limitation, the images used in our experiments are resized and cropped to 1024\\(\\times \\) 512 and the batch size is 1. More experimental settings will be available in the supplementary material.\n"]}
{"id": "1811.09885", "categories": "cs.CV math.DS", "paragraphs": ["Definition 4.4 Let \\(n\\in \\mathbb {N}\\)  and \\(u,v\\in \\mathbb {R}^{n}\\)  be two probability distributions, i.e. \\(0\\le u_i,v_i\\le 1\\)  for all \\(i=1,2,\\dots ,n\\)  and \\(\\sum _{i=1}^n u_i=\\sum _{i=1}^n v_i=1\\) . The cross entropy between \\(u\\)  and \\(v\\) is defined as:\n\\(H(u,v) := -u^T\\log (v).\\) \n\n", "Let \\(\\mathcal {D}\\)  be a dataset with \\(C\\)  classes of images. Given an input image \\(x^0\\in \\mathcal {D}\\)  to the network defined in Figure REF , let \\(y\\in \\mathbb {R}^C\\)  be the one-hot encoding label vector associated with \\(x^0\\)  (i.e. \\(y_i=1\\)  if \\(x^0\\)  is of Class \\(i\\) , and \\(y_i=0\\)  otherwise), and let \\(x^N\\in \\mathbb {R}^C\\)  be the output of the network. The label vector \\(y\\)  can be considered as the true distribution of \\(x^0\\)  over the \\(C\\)  possible classes. To obtain a predicted distribution of \\(x^0\\)  from the network and compare it with \\(y\\) , we apply the softmax normalization function to the output \\(x^N\\)  of the network, so that the loss to be minimized for each input \\(x^0\\in \\mathcal {D}\\)  is \\(H(y, S(x^N))\\) .\n", "Definition 4.5 \nLet \\(n\\in \\mathbb {N}\\)  and \\(u\\in \\mathbb {R}^{n}\\) . The softmax normalization of \\(u\\) is a vector in \\(\\mathbb {R}^n\\)  such that:\n\\(S(u)_i := \\dfrac{\\exp (u_i)}{\\sum _{j=1}^n\\exp (u_j)}, \\quad i=1,2,\\dots , n.\\) \n\n"]}
{"id": "1812.00500", "categories": "cs.CV", "paragraphs": ["As explained in Sec.\u00a0REF , we first train our network on each individual task to find the layers fit for each task along with other training parameters. The results are: \\(l_\\mathrm {R}=3\\)  (image caption retrieval), \\(l_\\mathrm {Q}=5\\)  (VQA), and \\(l_\\mathrm {G}=2\\)  (visual grounding). The training parameters were determined accordingly; see the supplementary material for details. We freeze all these parameters throughout all the experiments.\n", "We note here the training method used in all the experiments. We used the Adam optimizer with the parameters \\(\\alpha = 0.001\\) , \\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.99\\) , and \\(\\alpha \\text{ decay} = 0.5\\) . We employed a simple training schedule; we halve the learning rate by \u201c\\(\\alpha \\text{ decay}\\) \u201d after each \u201c# step\u201d or step size, which are determined above. All the weights in our network were initialized by the method of Glorot et al. [11]. Dropout is applied with probability of 0.3 and 0.1 over FC layers and LSTM, respectively. The dimension \\(d\\)  of the feature space is set to 1024.\n"]}
{"id": "1805.10254", "categories": "cs.CL", "paragraphs": ["For all models, we use a two-layer biLSTM as encoder and a two-layer unidirectional LSTM as decoder, with 200-dimensional hidden states in each layer. We apply dropout\u00a0[11] on RNN cells with a keep probability of 0.8.\nWe use Adam\u00a0[17] with an initial learning rate of 0.001 to optimize the cross-entropy loss. Gradient clipping is also applied with the maximum norm of 2.\nThe input and output vocabulary sizes are both 50k.\n", "Adding Pre-training.\nWe pre-train a two-layer seq2seq model with OP as input and target argument as output from our training set. After 20 epochs (before converging), parameters for the first layer are used to initialize the first layer of all comparison models and our models (except for the keyphrase decoder). Experimental results show that pre-training boosts all methods by roughly 2 METEOR\u00a0[9] points. We describe more detailed results in the supplementary material.\n"]}
{"id": "1801.02209", "categories": "cs.LG cs.AI", "paragraphs": ["We normalize each channel of the input frame to \\([0, 1]\\)  before feeding it into the neural network. Each of the training procedures includes a weight decay of \\(10^{-5}\\)  and a discounted factor \\(\\gamma =0.95\\) .\n", "DDPG: \nWe stack \\(k=5\\)  recent frames and use learning rate \\(10^4\\)  with batch size 128. We choose \\(\\alpha _\\textrm {DDPG}=100\\)  for all the settings except for the case with input signal of \u201cRGB+Depth\u201d on \\(\\mathcal {E}_{\\textrm {large}}\\) , where we choose \\(\\alpha _\\textrm {DDPG}=10\\) . We use an entropy bonus term with coefficient 0.001 on \\(\\mathcal {E}_{\\textrm {small}}\\)  and 0.01 on \\(\\mathcal {E}_{\\textrm {large}}\\) . We use exponential average to update the target network with rate 0.001. A training update is performed every 10 time steps. The replay buffer size is \\(7\\times 10^5\\) . We run training for 80000 episodes in all. We use a linear exploration strategy in the first 30000 episodes.\n", "A3C:  We clip the reward to the range \\([-1,1]\\)  and use a learning rate \\(1e-3\\)  with batch size 64. We launch 120 processes on \\(\\mathcal {E}_{\\textrm {small}}\\)  and 200 on \\(\\mathcal {E}_{\\textrm {large}}\\) . During training we estimate the discounted accumulative rewards and back-propagate through time for every 30 time steps unrolled. We perform a gradient clipping of 1.0 and decay the learning rate by a factor of 1.5 when the difference of KL-divergence becomes larger than 0.01. For training on \\(\\mathcal {E}_{\\textrm {small}}\\) , we use a entropy bonus term with coefficient 0.1; while on \\(\\mathcal {E}_{\\textrm {large}}\\) , the coefficient is 0.05. \\(\\alpha _{\\textrm {A3C}}\\)  is 1.0. We perform \\(10^5\\)  training updates and keep the best model with the highest training success rate.\n"]}
{"id": "1802.04431", "categories": "cs.LG stat.ML", "paragraphs": ["Each model is shallow with only two hidden layers and 80 units in each layer. We found this architecture provided enough capacity to predict individual channels well, and adding additional capacity provided little to no prediction benefits while increasing model sizes and training times. All channels do not necessarily require this amount of capacity and future improvements could include automated selection of appropriate model capacity based on channel complexity. Similarly, a sequence length \\(l_s = 250\\)  provided a balance between performance and training times. The difference in input dimensions for SMAP and MSL results from the missions each having different sets of command modules. Early stopping was used to prevent overfitting during model training, and not all models were trained for the full 35 iterations.\n", "If \\(L \\ge 1 - \\epsilon _{norm}\\)  values are classified as anomalous. In the next section, results generated using \\(l_{short} = 10\\)  and \\(\\epsilon _{norm} = \\lbrace 0.01, 0.0001\\rbrace \\)  are compared to the approach in Section REF . The effects of pruning (detailed in Section REF ) on this approach are also tested.\n"]}
{"id": "1812.02395", "categories": "cs.LG stat.ML", "paragraphs": ["where \\(\\eta \\)  is the learning rate and \\(M\\)  is the mini-batch size. The specific gradients of the parameters are omitted due to space limitations (see the Appendix).\nIn the training phase, the dynamic pooling is simply passed through the gradient to the unit selected as maximum, analogous to ordinary max-pooling.\nIn the mini-batch gradient descent, the learning rate \\(\\eta \\)  is controlled using the Adam optimizer with the hyperparameters recommended in\u00a0[28], and the mini-batches are set as \\(M=16\\)  examples. We used the same procedure for all the models we compared in our experiments.\nThe detailed settings of the hyperparameters \\(K\\) , \\(T_k\\) , \\(\\lambda \\) , \\(\\mu \\) , \\(l_0\\) , and \\(l\\)  are described for each experimental task in Section\u00a0.\n"]}
{"id": "1812.02619", "categories": "cs.CV", "paragraphs": ["We optimize Tube-CNN and TPN with the stochastic gradient descent (SGD) algorithm with momentum 0.9 and weight decay 0.0005 on mini-batches\u00a0[17].\nThe classification and regression are trained with the cross-entropy loss and smooth L1 loss\u00a0[9], respectively.\nWe fix tube length \\(T=10\\)  for all tube models.\nThis design choice is analyzed in Section\u00a0REF .\n"]}
{"id": "1808.00327", "categories": "cs.CV", "paragraphs": ["We apply the PatchGAN [13] to the discriminators, in which the discriminator tries to classify whether overlapping image patches are real or fake. Similar to EBGAN [39], we add the gaussian noise to the shared layers and generator. During training, Adam [12] optimization is applied with \\(\\beta _1\\) =0.5 and \\(\\beta _2\\) =0.999. We train the model on a single Titan X GPU with learning rate=0.0001. Each mini-batch contains three frontal view images, three homography view images and three bird view images. For the weighted factor \\(\\lambda _1\\) , we apply \\(\\lambda _1 = 10\\) , which is chosen by using a cross-validation method.\n"]}
{"id": "1805.08206", "categories": "cs.LG stat.ML", "paragraphs": ["VGG-16: For the first three datasets (CIFAR10, CIFAR100, and SVHN), we used an architecture inspired by the VGG-16 architecture [24]. We adapted the VGG-16 architecture to the small images size and a relatively small dataset size based on [18]. We trained the model for 250 epochs using SGD with a momentum value of \\(0.9\\) . We used an initial learning rate of 0.1, a learning rate multiplicative drop of 0.5 every 20 epochs, and a batch size of 128. A standard data augmentation was used including horizontal flips, rotations, and shifts. In this learning regime, we reached a validation error of 6.4% for CIFAR-10, 29.2% for CIFAR-100 and 3.54% for SVHN.\n", "Resnet-18: For ImageNet dataset, we used the Resnet-18 architecture [12]; we trained the model using SGD with a batch size of 256 and momentum of 0.9 for 90 epochs. We used a learning rate of 0.1, with a learning rate multiplicative decay of 0.1 every 30 epochs. The model reached a (single center crop) top 1 validation accuracy of 69.6% and top 5 validation accuracy of 89.1%.\n", "NN-distance: We implemented the NN-distance method using \\(k=500\\)  for the nearest neighbors parameter. We didn't implemented the two proposed extensions (embedding regularization, and adversarial training), this add-on will degrade the performance of \\(f\\)  for better uncertainty estimation, which we are not interested in. Moreover, running the NN-distance with this add-on will require to add it to all other methods to manage a proper comparison.\n"]}
{"id": "1805.08193", "categories": "cs.LG stat.ML", "paragraphs": ["The learning rate is initialized as 0.1, and decreases with the operation \\(\\emph {tf.train.exponential\\_decay}\\)  with the staircase in tensorflow. The corresponding decay factor is set \\(0.1\\) . The learning rate for generator and discriminator is fixed with \\(3e-4\\) .\n"]}
{"id": "1801.06769", "categories": "cs.CV", "paragraphs": ["For the SRR-net, we simply set the depth of the network to 20. We spend about 8 hours on training the SRR-net by using the Caffe\u00a0[22] and use Adam with weight decay of \\(10^{-6}\\)  and mini-batch size of 64. For DJRHR-net, we remove the batch normalization and pooling layer to get better regression effect. Besides, we set the growth rate \\(K\\)  to 12, the number of the denseblocks \\(L\\)  is 3. We use the pytorch to construct the network and use Adam with weight decay of \\(10^{-4}\\)  and mini-batch size of 10. We start with a learning rate of \\(10^{-3}\\)  and the learning rate decay of 0.95.\n"]}
{"id": "1808.06161", "categories": "cs.CL", "paragraphs": ["The token embeddings were pre-trained on a large corpus combining Wikipedia, PubMed, and PMC texts [27] using the word2vec toolThe word vectors can be downloaded at http://bio.nlplab.org/ (denoted as \u201cWord2vec-wiki+P.M.\u201d). They are fixed during the training phase to avoid over-fitting. We also tried other types of word embeddings, such as the word2vec embeddings pre-trained on the Google News datasethttps://code.google.com/archive/p/word2vec/ (denoted as \u201cWord2vec-News\u201d), word2vec embeddings pre-trained on the Wikipedia corpushttps://github.com/jind11/word2vec-on-wikipedia (denoted as \u201cWord2vec-wiki\u201d), GloVe embeddings pre-trained on the corpus of Wikipedia 2014 + Gigaword 5http://nlp.stanford.edu/data/glove.6B.zip (denoted as \u201cGlove-wiki\u201d), fastText embeddings pre-trained on Wikipediahttps://github.com/facebookresearch/fastText/blob/master/ pretrained-vectors.md (denoted as \u201cFastText-wiki\u201d), and fastText embeddings initialized with the standard GloVe Common Crawl embeddings and then fine-tuned on PubMed abstracts plus MIMIC-III notes (denoted as \u201cFastText-P.M.+MIMIC\u201d). The comparison results are summarized in the next section.\n", "The model is trained using the Adam optimization method [15]. The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch. For regularization, dropout [30] is applied to each layer. For the version of dropout used in practice (e.g., the dropout function implemented in the TensorFlow and Pytorch libraries), the model ensemble generated by dropout in the training phase is approximated by a single model with scaled weights in the inference phase, resulting in a gap between training and inference. To reduce this gap, we adopted the dropout with expectation-linear regularization introduced by [24] to explicitly control the inference gap and thus improve the generalization performance.\n"]}
{"id": "1801.08322", "categories": "cs.CV", "paragraphs": ["We built our models using Keras [43] with a TensorFlow backend [44]. In order to find the best models' structure, we evaluated a different number of gated layers from one to four. A smaller model with only one LSTM or GRU layer was not performing well on this task. Experiments with a larger number of gated layers and dense layers also failed to give improvements in performance. This might be due to overfitting problem. We got the best classification results using 2 gated layers for both LSTM and GRU models. Therefore, our LSTM and BLSTM models consist of two LSTM layer with \\(\\tanh \\)  function [45] as activation. For each heartbeat, the outputs of LSTM or BLSTM layers were given to the dense layer and the outputs of this layer were given to the softmax layer for classification (refer to Section ).\n", "We trained all these models using the training portion of the dataset and development portion of the data was used for hyper-parameter selection. We started training the network with a learning rate of 0.002 and learning rate was halved after every 5 epochs if the classification rate of the model did not improve on the validation set. This process continued until the learning rate reached below 0.00001 or until the maximum epochs i.e., 100 were reached. We used batch normalization after the dense layer for normalization of learned distribution to improve the training efficiency [46]. In order to accommodate the effect of initialization, we repeated each hyperparameter combination for three times and used the averaged prediction for validation and testing.\n"]}
{"id": "1802.09913", "categories": "cs.CL cs.NE stat.ML", "paragraphs": ["We use BiLSTMs with one hidden layer of 100 dimensions, 100-dimensional randomly initialised word embeddings, a label embedding size of 100. We train our models with RMSProp, a learning rate of \\(0.001\\) , a batch size of 128, and early stopping on the validation set of the main task with a patience of 3.\n"]}
{"id": "1809.02306", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["We preprocessed monolingual data and generated mini-batches for each language. For each iteration, our model alternately read mini-batches of each language, and updated its parameters every time it read one mini-batch. We trained our model for 10 epochs with the mini-batch size 64. The size of word embedding was set as 300, and the size of LSTM hidden states was also set as 300 for the forward and backward LSTMs, respectively. Dropout [21] is applied to the hidden state with its rate 0.3. We used SGD [5] as an optimizer with the learning rate 1.0. Our parameters, which include word embeddings, were uniformly initialized in [-0.1, 0.1], and gradient clipping [17] was used with the clipping value 5.0. We included in the vocabulary the words that were used at least a certain number of times. For the News Crawl corpus, we set the threshold as 3, 5, 5, 5 ,5, 10, and 20 for 50k, 100k, 150k, 200k, 250k, 300k and 1m sentences. For the Europarl corpus, we set the value as 10. We fed 10000 frequent words into the discriminator in [7] MUSE.\n"]}
{"id": "1811.04551", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["We use the convolutional and deconvolutional networks from [22], a GRU [9] with 200 units as deterministic path in the dynamics model, and implement all other functions as two fully connected layers of size 200 with ReLU activations [43]. Distributions in latent space are 30-dimensional diagonal Gaussians with predicted mean and standard deviation.\n", "We pre-process images by reducing the bit depth to 5 bits as in [31]. The model is trained using the Adam optimizer [30] with a learning rate of \\(10^{-3}\\) , \\(\\epsilon =10^{-4}\\) , and gradient clipping norm of 1000 on batches of \\(B=50\\)  sequence chunks of length \\(L=50\\) . We do not scale the KL divergence terms relatively to the reconstruction terms but grant the model 3 free nats by clipping the divergence loss below this value. In a previous version of the agent, we used latent overshooting and an additional fixed global prior, but we found this to not be necessary.\n", "For planning, we use CEM with horizon length \\(H=12\\) , optimization iterations \\(I=10\\) , candidate samples \\(J=1000\\) , and refitting to the best \\(K=100\\) . We start from \\(S=5\\)  seed episodes with random actions and collect another episode every \\(C=100\\)  update steps under \\(\\epsilon \\sim (0,0.3)\\)  action noise. The action repeat differs between domains: cartpole (\\(R=8\\) ), reacher (\\(R=4\\) ), cheetah (\\(R=4\\) ), finger (\\(R=2\\) ), cup (\\(R=4\\) ), walker (\\(R=2\\) ). We found important hyper parameters to be the action repeat, the KL-divergence scales \\(\\beta \\) , and the learning rate.\n"]}
{"id": "1805.05593", "categories": "cs.CL", "paragraphs": ["We employed pre-trained word embeddings trained by using the word2vec tool\u00a0[12] on the 2014 MEDLINE/PubMed baseline distribution. The vocabulary size was 215,840.\nThe embedding of the drugs, i.e., DRUG1 and DRUG2 were initialized with the pre-trained embedding of the word drug.\nThe embeddings of training words that did not appear in the pre-trained embeddings were initialized with the average of all pre-trained word embeddings. Words that appeared only once in the training data were replaced with an UNK word during training, and the embedding of words in the test data set that did not appear in both training and pre-trained embeddings were set to the embedding of the UNK word. Word position embeddings are initialized with random values drawn from a uniform distribution.\n"]}
{"id": "1809.03036", "categories": "cs.CV", "paragraphs": ["For our short-term model, the VGRU-r1 (MA), we trained on all action classes using our proposed multi-objective cost, calculating gradients over mini-batches of 32 samples (clipping gradient norms to 5) and optimizing parameters over \\(100,000\\)  iterations RMSprop [27] with initial learning rate \\(\\lambda = 0.0001\\)  and decayed by \\(0.8\\)  every 5000 iterations until 60,000 iterations. Drop-out [29], [19], with probability of 0.3, was applied only to the Body-RNN, which was further modified to use skip connections that connect input units to output units, as in [16].\nThe model was given 50 seed frames and tasked with predicting the next 10 subsequent frames (400 milliseconds). When training for this, the VTLN-RNN is unrolled backwards while the Body-RNN is unrolled forwards, in time, over 60 steps. (Note: MA stands for multi-action, SA for single-action.)\n", "For our long-term models, which were trained on single-action data, parameter optimization was carried out with RMSprop (\\(\\lambda = 0.0002\\) , decayed by \\(0.6\\)  every 2000 iterations) over \\(10,000\\)  iterations with mini-batches of 32, using, again, our proposed cost function. Models were fed in 50 seed frames and made to predict the next 100 frames (4 sec), which meant that the VTLN-RNN was unrolled backwards and the Body-RNN forwards 150 steps. The input vector to the Body-RNN consisted of joint angles appended with motion derivatives.\nVGRU-d refers to our proposed VTLN-RNN architecture where the VTLN-RNN and Body-RNN both contain only a single layer of 512 GRU cells. GRU-d refers to a 2-layer GRU model (512 units in each). Both VGRU-d and GRU-d models are trained with our proposed loss and make use of inputs augmented with motion derivatives. VGRU-ac refers to our VTLN-RNN architecture trained with auto-conditioning [30], using the recommended length of 5, serving as a baseline.\nFor all models (short and long-term), hyper-parameters were tuned on a separate validation set.\n{FIGURE}"]}
{"id": "1809.02940", "categories": "cs.CV", "paragraphs": ["R-FCN: ResNet-101 [24] is adopted as the backone of R-FCN, and online hard example mining (OHEM) [25] is used to train R-FCN. The learning rate is \\(0.0003\\)  and the momentum is \\(0.9\\) . A number of eye regions are segmented by R-FCN, which are further resized into \\(224\\times 224\\times 3\\) .\n", "CNN: The network architecture consists of five convolutional layers and three pooling layers, followed by three fully connected layers, as shown in Figure REF . Each convolutional layer is followed by a Relu layer [26], an effective activation function to improve the performance of the CNNs. In addition, the dropout strategy [27] is used in the first two fully connected layers in order to prevent overfitting. The network training is performed by the stochastic gradient descent method [28]. \\(L_2\\)  regularization with the weight decay \\(5\\times 10^{-4}\\)  is used in the network training. The dropout ratio is set as 0.5. The batch size is set as 32. The learning rate is initially set as 0.01 and the training is stopped after 5000 iterations.\n"]}
{"id": "1810.12715", "categories": "cs.LG cs.CR stat.ML", "paragraphs": ["For IBP, across all datasets, the networks were trained using the Adam\u00a0[30] algorithm with an initial learning rate of \\(10^{-3}\\) .\nWe linearly ramp-down the value of \\(\\kappa \\)  between 1 and \\(\\kappa _\\textrm {final}\\)  after a fixed warm-up period (\\(\\kappa _\\textrm {final}\\)  is set to both 0 or 0.5 and the best result is used).\nSimultaneously, we lineary ramp-up the value of \\(\\epsilon \\)  between 0 and \\(\\epsilon _{\\textrm {train}}\\)  (for Cifar-10 and Svhn, we use a value of \\(\\epsilon _{\\textrm {train}}\\)  that is 10% higher than the desired robustness radius).\nMnist is trained on a single Nvidia V100 GPU.\nCifar-10, Svhn and ImageNet are trained on 32 tensor processing units (TPU) [31] with 2 workers with 16 TPU cores each.\n", "The networks trained using [24] were trained using the schedule and learning rate proposed by the authors.\nFor [8], we used a learning rate schedule identical to IBP and, for the inner optimization, adversarial examples are generated by 7 steps of PGD with Adam\u00a0[30] and a learning rate of \\(10^{-1}\\) .\nNote that our reported results for these two methods closely match or beat published results, giving us confidence that we performed a fair comparison.\n{FIGURE}"]}
{"id": "1810.12546", "categories": "cs.CL", "paragraphs": ["\nDeepRNNSearch (GRU): a deep GRU-equipped RNNSearch model\u00a0[36] with 5 layers. We set the dimension of word embedding and hidden state to 620 and 1000 respectively.\n\nTransformer: a purely attentional translator\u00a0[30]. We set the dimension of word embedding and filter size to 512 and 2048 respectively. The model was trained with a minibatch size of 256.\n\n", "Our model with the CA structure, using only 63.1M parameters, processes 3993 words per second during training and generates 186 words per second during decoding, which yields substantial speed improvements over the GRU- and LSTM-equipped RNNSearch. This is due to the light matrix computation in recurrent units of ATR. Notice that the speed increase of ATR over GRU and LSTM does not reach 3x. This is because at each decoding step, there are mainly two types of computation: recurrent unit and softmax layer. The latter consumes the most calculation, which, however, is the same for different models (LSTM/GRU/ATR).\n"]}
{"id": "1810.12557", "categories": "cs.CL", "paragraphs": ["\nMaximum input length (max_length): specifies the maximum length of a sentence in tokens (sub-words in our case). Sentences longer than max_length are either excluded from the training (T2T) or cut to match the max_length (RNN). Lowering max_length allows us to use a higher batch size and/or bigger model but biases the translation towards shorter sentences. Since \\(99\\%\\)  of the training sentences are not longer than 70, we set max_length to 70.\n\nBatch size (batch_size) For T2T batch_size is the approximate number of tokens (subwords) consumed in one training step, while for ConvS2S and RNN batch_size is the number of sentence pairs consumed in one training step. Hence for consistency we define batch_size as the approximate average number of tokens consumed in one training step. In fact the number of tokens in a sentence is the maximum of source and target subwords from the pair of training sentences. During training this allows us to put as many training tokens per batch as possible while ensuring that batches with long sentences still fit in GPU memory. In contrast, if we fixed the number of sentence pairs in a training batch, the model can run out of memory if a batch has many long sentences.\n\nTraining epoch is one complete pass through the whole training set. The number of training steps can be converted to epochs by multiplying by the batch size and dividing by the number of subwords in the training data.\n\nModel size is number of trainable parameters of each model. Because of the difference in model structures, it is almost certain that two models with the same model size will not have the same training time.\n\n"]}
{"id": "1810.12443", "categories": "cs.CL", "paragraphs": ["Initialization. The size of the dimensions of character embeddings is 32 which are randomly initialized using a uniform distribution. We adopt the same initialization method for randomly initialized word embeddings that are updated during training. For IntNet, the filter size of the initial convolution is 32 and that of other convolutions is 16. We have used filters of size \\([3, 4, 5]\\)  for all the kernels. The number of convolutional layers are 5 and 9 for IntNet-5 and IntNet-9, respectively, and we have adopted the same weight initialization as that of ResNet. We use pre-trained word embeddings for initialization, GloVe [17] 100-dimension word embeddings for English, and fastText [0] 300-dimension word embeddings for Spanish, Dutch, and German. The state size of the bi-directional LSTMs is set to 256. We adopt standard BIOES tagging scheme for NER and Chunking.\n"]}
{"id": "1809.10966", "categories": "cs.CV", "paragraphs": ["We finetuned our models on \\(S=3\\)  source domains and tested on the remaining target. We splitted our training sets in 90% train and 10% validation, and used the best performing model on the validation set for the final test, following the validation strategy described in Section . For preprocessing, we used random zooming with rescaling, horizontal flipping, brightness/contrast/saturation/hue perturbations and normalization using ImageNet's statistics. We used a batch size of 96 (32 images per source domain) and trained using SGD with momentum set at 0.9 and initial learning rate at 0.01 and 0.007 for ResNet's and AlexNet's experiments respectively. We considered an epoch as the minimum number of steps necessary to iterate over the largest source domain and we trained our models for 30 epochs, scaling the learning rate by a factor of 0.2 every 10 epochs. We used the same setup to train our ResNet-18 Deep All baselines. We repeated each experiment 5 times, averaging the results.\n"]}
{"id": "1811.05163", "categories": "cs.CV", "paragraphs": ["In our experiments, the software tools are Matconvnet [16], CUDA 8.0, CUDNN V5.1, MATLAB 2014 and Visual Studio 2012. The hardware device is workstation configured with a Intel Xeon E3-1505 M v5 CPU @2.80 GHz, a NVIDIA Titan X GPU and 128 GB DDR3 Memory. Moreover, the training settings similar to [21], [23] are adopted and summarized as follows. All images in these two databases are scaled to \\(128\\times 128\\)  pixels, and each image is further augmented by the horizontal mirror and randomly rotating operations. The randomly rotating operation is applied to randomly rotate an image in ranges \\([-3^{\\circ }, 0^{\\circ }]\\)  and \\([0^{\\circ }, 3^{\\circ }]\\) . The weights in each layer are initialized based on a normal distribution \\(N(0, 0.01)\\) , and the biases are initialized to 0. The \\(L_2\\)  regularization weights \\(\\alpha \\)  in Eq. (REF ) is set as 0.005 on the VeRi [0] database, while for the larger VehicleID [2] database, it is set as 0.001. The size of mini-batch is 128 including 64 positive and 64 negative image pairs, and both positive and negative pairs are randomly selected from the whole database. The momentums are set to 0.9. The learning rates start with 0.01 and are gradually decreased along the training progress. That is, if the objective function is convergent at a stage, the learning rates are reduced to 1/10 of the current values, and the minimum learning rates are 0.001.\n"]}
{"id": "1807.01846", "categories": "cs.LG stat.ML", "paragraphs": ["To compute the output mass function given by Eq () in the binary case and by Proposition REF  in the multi-category case, we need to compute the weights of evidence. In the binary case, these weights depend on coefficients \\(\\beta _j\\)  and \\(\\alpha _j\\)  for \\(j=1,\\ldots ,J\\)  through (REF ). A learning procedure (such as likelihood maximization) gives us estimates \\(\\widehat{\\beta }_j\\)  of \\(\\beta _j\\)  for \\(j=0,\\ldots ,J\\) . Parameters \\(\\alpha _j\\)  are not identifiable, but are linked to \\(\\beta _0\\)  by Eq. (REF ). In the multi-category case, things are worse, because parameters \\(\\beta _{jk}\\)  are also not identifiable: we can easily check that adding any constant vector \\(c=(c_0,\\ldots ,c_J)\\)  to each vector \\(\\beta _k=(\\beta _{0k},\\ldots ,\\beta _{Jk})\\)  produces the same normalized plausibilities (REF ). Both parameters \\(\\beta _{jk}\\)  and \\(\\alpha _{jk}\\)  are, thus, underdetermined in that case.\n", "To identify the model parameters, we propose to apply the Least Commitment Principle introduced in Section REF , by searching for the parameter values that give us the output mass functions with minimal information content, the information content of a mass function \\(m\\)  being taken to be \\(I_p(m)\\)  defined by (REF ), with \\(p=2\\) . (The value \\(p=2\\)  is chosen because it lends itself to easy computation, as will be shown below). We will first deal with the binary case in Section REF  and proceed with the multi-category case in Section REF .\n"]}
{"id": "1804.02204", "categories": "cs.CL cs.LG stat.ML", "paragraphs": ["From preliminary experiments it was found that using batch sizes of roughly 25 hrs gave a good balance between the number of updates and using good gradient estimate. When the number of CG iterations is limited, initialising CG with a good estimate of the gradient is desirable, as within a few iterations a good descent direction can be found. However this comes at the cost of fewer HF/NG updates per epoch as larger batch sizes are needed to reduce the variance associated with the gradient estimates. In preliminary tests, it was found that increasing the batch sizes beyond 25hrs yielded no significant improvements. To make up the NG/HF minibatch, we followed Kingsbury's approach [4] and sampled only 1% of the training set.\n"]}
{"id": "1811.06017", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["Training setups: The loss function is the mean squared error (MSE) and is optimized with Adam optimizer [18] with learning rate=0.001, \\(\\beta _1\\) =0.9, \\(\\beta _2\\) =0.999. The batch size used in this work is 256 and models are trained for 1000 epochs.\n"]}
{"id": "1805.10163", "categories": "cs.CL", "paragraphs": ["We follow the setup of Transformer base model\u00a0[26]. More precisely, the number of layers in the encoder and decoder is \\(N=6\\) . We employ \\(h = 8\\)  parallel attention layers, or heads. The dimensionality of input and output is \\(d_{model} = 512\\) , and the inner-layer of a feed-forward networks has dimensionality \\(d_{ff}=2048\\) .\n"]}
{"id": "1807.04734", "categories": "cs.LG stat.ML", "paragraphs": ["\nFor training, validation, and testing, we used 630, 70, and 20 windows respectively. We trained CRsAE using mini-batch gradient descent back-propagation with the ADAM optimizer\u00a0[10] for a maximum of 60 epochs. In cases when the filters converged early, and the validation loss stopped to improve, we stopped the training. We picked the learned dictionary as the one with minimum validation loss. The number of training parameters was \\(K \\times C = 45 \\times 3 = 135\\) . The parameters to be tuned were \\(\\lambda \\) , \\(L\\) , the learning rate of the ADAM optimizer, the number of FISTA iterations \\(T\\) , and the mini-batch size \\(B\\) . Below, we discussed how we selected each.\n", "Regularization parameter \\(\\lambda \\). Let \\(\\mathbf {H}_0\\)  denote the initial dictionary estimate: \\(\\mathbf {y}_j = \\mathbf {H}_{0} \\mathbf {x}_j + \\underbrace{(\\mathbf {H}-\\mathbf {H}_{0}) \\mathbf {x}_j + \\mathbf {v}_j}_{\\mathbf {n}_j}\\) . The observations contain two sources of noise, namely observation noise, and noise from lack of knowledge of \\(\\mathbf {H}\\) . For simulated data, we can compute both quantities explicitly. If we let \\(\\hat{\\sigma }_n = \\frac{1}{N} \\left|\\left|n_j\\right|\\right|_2\\) , we can use \\(\\lambda \\approx \\hat{\\sigma }_n \\sqrt{2\\log ( C\\times N_e)}\\)  as suggested in\u00a0[11]. For real data, we can estimate \\(\\left|\\left|\\mathbf {v}_j\\right|\\right|_2\\)  from \u201csilent\u201d periods. Assuming \\(\\mathbf {H}_0\\)  is close enough, an assumption well-justified by the dictionary learning literature\u00a0[1], we propose to estimate the cumulative noise by scaling this estimate by a constant factor of \\(1.1\\)  to 2. In principle, this argument suggests we should decrease \\(\\lambda \\)  as the estimate of the dictionary improved. The rate at which this occurs should depend on the rate at which the dictionary estimate improves. To our knowledge, there is no theory characterizing this. Therefore, we do not decrease \\(\\lambda \\)  in the results we report.\n", "Picking \\(L\\)  and the learning rate. \\(L\\)  must be greater than the maximum eigenvalue of \\(\\mathbf {H}^\\textrm {T}\\mathbf {H}\\) \u00a0[9]. We used an existing collection of neural action potentials to estimate \\(L\\) . We set \\(L = 26\\) . We varied the learning rate of the optimizer from \\(10^{-5}\\)  to \\(10^{-1}\\)  and chose the one associated with the sharpest drop in the validation loss function as in\u00a0[12].\n", "Batch size \\(B\\). We chose \\(B = 16\\) . We found this to be a choice for which the ADAM optimizer successfully avoided local optima of the loss function.\n", "Figure REF  demonstrates the ability of CRsAE to recover the filters used in the simulation in the presence of noise. For each point in Figure\u00a0REF , we estimated the recovery error by averaging over 7 independent simulations. We note that we only used 700 examples per simulation, which took \\(\\approx 1\\)  hour on a GPU! Increasing the number of simulations and the number of training examples would let the errors converge to values much closer to 0. Figure\u00a0REF  demonstrates that, for one of the best simulations with 16 dB SNR, the errors do converge to 0.\n{FIGURE}", "We also compared the dictionary-learning performance of CRsAE to the network in [4] where the encoder consists of 3 ISTA iterations, and the decoder is linear and unconstrained. We term this architecture LCSC(3). Figure\u00a0REF  compares the true dictionary to that estimated using LCSC(3) when the SNR is 16 dB. We considered two cases for CRsAE. In the first, \\(\\mathbf {H}_0\\)  was a random perturbation of the true dictionary \\(\\mathbf {H}\\)  such that the error (Eq.\u00a0REF ) was between \\(0.4\\)  to \\(0.5\\) . In the second, the entries of \\(\\mathbf {H}_0\\)  were assumed to be i.i.d. Gaussian. For LCSC(3), we initialized the dictionary as in the first case for CRsAE. We also allowed \\(\\lambda \\)  to be trainable as in\u00a0[4]. In spite of the fact LCSC(3) converged to a solution with small reconstruction error, unlike CRsAE, it was not able to learn the true filters.\n{FIGURE}{FIGURE}"]}
{"id": "1805.10981", "categories": "cs.LG q-bio.NC stat.ML", "paragraphs": ["Figure REF  shows the activation patterns and the corresponding informative time-windows of the components with the maximum contribution to the decoding of each class in the LF-CNN model, trained on the pooled data from Experiment 1 and updated using the pseudo-real time update procedure described above on single held-out subject. For all of the five classes, our model extracted spatial patterns whose source estimates showed overall good correspondence to the locations and lateralizations of the peaks of the evoked responses (Figure REF ).\n", "We also estimated spatial properties of the latent components that had the least overall contribution to any of the classes. Inspecting the weights of the output (classification) layer (Figure REF ) further allowed us to identify 5 components which provided minimum contribution to either class. Although none of these components directly corresponded to known signatures of e.g. oculomotor artifacts, their overall limited contribution to either class may suggest that these patterns were used for out-projecting irrelevant activity.\n{FIGURE}"]}
{"id": "1807.06216", "categories": "cs.CV", "paragraphs": ["As TNRD model serves as a strong baseline, we initialize the parameters \\(\\Theta \\)  using the TNRD model.\nWe tested the trained models on the 68 images test dataset [17].\nWe evaluated the denoising performance using PSNR [5] and SSIM [23].\n"]}
{"id": "1807.06233", "categories": "cs.CV", "paragraphs": ["Except for our data augmentation strategy, we use the same training setup used in SSD (e.g., matching strategy, hard negative mining, and multi-task loss function). We use VGG-16 pretrained model on ImageNet in two parallel CNN pipelines. The stochastic gradient descent (SGD) are used with the mini-batch size of 2 and the momentum parameter of \\(0.9\\) . We set the initial learning rate to \\(0.0005\\) . We set the weight decay parameter applied to L2 regularization term to \\(0.0005\\) .\n"]}
{"id": "1807.02305", "categories": "cs.CL", "paragraphs": ["The vocabulary is collected from the CNN/Daily Mail training data.\nWe lower-case the text and there are 732,304 unique word types.\nWe use the top 100,000 words as the model vocabulary since they can cover 98.23% of the training data.\nThe size of word embedding, sentence level encoder GRU, document level encoder GRU are set to 50, 256, and 256 respectively.\nWe set the sentence extractor GRU hidden size to 256.\n"]}
{"id": "1807.02232", "categories": "cs.CV", "paragraphs": ["We propose the model to work for various resolutions, so we train the model with materials of various scales. The images are cropped and downsampled to three scales, namely \\(1792\\times 1024\\) , \\(1344\\times 768\\) , and \\(896\\times 512\\) . Using these images with different scales, our network can work for videos from high resolution to low resolution. Further, to reduce the gap between the distribution of the training set and the test set, the images are previously encoded using HEVC. We set the Quantization Parameter (QP) to \\(22, 27, 32, 37\\)  and use the reconstructed blocks in the decoding process to form the training pairs. We randomly sample about 3,000,000 pairs to train the model. A training process takes about 4 hours on an NVIDIA GTX 1080 GPU. Adam optimizer [58] is used for training. The network is implemented using TensorFlow [59].\n"]}
{"id": "1807.02371", "categories": "cs.CV cs.RO", "paragraphs": ["The training setup is depicted in figure\u00a0REF . We use a central machine to run the RL algorithm which communicates with 9 instances of the game split over 2 machines. Each of the agents communicates via TCP with a WRC6 instance through a dedicated API specifically developed for this work.\nIt allows us to retrieve in-game info, compute the reward and send control back to the game. To speed up the pipeline and match the CNN input resolution, some costly graphics effects were disabled and we used a narrower field of view compared to the in-game view, as shown in figure\u00a0REF .\nThe game's clock runs at 30FPS and the physical engine is on hold as it waits for the next action.\n", "Three tracks - a total of 29.6km - were used for training (3 instances of each) and we reserved two tracks for testing the generalization of the learning which is discussed in section\u00a0. The agents (cars) start and respawn (after crashes) at a random checkpoint (always at 0 km/h).\n"]}
{"id": "1804.06958", "categories": "cs.CV", "paragraphs": ["Showing the effectiveness of the proposed A-CCNN, we use the same training parameters as in\u00a0[1], except for two HPs, which are the patch sizes and \\(\\Sigma \\) 's, for people counting and density estimation.\nThese two HPs are empiracally determined on the traing data set.\nSimilar to the approach in\u00a0[1], a stochastic gradient decent algorithm is used during training.\nThe momentum, the learning rate and the weight decay are set to be 0.9, 0.0001 and 0.001 respectively.\nAfter 25 epochs, the model can reach a local optimum.\n"]}
{"id": "1804.06898", "categories": "cs.CL cs.AI", "paragraphs": ["Coherence models We train and test the LC model described in Section REF  on the synthetic dataset and evaluate it using PRA and TPRA. During pre-processing, words are lowercased and initialized with pre-trained word embeddings [22]. Words that occur only once in the training set are mapped to a special UNK embedding. All network weights are initialized to values drawn randomly from a uniform distribution with scale \\(=0.05\\) , and biases are initialized to zeros. We apply a learning rate of \\(0.001\\)  and RMSProp \u00a0[17] for optimization.\nA size of 100 is chosen for the hidden layers (\\(d_{lstm}\\)  and \\(d_{cnn}\\) ), and the convolutional window size (\\(m\\) ) is set to 3. Dropout\u00a0[15] is applied for regularization to the output of the convolutional operation with probability \\(0.3\\) . The network is trained for 60 epochs and performance is monitored on the development sets \u2013 we select the model that yields the highest PRA value.Our implementation is available at https://github.com/Youmna-H/Coherence_AES\n", "We use as a baseline the LC model that is based on the multiplication of the clique scores (similarly to [9]), and compare the results (LCmul) to our averaged approach. As another baseline, we use the entity grid (EGrid) [2] that models transitions between sentences based on sequences of entity mentions labeled with their grammatical role. EGrid has been shown to give competitive results on similar coherence tasks in other domains. Using the Brown Coherence Toolkit [6],https://bitbucket.org/melsner/browncoherence we construct the entity transition probabilities with length = 3 and salience = 2. The transition probabilities are then used as features that are fed as input to an SVM classifier with an RBF kernel and penalty parameter \\(C = 1.5\\)  to predict a coherence score.\n", "Combined models\nAfter training the LC and LSTMT&N models, we concatenate their output vectors to build the Baseline: Vector Concatenation (VecConcat) model as described in Section REF , and train a Kernel Ridge Regression model.We use scikit-learn with the following parameters: alpha=\\(0.1\\) , coef0=1, degree=3, gamma=\\(0.1\\) , kernel=`rbf'.\n"]}
{"id": "1801.00968", "categories": "cs.CV cs.GR", "paragraphs": ["We implement our network in Tensorflow on an NVIDIA GeForce GTX 1080Ti graphics card. We collect 1449 RGB/D image pairs from NYU data set\u00a0[20], 1000 RGB/D image pairs for training and 449 RGB/D image pairs for testing. We augment the training data by clip, rotation and mirror and generate 180,000 training patch pairs of size 128\\(\\times \\) 128. The batch size is set to 36. The network is trained with 200,000 steps and the initial learning rate is 1e-3, which decays to 0.8 times per 10000 steps. We train our network for joint image SR. We get the low-resolution target image from a ground-truth image using nearest-neighbor down-sampling. When our JCNP model is trained with the augmented RGB/D data pairs, it can be directly applied to several different joint image SR tasks, including depth map SR, chromaticity map SR and saliency map SR. In experiments, we provide the average root mean squared errors (RMSEs) of test data set, except for the visual results, to evaluate the results.\n"]}
{"id": "1803.06554", "categories": "cs.CV cs.AI eess.IV", "paragraphs": ["The network structure for YOLO is basically the same as the default structure of YOLOv2 [17] except the last layer. YOLOv2 divides each image into 13 by 13 grids. For our application, we predict five base bounding boxes for each grid in the image. For each box, there are 4 numbers (for the top left and bottom right coordinates of the AABB), 1 confidence score and 5 class scores. Therefore, we change the filter size to \\(13 \\times 13 \\times 50\\)  (\\(5 \\times (4 + 1 + 5) = 50\\) ) in the last layer.\nWe also change the learning rate to \\(10^{-5}\\)  to avoid divergence. We use a batch size of 64, a momentum of 0.9 and a decay of 0.0005, which are the same as those in the original YOLOv2 configuration.\n", "For Faster R-CNN, the model is trained with a VGG net. Most parameters are set to be the same as the original Faster R-CNN [18]. The changes are as follows: the number of classes is modified to five; the number of outputs in the class score layer is modified to five; the number of outputs in the bounding box prediction layer is modified to 20 (\\(4 \\times 5\\) ).\n"]}
{"id": "1810.04142", "categories": "cs.CL", "paragraphs": ["We train CMX on the concatenation of three datasets: (a) GY-Mix's training portion, (b) synthetic codemixed data and (c) a monolingual corpus that covers 100 languages. Every token in the training set spawns a training instance. Our training set consists of 38M tokens in total, which is on the same magnitude as the sizes of training data reported in previous work [13], [11].\n", "We use mini-batched averaged stochastic gradient descent (ASGD) [6] with momentum [9] and exponentially decaying learning rates to learn the parameters of the network. We fix the mini-batch size to 256 and the momentum rate to 0.9. We tune the initial learning rate and the decay step using development data.\n"]}
{"id": "1810.04158", "categories": "cs.CV", "paragraphs": ["\nWeights are initialized from a zero-centered Gaussian distribution, with a standard deviation of \\(0.02\\)  ;\n\nThe Adam optimizer\u00a0[19] is used, with \\(\\beta _1 = 0.5\\) ;\n\nThe base learning rate is initialized at \\(2e^{-4}\\) .\n\n"]}
{"id": "1811.11161", "categories": "cs.CL", "paragraphs": ["For the model, we initialize the word embeddings using fasttext embedings\u00a0[11]. The model is trained using mini-batch SGD with Adam optimizer\u00a0[9] with standard parameters to minimize the class weighted cross-entropy loss. In our experiments, we use 128 dimensions for the LSTM hidden states and 256 dimensions for the hidden state in the decoder. Similar to\u00a0[21], we pre-train an LSTM encoder using live data in each of the languages and use this model to initialize the parameters of the LSTM-based encoders. All model setups are trained for 40 epochs. For evaluation on test set we pick the best model based on performance on dev set. For evaluation, we only select those slots as the final hypothesis, whose \\(\\tau > 0.5\\)  and occur in the context of the conversation. For each utterance, independent carryover decisions are taken for each candidate slot. We use standard definitions of precision, recall and F1 by comparing the reference slots with the model hypothesis slots. If an entity type is repeated in the current turn then we do not carry this from dialogue history.\n"]}
{"id": "1811.11165", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["In the experiments on Clothing1M (Section\u00a0REF ), we used two GAN configurations: CT-GAN for AC-GAN/rAC-GAN and SN-GAN for cGAN/rcGAN. We defined the network architectures and training settings while referring to the source code provided by the authors of SN-GAN\u00a0[47] (which is used for \\(64 \\times 64\\)  dog and cat image generation).https://github.com/pfnet-research/sngan_projection The reason why we refer to this source code is that there is no previous study attempting to learn a generative model using Clothing1M, to the best of our knowledge. We experimentally confirm that its settings are reasonable for Clothing1M with no hyperparameter tuning.\n"]}
{"id": "1806.11534", "categories": "cs.CV cs.LG cs.RO", "paragraphs": ["We use Adam optimizer [42] with a learning rate of \\(10^{-5}\\) , \\(\\beta _1\\)  of \\(0.9\\)  and \\(\\beta _2\\)  of \\(0.999\\) . The CNNs are initialized with the pre-trained VGG16 weights on ImageNet and the fully connected layers (which includes the weights of the binary random variables \\(\\mathbf {y}\\) ) are initialized by sampling from a truncated normal distribution (a normal distribution in which values sampled more than 2 standard deviations from the mean are dropped and re-picked) with 0 mean and \\(10^{-3}\\)  standard deviation.\n"]}
{"id": "1805.00912", "categories": "cs.CL cs.AI", "paragraphs": ["The optimization objectives for classification and regression problems are cross-entropy loss and mean square error respectively, which we minimize by using Adadelta [52] or Adam [23] optimizer. All trainable weight matrices are initialized by Glorot Initializer [11], and all the biases are initialized as zeros. We use 300D (except 200D for SRL task) GloVe 6B pre-trained vectors [35] to initialize the word embeddings. The embedding for a word in the training set but not in GloVe is randomly initialized by sampling from uniform distribution between \\([-0.05, 0.05]\\) . The word embeddings will be fine-tuned during the training phase. We also apply Dropout with keep probability \\(p_{kp}\\) , and L2 regularization with weight decay factor \\(\\gamma \\)  to all the model for avoiding overfitting. The unspecified activation functions for all fully-connected layers appearing in models are set to \\(\\operatornamewithlimits{relu}\\)  [12]. The activation function \\(\\sigma _t (\\cdot )\\)  applied to token2token alignment scores is set to \\(\\log (\\operatornamewithlimits{sigmoid}(\\cdot ))\\)  unless otherwise specified.\n", "For fair and reliable comparisons with baselines and prior works, on SNLI and sentence classification tasks, we follow training setup and hyperparameters used in corresponding prior works, and only tune the dropout probability for different baseline or ablation models, without any other trick (e.g., learning rate schedule, batch/layer normalization, etc.); on SRL, we directly employ the training setup and the well-tuned hyperparameters used in the prior state-of-the-art work [46] based on multi-head self-attention mechanism, without tuning them specifically for our proposed model. Besides, for the language model based transfer learning for SNLI and MultiNLI tasks, we use the pretrained model provided by [36]\u00a0radford2018improving. And, we use the language model as the auxiliary task for models' universality with the coefficient of \\(0.3\\) , and use other hyper-parameters (e.g., initial learning rate, optimizer, leaning rate schedule, epoch number) given by [36]\u00a0radford2018improving.\n", "For SNLI dataset (natural language inference), we set \\(p_{kp} = 0.65\\)  and \\(\\gamma = 5\\times 10^{-5}\\) , and use Adadelta as the optimizer with mini batch size of 128. And, we do not use the attention dropout for this benchmark. Besides, the activation function for fully-connected layer is set to \\(\\operatornamewithlimits{elu}\\)  [8]. The training procedure is completed within 600K steps, approximately costing 12 hours.\n", "For CoNLL-05 dataset (semantic role labeling), we use the same hyper-parameters provided in [46] rather than tune them for a fair comparison. The keep probabilities of dropout for fully-connected layer and residual connection\u00a0[14] are set to \\(0.9\\)  and \\(0.8\\)  respectively. The attention dropout with keep probability of \\(0.9\\)  is applied to both source2token and token2token alignment scores, which equals to setting the probability to \\(0.81\\)  in MTSA. And, the activation function \\(\\sigma _t (\\cdot )\\)  applied to the token2token alignment scores is set to \\(\\operatornamewithlimits{identity}(\\cdot )\\) . Besides, different from using fixed positional embedding in [46], we remove it and only use the forward and backward masks in MTSA to encode bi-directional order information. The training will finish within about 20 hours by using Adadelta optimizer.\n", "For CR, MPQA and SUBJ datasets, we set \\(p_{kp} = 0.5\\)  and \\(\\gamma = 10^{-4}\\)  for these three benchmarks. And we apply attention dropout with keep probability of \\(0.8\\)  to CR and MPQA. Different from the other experiments in this paper, we here use Adam as the optimizer to train the models, and do not use any learning rate decay trick. The training procedure is completed within 1000 batch steps.\n", "For TREC dataset (question-type classification), we set \\(p_{kp} = 0.5\\)  and \\(\\gamma = 10^{-4}\\)  and do not apply the attention dropout. The training procedure is completed within 80K steps by using Adadelta optimizer.\n", "For SST-5 dataset (sentiment analysis), we set \\(p_{kp} = 0.7\\)  and \\(\\gamma = 10^{-4}\\)  and do not apply the attention dropout. The training procedure is completed within 120K steps by using Adadelta optimizer.\n"]}
{"id": "1802.09660", "categories": "cs.LG cs.NE", "paragraphs": ["We use the standard \\(tanh\\)  for activation function of nodes in the networks and the mean square root error function (MSE) for the subsequent gradient minimisation. The artificially generated training sets are split in ratios of 0.7, 0.15 and 0.15 for training, internal cross validation and generalisation testing, respectively.\n"]}
{"id": "1812.01598", "categories": "cs.CV cs.GR", "paragraphs": ["As explained in the main paper, we use Adam model introduced in [22] for total body motion capture. The model parameters \\(\\Psi \\)  include the shape parameters \\(\\phi  \\in \\mathbb {R}^{K_\\phi }\\) , where \\(K_\\phi =30\\)  is the dimension of shape deformation space, the pose parameters \\(\\theta  \\in \\mathbb {R}^{J \\times 3}\\)  where the \\(J=62\\)  is the number of joints in the modelThe model has 22 body joints and 20 joints for each hand., the global translation parameters \\(t \\in \\mathbb {R}^3\\) , and the facial expression parameter \\(\\sigma  \\in \\mathbb {R}^{K_\\sigma }\\)  where \\(K_\\sigma =200\\)  is the number of facial expression bases.\n"]}
{"id": "1812.10235", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["The layer sizes for both the LSTM and BLSTM networks in our model are chosen as 200. Based on the size of our dataset, the number of hidden layers is chosen as 2 and Adam optimization is used as in [8]. The size of word embedding is 300, which are initialized randomly at the beginning of experiment.\n"]}
{"id": "1812.10234", "categories": "cs.LG cs.CL stat.ML", "paragraphs": ["Slot Filling: For slot filling task, the pre-trained DNN model \\(f_{dnn}\\)  has the same set-up as in [11], by using an attention based bi-directional LSTM. The number of states in LSTM cell is 128. The randomly initialized word embedding size is also 128.The batch size is 16 and the dropout rate for non-recurrent connection is 0.5.\n", "The DAT model \\(f_{dat}\\)  used for Slot fill and NER tasks are almost the same except the batch size. The network structure chosen to estimate the action-value function \\(Q\\)  is an LSTM structure with 100 states. The averaged word vector in a reinforcement learning state is chosen as a trigram, i.e. n=3. The discount factor \\(\\gamma \\)  in (REF ) is selected as 0.5, 0.7 and 0.9 for difference experiments. The minibatch size is \\(K=16\\)  for slot filling task and \\(K=10\\)  for NER in order to keep the same training batch size as \\(f_{dnn}\\)  in different tasks, and the replay memory size is pre-defined as \\(\\mu =5,000\\) . The thresholds for both experiment are set as \\(T_r=0.95\\) .\n"]}
{"id": "1806.01523", "categories": "cs.CL", "paragraphs": ["For training configurations, we trained for 10 epochs using AdaDelta [26] with \\(\\rho =0.95\\)  and \\(\\epsilon =1.\\mathrm {e}{-6}\\) . We also employed early stopping with patience set to 3. We split our data using 80% training, 10% validation, and 10% test for the fully supervised scenario. For the active learning scenario, we further split the training data into labeled and unlabeled data. We used two kinds of split, 50:50 and 85:15. For the 50:50 scenario, we queried 100 sentences for each epoch. For the 85:15 scenario, we used a smaller query of 10 sentences in an epoch to keep the number of queries less than the number of available fully supervised training data in 10 epochs. This number of queried sentences was obtained by tuning on the validation set.\n"]}
{"id": "1801.00693", "categories": "cs.CV", "paragraphs": ["For both the baseline model and our adversarial auto-encoder model, we used \\((a,b) = (9,1)\\)  for the weighted classification loss. The CNN was trained using an RMSProp Optimizer with a momentum of 0 and a learning rate of \\(10^{-4}\\) . The encoder and decoder of the ssDAAE were trained with the same optimizer with same learning are and momentum as the CNN. We found setting coefficients \\((\\alpha , \\beta , \\eta ) = (0.1, 1, 0.1)\\)  to work well. When training the discriminator training, the same optimizer and learning rate were used, but the momentum was set to \\(0.2\\) .\n"]}
{"id": "1809.00582", "categories": "cs.CL", "paragraphs": ["We validated model hyperparameters on the development set. We did not tune the dimensions of word embeddings and LSTM hidden\nlayers; we used the same value of \u00a0600 reported in Wiseman et\nal.\u00a0wiseman2017challenges. We used one-layer pointer\nnetworks during content planning, and two-layer LSTMs during text\ngeneration. Input feeding (Luong et al.\u00a0[26]) was\nemployed for the text decoder. We applied dropout\u00a0(Zaremba et\nal.\u00a0[44]) at a rate of\u00a0\\(0.3\\) . Models\nwere trained for 25\u00a0epochs with the Adagrad optimizer\u00a0(Duchi et\nal.\u00a0[10]); the initial learning rate\nwas\u00a0\\(0.15\\) , learning rate decay was selected from \\(\\lbrace 0.5, 0.97\\rbrace \\) , and\nbatch size was\u00a05. For text decoding, we made use of\nBPTT\u00a0[31] and set the truncation size to\u00a0100.\nWe set the beam size to\u00a05 during inference. All models are\nimplemented in OpenNMT-py [18].\n"]}
{"id": "1810.03444", "categories": "cs.CL", "paragraphs": ["We preserve most of the training settings from [15] to enable a fair comparison with the original Transformer. Specifically, we use the Adam optimizer [7] with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.98\\) , and \\(\\epsilon =10^{-9}\\) . We follow a similar learning rate schedule with \\(warmup\\_steps\\)  of 16000: \\(LearningRate = 2 * d^{-0.5} * \\min (step\\_num^{-0.5}, step\\_num * warmup\\_steps^{-1.5})\\) .\n", "We trained our models and the baseline on a single GPU for 500,000 steps.We were unable to replicate state-of-the-art results in [15] because of limited GPUs. Hence, we conducted all the experiments including Transformer base in an identical setup for fair comparisons. The batches were formed by sentence pairs containing approximately 4096 source and 4096 target tokens. Similar to [15], we also applied residual dropout with 0.1 probability and label smoothing with \\(\\epsilon _{ls} = 0.1\\) . Our models are implemented in the tensor2tensorhttps://github.com/tensorflow/tensor2tensor library [16], on top of the original Transformer codebase. We trained our models on the standard WMT'16 English-German dataset constaining about 4.5 million sentence pairs, using WMT newstest2013 as our development set and newstest2014 as our test set.\nWe used byte-pair encoding [12] with combined source and target vocabulary of 37,000 sub-words for English-German. We took the average of the last 5 checkpoints (saved at 10,000-iteration intervals) for evaluation, and used a beam search size of 5 and length penalty of 0.6 [19].\n"]}
{"id": "1808.09935", "categories": "cs.LG stat.ML", "paragraphs": ["The training dataset class distribution is heavily skewed with about 92% samples belonging to class 0. Hence, we use weighted-binary-cross-entropy as our loss function to penalize the classifier more heavily on mis-classification of a segment boundary. The loss function is defined as \\(loss = -\\frac{1}{N} \\sum _{i=1}^{N}(t \\log (o) + \\frac{f_1}{f_0} (1 - t) \\log (1 - o)))\\)  where \\(t\\)  and \\(o\\)  are the target and the predicted outputs respectively. \\(f_0\\)  and \\(f_1\\)  are the frequencies of class 0 and class 1 respectively.\n", "We use `AdaDelta' [32] as the optimizer and use dropouts of 0.2 \u2013 0.3 for input and recurrent gates in the recurrent layers. We also use dropouts of 0.3 after the dense fully connected layers to prevent over fitting on the training dataset. We use filters of sizes \\(\\lbrace 2, 3, 4, 5\\rbrace \\)  with 200 filters for each of the sizes. The recurrent layers have 600 neurons.\n"]}
{"id": "1811.03087", "categories": "cs.LG stat.ML", "paragraphs": ["\nDiscussion. We directly note that: (i) \\(\\overline{m}[\\nu _2({\\mathbf {x}}^{k})]\\)  and \\(\\underline{m}[\\nu _2({\\mathbf {x}}^{k})]\\)  are random variables which depend on \\({\\Theta ^{k-1}}\\) , while \\(\\underline{s}[\\nu _2({\\mathbf {x}}^{k})]\\)  is a random variable which depends on \\({\\Theta ^k}\\); (ii) \\(\\underline{m}[\\nu _2({\\mathbf {x}}^{k})] < 0\\)  by log-concavity; (iii) \\(\\underline{s}[\\nu _2({\\mathbf {x}}^{k})]\\)  is centered with \\(\\mathbb {E}_{{\\theta ^k}}[ {\\hspace{0.5pt}}\\underline{s}[\\nu _2({\\mathbf {x}}^{k})] {\\hspace{0.5pt}}]=0\\)  and \\(\\mathbb {E}_{{\\Theta ^k}}[ {\\hspace{0.5pt}}\\underline{s}[\\nu _2({\\mathbf {x}}^{k})] {\\hspace{0.5pt}}]=0\\) .\n", "We further note that each channel provides an independent contribution to \\(\\nu _2({\\mathbf {x}}^{k})= \\frac{1}{N_k}\\sum _{{\\textnormal {c}}}\\nu _{2,{\\textnormal {c}}}({\\mathbf {x}}^{k})\\) , implying for large \\(N_k\\)  that \\(\\underline{\\delta } \\nu _2({\\mathbf {x}}^{k})\\)  has low expected deviation to 1 and that \\(|\\log \\underline{\\delta } \\nu _2({\\mathbf {x}}^{k})| \\ll 1\\) , \\(|\\underline{m}[\\nu _2({\\mathbf {x}}^{k})]| \\ll 1\\) , \\(|\\underline{s}[\\nu _2({\\mathbf {x}}^{k})]| \\ll 1\\)  with high probability. The term \\(\\overline{m}[\\nu _2({\\mathbf {x}}^{k})]\\)  is thus dominating as long as it is not vanishing. The same reasoning applies to other positive moments, e.g. \\(\\mu _2({\\mathbf {x}}^{l})\\) , \\(\\mu _2({\\mathop {}\\!\\mathrm {d}{\\mathbf {x}}}^{l})\\) .\n", "\nWe introduce the notation \\(a\\simeq b\\)  when \\(a(1+\\epsilon _a) = b (1+\\epsilon _b)\\)  with \\(|\\epsilon _a|\\ll 1\\) , \\(|\\epsilon _b| \\ll 1\\)  with high probability. And the notation \\(a\\lesssim b\\)  when \\(a(1+\\epsilon _a) \\le b (1+\\epsilon _b)\\)  with \\(|\\epsilon _a|\\ll 1\\) , \\(|\\epsilon _b| \\ll 1\\)  with high probability. From now on, we assume that the width is large, implying\n\\(\\delta {\\chi }^{l}= \\exp \\big ( \\overline{m}[{\\chi }^{l}] + \\underline{m}[{\\chi }^{l}] + \\underline{s}[{\\chi }^{l}]\\big ) \\simeq \\exp \\big ( \\overline{m}[{\\chi }^{l}]\\big ).\\) \n"]}
{"id": "1802.04765", "categories": "cs.LG cs.AI cs.RO stat.ML", "paragraphs": ["The policy network models a Gaussian distribution by outputting a state dependant mean. We use a state independent standard deviation that normalized with respect to the action space and multiplied by \\(0.1\\) .\nWe also use a version of epsilon greedy exploration where with \\(\\epsilon \\)  probability an exploration action is generated.\nFor all of our experiments we linearly anneal \\(\\epsilon \\)  from \\(0.2\\)  to \\(0.1\\)  in \\(100,000\\)  iterations and leave it from that point on.\nEach training simulation takes approximately 5 hours across 8 threads.\nFor network training we use SGD with momentum.\nDuring the distillation step we use gradually anneal the probability of selecting an expert action from 1 to 0 over \\(10,000\\)  iterations.\n"]}
{"id": "1805.09355", "categories": "cs.CL cs.LG cs.NE", "paragraphs": ["As input to the SDSN network we use 300-dimensional dependency-based word embeddings by Levy2014a.\nLayers \\(m_1\\)  and \\(m_2\\)  also have size 300 and layer \\(h\\)  has size 100.\nFor regularisation, we apply dropout to the embeddings with \\(p=0.5\\) .\nThe margin \\(R\\)  is set to 1 for the supervised pre-training stage.\nThe model is optimised using AdaDelta [28] with learning rate \\(1.0\\) .\nIn order to control for random noise, we run each experiment with 10 different random seeds and average the results.\nOur code and detailed configuration files will be made available online.http://www.marekrei.com/projects/sdsn\n"]}
{"id": "1811.08674", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["Based on the pre-training dataset, we designed an architecture for the GNN model comprising an encoder with a receptive field of 2 as described in Section\u00a0REF , obtained from the range \\([1,\\dots ,10]\\) . Validation accuracy and validation loss on the pre-training dataset used to obtain the optimal number of GNN layers is depicted in Figure\u00a0REF . Each of the MLPs, \\(g_{\\dots }(\\cdot )\\) , used in the encoder in Equations\u00a0(REF )\u2013() has two hidden layers chosen from the set \\(\\lbrace 1,2,3,4\\rbrace \\)  and the number of channels per layer parameter \\(E=8\\)  chosen from \\(\\lbrace 4,8,16,\\dots ,256\\rbrace \\) . A dropout rate of \\(0.5\\)  was used between each layer in the MLPs, chosen from the set \\(\\lbrace 0, 0.1, \\dots , 0.9\\rbrace \\) . The number of training epochs for the GNN model was set to 500. Batch size of 12 was used during training. Note that the GNN model can handle entire graphs utilising efficient sparse matrix operations and we do not require to subsample the graph as in the case of MFN model, as described in Section\u00a0REF .\n{FIGURE}"]}
{"id": "1805.01676", "categories": "cs.CL", "paragraphs": ["We built our neural machine translation (NMT) system by using Nematus [17], an open-source NMT toolkit which implements the encoder-decoder NMT architecture with attention mechanism. Our system is based on the NMT system in [16]. We built an ensemble model consisting of 4 independent models, which are the cross product of two different deep RNN architectures, i.e., deep stacked RNN and deep transition RNN, and two different recurrent unit functions, i.e., GRU and LSTM.\n", "Training for each individual model progresses by updating the model parameters at each mini-batch of 40 sentence pairs to minimize the negative log-likelihood loss function on the parallel training data. We use the Adam algorithm [7] with learning rate of \\(0.0001\\) . At each update, we clip the gradient norm to \\(1.0\\) . We apply layer normalization [0] on the model parameters for faster convergence and tie the target-side embedding with the transpose of the output weight matrix [14]. Model parameters are saved at every checkpoint of 10,000 update iterations. At this stage, the negative log-likelihood loss function on the development set is checked. Training stops when there has been no improvement over the lowest loss function value on the development set for 10 consecutive checkpoints.\n"]}
{"id": "1811.07738", "categories": "cs.CV", "paragraphs": ["The proposed architecture is implemented in PyTorch.\nWe use AdamW\u00a0[27] as optimization method with a learning rate of \\(0.001\\) .\nA combined binary cross-entropy and Jaccard loss function \\(L_{JBCE}\\)  with a weighting factor \\(w=0.3\\)  as proposed in [20] is utilized:\n\\(L_{JBCE} = L_{BCE} + w * (1-J),\\) \n", "For the DRIVE dataset we adopt the training-test split as proposed by the authors of the dataset (20 training and 20 test images)\u00a0[42]. For CHASE_DB1 we follow the suggestion of [17], where the first 8 images form the training set and the remaining 20 the test set. This split is also used in\u00a0[32].\n", "For HRF we adopt the split as proposed by Orlando \u00a0[32] and adapted in\u00a0[44], who introduced the only supervised methods that have so far been evaluated on this dataset.\nThe training set contains the first five images of each category (healthy, diabetic retinopathy and glaucoma); the remaining 30 images define the test set.\n", "No other preprocessing is conducted and during training, a set of random augmentations are applied: rotations, horizontal and vertical flips, elastic distortions and changes in brightness, contrast, saturation and hue.\nImage augmentations are commonly used in biomedical image analysis tasks, especially when working with small datasets, as they can improve accuracy and generalization\u00a0[13].\nWe limit the rotations to a range from \\(-15\\)  to \\(+15\\)  degrees. Brightness, contrast, saturation and hue are changed by a small random factor in the range \\(\\mathbb {R} \\cap [1-c,1+c]\\) , where \\(c\\)  determines the freedom of change. The following values are selected: \\(c_{brightness} =0.3\\) , \\(c_{contrast}=0.3\\) , \\(c_{saturation}=0.02\\)  and \\(c_{hue}=0.02\\) .\nElastic transformations are governed by the grid size and the magnitude, for which we selected values of \\(8\\times 8\\)  and 1 respectively\u00a0[9].\n", "Since our datasets contain a very small number of training images we derive separate validation sets from the training sets, using the same random augmentations.\nDuring training, the model with the highest dice score on the validation set is selected.\nTraining is stopped after 300 epochs.\n", "On DRIVE we use a batch-size of 4 for ERFNet and M2U-Net and a batch size of 2 for U-Net due to it's higher memory footprint.\nOn CHASE_DB1 we use a batch-size of 2 for ERFNet and M2U-Net.\nThe U-Net can not be trained on CHASE_DB1 and HRF without reverting to a patch-based training approach, due to insufficient GPU memory in our training setup (11 GB).\nSimilarly the memory requirement of ERFNet for HRF is too large for our setup.\nAs a consequence we train only M2U-Net on HRF, using a batch-size of 1.\n"]}
{"id": "1806.05559", "categories": "cs.CL cs.LG stat.ML", "paragraphs": ["The models of our approach are implemented in TensorFlow\u00a0[0]. We use a BiRNN with a single layer in each direction with 512-dimensional word embeddings and 512-dimensional recurrent states. We use LSTM as recurrent activation functions. The hidden layer of the feed-forward neural network has 256 hidden units. To train our models, we use Adam\u00a0[19] with a learning rate of 0.0002 and a minibatch of 128 examples. Models are trained for a total of 15 epochs. To avoid exploding gradients, we apply gradient clipping such that the norm of all gradients is no larger than 5\u00a0[28]. We apply dropout to prevent overfitting with a probability of 0.2 and 0.3 for the non-recurrent input and output connections respectively\u00a0[36].\n", "To train the NMT systems, we use the PyTorch implementation of OpenNMT\u00a0[20].https://github.com/OpenNMT/OpenNMT-py The NMT systems are one layer BiLSTMs with an attention mechanism\u00a0[5]. The dimensions of the word embeddings and recurrent states for the encoder and decoder are all set to 256. The systems are trained for 10 epochs with minibatch of 64 sentence pairs using SGD with an initial learning rate of 1.0 and linear decay.We could have trained NMT systems for more epochs to obtain better BLEU scores, but our objective is not to compare SMT with NMT systems. The norm of the gradient is clipped such that it is not greater than 5.\n"]}
{"id": "1803.09065", "categories": "cs.CL", "paragraphs": ["Binary vectors of 4 different sizes are produced: 64, 128, 256 and 512 bits.\nThe optimal hyperparameters are found using a grid search and selected to\nminimize the reconstruction loss and the regularization loss described in\nSection . The model uses a batch size of 75, 10 epochs for\ndict2vec and fasttext, and 5 epochs for GloVe (the autoencoder converges\nfaster due to the smaller vocabulary) and a learning rate of 0.001. The\nregularization hyperparameter \\(\\lambda _{reg}\\)  depends on the starting vectors\nand the binary vector size. It varies from 1 to 4 in the experiments but its\ninfluence on the performance is small (\\(\\sim \\) 2% variation).\n"]}
{"id": "1803.09092", "categories": "cs.CV", "paragraphs": ["In the video generation and segmentation experiments, we performed gradient-descent using ADAM, with an initial learning rate of 0.0002, \\(\\beta _1 = 0.5\\) , \\(\\beta _2 = 0.999\\)  and batch size of 16. For video generator training we used \\(\\alpha =1\\) , \\(s_{\\alpha }=0.2\\)  and \\(E_{\\alpha }=2\\)  (values set empirically).\nWe trained the video generation models for 25 epochs and the video segmentation ones for 200 epochs. For video action recognition, we used SGD with momentum and dampening of 0.9, weight decay of 0.001 and learning rate of 0.1, reduced by a factor of 10 when no improvement in validation accuracy occurred for 10 epochs. Batch size was 128 and the number of epochs was 130.\n"]}
{"id": "1804.08872", "categories": "cs.CV", "paragraphs": ["As mentioned above, we evaluated ResNet50 and InceptionV3 for the classification task.\nBoth architectures were initialized with pre-trained weights from the ImageNet dataset and trained using cross-entropy as a cost function minimized by stochastic gradient descent.\nBatch normalization was applied.\nThe initial learning rates for both architectures were set to \\(3\\cdot 10^{-5}\\)  in order to protect the pre-trained weights.\nTraining was performed with a batch size of 48.\nAnalysis of the validation accuracy showed no significant gain after five epochs, thus early stopping was applied in order to avoid overfitting\n"]}
{"id": "1805.11762", "categories": "cs.CL", "paragraphs": ["Before performing interactive adversarial learning with RL, we pretrain the dialog agent and the discriminative reward function with offline supervised learning on DSTC2 dataset. We find this being helpful in enabling the adversarial policy learning to start with a good initialization. The dialog agent is pretrained to minimize the cross-entropy losses on agent action and slot value predictions. Once we obtain a supervised training dialog agent, we simulate dialogs between the agent and the user simulator. These simulated dialogs together with the dialogs in DSTC2 dataset are then used to pretrain the discriminative reward function. We sample 500 successful dialogs as positive examples, and 500 random dialogs as negative examples in pretraining the discriminator. During dialog simulation, a dialog is marked as successful if the agent's belief tracking outputs fully match the informable\u00a0[6] user goal slot values, and all user requested slots are fulfilled. This is the same evaluation criteria as used in [28] and [14]. It is important to note that such dialog success signal is usually not available during real user interactions, unless we explicitly ask users to provide this feedback.\n", "During supervised pretraining, for the dialog agent we use LSTM with a state size of 150. Hidden layer size for the policy network MLP is set as 100. For the discriminator model, a state size of 200 is used for the bidirectional LSTM. We perform mini-batch training with batch size of 32 using Adam optimization method [9] with initial learning rate of 1e-3. Dropout (\\(p=0.5\\) ) is applied during model training to prevent the model from over-fitting. Gradient clipping threshold is set to 5.\n", "During interactive learning with adversarial RL, we set the maximum allowed number of dialog turns as 20. A simulation is force to terminated after 20 dialog turns. We update the model with every mini-batch of 25 samples. Dialog rewards are calculated by the discriminative reward function. Reward discount factor \\(\\gamma \\)  is set as 0.95. These rewards are used to update the agent model via policy gradient. At the same time, this mini-batch of simulated dialogs are used as negative examples to update the discriminator.\n"]}
{"id": "1803.07187", "categories": "cs.CV eess.IV math.NA", "paragraphs": ["For the segmentation of the the training region \\(D_1\\)  within the inpainting domain \\(D\\)  we use the activecontour MATLAB function by which the Chan-Vese algorithm can be called.\nFor this we fixed the maximum number of iterations to maxiter\\(=1000\\)  and use the default value as a tolerance on the relative error between iterates as a stopping criterion.\nWe use the default values for the parameters \\(\\mu \\)  and \\(\\nu \\)  in (REF ).\nThe subsequent labelling phase was performed by means of the standard MATLAB kmeans function after specifying a total of \\(K=35\\)  labels to assign.\nThe labelling was iteratively repeated 5 times to avoid misclassification. Once the detection of the inpainting domain is completed, in order to provide a good initialisation to the exemplar-based model we use the TV inpainting model (REF ) with (REF ) with the value \\(\\lambda =1000\\)  and a maximum number of iterations equal to maxiter2\\(=1000\\)  with a stopping criterion on the relative error between iterates depending on a default tolerance.\nFinally, we followed [34] for the implementation of the exemplar-based inpainting model: for this we specified 12 propagation of iterations and tested different size of the patches. In order to avoid memory shortage, we restricted to patches of the size \\(5\\times 5\\) , \\(7\\times 7\\)  and \\(9\\times 9\\) .\n"]}
{"id": "1812.04180", "categories": "cs.CV", "paragraphs": ["For ResNet, we kept the same training schedule as AIG[48], and follow the standard ResNet training procedure: batch size of 256, momentum of \\(0.9\\) , and weight decay of \\(10^{-4}\\) . For the weight decay for gate parameters, we use \\(\\frac{20}{|\\mathcal {G}|}\\cdot 10^{-4}\\) . We train for 100 epochs from a pretrained model of the appropriate architecture with step-wise learning rate starting at \\(0.1\\) , and decay by \\(0.1\\)  after every 30 epochs. This is the same training schedule as [48]. We use standard training data-augmentation (random resize crop to 224, random horizontal flip) and standard validation (resize the images to \\(256\\times 256\\)  followed by a \\(224\\times 224\\)  center crop).\n"]}
{"id": "1002.0773", "categories": "cs.CL", "paragraphs": ["This Section presents further analysis of the results in Section\u00a0REF . We will develop a simple framework that we will use to examine what happens to the model parameters after 100 iterations of extended Baum-Welch. We shall demonstrate that extended Baum-Welch expands the space that the model means occupy and that this expansion appears to be consistent with steady decrease in the sequences \\((\\operatorname{num}(x, w ; \\theta _k, \\kappa , \\mathcal {V}_{mle}, \\mathcal {R}_{mle}))\\)  and \\((\\operatorname{den}(x, w ; \\theta _k, \\kappa , \\mathcal {V}_{mle}, \\mathcal {R}_{mle}))\\)  that we saw in Section\u00a0REF . We also show that extended Baum-Welch appears to be shrinking the model variances, but not in a dramatic way.\n", "This is called the Mahalanobis distance. The set of points in \\(\\mathbb {R}^d\\)  satisfying \\(d_{\\Sigma ^{-1}}(x, 0) = 1\\)  is an ellipsoid\n\\(Ell(\\Sigma ^{-1}) \\equiv \\lbrace x \\in \\mathbb {R}^d : x^t \\Sigma ^{-1} x = 1\\rbrace ,\\) \n", "Then the collection of vectors \\(\\lbrace \\sqrt{\\lambda _i} u_i\\rbrace _{i=1}^d\\)  are the semi-principal axes of \\(Ell(\\Sigma ^{-1})\\) . The ratio \\(c (\\Sigma ) \\equiv \\sqrt{\\lambda _d / \\lambda _1}\\)  gives a sense of how elongated the ellipsoid \\(Ell(\\Sigma ^{-1})\\)  is: \\(\\sqrt{\\lambda _d / \\lambda _1} \\ge 1 \\)  with equality \\(\\iff \\)  \\(Ell(\\Sigma ^{-1}) = S^{d-1}\\) .The quantity \\(c^2\\)  is the condition number of the matrix \\(\\Sigma \\) . Also \\(\\sqrt{1 - 1 / c^2}\\)  is the eccentricity of the two-dimensional ellipse formed by the intersection of \\(Ell(\\Sigma ^{-1})\\)  with the plane through the origin spanned by \\(u_1\\)  and \\(u_d\\) .\n", "First we examine the scatter of the collection of the 1500 state means in \\(\\mathbb {R}^{39}\\) . We treat these means as points in \\(\\mathbb {R}^{39}\\)  and compute the total mean vector (or centroid)\n\\(\\bar{\\mu } = \\frac{\\sum _{j=1}^{1500} \\mu _j}{1500},\\) \n", "For each state \\(j\\)  let \\(\\sigma _{mle, j}\\)  and \\(\\sigma _{mmi, j}\\)  denote the vectors of standard deviations for state \\(j\\)  before and after 100 iterations of extended Baum-Welch, and define\n\\(V_j \\equiv \\log \\frac{\\prod _{i=1}^{39}\\sigma _{mmi, j, i} }{\\prod _{i=1}^{39}\\sigma _{mle, j, i} }.\\) \n"]}
{"id": "2207.10213", "categories": "cs.CV", "paragraphs": ["We train E2E-Spot using 100 frame long clips by default and a batch size of 8 clips.\nBatches are formed by randomly sampling clips from the training videos.\nWe group every 625 training steps into a training cycle (i.e., a pseudo-epoch of 500K frames).\nA single cycle runs in approximately 8.5 and 14 minutes on a single A5000 GPU\u00a0[43] for the 200MF and 800MF variants, respectively.\nAll variations of E2E-Spot are trained for 50 cycles on the Tennis, Figure Skating, and FineDiving datasets.\nWe train the 200MF model for 100 cycles and the 800MF model for 150 cycles on FineGym and SoccerNet-v2, due to the larger dataset sizes (see\u00a0sec:suppdataset).\nTraining is performed with AdamW\u00a0[42], setting a base learning rate of \\(10^{-3}\\) , with 3 linear warmup cycles followed by cosine decay\u00a0[41].\n", "We train the TCN, MS-TCN, GRU, ASFormer, and GCN models on randomly sampled, 500 frame long clips \u2014 with a batch size of 50, a train-val cycle of 40 steps (1M frames), and for 50 cycles.\nUpdates are performed using AdamW\u00a0[42] with a base learning rate of \\(10^{-3}\\) , linear warmup (3 cycles), and cosine annealing\u00a0[41].\nThe StridedTransformer and NetVLAD++\u00a0[22] baselines make singular predictions on a window of frames.\nWe train these with a batch size of 100 clips, train-val cycles of 1,000 steps, and for 50 cycles.\nWe use the same AdamW\u00a0[42] optimizer and LR schedule as the other models.\nValidation mAP, computed at the end of every training cycle, is used for model selection.\n"]}
{"id": "2205.05128", "categories": "cs.CL cs.LG", "paragraphs": ["We use 2.4447e-4 as the learning rate for training HaRT, with 1 user train batch size, 15 users eval batch size and early stopping patience set to 3. For GPT-2HLC, we use the default settings from [47] with train and eval batch size set to 60 and early stopping patience set to 3.\n"]}
{"id": "2212.10766", "categories": "cs.CV", "paragraphs": ["\nCIFAR-10 and CIFAR-100. For all the experiments on CIFAR, we train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 128. The network is trained for 450 epochs. We set the initial learning rate as 0.02, and reduce it by a factor of 10 after 225 epochs. The warm up period for the DNN is 10 epochs. The weight \\(\\lambda ^{(\\mathcal {U^{\\prime }})}\\)  is set to \\(\\lbrace 0,25,50,150\\rbrace \\)  as in DivideMix.\n\nClthing1M. We train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.001, and a batch size of 32. The model is trained for 80 epochs. The warm up period for the DNN is 1 epoch. The initial learning rate is set as 0.002 and reduced by a factor of 10 after 40 epochs. For each epoch, we sample 1000 mini-batches from the training data. The weight \\(\\lambda ^{(\\mathcal {U^{\\prime }})}\\)  is set to 0.\n\nWebVision. We train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.001, and a batch size of 32. The model is trained for 100 epochs. The warm up period for the DNN is 1 epoch. The initial learning rate is set as 0.01 and reduced by a factor of 10 after 50 epochs. For each epoch, we sample 1000 mini-batches from the training data. The weight \\(\\lambda ^{(\\mathcal {U^{\\prime }})}\\)  is set to 0.\n\n"]}
{"id": "2211.07277", "categories": "cs.CV cs.LG", "paragraphs": ["We train convolutional neural networks (CNNs) and Vision Transformers (ViTs) using our methodology. Among the CNN models, similar to [16], we show results on ResNet50, ResNet101, and ResNet152. For training the ResNet models, we supplemented ImageNet data with an equal number of augmented images. We trained them for 100 epochs with a starting learning rate of \\(0.2\\)  which is reduced by a factor of 10 at the 30th, 60th and 90th epoch. Similar to TSD, while training the ResNet models, we also use auxiliary batch norm\u00a0[23]. As ViTs are compute-intensive, we finetune ImageNet pretrained ViT models for 20k steps with a cosine learning rate schedule with a starting learning rate of \\(0.01\\) . The stochastic gradient descent (SGD) with a momentum of \\(0.9\\)  is used to train the models. We train all our models on 8 A100 GPUs with a batch size of 512 for ResNets, 256 for ViT-Small and ViT-Base and 128 for ViT-Large. We find that values \\(\\alpha =4\\) , \\(\\beta =1\\) , and \\(\\eta =0.65\\)  produce the best results for all models.\n"]}
{"id": "2212.10179", "categories": "cs.CL", "paragraphs": ["We set \\(k=10\\)  when applying the top-\\(k\\)  sampling method to find candidate tokens. For a fair comparison, we set the batch size to 4 for all experiments.\n"]}
{"id": "2209.11379", "categories": "cs.LG cs.AI", "paragraphs": ["Models are trained using Adam optimizer [13] with a fixed batch-size of 1024. En\\(\\rightarrow \\) {Zh, Fr} models are trained for 530038 steps while the rest of models (due to smaller training data size) are trained for 397529 steps. For all the runs, we use 40K steps of linear warm-up and then use a learning rate schedule of the form\n\\(\\frac{\\eta }{\\sqrt{t}}, \\qquad \\eta : \\mbox{base learning rate,} \\qquad t: \\mbox{training step.}\\) \n", "\nFor each model run, we sweep for \\(\\eta \\)  in the grid \\(\\lbrace 0.05, 0.1, 0.5, 1.0, 2.5, 5.0\\rbrace \\) . Often times, \\(\\eta = 0.5\\)  yields the optimal performance and \\(\\eta = 5.0\\)  diverges. For sampling experiments, we sweep the rate for En\\(\\rightarrow \\) Fr in the grid \\(\\lbrace i / 10 \\rbrace _{i=1}^9\\) . This determines the rate for the other language pair automatically. As such, to derive each scalarization front, we train a total of 54 models.\n", "When examining the generalization performance (left hand side of Figures REF , REF , and REF ) we use early stopping: we evaluate the model every 5000 steps and use the step that yields the smallest average validation loss for the two tasks. For En\\(\\rightarrow \\) {Zh, Fr} and En\\(\\rightarrow \\) {De, Fr} models, it is often the case that the final step is the optimal step. As such early stopping doesn't significantly change the picture. For En\\(\\rightarrow \\) {Ro, Fr}, performance statistics change noticeably with early stopping but the overall qualitative picture remains the same. For the training performance (right hand side of Figures REF , REF , and REF ) we report the final step training statistics.\n"]}
{"id": "2210.10246", "categories": "cs.LG cs.PF", "paragraphs": ["We also evaluate on other hardware platforms as well as model parameters. First, we use an increased hidden layer size for various configurations. These experiments are conducted on a platform with an NVIDIA A100 GPU [42] across sequence lengths of 128 and 512. We maintain the hidden layer size \\(H\\)  to the number of attention heads \\(A\\)  ratio of 64 which is in line with prior works [62], [11]. The results are shown in Figure REF . The figure demonstrates two important generalizations of Tempo. First, note that even on newer and more advanced GPUs, Tempo continues to provide a tangible benefit. Second, across larger hidden layer sizes Tempo consistently demonstrates a clear improvement over the baseline (as shown in the figure, this can be as high as a 39% speedup over Baseline which corresponds to a 16% speedup over Checkpoint). The speedup over Checkpoint is as high as 20% . We conclude that Tempo will continue to be applicable to new hardware and larger models.\n{FIGURE}", "We also conduct experiments on BERT\\(_{LARGE}\\)  (modified to use 12 Layers instead of 24 for more data points) for sequence lengths larger than 512. Figure\u00a0REF  shows the results for this experiment, where we demonstrate that Tempo outperforms Baseline on longer sequence lengths as well which can be as high as a 27% speedup over Baseline. At the same settings, we observe 16% speedup over Checkpoint. Tempo also outperforms Checkpoint by as much as 20%. We conclude that yet again Tempo will be able to take advantage of modern hardware, as well as remain advantageous as sequence lengths increase. Note that the largest sequence length of 3072 on Baseline does not have enough memory to run.\n{FIGURE}"]}
{"id": "2202.06725", "categories": "cs.LG cs.AI", "paragraphs": ["We train our model for 800k steps on the provided training data on a standard MSE loss using the ADAM optimizer [5]. The learning rate schedule contains 2k steps of warm-up. After warm-up, the learning rate is \\(0.002\\)  and decays from there exponentially at a rate of \\(0.98\\)  every 100 steps. The minimal learning rate is \\(0.0002\\) . At each step, we sample valid starting frames for the seed sequences (model input), which are frames that correspond to a time between 00:00 and 22:00 o'clock. We take the average over the gradient of 16 successive samples to update the model parameters. This effectively corresponds to taking a batch size of 16. We submitted the exact same model to both competitions. Hence, we consider the temporal generalization problem from the core challenge as a kind of spatial generalization problem as well. This is possible as data from 2020 is included in the training set, just for different cities than used in the evaluation.\n"]}
{"id": "2210.00430", "categories": "cs.LG cs.CR", "paragraphs": ["On MNIST, we use LeNet5 for the classifier and 2 layers MLP (with hidden size 256 and 784) for the encoder and decoder of conditional VAE. For standard training of the classifier, we use 30 epochs, batch size 128, learning rate \\(10^{-3}\\) , and weight decay \\(5\\times 10^{-4}\\) . For the CVAE, we use 20 epochs, learning rate \\(10^{-3}\\) , batch size 64, and latent size 10. For standard adversarial training and, we use \\(\\varepsilon =0.3\\)  for FGSM and PGD. in PGD, we use 40 steps for the inner part. Adversarial training start after 10 epochs standard training. For generative adversarial training, we use \\(\\varepsilon =1\\)  in the latent space with FGSM and PGD. We use 40 steps PGD for latent space adversarial training. Adversarial training start after 10 epoches standard training. In the attack part, we use the same \\(\\varepsilon \\)  as the adversarial training part.\n", "On CIFAR-10, we use ResNet32 for the classifier and 4 layers CNN for the encoder and decoder of conditional VAE. For standard training of the classifier, we use 200 epochs, batch size 128. The learning rate schedule is 0.1 for the first 100 epochs, 0.01 for the following 50 epochs, and 0.001 for the last 50 epochs. The weight decay is \\(5\\times 10^{-4}\\) . For the CVAE, we use 200 epochs, learning rate \\(10^{-3}\\) , batch size 64, and latent size 128.\n", "For standard adversarial training, we use \\(\\varepsilon =8/255\\)  for FGSM and PGD. in PGD, we use 10 steps for the inner part. For generative adversarial training, we use \\(\\varepsilon =0.1\\)  in the latent space with FGSM and PGD. Since we see that the modeling power of VAE in CIFAR10 is not good enough. For each of the image, the encode variance is very small. When we add a small perturbation to the encode mean value, the output image are blured. Hence we only use a small \\(\\varepsilon =0.1\\) . In the attack part, we use \\(\\varepsilon =8/255\\)  for norm-based attacks and \\(\\varepsilon =0.1\\)  for generative attack on the test set.\n"]}
{"id": "2201.04684", "categories": "cs.CV", "paragraphs": ["For all the baselines and our methods, we use DeepLab-v3\u00a0[8] with Resnet-50 [29] as the image backbone model. We use the SGD optimizer with learning rate \\(0.01\\) , momentum \\(0.9\\)  and weight-decay \\(0.0001\\) . We use polynomial learning rate decay with power \\(0.9\\) . We use batch size 64 for all tasks and train for 200 epochs. For augmentation, we use random resize with scales from \\(0.5-2.0\\) . random crop and random horizontal filp. We use resolution 224 for both training and testing. When training with our synthetic dataset, we use the same training schedule and augmentation policy as described here.\n"]}
{"id": "2211.05295", "categories": "cs.CV cs.LG eess.IV", "paragraphs": ["For hardware, a Linux station composed of I7 12700K CPU and two 24G RTX3090Ti GPUs is used in this research for training. Pytorch is used as framework. We use stochastic gradient descent as the optimizer with the initial learning rate \\(0.01\\) , the momentum \\(0.9\\)  and batch size 32. The experiments set a fixed random seed, and all training datasets are augmented (flip, contrast, brightness, saturation).\nThe models are trained 50 epochs, and every 5 epochs the validation datasets is tested and \\(IoU\\) , \\(PA\\)  and \\(OII\\)  are estimated.\nAll following experiments are performed with the same configuration.\n"]}
{"id": "2211.02077", "categories": "cs.CV cs.AI cs.LG cs.MM", "paragraphs": ["Network Architecture: For all of our experiments, we use modality-agnostic Transformer variant in [10], VATT-MA-Medium. Specifically, we use a 12-layer Transformer, in which each layer has a feed-forward MLP with hidden size of 4096, and 16 attention heads with a hidden size of 1024.\n", "Pre-training Hyperparameter: We strictly follow the setting in [10], pre-training VATT from scratch with Adam optimizer with an initial learning rate of 1e-4, 10k warmup steps, 500k steps in total, a batch size of 2048\nand using a cosine learning rate scheduler to anneal the learning rate from 1e-4 to 5e-5. Our framework is implemented in Tensorflow 2.8, and train with 256 TPUV3s, it took a total of 3 days to train our models.\n"]}
{"id": "2211.02100", "categories": "cs.LG cs.AI", "paragraphs": ["When pretraining\u00a0CVL, we first optimize the critic on unlabeled data from dataset for all the semantically related tasks, i.e. tasks which belong to the same domain, and then finetune both the critic and the policy on reward-labeled data from the target task. Semantically related tasks in MetaWorld are easily identifiable by their domain name, e.g. drawer-open and drawer-close belong to the drawer domain. We use a similar approach when pretraining CQL+UDS, where we perform TD updates with all rewards equal to 0 during the pretraining phase.\n"]}
{"id": "2211.07371", "categories": "cs.CV", "paragraphs": ["The network architecture of the encoder model is ResNet-50 [18], which is one of the widely used architectures in recent SOTA FR [13], [0], [22], [3]. Following [17], the momentum encoder is updated with a momentum coefficient of 0.999 [17] and the temperature value \\(\\tau \\)  of contrastive loss is set to 0.07 [17]. The feature representation dimensions is initially set to 512-D in the results presented in Sections REF , REF , REF .\nLater, in Section REF  we present an ablation study on the optimal feature representation dimensions of 128, 256, 512, and 1024-D. The queue size is set to 32768 based on sensitivity study presented in Section REF .\nAn optimizer Stochastic Gradient Descent (SGD) is used with initial learning rate of 0.1. The momentum is set to 0.9 and the weight decay to 5e-4. The learning rate is divided by 10 after 8, 16, 24, and 32 epochs. The models presented in Sections REF , REF , REF , REF  are trained for 40 epochs in total with a batch size of 512 on 100K synthetic images.\nAll models are implemented using PyTorch [27] and trained on two CPU 16 core Intel Xeon Gold 5218 and four NVIDIA Quadro RTX6000 GPUs.\n{FIGURE}"]}
{"id": "2211.04198", "categories": "cs.CL", "paragraphs": ["We fine-tune XLM and mBERT models for 10 epochs over the parallel fine-tuning corpus for each language pair, with a batch size of 8. We use AdamW[23] with learning rate of 1e-5. The dropout rate is set to 0.1. The training process typically takes 2 to 3 hours. The hyper-parameters are tuned based on the development set performances. Regarding the threshold \\(c\\)  in the word alignment prediction, it is set to 1e-6 for Ro-En and 0.1 for the others. Regarding the hyper-parameters in integrating the various third-party supervisions, \\(f\\)  is set to 0.45 and \\(\\lambda \\)  is set to 0.5 for all language pairs.\n"]}
{"id": "2209.15270", "categories": "cs.CV", "paragraphs": ["We build our Chinese model following the similar settings except initializing the textual encoder with ERNIE [32], [33], [34] and training with a total batch size of 23200. Besides, we explore another visual backbone widely used in VLP (i.e., Vision Transformer (ViT) [35]) as our image encoder (the details in Appendix  ).\n"]}
{"id": "2209.15301", "categories": "cs.CL", "paragraphs": ["We adopt the BART encoder-decoder model [23], as it set a state of the art in abstractive summarization benchmarks. We train our model using the HuggingFace implementation [36], on a learning rate of \\(2 \\cdot 10^{-6}\\) . The question matching pool retrieved by TF-IDF is comprised of \\(k = 32\\)  knowledge base FAQs. Our answer selection loss \\(\\mathcal {L}_\\mathrm {sel}\\)  is optimized to select up to \\(n = 3\\)  sentences. We use \\(\\lambda = 0.01\\)  and \\(\\gamma = 0.01\\)  as weights for the self-supervised losses. The BART encoder is used for embedding sentences for question matching and answer selection.\n", "We train for 50 epochs for MeQSum, and 20 epochs for HealthCareMagic. Each training epoch takes about 10 minutes for MeQSum, and about 35 minutes for HealthCareMagic. Inference takes 1 minute for the MeQSum test set and 3 minutes for the HealthCareMagic test set. The best checkpoint is selected based on the lowest loss value \\(\\mathcal {L}\\)  on the dev set.\n"]}
{"id": "2208.09694", "categories": "cs.CV cs.LG", "paragraphs": ["In all settings, we train segmentation networks from ImageNet pre-trained weights using stochastic gradient descent (SGD) with the momentum (0.9) and the weight decay (0.0001) with batch size 8. The learning rate is initialized to be 0.01 and is multiplied by \\((1-\\frac{iter}{max\\_iter})^{0.9}\\) . For the number of training steps, we sweep over {80K, 160K, 320K, 640K, 1280K} as the optimal iteration number depends on training strategies. We apply standard augmentations, including random scaling (from 0.5 to 2.0) and random horizontal flipping, except for the Distill-Aux setting, where we have massive data and do not need to care about overfitting.\nIn all settings, patches of size 512 \u00d7 1024 are randomly cropped from (possibly augmented) images.\n{FIGURE}", "Similar to semantic segmentation, we train detection networks from ImageNet pre-trained weights using stochastic gradient descent (SGD) with the momentum (0.9) and the weight decay (0.0001) with batch size 32. The learning rate is initialized to be 0.01 and is multiplied by \\((1-\\frac{iter}{max\\_iter})^{0.9}\\) . For the number of training steps, we sweep over {80K, 160K, 320K, 640K}. We apply standard augmentations, including random scaling (from 0.5 to 2.0) and random horizontal flipping, except for the Distill-Aux setting as in semantic segmentation.\n"]}
{"id": "2208.01762", "categories": "cs.CV", "paragraphs": ["We follow previous works\u00a0[5], [46], [11], [61] and train our model on the conventional training set which contains 1,485 samples from the NJU2K-train\u00a0[14] and 700 samples from the NLPR-train\u00a0[31].\nFor testing benchmarks, we observe that the depth quality within each dataset varies, which is mainly due to acquisition methods. Specifically, DES [2] contains 135 images of indoor scenes captured by a Kinect camera. SIP [4] provides a human dataset that contains 929 images captured by a mobile device. Therefore, these two datasets can be considered moderate with less noisy depths.\n"]}
{"id": "2210.07365", "categories": "cs.CL", "paragraphs": ["We finetune our models on the TweetEval (Section\u00a0REF ) and ToxiGen (Section\u00a0REF ) datasets.\nWe use the same training budget and hyperparameter setup for all the models.\nWe retrieve pretrained models and tokenizers from the HuggingFace Hub [28]. We set a maximum sequence length of 256, batch size of 32, and peak learning rate of 0.00002 with linear warmup scheduling, increasing it during 10% of the total training steps. On ToxiGen, we train for a maximum of 3 epochs, on TweetEval, for a maximum of 10. We evaluate every 500 steps and use the checkpoint with the best validation loss for testing.\nFor TweetEval, we repeat experiments on 5 initialization seeds.\n"]}
{"id": "2208.12343", "categories": "cs.CV eess.IV", "paragraphs": ["In the first, \u201cpretraining\u201d stage, we use a weighted sum of L1 loss, SSIM loss, and our three aforementioned \u201cBokeh\u201d losses, using the depth map as a greyscale mask to separate the foreground from the background. The final loss-function for this stage was given by\n\\(L_{pretrain} = 0.5 \\times L_{1} &+ 0.05 \\times L_{SSIM} + 0.005 \\times L_{edgediff} \\\\&+ 0.1 \\times L_{backblur} + 0.005 \\times L_{foreedge} \\nonumber \\) \n", "In the second stage, the model is finetuned with a weighted sum of L1 loss, SSIM loss, VGG perceptual loss, and an adversarial loss from the dual-scale PatchGAN discriminator that is trained jointly in the second stage using the WGGAN-GP method [8]. This second stage refines the distorted edges and the background to generate a realistic bokeh effect. The refinement loss thus took the form\n\\(L_{refinement} = 0.5 \\times L_{1} &+ 0.1 \\times L_{VGG} + 0.05 \\times L_{SSIM} + L_{adv}\\) \n"]}
{"id": "2201.13063", "categories": "cs.CV cs.AI cs.GR", "paragraphs": ["We found it beneficial to use one-cyclic learning rate scheduling, following recommendations of\u00a0[38], with the maximum learning rate of 0.002. We train all models for 350 epochs with Adam optimizer\u00a0[17] and batch size of\u00a030 with early stopping enabled for when the model does not improve for consecutive 100 epochs. The training pipeline is implemented with PyTorch\u00a0[31], PyG\u00a0[10], and Weights and Biases\u00a0[3].\n"]}
{"id": "2205.01068", "categories": "cs.CL cs.LG", "paragraphs": ["For weight initialization, we follow the same settings provided in the Megatron-LM codebase,https://github.com/NVIDIA/Megatron-LM/blob/main/examples/pretrain_gpt3_175B.sh using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a \\(1.0/\\sqrt{2 L}\\)  term where \\(L\\)  is the total number of layers. All bias terms are initialized as 0, and all models are trained with ReLU activation and a sequence length of 2048.\n", "We use a dropout of 0.1 throughout, but we do not apply any dropout to embeddings. We clip gradient norms at 1.0, except for some mid-flight changes that reduce this threshold down from 1.0 to 0.3 (see Section\u00a0REF ). We also include a gradient predivide factor to reduce the risk of over/underflows when computing the gradient across all ranks (splitting the division by the world size of \\(N\\)  into two division operations by \\(\\sqrt{N}\\) ).\n"]}
{"id": "2204.04213", "categories": "cs.LG cs.AI q-bio.QM", "paragraphs": ["Datasets.\nWe merge two datasets from the Deeploc dataset [0] and the Enzyme dataset\u00a0[10], and perform pretraining on the merged dataset.\nFor the Deeploc dataset, we acquire the available protein structures from the alphafold protein database\u00a0https://alphafold.ebi.ac.uk/.\nFor the Enzyme dataset, we use PDB files in the Protein Data Bank\u00a0[3] to obtain protein structures.\nBesides, we exclude proteins with a sequence length of more than 400 residues, which results in a total of around 40,000 protein structures for pretraining.\n", "Training details.\nFor the GNN model, we set the dimension of hidden representation as 1280 and the layer number as 2 in our experiments.\nThe threshold to determine whether there is an edge between two residues is set as 7 \u00c5\u00a0which is consistent with previous study [23].\nWe parameterize both \\(\\rm {NN}_{dis}(\\cdot )\\)  and \\(\\rm {NN}_{ang}(\\cdot )\\)  as two fully-connected layers with a ReLU activation in the middle.\nFor \\(\\rm {NN}_{dis}(\\cdot )\\) , we set \\(T\\) =30 and apply softmax to the output logits.\nBesides, we adopt the available protein BERT model in [7] as the pretrained protein language model.\nWe use the cosine learning rate decay schedule for a total of 10 epochs for pretraining.\nWe set the learning rate for the GNN model as \\(0.001\\)  and the learning rate for the protein LM as \\(0.0001\\)  in the pseudo bi-level optimization scheme.\nThe Adam optimizer is adopted to update the GNN parameters with \\(\\beta _1 = 0.9\\)  and \\(\\beta _2 = 0.999\\) .\n"]}
{"id": "2204.04292", "categories": "cs.LG", "paragraphs": ["Meta training details\u00a0\u00a0\u00a0The population is 1,000 individuals and the maximum graph size is 60 nodes. All are initialized using SAC as a warm-start (see Appendix ). For the RL algorithm evaluation, we use 10 different seeds \\(S_{train}\\) , fix the number of evaluation episodes \\(N_{eval}\\)  to 20, and normalize all fitness scores to the range [0, 1]. We set \\(\\kappa = 1\\)  in Equation\u00a0REF . Additional details are in Appendix .\n"]}
{"id": "2202.11490", "categories": "cs.LG cs.DC eess.SP", "paragraphs": ["We apply a batch size of 32. The initial learning rate is \\(0.005\\)  and decays under cosine rule. When training SuperNet, we use Adam to optimize \\(\\alpha \\)  and momentum SGD to optimize \\(\\mathbf {w}\\) . The weight decay of \\(\\mathbf {w}\\)  is \\(4\\times 10^{-5}\\) , while we do not use weight decay for \\(\\alpha \\) .\n", "We apply a batch size of 128. The initial learning rate is \\(0.025\\)  and decays under cosine rule. When training SuperNet, we use Adam to optimize \\(\\alpha \\)  and momentum SGD to optimize \\(\\mathbf {w}\\) . The weight decay of \\(\\mathbf {w}\\)  is \\(3\\times 10^{-4}\\) , while we do not use weight decay for \\(\\alpha \\) .\n", "We moved the FDNAS normal net to ImageNet for training to evaluate the generalization performance on larger image classification tasks. Following the general mobile setting\u00a0[14], we set the input image size to \\(224\\times 224\\) . We set the FDNAS normal net's Layer 1, 3, 6, 8, and 16 as the downsampling layers. The model's FLOPs are constrained to below 600M.\nWe use an SGD optimizer with a momentum of 0.9. The initial learning rate is 0.4 and decays to 0 by the cosine decay rule. Then the weight decay is kept consistent at \\(4\\times 10^{-5}\\) . The dropout rate is 0.2.\n"]}
{"id": "2211.09117", "categories": "cs.CV", "paragraphs": ["We set the input image resolution as 256x256 to be consistent with previous generative models. After passing through the VQGAN tokenizer, the token sequence length is 16x16 (256 tokens). Following MAE [25], we use strong random crop and resize (0.2 to 1) and random flipping as our default augmentations. We also trained models with a weaker version of random crop and resize (range from 0.8 to 1), which we call \u201cw.a.\" in the results. We pre-train base- and large-size vision Transformers [19], i.e., ViT-B and ViT-L, respectively. We use AdamW to train the model for 1600 epochs with batch size of 4096 for ViT-B, and batch size of 2048 for ViT-L. We use a cosine learning rate schedule with an 80-epoch warmup. The base learning rate is \\(1.5\\times 10^{-4}\\)  for both ViT-B and ViT-L, and is further scaled by batchsize/256. More details are in the Appendix.\n"]}
{"id": "2207.14287", "categories": "cs.CV cs.LG", "paragraphs": ["We implemented our models using PyTorch,\nwith distributed training across eight A100 GPUs. We used grid search to choose training parameters, that include: view synthesis weight \\(\\lambda _s = 5.0\\) , virtual camera loss weight \\(\\lambda _v=0.5\\) , virtual camera projection noise \\(\\sigma _v = 0.25\\) , canonical jittering noise \\(\\sigma _t=\\sigma _r=0.1\\) , and batch size \\(b=32\\)  (4 per GPU). We use the AdamW optimizer\u00a0[28], with standard parameters \\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\) , a weight decay of \\(w=10^{-4}\\) , and an initial learning rate of \\(lr=2 \\cdot 10^{-4}\\) . For our stereo experiments, we train for 200 epochs, halving the learning rate every 80 epochs. For our video experiments, we train for 100 epochs, halving the learning rate every 40 epochs. Higher-resolution fine-tuning is performed for 50 epochs for stereo experiments, and 10 epochs for video experiments, with \\(lr=2 \\cdot 10^{-5}\\) .\n"]}
{"id": "2203.12574", "categories": "cs.CL cs.LG", "paragraphs": ["We use GPT2\u2013small, a 12 layer transformer-based LM comprising of \\(\\sim \\) 124M parameters, as the teacher model and\na six-layer version of GPT\u20132 as the student model.\nWe use OpenWebText corpus, which is an open-source reproduction of WebText corpus that was used to train GPT\u20132 in [36]. Due to limitations in computational budget,\nwe use 10% of the corpus for training.\nWe used the knowledge distillation procedure presented in \u00a0[38], but without the cosine loss between representations during knowledge transfer because adopting knowledge distillation for fair learning requires correcting the `biased knowledge' from the teacher, but it is hard to amend biased contextual representations. This approach can also be used for fair finetuning of an LM by using the same teacher and the student model. In that case, one may initialize with the pre-trained teacher's weights. For fair finetuning experiments, we use GPT2\u2013small as both the teacher and the student. Details on training, text generation, and hyperparameters are provided in Appendix\u00a0.\n"]}
{"id": "2203.12485", "categories": "cs.CV", "paragraphs": ["To aid reproducibility, we report training parameters and hyperparameters. We use identical training parameters and align with our baseline\u00a0[21] where possible. We use the Ranger optimiser yong2020gradient and batch sizes of 8, a learning rate of \\(1e{-}4\\)  with an exponential learning rate decay. We train all considered methods for 50 epochs.\n"]}
{"id": "2204.11370", "categories": "cs.CV cs.AI cs.RO", "paragraphs": ["The neural networks have been trained with an RMSprop optimizer and with a learning rate of \\(10^{-3}\\) . The selected minibatch size was 128. We set a limit of training of 4000 episodes for the scenario, and the replay memory has size 30000. The limit was defined empirically since good results could be observed around these values.\n"]}
{"id": "2206.02593", "categories": "cs.LG cs.AI cs.IR", "paragraphs": ["Cascade Model: In CM (sec:cascade model), the value of a list depends only on the attraction probabilities of its items. Let \\(L_{\\theta , a}\\)  be the LCB on the attraction probability of item \\(a\\) , that is \\(\\theta _a \\ge L_{\\theta , a}\\) . If we apply LCB on attraction probabilities \\(\\theta \\) , then\n\\(\\displaystyle 1 - \\prod _{k = 1}^K (1 - L_{\\theta , A_k})\\le 1 - \\prod _{k=1}^K (1 - \\theta _{A_k})\\) \nand we get the LCB for the whole list \\(A\\)  as we have an upper bound on each term in the product. Suppose that we have LCBs for all model parameters. Then we have an LCB for any fixed list. Now suppose that we have \\(1 - \\delta \\)  LCBs for each model parameter. Then, by the union bound, all model parameter LCBs hold jointly with probability at least \\(1 - \\delta \\left|\\mathcal {E}\\right|\\) , and so do the LCBs for all lists.\n", "Dependent-Click Model: Apart from attraction probabilities \\(\\theta \\) , DCM (sec:dependent-click model) has position parameters \\(\\lambda \\)  expressing the probability that a user continues exploring further down the list after a click. Let \\(L_{\\lambda , k}\\)  be LCB on the probability that satisfactory click occurred on position \\(k\\) , that is \\(1 - \\lambda _k \\ge L_{\\lambda , k}\\) . If we apply LCB on attraction probabilities \\(\\theta \\)  and position probabilities \\(\\lambda \\) , then\n\\(\\displaystyle 1 - \\prod _{k = 1}^K (1 - L_{\\lambda ,k}L_{\\theta , A_k})\\le 1 - \\prod _{k=1}^K (1 - (1 - \\lambda _k)\\theta _{A_k})\\) \nand we get the LCB for the whole list \\(A\\)  as we have an upper bound on each term in the product. Now suppose that we have \\(1 - \\delta \\)  LCBs for each model parameter. Then, by the union bound, all model parameter LCBs hold jointly with probability at least \\(1 - \\delta (\\left|\\mathcal {E}\\right| + K)\\) , and so do the LCBs for all lists.\n", "Position-Based Model: Let \\(L_{p, k}\\)  be the LCB on the examination probability of position \\(k\\) , that is \\(p_k \\ge L_{p, k}\\) . If we apply LCB on attraction probabilities \\(\\theta \\)  and position probabilities \\(p\\)  in PBM (sec:position-based model), then\n\\(\\displaystyle \\sum _{k=1}^KL_{p,k}L_{\\theta , A_k}\\le \\sum _{k=1}^Kp_k\\theta _{A_k}\\) \nand we get the LCB for the whole list \\(A\\)  as we have a lower bound on each term in the sum. Now suppose that we have \\(1 - \\delta \\)  LCBs for each model parameter. Then, by the union bound, all model parameter LCBs hold jointly with probability at least \\(1 - \\delta (\\left|\\mathcal {E}\\right| + K)\\) , and so do the LCBs for all lists. The single-variable LCBs are presented in section:lower confidence bounds. All variables are means of Bernoulli random variables, which we use in the derivations.\n", "Bayesian Lower Confidence Bounds: Let \\(Z\\)  be the mean of a Bernoulli random variable and \\(z\\)  be its value. Let \\(Y_1, \\dots , Y_n\\)  be \\(n\\)  i.i.d. observations of \\(Z\\) . We define \\(n_+ = \\sum _{i = 1}^n Y_i\\)  and \\(n_- = n - n_+\\) . In the Bayesian setting, we make an additional assumption that \\(Z \\sim \\mathrm {Beta}(\\alpha , \\beta )\\) . By definition, \\(Z \\mid Y_1, \\dots , Y_n \\sim \\mathrm {Beta}(\\alpha + n_+, \\beta + n_-)\\) . Thus \\(1 - \\delta \\)  LCB on \\(Z\\)  is\n\\(L_Z = \\max \\left\\lbrace \\ell \\in [0, 1]: \\int _{z = 0}^\\ell \\mathrm {Beta}(z; \\alpha + n_+, \\beta + n_-) \\mathop {}\\!\\mathrm {d}z \\le \\delta \\right\\rbrace \\,.\\) \n", "Frequentist Lower Confidence Bounds: If the prior on \\(Z\\)  is unknown or poorly estimated, Bayesian estimates could be biased. In this case, Hoeffding's inequality would be preferred, since it provides a confidence interval for any random variable on \\([0, 1]\\) . Specifically, let \\(Z\\)  be set to any value in \\([0, 1]\\) , and all other quantities be defined as in the Bayesian estimator. Then \\(1 - \\delta \\)  LCB on \\(Z\\)  is\n\\(L_Z = \\frac{n_+}{n_+ + n_-} - \\sqrt{\\log (1 / \\delta ) / (2n)}\\,,\\) \n", "\nThe empirical Bayes [30] is a statistical procedure that chooses \\((\\alpha , \\beta )\\)  that maximize \\(\\mathcal {L}(\\alpha , \\beta )\\) . To find the maximizer, we search on a grid. For instance, let \\(\\mathcal {G}= [m]\\)  for some integer \\(m > 0\\) . Then we search over all \\((\\alpha , \\beta ) \\in \\mathcal {G}^2\\) .\n"]}
{"id": "2206.02539", "categories": "cs.CV cs.LG", "paragraphs": ["Training was performed using the implementation of FBF adversarial training in [10], modified for use with non-classifiers, using inputs in the space \\([-1, 1]^{3\\times 720\\times 1280}\\) . The dataset used for development was the TuSimple lane detection challenge [24], with a train/validation/test split of 3082/181/363 images. We used the SGD optimizer with a momentum factor of \\(0.9\\) , a perturbation bound of \\(\\varepsilon = 8/255\\)  for both FBF and TRADES, \\(N = 10\\)  steps of size \\(\\eta = \\varepsilon /10\\)  for TRADES, and a regularization weighting of \\(\\beta = 2.0\\times 10^{-5}\\)  that was tuned through experimentation. The TRADES repository [33] recommends \\(1 \\le \\beta \\le 10\\)  for training a classifier for which the KL divergence is calculated over a one-dimensional array of class probabilities. As our KL divergence was calculated over a \\(320\\times 180\\) -dimensional array of probabilities, this translates to a recommendation of \\(1.74\\times 10^{-5} \\le \\beta \\le 17.4\\times 10^{-5}\\) , in line with our choice of \\(\\beta \\) . We used \\(L^{\\infty }\\) -norm perturbations for TRADES, corresponding to the use of FGSM in FBF adversarial training.\n"]}
{"id": "2203.10945", "categories": "cs.CL", "paragraphs": ["AraBART pretraining took approximately 60h. The pretraining was carried out on 128 Nvidia V100 GPUs which allowed for 25 full passes over the pretraining corpus. We used the Adam optimizer with \\(\\epsilon = 10^{-6}\\) , \\(\\beta _1=0.9\\) , and \\(\\beta _2=0.98\\)  following [32]. We use a warm up for 6% of the pretraining were the learning rate linearly increases from 0 to 0.0006, then decreases linearly to reach 0 at the end of the pretraining. We fixed the update frequency to 2 and we use a dropout 0.1 in the first 20 epochs and we changed it to 0 in the last 5 epochs. Finally we used FP16 to speed-up the pretraining. The pretraining is done using Fairseq [37].\n"]}
{"id": "2202.11822", "categories": "cs.CL", "paragraphs": ["Our data sampling strategy consists in first randomly selecting a data source (either monolingual or parallel data) with equal probability, then sampling uniformly from the datasets in the selected source. We continue training with the same learning rate schedule and Adafactor optimizer [33] states as mT5. For parallel data, we use the standard cross-entropy objective. For monolingual data, we use the MASS [34] objective. We additionally use a dropout rate of 0.1 and label smoothing set to 0.1.\n"]}
{"id": "2204.05660", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["All the experiments were ran with the following hyper parameters, batch size was kept at 16 where as the eval batch size was 5. The maximum number of epoch ran for the experiments were 5 with the warm-up kept at 0.06. The learning rate used was 1.5e-5 and the weight decay was 0.01.\n"]}
{"id": "2211.02145", "categories": "cs.CV", "paragraphs": ["We feed random noise to initialize the decomposition network in the same way as Omnimatte\u00a0lu2021omnimatte.\nWe set up one discriminator for the foreground layer, and one for the residual layer.\nTheir receptive fields are at three scales: 16\\(\\times \\) 16, 32\\(\\times \\) 32, and 64\\(\\times \\) 64. The patches are sampled over the entire frame, so that the discriminator loss \\(L_\\textit {adv}\\)  provides an alternative supervision to zero-alpha regions that cannot be supervised by \\(L_\\textit {recon}\\)  (\\(L_\\textit {RGB-warp}\\)  and \\(L_{\\alpha \\textit {-warp}}\\)  are other supervisions on those regions). In Stage 1, we use SIFT features\u00a0[26] and FLANN-based matcher to pre-compute a homography for each frame. Thus the model only needs to estimate a background canvas and warp it to each frame with an appropriate homography. Then we freeze the environment layer in the later stages to simplify the factorization optimization.\nWe use all aforementioned losses and train on NVIDIA RTX A6000 for 1200 epochs in both Stage 1 and 3, and only use \\(L_\\textit {adv}\\)  in Stage 3.\nWe use the ADAM optimizer\u00a0[16] when training the decomposition network and discriminators, with learning rate 0.001 and batch size 16. When applying strategies in Section REF , we use 3 temporal scales: the frame at time step \\(t\\)  is paired with \\(t+1\\) , \\(t+4\\) , and \\(t+8\\)  as inputs. All flow-related losses are optimized for all scales.\n", "where \\(\\lambda _1 = \\lambda _2 = \\lambda _3 = 0.005\\) , \\(\\lambda _4 = \\lambda _5 = \\lambda _7 = 0.0005\\) , \\(\\lambda _6 = 25\\) , and \\(L_\\textit {mask-init}\\)  is turned off after it falls below 0.05.\n"]}
{"id": "2211.02213", "categories": "cs.CV", "paragraphs": ["In all experiments, we choose YOLOv5 with Large parameters (YOLOv5-L) as the detector for its comparable detection accuracy with state-of-the-art object detection methods in about real-time. All training and testing images are padded and resized in shape \\((960, 960, 3)\\) . During training, each batch consists of two pairs of images: \\((\\mathbf {I}^s, \\mathbf {I}^s_{f})\\)  with labels and \\((\\mathbf {I}^t, \\mathbf {I}^t_{f})\\)  without labels. We can set the batchsize to 10 in one 24GB GTX3090 GPU. Total epochs are 200. The \\(\\gamma \\)  in EMA for the teacher model is set to \\(0.99\\) . In filters \\(\\mathcal {G}_\\mathcal {B}[\\cdot ]\\)  and \\(\\mathcal {G}_\\mathcal {C}[\\cdot ]\\) , we set the IoU threshold \\(\\tau _{box}=0.3\\)  and the category score threshold \\(\\tau _{cls}=0.8\\) . Other not mentioned settings keep consistent with the setup in YOLOv5 [20]. For the hyper-parameters in SSDA-YOLO, we set \\(\\alpha =0.005\\)  and \\(\\beta =2.0\\)  in the overall loss \\(\\mathcal {L}\\) . And the consistency loss \\(\\mathcal {L}_{con}\\)  adopts L2 distance. These selected settings will be discussed in our ablation studies.\n"]}
{"id": "2211.02162", "categories": "cs.CL", "paragraphs": ["For training on all datasets with BART, we first follow the hyperparameter setting provided by the original BART training script for XSumhttps://github.com/pytorch/fairseq/blob/main/examples/bart/README.summarization.md except that we set the total number of update steps to \\(30{,}000\\)  for TempWikiBio and \\(35{,}000\\)  for the content transfer dataset.\nIn addition, we adjust the accumulated batch size for training on TempWikiBio to have \\(65{,}536\\)  tokens in each batch.\nWe then tune the learning rates on TempWikiBio and the content transfer dataset by searching through \\(1\\times 10^{-5}\\) , \\(3\\times 10^{-5}\\) , and \\(5\\times 10^{-5}\\)  with the model without prompts. Based on the BLEU-4 scores on the development sets, we choose \\(5\\times 10^{-5}\\)  for TempWikiBio and \\(3\\times 10^{-5}\\)  for the content transfer dataset.\nEach model is trained for one run with one random seed due to the high computational cost of fine-tuning large models.\nFor experiments with T5, we follow the default parameters suggested by HuggingFace.\n"]}
{"id": "2206.03113", "categories": "cs.CV", "paragraphs": ["The proposed method is implemented with PyTorch.\nThe original input and output size of the inpainting model are both \\(256\\times 256\\)  as many other inpainting methods. And the input size to Haar transform is upsampled to \\(512\\times 512\\)  from \\(256\\times 256\\) .\nWe train the model in 240k steps in Celeba-HQ and 1000k steps in Places2 in Adam optimizer\u00a0[50] of \\(\\beta _1=0\\)  and \\(\\beta _2=0.9\\) . The initial learning rates are \\(2e-4\\)  and \\(2e-5\\)  for the generator and discriminator respectively. Then, the learning rate is decayed with 0.5 for \\(1/5\\)  of the total steps. Our model is trained in Pytorch v1.3.1, and costs about 1 day to train in Celeba-HQ and 5 days to train in Places2 with a single NVIDIA(R) Tesla(R) V100 16GB GPU.\n"]}
{"id": "2212.10405", "categories": "cs.CL cs.SI", "paragraphs": ["We implement AnnoBERT and the BERT baseline using the Huggingface transformers library [50]. Models are trained for 4 epochs (except LEAM for 15 epochs), with a learning rate of \\(1e^{-5}\\) , cross-entropy loss, and a batch-size of 32. Adam [26] is used as the optimiser, with Tanh activation function. LEAM model uses FastText embeddings of dimension 300 [19] as the encoder, and softmax function in the output layer. We report average results for 10 separate runs of each model.\n"]}
{"id": "2203.06456", "categories": "cs.LG", "paragraphs": ["Data is generated by solving the problem (REF ) using a high precision numerical scheme with a pseudo-spectral method for spatial discretization and 4th order Runge-Kutta for temporal discretization (with time step size \\(\\delta t = 0.01\\) ). We assume periodic boundary conditions and the initial value\n\\(u(x, y, 0) = \\sum _{|k|, |l| \\le N} \\lambda _{k,l} cos(kx + ly) + \\gamma _{k, l} sin(kx + ly)\\) \n"]}
{"id": "2204.02791", "categories": "cs.CV", "paragraphs": ["The training data consists of three parts: 1) all training data from the \\(\\textit {DAVIS}_{\\textit {16}}\\)  , including 30 videos with about 2K frames; 2) a subset of the training set of \\(\\textit {YouTube-VOS}\\)   selected 18K frames, which is obtained by sampling images containing a single object per sequence; 3) \\(\\textit {DUTS-TR}\\)  which is the training set of \\(\\textit {DUTS}\\)   has more than 10K images. The training process is divided into two stages. Stage 1: we first pre-train our network for 200K iterations on a subset of \\(\\textit {YouTube-VOS}\\) . During this training period, the entire network is trained using Adam  optimizer (\\(\\beta _{1}=0.9\\)  and \\(\\beta _{2}=0.999\\) ) with a learning rate of \\(10^{-6}\\)  for the FEM, \\(10^{-5}\\)  for the ACM and APM decoder, and the MCM is set as \\(10^{-4}\\) . The batch size is set to 8, and the weight decay is 0. Stage 2: we fine-tune the entire network on the training set of \\(\\textit {DAVIS}_{\\textit {16}}\\)  and \\(\\textit {DUTS}\\)  with our joint training strategy. In this stage, the Adam optimizer is used with an initial learning rate of \\(10^{-7}\\) , \\(10^{-6}\\) , and \\(10^{-5}\\)  for each of the above-mention modules, respectively. We train our network for 155K iterations in the second training stage. Data augmentation (e.g., scaling, flipping, and rotation) is also adopted for both image and video data. Our IMCNet is implemented in PyTorch . All experiments and analyses are conducted on an NVIDIA TITAN RTX GPU, and the overall training time is about 72 hours.\n"]}
{"id": "2202.07036", "categories": "cs.LG", "paragraphs": ["For all experiments we use Nvidia Tesla V100-SXM2 GPUs with 32 GB VRAM equipped with Core Xeon CPUs and 192 GB RAM. We use the Adam optimizer with a learning rate of \\(10^{-4}\\) . We run each experiment for 1,000 epochs with a batch size of 50 (unless stated differently) and report results for the best epoch. We split each dataset into five approx. 80/20 train/test splits, and report the mean and standard deviation of the WER and CER. We use our OnHW-equations, OnHW-words500(R), OnHW-wordsRandom and OnHW-wordsTraj as well as the IAM-OnDB\u00a0 and VNOnDB-words\u00a0 datasets for the sequence-based classification task, and the OnHW-symbols, split OnHW-equations and OnHW-chars\u00a0 datasets for the single character-based classification task. Each model is trained from scratch for every dataset. We make use of the time-series classification toolbox tsai\u00a0 that contains a variety of state-of-the-art techniques , , , , , , , , , , , , , .\n"]}
{"id": "2205.03512", "categories": "cs.CL", "paragraphs": ["For the joint related work tagger training, we use GeForce GTX 1080 11 GB GPUs. The training process lasts 2.5 hours on a single GPU using Huggingface's [56] SciBERT, BERT-base or Roberta-base as the paragraph encoders, and it lasts 6.5 hours using LED-base encoder. We train the models for 15 epochs. It takes approximately one week to run the hyper-parameter search using five-fold cross-validation for all language models, using 8 GPUs in total.\n", "For training the citation span generation model, we use Tesla V100s-PCIE-32GB GPUs. The training process lasts for 2 days on a single GPU. We run the training for a maximum of 3 epochs with early stopping based on the validation loss.\n"]}
{"id": "2212.07652", "categories": "cs.CV", "paragraphs": ["Datasets: We mainly expect to evaluate the association quality of BPJDet, while maintaining high object detection accuracy. Three public datasets including CityPersons , CrowdHuman  and BodyHands  are chosen. The former two are for pedestrian detection tasks. In CityPersons, it has 2,975 and 500 images for training and validation, respectively. In CrowdHuman, there are 15,000 images for training and 4,375 images for validation. Following BFJDet , we use its re-annotated box labels of visible faces for conducting corresponding experiments. The last dataset BodyHands is for hand-body associations tasks. It has 18,861 and 1,629 images in train-set and test-set with annotations for hand and body locations and correspondences. We implement body-hand joint detection task in it for comparing.\n", "Implementation Details: We adopt the PyTorch 1.10 and 4 RTX-3090 GPUs for training. Depending on the dataset complexity and scale, we train on the CityPersons, CrowdHuman and BodyHands datasets for 100, 150 and 100 epochs using the SGD optimizer, respectively. Following the original YOLOv5  architecture, we train three kinds of models including BFJDet-S/M/L by controlling the depth and width of bottlenecks in \\(\\mathcal {N}\\) . The shape of input images is resized and zero-padded to \\(1536\\times 1536\\times 3\\)  following settings in JointDet  and BFJDet . We adopt the data training augmentation but leave out test time augmentation (TTA). As for many hyperparameters, we keep most of them unchanged, including adaptive anchors boxes \\(\\mathcal {B}^s\\) , the grid balance weight \\(w_s\\) , and the loss weights \\(\\alpha \\!=\\!0.05\\) , \\(\\beta \\!=\\!0.7\\)  and \\(\\gamma \\!=\\!0.3\\) . We set \\(\\lambda \\!=\\!0.015\\)  based on ablation studies. When testing, we use thresholds \\(\\tau ^b_{conf}\\!=\\!0.05\\) , \\(\\tau ^b_{iou}\\!=\\!0.6\\) , \\(\\tau ^p_{conf}\\!=\\!0.1\\) , \\(\\tau ^p_{iou}\\!=\\!0.3\\)  and \\(\\tau ^{inner}_{iou}\\!=\\!0.6\\)  for applying \\(\\mathsf {NMS}\\)  on \\(\\widehat{\\mathbf {O}}\\) .\n{FIGURE}"]}
{"id": "2212.04983", "categories": "cs.LG", "paragraphs": ["Optimization hyperparameters. We use the Adam optimizer with a learning rate 0.01 and weight decay of 0.0005. All models are trained for 200 epochs with no weight scheduling. We add a dropout layer with rate \\(p=0.5\\)  after each GNN layer during training. We apply no early stopping and the optimal model is selected with its performance on the validation set. The test set is never touched during training.\n", "Train/val/test split. The evaluation procedures of GNNs on node classification tasks have suffered overfitting bias from using a single train-test split. [19] showed that different splits could significantly affect the performance and ranking of models. In all our experiments on node classification tasks, we apply the split setting in [36], which utilizes 10% samples for training, 10% samples for validating, and 80% samples for testing. We generate 20 random splits and for each split we train 10 models with different random initialization. We report the mean and standard deviation of the accuracy of the 200 random models in our results.\n"]}
{"id": "2203.10852", "categories": "cs.CV cs.AI", "paragraphs": ["For self-supervised learning for generating brain networks, the autoencoder adopts mean squared error loss (MSELoss), the Adam optimizer with a weight decay of 0.0005 and a batch size of 50. We implement the following hyperparameters: 1000 training epochs. We set the initial learning rate as 0.001, and the learning rate is reduced to 90% after every 50 epochs. For edge reconstruction, we adopt the SGD optimizer\u00a0[26] with a weight decay of 0.0005 and a batch size of 50 with 1000 training epochs. We set the initial learning rate as 0.001, and the learning rate is reduced to 90% after every 50 epochs.\n", "For multi-modal learning, we adopt the SGD optimizer to optimize the network with a weight decay of 0.0005 and a batch size of 20. We implement the following hyperparameters: 1000 training epochs; a mini-batch size of 20. We set the initial learning rate as 0.001, and the learning rate is reduced to 90% after every 50 epochs. Data augmentation is performed by rotating both images and points cloud data with the same angles.\n", "For population graph-based-GNN, we adopt the Adam optimizer\u00a0[26] with a weight a batch size of 20. We apply binary cross-entropy loss for patient classification. We implement the following hyperparameters: 200 training epoch; a mini-batch size of 20. We set the initial learning rate as 0.001, and the learning rate is reduced to 90% after every 50 epochs.\n"]}
{"id": "2212.01612", "categories": "cs.CL", "paragraphs": ["During training, we finetune the models by AdamW [18] optimizer. In all experiments, we use the grid search to find the learning rate for the embeddings within \\([1\\times 10^{-6}, 5\\times 10^{-5}]\\) . We use a learning rate of \\(5\\times 10^{-6}\\)  and a batch size of 4 for task model training. Following ITA [28], we use the cross-view alignment loss to minimize the KL divergence between the output distributions of retrieval based input and original input. For MoE, we use the same learning rate and a batch size of 64 instead. The task models are trained for 10 epochs and the MoE models are trained for 50 epochs. All of the results are averaged from 3 runs with different random seeds.\n"]}
{"id": "2208.05838", "categories": "cs.CV", "paragraphs": ["Dataset pre-processing.\nFor the PCD dataset, original images are cropped into\n224\u00d7224. By sliding 56 pixels in width and data augmentation of plane rotation, each image pair is expanded into 60 patches with a 224\u00d7224 resolution. In total, 12000 image pairs are generated. As the input, the image pairs will be resized into 256\u00d7256.\nFor the VL-CMU-CD dataset, we follow the random training and testing splits reported in [9], [3]. Nine hundred thirty-three image pairs (98 sequences) for training and 429 (54 sequences) for testing are resized into a 256\u00d7256 resolution. Note that only images belonging to the train set (without labels) are used to train the DSP model. For data pre-processing, training, and validation split, we follow the steps mentioned in [3] for pretraining in VL-CMU-CD and PCD datasets.\n", "Architecture. The proposed DSP method has a Siamese architecture that consists of ResNet50 [11] (without the final classification layer) as a feature extractor followed by a projector network. We use dilated convolutions and reduce the input image size by a stride of 16 to output feature vectors of size \\(16 \\times 16 \\times 2048\\) . Then, the feature vectors are passed to the projection head by applying a 2-D adaptive average pooling. The projector network has two linear layers, each with a hidden layer size of 512 output units. Owing to the high computational requirements, the output of the projector network was modified to generate embeddings of size \\(1 \\times 256\\) . The first layer of the projector is followed by a batch normalization layer and rectified linear units. Then, an absolute difference is applied to the output embedding to get difference embeddings. The Barlow Twins loss function is applied on the difference embeddings of size \\(1 \\times 256\\)  to generate a cross-correlation matrix of shape \\(256 \\times 256\\) . Additionally, as discussed in Section\u00a0, we incorporate Invariant prediction and Change Consistency regularization loss in our DSP framework. As shown in Figure\u00a0REF , the output of the projector before the difference is used to calculate the invariant prediction loss while the change consistency regularizer is applied to the output of the feature extractor.\n", "Training and Optimization. We follow the optimization protocol described in Barlow Twins. We use the LARS optimizer [29] and train for 400 epochs with a batch size of 16 on two NVIDIA RTX-2080 Ti GPU. We use a learning rate of 0.003, multiply the learning rate by the batch size, and divide it by 256. The learning rate is reduced by a factor of 1000 using a cosine decay schedule [16]. We use a weight decay parameter of 1x10\\(^-6\\) . Finally, the loss balancing parameters \\(\\alpha \\)  and \\(\\beta \\)  used for training our pretraining model is 100 and 3000 respectively. We tuned these parameters by cross-validation on the validation sets of the VL-CMU-CD dataset and balanced these losses to the same scale.\nFor self-supervised Barlow Twins training, we followed the exact data preprocessing, training and optimization procedure.\n"]}
{"id": "2205.01703", "categories": "cs.CL", "paragraphs": ["For the pretrained language model checkpoints, we use the 125 million parameters (125M) and the 1.3 billion parameters (1.3B) dense model from [2]. These pretrained models have shown results comparable to GPT3 across various tasks.\n", "For self-supervised training, we use a subset of documents from the RoBERTa training corpus [40] that contains four domains: BookCorpus plus Wikipedia, CC-News, OpenWebText, and Stories. Specifically, we randomly sample 100k documents from each domain except Stories where we only sample 10k documents as the documents there are much longer than the others. The final training data contains approximately 1 million instances with 250k training instances per task.The average numbers of example per instance for each data source are: 6.9 for BookCorpus plus Wikipedia, 5.3 for CC-News, 3.5 for OpenWebText, and 7.2 for Stories. For the 125M model, we train for 10 epochs, which takes roughly 1 day on a V100 GPU. For the 1.3B model, we train for 5 epochs, which takes roughly 3 days on 2 V100 GPUs.\n"]}
{"id": "2207.04614", "categories": "cs.CV cs.GR", "paragraphs": ["We train our network by adopting the strategies of CondInst\u00a0[59] and AdelaiDet\u00a0[69].\nFirst, we adopt the weights of ResNeXt-101-BiFPN\u00a0[70], [71] trained on ImageNet\u00a0[72] to initialize the backbone network parameters, set the mini-batch size as two, and optimize our network on one NVidia RTX 3090 GPU.\nSecond, we set the base learning rate as \\(0.001\\) , adopt a warm-up\u00a0[73] strategy to linearly increase the learning rate from \\(0.0001\\)  to \\(0.001\\)  in the first \\(1,00\\)  iterations, reduce the learning rate to \\(0.0001\\)  after \\(40,000\\)  iterations, and stop the learning after \\(45,000\\)  iterations.\nThird, we re-scale the input images, such that the longer side is smaller than \\(1,333\\)  and the shorter side was smaller than 640, without changing the image aspect ratio.\nLastly, we apply random horizontal flip to the input images as data augmentation.\n"]}
{"id": "2206.13155", "categories": "cs.CV cs.CL cs.MM", "paragraphs": ["Following previous works[30], [2], we initialize the weight of Bi-VLDoc model with the existing pre-trained models. For 24-layer transformer encoder layers and the text embedding layer, we use the RoBERTa-Large[20] to initialize. For the visual part in the Bi-VLDoc, we use the backbone of a Mask R-CNN[43] model trained on PubLayNet[44] following LayoutLM v2[2]. The cross attention layer and the rest of the parameters in the model are randomly initialized. The Bi-VLDoc is trained by using Adam optimizer with the learning rate of 2e-5 and a linear\ndecay learning rate schedule. We use batch size 384 to train Bi-VLDoc for 10 epochs on the IIT-CDIP dataset.\n", "For pre-training on IIT-CDIP dataset, we set the maximum sequence length to 512 and keep the head and the tail of the text sequence if the text is too long. All text tokens are set to the segment 0 and visual feature tokens are set to the segment 1. The output of the visual backbone is transformed by a pooling layer to 49 (\\(7\\times 7\\) ) image tokens. Following [5], we mask 15% text tokens in which 80% are replaced by the [MASK] token , 10% are replaced by a random token, and 10% keeps unchanged. In BTIA, 15% text blocks are covered and the visual parts performs image-text-alignment according to the text blocks that contains [MASK] token from MVLM. In TIPA, the 2D-positions of 15% text blocks are masked by (0,0,0,0).\n"]}
{"id": "2205.10955", "categories": "cs.LG", "paragraphs": ["\nWe use the popular ResNet50 architecture [12]\nfor image classification with ADAM optimizer.\n\nWe choose cross-entropy or top-1 accuracy as loss function.\n\nThe model's weights have been chosen randomly. As the classes are\nbalanced, there is no need for class weights.\n\n20% of the training set \\(T_{N}\\)  is held out as validation data.\nIt is used to determine when the model has converged. In models with\nequal training volume the same data is held out.\n\nWe use early stopping on the validation-accuracy with a patience of\n50 epochs, and a maximum of 100 training epochs.\n\nThe only data-augmentation used is a 50% chance to horizontally flip\nthe image and rescaling to 224x224 pixels.\n\nValidation accuracy and loss is either reported for the model that\ntriggered the early stopping, or the best model found.\n\nTo evaluate the trained models, we classify 900 images of the overall\nheld out test set. This is independent of \\(N\\) , the volume of the\ntraining set and validation set used.\n\nDue to random choices each configuration of training is repeated 5 times. We plot the mean loss of these 5 models, as well as plus/minus one standard deviation from the mean.\n\n"]}
{"id": "2209.10063", "categories": "cs.CL cs.AI", "paragraphs": ["Figure REF  shows the scaling of performance with GPT-3 generator parameters, including Ada-150M, Babbage-1.3B, Curie-6.7B and Davinci-175B. We note that for both FiD and our GenRead , we use the FiD-xl with 10 input documents either retrieved from Wikipedia or generated by GPT-3.\nThe performance of both TriviaQA and WebQ continues to improve as the generator model parameters increase, as does the slope.\nOnly with the largest size GPT-3, GenRead can outperform the DPR-FiD. This indicates using large language model to generate contextual documents is an \u201cemergent ability\u201d of scaling, which is not present in smaller models but is present in larger models\u00a0[45].\n"]}
{"id": "2209.10080", "categories": "cs.LG stat.ML", "paragraphs": ["The training settings are the same for CIFAR-10 and CIFAR-100. All ConvNets are trained for 4000 epochs with SGD with momentum \\(0.9\\) , fixed learning rate \\(1\\mathrm {e}-3\\) , batch size 128, and no weight decay. All learned layers are initialized with Pytorch's default weight initialization (version 1.11.0). To stabilize prolonged training in the absence of batch normalization, we use learning rate warmup: starting from a base value of \\(1\\mathrm {e}-4\\)  the learning rate is linearly increased to \\(1\\mathrm {e}-3\\)  during the first 5 epochs of training, after which it remains constant at \\(1\\mathrm {e}-3\\) .\n", "All ResNets are trained for 4000 epochs using Adam with base learning rate \\(1\\mathrm {e}-4\\) , batch size 128, and no weight decay. All learned layers are initialized with Pytorch's default initialization (version 1.11.0). All residual networks are trained with data augmentation, consisting of \\(4-pixel\\)  random shifts, and random horizontal flips.\n"]}
{"id": "2203.11926", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["We train FocalNet-B and FocalNet-L for 90 epochs with a batch size of 4096 and input resolution \\(224 \\times 224\\) . The initial learning rate is set to \\(10^{-3}\\)  after a warmup of 5 epochs. We set the the stochastic depth drop rates to \\(0.2\\)  for both networks. For stability, we use LayerScale\u00a0[57] with initial value \\(10^{-4}\\)  for all layers. The other settings follow those for ImageNet-1K. After the pretraining, we finetune the models on ImageNet-1K for 30 epochs with initial learning rate of \\(3\\times 10^{-5}\\) , cosine learning rate scheduler and AdamW optimizer. The stochastic depth drop rate is set to \\(0.3\\)  and both CutMix and Mixup are muted during the finetuning.\n"]}
{"id": "2210.03380", "categories": "cs.CL cs.AI", "paragraphs": ["We use the pre-trained uncased BERT-base [8] as the encoder with 768-dim embedding, the learning rate is 2e-5, and the coefficient of L2-regularization \\(\\lambda \\)  is set to 1e-5. Adam is utilized as the optimizer. The mini-batch is set to 32. For contrastive learning loss, we set the hyper-parameters \\( \\tau =0.07 \\) , \\( \\eta =0.1 \\)  and dropout probablity \\( p=0.1 \\) . We use LDA [3] to generate topic words for masking sentences, where T = 6 and K = 5. For BiCond and CrossNet, the word embeddings are initialized with the pre-trained 200-dimensional GloVe vectors [25]. The reported results are averaged scores of 10 runs to obtain statistically results.\n"]}
{"id": "2210.12582", "categories": "cs.CL cs.AI", "paragraphs": ["We adopt the VOA corpus constructed by\u00a0[11] for sparse latent type pre-training, which was extracted from 108,693 multimedia news articles openly available on the Voice of America website between 2006 and 2017.\nWe use the bert-base-uncased version of the BERT\u00a0[3] model as our encoder, and a single transformer decoder layer to reconstruct the sentence, following [7], [16]. While our approach is generally applicable for both encoder-only Masked Language Model (MLM) and the encoder-decoder Denoising Language Model (e.g. BART [10]), we focus on MLM because MLM is more widely used in the downstream information extraction tasks.\nThe implementation details can be found in Appendix\u00a0.\n"]}
{"id": "2212.04613", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["Pretraining is performed with the provided InsLoc implementation [46]. Faster R-CNN [29], with a ResNet-50 backbone and FPN, serves as the trained detector. With high computational costs for contrastive pretraining, these experiments consider a fixed pretraining budget of 200 epochs. For COCO, pretraining is performed with per-GPU batch size 64 and learning rate 0.03 on 4 NVIDIA Quadro RTX 5000 GPUs with memory 16 GB. For ImageNet-Subset, pretraining is performed with per-GPU batch size 32 and learning rate 0.015 on 2 NVIDIA GeForce GTX 1080 Ti GPUs with memory 11 GB. All pretraining uses a dictionary size of K=8,192. Full finetuning of all layers is performed within the Detectron2 [41] framework with a 24k iteration schedule, a learning rate of 0.02, and a batch size of 4 on 2 NVIDIA GeForce GTX 1080 Ti GPUs, unless otherwise noted.\n"]}
{"id": "2212.04663", "categories": "cs.LG cs.NA math.NA", "paragraphs": ["In 1D examples, we use the modified fully connected architecture with depth of 5 layers and width of 100 neurons for both branch and trunk nets, and the design of the optional layers \\(P\\)  and \\(D\\)  in Figure\u00a0REF  will be detailed in each of the following examples. The batch size is chosen to be 100 with ADAM optimizer, where the initial learning rate \\(lr=0.001\\)  and a 0.95 decay rate in every 5000 steps. Same architecture is used in 2D examples except that a depth of 6 layers is used. In the transfer learning step, to solve the optimization problem (REF ), we use\nthe lstsq function (with rcond=1e-6) from Numpy[8] for linear operators and the leastsq function in Scipy[21] (using default setting with \\(ftol=\\) 1e-5, \\(xtol=\\) 1e-5) for nonlinear operators. All of the neural networks are trained on a single K40m GPU, and the prediction step is computed on a AMD Ryzen 7 3700x Processor.\n"]}
{"id": "2210.08355", "categories": "cs.CL", "paragraphs": ["We implemented all models based on\nPyTorch\u00a0[29] with\nPyTorch\u00a0Lightning\u00a0[4]\nand used language models from Transformers\u00a0[35].\nWe used base models, such as BERT-base-cased, RoBERTa-base, for all the experiments.\nThe dimension of hidden layers in FFNs was set to 512, and the dropout rate was set to 0.2.\nBy following [8], we employed\nspan-based batch rather than document-based batch.\nThe mini-batch size is 5 spans/action.\nWe optimized all models with the AdamW\u00a0[24] optimizer.\nWe used a learning rate of 1e-5 for language models and 1e-5/2e-4 for other parametersThe learning rate was determined by using the development dataset. such as FFN and biaffine layers.\nWe scheduled the learning rate by linear warm-up, which increases the learning rate linearly during the first epoch and then decreases it linearly to 0 until the final epoch.\nWe trained the model up to 20 epochs and applied early stopping with a patience of 5 by monitoring the fully labeled span F1 score\non the development dataset.\nDetails of other hyperparameters are in Appendix .\n"]}
{"id": "2208.08118", "categories": "cs.CV", "paragraphs": ["In our experiments, we set \\(\\lambda _{rec}=50\\) , \\(\\lambda _{region}=100\\)  and \\(\\lambda _{sync}=0.05\\) . We provide the details regarding pre-processing and training settings in supplementary file on our project page.\n", "Datasets:\nWe train our model using AVSpeech\u00a0[15] and VoxCeleb2\u00a0[7] datasets; both containing talking-face videos spanning a wide variety of identities, languages and poses. For AVSpeech data, we extract the face tracks using an off-the-shelf face detector\u00a0[55]. We curate a set of 50 hours for training and \\(\\sim 3\\)  hours from the official test split for testing and verified it for accurate lip-sync using SyncNet\u00a0[8]. We also benchmark our model on VoxCeleb2 data which comprises face tracks with a fair amount of background. Owing to computational limitations, we randomly sample a subset of 100 hours for training and use the full official test split for testing. Note that there are no overlaps between the identities used in training and testing sets in both datasets. The code, models and file-lists are released on our website for reproducibility and future research.\n"]}
{"id": "2203.14632", "categories": "cs.CL", "paragraphs": ["Mapping: Using fasttext [15] with the default parameters These are 300-dimensional vectors with 10 negative samples, a sub-sampling threshold of 1e-5 and 5 training iterations, we first gather monolingual word embeddings for each of the respective languages. After this, we map the embeddings to a cross-lingual space using VecMap [1] in the unsupervised mode as we do not have any bilingual dictionaries. In this mode an initial solution is found using heuristics and iteratively refined.\n"]}
{"id": "2204.01341", "categories": "cs.CV cs.AI", "paragraphs": ["In Eq.\u00a0REF , \\(z_{i}\\)  is the output in the \\(i\\) th node, C is the number of output nodes, representing the number of classified categories.\nThe classification prediction can be converted into the probabilities by using the Softmax function, which distributes in the range of [0, 1], and the sum of probability is 1.\nBecause the image segmentation for yeast counting is to distinguish the foreground and the background. Hence it is a binary classification, the Eq.\u00a0REF  can be rewritten as Eq.\u00a0REF .\n\\(Softmax(z_{1})=\\frac{e^{z_{1}}}{e^{z_{1}}+e^{z_{2}}}=\\frac{1}{1+e^{-(z_{1}-z_{2})}}=Sigmoid(\\beta )\\) \n", "Adam optimizer is applied to minimize the loss function, which can adjust the learning rate automatically by considering the gradient momentum of the previous time steps\u00a0[34].\nIn our training process, learning rate is set as 0.001 and batch size is 8.\nThe epoch is set as 100 by considering the converge speed of experimental models, the example of loss and intersection over union (IoU) curves of models is shown in Fig.\u00a0REF .\nThough there are 92,319,298 params to be trained in PID-Net, but it can converge rapidly and smoothly, without over fitting.\nThere is a jump in loss and IoU plots for all 3 tested networks from 40 to 80 epochs, which is caused by the small batch size. Small batch size may lead to huge difference between each batch, and the loss and IoU curves may jump while convergence.\n{FIGURE}"]}
{"id": "2204.01366", "categories": "cs.LG cs.CV", "paragraphs": ["fig:appendix-gcn-mean shows mean evaluation plots of five training runs of GCN_W_BN on \\(3.15M\\)  instances of RandomMP.\nNo CCL was applied in the first \\(3M\\)  instances.\nThen, \\(\\alpha \\)  was linearly increased over \\(100k\\)  instances to \\(\\alpha =0.01\\) .\nAfterwards, the training continued for \\(50k\\)  instances with \\(\\alpha =0.01\\) .\nThe node embedding dimensionality was set to 128 and the number of GCN layers to 20.\nThe MLP edge classifier consists of 2 hidden layers with 256 neurons.\nOptimization was performed with Adam\u00a0[31] (\\(0.001\\)  learning rate, \\(5\\cdot 10^{-4}\\)  weight decay, \\((0.9, 0.999)\\)  betas) and a batch size of 200.\nEach training was performed on a MEGWARE Gigabyte G291-Z20 server on one NVIDIA Quadro RTX 8000 GPU and took 24hrs on average, whereof the training time of the last \\(150k\\)  (CCL) instances took around 18hrs.\n{FIGURE}"]}
{"id": "2210.02414", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["Self-Supervised Blank Infilling (95% tokens).\nRecall that GLM-130B uses both [MASK] and [gMASK] for this task.\nSpecifically, [MASK] is used to mask consecutive spans in 30% of training tokens for blank infilling. The lengths of spans follow a Poisson distribution (\\(\\lambda =3\\) ) and add up to 15% of the input.\nFor the other 70% tokens, the prefix of each sequence is kept as context and [gMASK] is used to mask the rest of it.\nThe masked length is sampled from the Uniform distribution.\n"]}
{"id": "2209.07118", "categories": "cs.CL cs.CV", "paragraphs": ["Implementation Details\u00a0\nFor the uni-modal encoders, we use the vision encoder with CLIP-ViT-B [41] (\\(L_{v}=12\\) ) and the language encoder with RoBERTa-base [32] (\\(L_{l}=12\\) ).\nFor the multi-modal fusion module, we set the number of Transformer layers \\(L_{m}=6\\) , and the dimension of the hidden states \\(D=768\\)  with the number of heads set to 12.\nFor knowledge representation and injection, we set the dimension of the hidden states \\(D_{e}=256\\) .\nFor the pretext tasks, we adopt (knowledge-enhanced) MLM, MIM [16], and ITM, where the masking ratios of MLM and MIM are set to 15% and 75%, respectively.\nFor the optimization, the models are trained with AdamW optimizer [33] for 100,000 steps with the learning rates for the uni-modal encoders and the remaining parameters set to 1e-5 and 5e-5, respectively.\nThe warm-up ratio is set to 10%, and the learning rate is linearly decayed to 0 after warm-up.\nBesides, we use center-crop to resize each image to the size of 288\\(\\times \\) 288.\n"]}
{"id": "2209.07098", "categories": "cs.CV cs.CL", "paragraphs": ["We conduct our experiments on two datasets, i.e., ROCO [19] and MedICaT [22], where the former contains over 81,000 medical image-text pairs and the latter consists of over 217,000 medical images with their captions and inline textual references.\nFor ROCO, we adopt their official splits, and for MedICaT, we randomly sample 1,000 images for validation, 1,000 images for testing, and the remaining images are used for training.\nFor pre-training, we use the training set of ROCO and MedICaT to train models with the pre-training tasks presented in Section  together with the common image-text matching task [1] by default.\n"]}
{"id": "2209.13948", "categories": "cs.CV", "paragraphs": ["Our training configuration directly follows Deformable-DETR [63]. Obj2Seq takes 16 images as a training batch. It is trained with an AdamW optimizer [37] for 50 epochs, with \\(\\beta _1=0.9, \\beta _2 = 0.999\\)  and weight decay \\(1\\times 10^{-4}\\) . The initial learning rate is \\(2\\times 10^{-4}\\) , and it decays by \\(0.1\\)  after the 40th epoch. As to the input images, we apply scale augmentation and scale augmentation as in [3], [63].\nWe train Obj2Seq with 16 Nvidia V100 GPUs.\n"]}
{"id": "2201.12093", "categories": "cs.CL", "paragraphs": ["We follow common practices and carry out preliminary grid search on the development set of STSb to decide the hyper-parameter configuration. The learning rate is set to 3e-5 for base models and 1e-5 for large models, respectively. Except for learning rate, We use the same training hyper-parameters for all experiments with the batch size of 64 and the maximum length of 32. The temperature parameter \\(\\tau \\)  is set to 0.05, and the dropout probability is set to 0.1. We train our model for 1 epoch and evaluate the model on the STSb development set every 125 steps, and keep the best checkpoint by following [14].\n"]}
{"id": "2204.05862", "categories": "cs.CL cs.LG", "paragraphs": ["where \\(\\lambda _{\\rm KL}\\ge 0\\)  is a hyperparameter. In practice we use a very small value of \\(\\lambda _{\\rm KL} = 0.001\\) , which likely has a very minor impact during most of RL training (as \\(D_{\\rm KL} < 100\\)  typically), and might actually be wholly unnecessary. More details about RL are provided in REF .\n", "In order to produce additional prompts (i.e. the human side of the conversations) for RLHF training, we used a large LM to generate them. For this purpose, we simply used few-shot learning, creating a context with about 10 existing high-quality human queries, and then sampling to generate more. We find that the sample efficiency of RLHF is roughly the same on the original crowdworker-written prompt dataset and the model-generated one, so we combine the two for greater diversity during RLHF training. We used 137k prompts from the `static' dataset, and 369k model-generated prompts.\n"]}
{"id": "2211.11602", "categories": "cs.LG cs.HC cs.MA", "paragraphs": ["For behavioural cloning, we used the same setup as in [17]. Observations from human-human episodes were sequentially given to the agent to obtain movement and language actions distributions for each timestep and we maximized the likelihood of the ground truth actions taken by humans. We used different coefficient for movement and language actions (\\(w^{\\text{BC}}_{\\text{move}} = 1\\)  and \\(w^{\\text{BC}}_{\\text{lang}} = 5\\) , respectively).\n", "For contrastive self-supervised learning, we used the same technique as in [16], [17] wherein the agent must predict whether vision and language embeddings match (i.e., they are produced from a trajectory from the dataset as normal), or they do not match (i.e., visual embeddings are produced from the input image of one trajectory in the dataset, and language embeddings are produced from the language input from a different trajectory). We implemented this by adding an MLP discriminator that produces a binary prediction and used the same batch of data used for the behavioural cloning for the \u201cmatches\u201d, and a shuffling of vision and language embeddings for the mis-matches. We added this auxiliary loss to the total loss with a weight \\(w^{\\text{CSS}}=1\\) .\n", "For reinforcement learning, we used V-trace [13]. The value function baseline was implemented in the agent by an additional MLP head with a hidden layer size of 512 taking in the same inputs as policy heads do. Both the movement and language policy shared the same rewards and value function. We used a discount factor of \\(\\gamma =0.96\\)  and different coefficients for weighing the movement policy updates (\\(w^{\\text{RL}}_{\\text{move}}=0.5\\) ), language policy updates (\\(w^{\\text{RL}}_{\\text{lang}}=0.1\\) ) and the value function loss (\\(w^{\\text{V}}=1\\) ).\n", "In the RL experiments, we initialize the agent's parameters with those of a pretrained BC agent and continue training with the BC, CSS and RL objectives. Additionally, we add a loss that penalises the KL divergence between the training policy and the initial BC policy. In practice, we found that the only component of the policy that required this penalty was the one controlling when to produce language outputs, for which we used a weight of \\(w^{\\text{KL}}_{\\text{lang}}=0.001\\) .\n"]}
{"id": "2206.06363", "categories": "cs.CV cs.LG", "paragraphs": ["We use a DeepLab-v3\u00a0[13] segmentation model with dilated\u00a0[86] ResNet-50 backbone\u00a0[36] to facilitate a fair comparison with\u00a0[75]. The weights are initialized via self-supervised MoCo\u00a0[16] pre-training on ImageNet. We train the segmentation model for 45 epochs using batches of size 16. The weights are updated through SGD with momentum \\(0.9\\)  and weight decay \\(10^{-4}\\) . The learning rate is \\(2\\cdot 10^{-3}\\)  at the start and reduced to \\(2\\cdot 10^{-4}\\)  after 40 epochs. Further, we use confidence threshold \\(\\tau = 0.9\\)  to select the most confident masks from our Mask R-CNN model (see Section\u00a0REF ). We keep the mask with the largest confidence score when thresholding excludes all predictions in an image from being used. The cross-entropy loss in Eq.\u00a0REF  uses the top-20% hardest pixels. Following\u00a0[75], we freeze the first two ResNet blocks to increase speed.\n"]}
{"id": "2206.06336", "categories": "cs.CL", "paragraphs": ["We use sinusoidal position embeddings\u00a0[103] for the language model.\nThe number of layers is \\(L=24\\) , each layer consists of \\(A=32\\)  attention heads and the hidden dimension is \\(H=2048\\) .\nThe number of parameters is about 1.3B.\nFor the non-causal part, we use encoder-only Transformers, where \\(A=16\\) , \\(H=1024\\) , \\(L=24\\) .\nWe utilize the learnable position embedding and relative position bias\u00a0[81] for the non-causal model.\nThe number of parameters is about 366M.\nWe use DeepNorm\u00a0[107] for Transformers.\nThe connector module is a linear projection layer in our implementation.\n", "The maximum input lengths for non-causal and semi-causal models are 512 and 2048, respectively.\nWe randomly sample random spans whose lengths are between 64 and 128, and feed them to the non-causal part.\nThe total length of non-causal spans is 25% of the original sequence length.\nThe spans do not cross document boundaries.\nWe pretrain the semi-causal language model from scratch.\nThe non-causal module is initialized from a pretrained bidirectional encoder, using the replaced token detection task\u00a0[20].\nDuring pretraining, we freeze all parameters of the non-causal encoder except the last two layers.\nWe pretrain MetaLM for 300k steps with a batch size of 1024 and use Adam\u00a0[48] for optimization.\nWe disable dropout of the semi-causal model and set the dropout rate of the non-causal model to 0.1.\nWe use a learning rate of 6e-4 with warm-up.\nPlease refer to Appendix\u00a0REF  for more pretraining details.\n", "We use a 12-layer non-causal vision-language encoder and a 24-layer language model.\nThe universal task layer follows the same network architectures and configurations of GPT-2\u00a0[78].\nThe hidden size is 1024, and there are 16 attention heads.\nWe employ sinusoidal position embeddings\u00a0[103].\nThe number of parameters is 353M.\nFor the non-causal encoder, we use a vision-language model pretrained as in VLMo\u00a0[109].\nThe number of parameters is 192M.\nWe use 224x224 resolution during pretraining for images.\nThe connector is a three-layer feed-forward network.\nMore details about hyper-parameters can be found in Appendix\u00a0REF .\n", "We pretrain MetaLM for 350k steps with 256 batch size. We use AdamW optimizer with \\(\\beta _1=0.9\\)  and \\(\\beta _2=0.98\\) . The learning rate is 1e-4 and weight decay is 0.01. We use linear decay and apply warm-up at the first 2,500 steps.\nThe dropout rate is set to 0.1.\n"]}
{"id": "2209.13192", "categories": "cs.CL", "paragraphs": ["Our systems\nare implemented on Fairseq-ST [67], following the default settings unless stated otherwise.\nWe adopt a CTC compression to the 8th encoder layer [41], [16] that also reduces RAM consumption.\nBoth the Conformer and Transformer layers have a 512 embedding dimension and 2,048 hidden units in the linear layer. We set dropout at 0.1 in the linear and attention layers. We also set dropout at 0.1 in the Conformer Convolutional layers and a kernel size of 31 for the point- and depth-wise convolutions.\nFor the constrained data condition, we train a one-to-many multilingual model that prepends a token representing the selected target language\nfor decoding\n[24] on all the 7 languages of MuST-Cinema.\nConversely, for the unconstrained data condition we develop two separate bilingual models for each language (en-{de,es}).\nFor inference, we set the beam size to 5 for both subtitles and captions.\n", "We train with Adam optimizer [33] (\\(\\beta _1=0.9\\) , \\(\\beta _2=0.98\\) ) for 100,000 steps. The learning rate is set to increase linearly from 0 to \\(2e^{-3}\\)  for the first 25,000 warm-up steps and then to decay with an inverse square root policy. For fine-tuning, we set a constant learning rate of \\(1e^{-3}\\) .\nThe vocabularies are based on SentencePiece models [52] with size 8,000 for the source language. For all the target languages of the constrained data conditions, a shared vocabulary is built with a size of 16,000 while, for the unconstrained data condition, we build a vocabulary for each language (German and Spanish) with a size of 16,000.\nIn the case of ASR training, we employ on the target side the same source language vocabulary of size 8,000 used in the translation settings.\nThe MT model is trained using the standard Fairseq multilingual machine translation task [48] hyper-parameters, with the same source and target vocabularies of the ST task.\n", "Training is performed on 4 NVIDIA A40 with 40GB of RAM, max tokens of 40k per mini-batch and an update frequency of 2, except for the MT models for which 8 NVIDIA K80 are used with 4k max tokens and an update frequency of 1.\n"]}
{"id": "2203.15082", "categories": "cs.CV", "paragraphs": ["We train the IDUS with mini-batch size fifteen on an NVIDIA Titan X GPU (12GB) with the PyTorch package [53]. The loss function we use is the mean of dice loss [54] and cross-entropy loss. We use weights \\(w_m^{ept} = 1/r_m\\)  and \\(w_m^{dice} = 1/\\sqrt{r_m}\\) , where \\(r_m\\)  was the proportion of \\(m\\) th class label in training samples, to balance the class weights of two losses, respectively. Batch normalization layers [55] are added after each convolutional layer (except the segmentation head) to accelerate the loss convergence. The model's parameters are initialed by a uniform distribution following the scaling of [56] and optimized by Adam [57] algorithm with weight decay value \\(10^{-9}\\) . In every iteration, the learning rate of the model is started with \\(10^{-4}\\)  and decreased by a drop rate \\(0.1\\)  every 100 epochs. The updating interval of superpixel labels and boundaries is 200 and we do this five times (five iterations) throughout training. Consequently, we train for 1000 epochs where every 200 epochs we update the superpixel labels and boundaries.\n", "In IDSS, the training configuration of unsupervised learning is the same as that of IDUS. For the supervised part, the learning rate was set to \\(10^{-4}\\)  and decreased by \\(0.1\\)  every eighty epochs. We first update the parameters of the decoder and segmentation head. After forty epochs, we add the encoder parameters to the optimizer to jointly optimize.\n"]}
{"id": "2206.08640", "categories": "cs.CV cs.AI", "paragraphs": ["We use a CNN with dropout rate 20%, convolutional layers with kernel size 4 and filter size 200. The temporal cell (LSTM, BiLSTM or TCN) contains 100, 100 or 120 neurons, respectively. We interpolate the time-series to 64 time steps, and train the model for 2,000 epochs with early stopping and a batch size of 50.\n"]}
{"id": "2206.08653", "categories": "cs.LG cs.AI cs.CV", "paragraphs": ["Image classification: We conduct experiments across two different backbones (Efficientnetv2S [25] and Mobilenetv2 [26]) initialized using imagenet pre-trained weights followed by a dropout layer with probability 0.4 and a final sigmoid classification layer. We use an image size of 224 x 224 and standard augmentations like Horizontal Flip, Rotation, Contrast, Translation, and Zoom. It is important to note that state-of-the-art approaches [27] use larger image sizes for training (448 x 448), autoaugment [28] and cutout [29] for augmentations, one-cycle learning rate scheduler [30] among other tricks. We are not trying to compete with the state-of-the-art results on multi-label classification and want to demonstrate the value of adding hierarchical knowledge.\n", "Text classification: We conduct experiments by using small bert [31] (uncased, L=2, H=768, A=12) and [32] embeddings as base extractor initialised by weights trained on Wikipedia [33] and Bookscorpus datasets [34]. All text sentences are first converted to lower case.\n"]}
{"id": "2206.08657", "categories": "cs.CV cs.CL cs.LG", "paragraphs": ["We use four public image-caption datasets for pre-training: Conceptual Captions (CC)\u00a0[61], SBU Captions\u00a0[51], MSCOCO Captions\u00a0[3], and Visual Genome (VG)\u00a0[30]. The total number of the unique images in the combined training data is 4M.\nThe statistics of these datasets are shown in Appendix.\nWe pre-train Bridge-Tower\\(_{\\text{BASE}}\\)  for 100k steps on 64 NVIDIA A100 GPUs with a batch size of \\(4,096\\) . The learning rate in pre-training is set to \\(1e^{-5}\\) . No data augmentation is used except for center-crop\u00a0[54], [12]. The image resolution in pre-training is set to \\(288 \\times 288\\) . Other hyperparameters remain unchanged based on the ablation experiments.\n"]}
{"id": "2207.04186", "categories": "cs.CV", "paragraphs": ["For the baseline, we follow [14] to train a BYOL model for 300 epochs (top-1: 73.0) and 1,000 epochs (top-1: 74.3). For our framework, if not explicitly stated otherwise, we use \\(V=2, K=8, s_{\\text{base}}=0.9, s_{\\text{view}}=0.6\\) . We use LARS optimizer with a base initial learning rate \\(0.3\\times \\text{batch size}/256\\)  for 300 epochs and \\(0.2\\times \\text{batch size}/256\\)  for 1,000 epochs, with a cosine learning rate decay, and a warm up period of 10 epochs.\n"]}
{"id": "2212.06742", "categories": "cs.CL cs.LG cs.PL cs.SE", "paragraphs": ["We use the same T5 architecture with a 12-layer encoder, a 12-layer decoder, 768 hidden units (\\(d_\\text{model}\\) ), 12 heads, 2048 feedforward linear units (\\(d_\\text{ff}\\) ), GELU activations, a dropout\u00a0[36] rate as 0.1, and no embedding tying. [3] find no difference between training from pre-trained model weights and that from scratch, except that the former converges more quickly. To this end, we use mT5 checkpointhttps://github.com/google-research/multilingual-t5#released-model-checkpoints for initialization, which already contains strong multilingual NL representations.\n", "For pre-training, we set the maximum length (L) of 512/1024, a micro-batch size of 8/4 with a gradient accumulation step of 15. We utilize the Adafactor\u00a0[34] optimizer and a linear warmup of 1000 steps with a peak learning rate of 1e-4. All pre-training tasks are run on a cluster of 32 NVIDIA A100 GPUs with 40G memory for 100,000 training steps. To accelerate the pre-training, we utilize the ZeRO stage1 approach\u00a0[31] for partitioning optimizer states and enable BFloat16 half-precision format for mixed-precision training. The total pre-training time lasts around four weeks.\n"]}
{"id": "2210.11610", "categories": "cs.CL", "paragraphs": ["We follow previous studies\u00a0[43], [40] and conduct our experiments on an autoregressive Transformer-based language model with 540 billion parameters. The CoT examples for each dataset are listed in Appendix\u00a0REF .\nWe generate \\(m=32\\)  reasoning paths for each question in a training set. Since each reasoning path is augmented into four formats in Sec.\u00a0REF , the final training samples are up to the size of \\(128\\times |\\mathcal {D}^\\mathtt {train}|\\) , with \\(|\\mathcal {D}^\\mathtt {train}|\\)  being the size of the corresponding training set.\nFor all datasets except DROP, we use the whole training set; To reduce the training burden, we sample 5k examples from the non-football and football partition of the DROP dataset, and sample 5k examples from ANLI-A2 and ANLI-A3.\nFor each dataset, we fine-tune the model for 10k steps with a learning rate of 5e\\(-5\\)  and a batch size of 32.\nFor multiple path decoding, we use a sampling temperature of \\(T=0.7\\)  with the pre-trained model as suggested by\u00a0[40]. We use \\(T=1.2\\)  for the language model after self-improvement (LMSI). We set the maximum number of decoded steps to 256 for all experiments.\n"]}
{"id": "2209.02522", "categories": "cs.CV", "paragraphs": ["In our work, we leverage the work of Jia \u00a0[14] as our baseline.\nWe train all models using a learning rate of \\(1e-4\\) , weight decay of \\(5e-4\\) , and the Adam optimizer if otherwise stated.\nRegarding the learning rate schedule, we apply a plateau scheduler that reduces the learning rate by a factor of 0.1 if the validation results do not improve for four epochs.\nBatches of size 64 are used per default, and gradient clipping is applied after computing the gradients based on the weighted cross-entropy loss function\u00a0[15].\nThe backbone networks are initialized with pre-trained weights from the ImageNet dataset\u00a0[2].\nMore details can be found in the supplementary material.\n"]}
{"id": "2211.14133", "categories": "cs.LG", "paragraphs": ["We pretrain BERT-Base [6] on the English Wikipedia (Phase 1 only) by NVLAMB and K-FAC.\nFor NVLAMB, we set mini-batch size 8,192, max sequence length 128, weight decay 0.01, base learning rate \\(6\\cdot 10^{-3}\\) , total training steps 7,038, and linear learning rate warming up steps 2,000.\nThe learning rate at the \\(t\\) -th step after warm-up is determined by the polynomial decay: \\(\\eta _t=\\mathrm {base\\_lr}\\times (1-t/\\mathrm {total\\_steps})^{0.5}\\) .\nFor K-FAC, the same hyperparameters are used except that the number of learning rate warming up steps is reduced to 600, resulting in larger learning rates than NVLAMB until the 2,000th step.\nThe pretraining loss versus the number steps is shown in Figure\u00a05 (left).\nfig:bertbaselr shows the learning rate schedule.\n", "Setting the micro-batch size to 32 (maximum number of powers of 2 that can be placed on a P100 GPU) would require 256 GPUs to run training with mini-batch size 8,192.\nHowever, to reduce total GPU hours and energy and CO\\(_2\\)  overheads, we simulate this training by using 32 GPUs and accumulating the micro-batch gradient over 8 steps before updating parameters (\\(32\\times 32\\times 8 =8,192\\) .)NVLAMB on 32 GPUs takes 3.74 seconds per parameter update (with a mini-batch of size 8,192) while 128 GPUs takes 1.23 seconds. Hence the speedup (3.04x) is not linear to the number of GPUs.\n", "In addition to this, the training is done using simple data parallelism without pipelines for reducing GPU hours.\nThis is because the entire BERT-Base model fits into the P100 GPU device memory (16 GB), and in this case data parallelism without any model partitioning saves the most GPU hours on 32 GPUs in the GPU cluster we use (although it increases the communication cost of the allreduce of gradients for data parallelism.)\nWhile the target of model partitioning is a model that is too large to fit in the memory of a single device, our study simulates the effects of pipelining with relatively small Transformers (i.e. BERT-Base and -Large) compared to today's GPU memory limitations.\nYet, the same techniques, discussions, and benefits of pipelining (and PipeFisher) described in our study are applicable to even larger Transformers.\n", "The choice of the parallel training strategy does not affect the convergence of NVLAMB as long as the micro-batch gradients are synchronizedWe use the fp32 precision for every quantity (parameters, gradients, optimization state) in training, so we assume that the effect of the numerical precision is negligible., which is the case in all of our experiments.\nFor K-FAC, we use data- and matpltpurpleinversion-parallel K-FAC (Figure\u00a02 (ii,b)), and the matpltgreencurvature and matpltpurpleinverse matrices are refreshed once in 10 steps.\nWe assume this does not affect the convergence by PipeFisher because it refreshes the matrices more frequently (once in 5-10 steps) in this BERT-Base setup as described in Figure\u00a05.\n{FIGURE}"]}
{"id": "2208.06061", "categories": "cs.CL", "paragraphs": ["Both the standard Transformer and the TP-Transformer (TPT) use 6 layers and 8 heads per layer. TPT has key/value/query/role dimensions of 64, whereas the standard Transformer has key/value/query dimensions of 80. The reason for this increase is so that the resulting models match in terms of parameter count, and we add parameters are the most homologous area. The standard Transformer has 74,375,936 parameters, and the TP-Transformer has 74,385,152 parameters. Both networks use a token dimension of 512, a feedforward dimension of 2048, and 32 relative positioning buckets [35]. The input vocabulary size is 50,000. We set a training batch size of 80 per GPU and used the Adafactor [36] optimizer with square root learning rate decay. Throughout the model, we used a commonly used dropout rate of .1.\n"]}
{"id": "2205.09470", "categories": "cs.LG cs.AI cs.DC", "paragraphs": ["The prevailing Transformer-encoder was adopted as the backbone of the model in Scenario-I. For the generators, a structure with 12 layers, 768 hidden units, 12 heads was employed. For the discriminator, a structure with 24 layers, 1,024 hidden units, 16 heads was put to use. The activation function used is GeLU [48]. In order to maximize the utility of existed models, we initialized the parameters of the generators with \\(\\textsc {ERNIE-M}_{Base}\\) , and the discriminator with \\(\\textsc {ERNIE-M}_{Large}\\) , respectively. We used the Adam optimizer [49] to train ERNIE-M Extra; the learning rate was scheduled with a linear decay with 10K warm-up steps, and the peak learning rate was \\(1e-4\\) . The hyperparameter \\(\\lambda \\)  and \\(\\gamma \\)  were set to 50 and 1 respectively. The training was separated into two steps, i.e. intra-cluster and inter-cluster. During intra-cluster training, we conducted the pre-training experiments using 64 NVIDIA A100-40GB GPUs with 2,048 batch size and 512 max length. During inter-cluster training, we used 8 NVIDIA V100-32GB GPUs and 64 Ascend 910-32GB NPUs and keep other hyperparameters the same.\n"]}
{"id": "2205.09443", "categories": "cs.CV", "paragraphs": ["The hyper parameter settings differ a lot in previous works for skeleton-based action recognition using GCN.\nIn PYSKL, we use the same hyper parameter setting to train all GCN models.\nWe set the initial learning rate to 0.1, batch size to 128, and train each model for 80 epochs with the CosineAnnealing LR scheduler.\nFor the optimizer, we set the momentum to 0.9, weight decay to \\(5\\times 10^{-4}\\) , and use the Nesterov momentum.\nWe find that for most GCN networks, the new hyper parameter setting leads to better recognition performance than previous settings that use the MultiStep LR scheduler.\n"]}
{"id": "2212.00942", "categories": "cs.CV", "paragraphs": ["All the models are implemented based on PyTorch and trained using NVIDIA RTX 3090. The batch size is 64. Adam optimizer is used with \\(\\beta _1\\) =0.9, \\(\\beta _2\\) =0.999 and \\(\\epsilon \\) =1e-8. We use cross-entropy loss as the loss function.\n"]}
{"id": "2201.09199", "categories": "cs.LG cs.AI", "paragraphs": ["We evaluate MLAS under different pre-training parameters in this set of experiments.\nATT and SEQ are not included in this set of experiments since they only utilize one data type. Output dimension is set to 5. Minimum cluster size is set at 50.\nFigure\u00a0REF  presents the results under different pre-training parameters. This confirms that our proposed MLAS method is not sensitive to different pre-training parameters.\n"]}
{"id": "2208.01448", "categories": "cs.CL cs.LG", "paragraphs": ["We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens (total of 1 trillion token updates). We used Adam optimizer\u00a0[35] with \\(lr=1e^{-4}\\)  with linear decay to \\(lr=5e^{-6}\\)  over 500k updates. We used weight decay of \\(0.1\\)  on all parameters except biases and layernorms. Finally, we trained the model in BFloat16 which helped with the stability of training\u00a0[59].\n", "We used DeepSpeed's ZeRO Stage 3\u00a0[61] to partition model weights, optimizer states, and gradients across all GPU workers, allowing us to train the model with high throughput. We relied on an internal and optimized version of DeepSpeed that we have since open-sourced\u00a0[6] to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.\n"]}
{"id": "2203.08243", "categories": "cs.LG cs.CV", "paragraphs": ["Numerically, the learning rate for parameter \\(z\\)  is always changing during the primal-dual algorithm process. Thurs, we propose to use a dynamic learning rate for the parameter \\(z\\)  that controls the budget constraint. We use a four-step schedule of \\(\\lbrace 1,5,9,13,17\\rbrace \\)  in practice.\n"]}
{"id": "2209.00642", "categories": "cs.CV cs.CL cs.SD eess.AS", "paragraphs": ["In our experiments we set \\(\\lambda _{r}=10\\) , \\(\\lambda _{k_{global}}=5\\) , \\(\\lambda _{k_{local}}=5\\)  and \\(\\lambda _{voice}=5\\) . We follow the pre-processing procedures of Lip2Wav\u00a0[35] to detect and extract face crops from training videos. We create video inputs by randomly sampling a window of \\(T=25\\)  (1 sec.) contiguous face crops resized to \\(96\\times 96\\) . The corresponding audio segment is sampled at 16kHz. We compute STFT with a hop length of \\(10ms\\)  and a window length of \\(25ms\\) . We finally obtain melspectrograms with 80 mel-bands and \\(T^{\\prime }=100\\)  mel time-steps (1 sec.). We use a batch size of 32 and RMSProp\u00a0[39] optimizer with an initial learning rate of \\(0.00005\\)  for both the generator and the discriminator, which is advised for training a WGAN model\u00a0[21]. The generator is trained every five discriminator iterations following\u00a0[21]. As this is a WGAN, the discriminator loss shows the progress of training and correlates with the quality of the generated samples. Hence, we stop the training once the discriminator loss does not improve for 10 epochs. During inference, we feed the speaker embedding and the lip distribution to the decoder instead of the content distribution. Since our model can take a variable number of time steps as input, it can directly generate for any length of video without any further changes.\n"]}
{"id": "2211.00310", "categories": "cs.LG", "paragraphs": ["We evaluate SADT by classification tasks using CIFAR10 and CIFAR100 datasets. The neural architectures used are Simple CNN (custom model of 3 conv and 3 dense layers), VGG [32], and InceptionResNet [33]. The SADT variants are compared against related works in model generalization: Gradient Centralization (GC) [8], Adaptive Gradient Clipping (AGC) [9], and Sharpness-Aware Minimization (SAM) [0]. In particular, self-distillation methods were not included to the comparison, as they require multiple training rounds, which makes them incompatible with our single-round experimental setting. In order to make the training landscape uniform for all methods considered, the training always starts from the same initial point, and all methods use the same optimizer (Adam), learning rate scheduler (cosine-decay with initial rate of 0.0001), batch size BS (512 and 2048), epoch count (200 for BS 512 and 370 for BS 2048), and data augmentation scheme (CutMix [10]).\n{FIGURE}"]}
{"id": "2212.05289", "categories": "cs.LG", "paragraphs": ["The weights of the TSCNNs are initialized with a Gaussian distribution \\(\\sim N(0, 0.01)\\) . The network is trained using backpropagation to minimize the binary cross-entropy (BCE) loss function:\n\\(BCE(y, \\hat{y}) = -(y\\log (\\hat{y})+(1-y)\\log (1-\\hat{y})),\\) \n", "where \\(y\\)  denotes the true label and \\(\\hat{y}\\)  denotes the predicted label.\nAll models are trained using the Adam optimizer with a learning rate of 0.00025\u00a0[26].\nThe dropout rate and the batch size are set to 50% and 64, respectively. All the experiments are conducted on a laptop with AMD-Ryzen 7-5800H 3.20-GHz and 16-GB memory.\n"]}
{"id": "2207.06893", "categories": "cs.CV", "paragraphs": ["All experiments are implemented and conducted using the PyTorch framework, on a server platform with 4 V100 GPUs. All models are trained for 300 epochs from scratch with binary weights and activation. The initial learning rate is set to 2e-4 and halved every 200 epochs. The mini-batch size is set to 16 and the ADAM\u00a0[8] optimizer is adapted.\n"]}
{"id": "2207.08457", "categories": "cs.LG cs.AI stat.ME", "paragraphs": ["The policy network for the experiment in Section  has a fully connected layer of size 30, followed by an LSTM layer of size 30. The actor-network has one fully connected layer of size 30, the critic-network one fully connected layer of size 10. The length of each episode was set to 10 and the model trained for 5 million training steps. For all other parameters, the default values were used.\n", "The following configuration for the policy network worked best after preliminary experiments for the 3-variable (4-variable) environments: One (two) fully connected layer(s) of size 30 (40) followed by an LSTM layer of size 30 (160). Its outputs are fed into a fully connected layer of size 30 (60) for the actor-network and one of size 10 (20) for the critic-network. For this experiment, we set the episode length to 20. The network was evaluated every 500000 steps in both cases. All runs were stopped after 14 evaluations of the policy on the test setwere worse than the best policy found so far to avoid overfitting. In the 3-variable set, we use the first 18 environments for training and the last 5 for testing. In the 4-variable set, we use the first 500 environments for training and the last 42 for testing. The best model for the 3-variable (4-variable) environments is obtained after 14.5 (31) million training steps.\n"]}
{"id": "2212.02802", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["We optimize the learnable parameters jointly on 77294 videos of VoxCeleb1 dataset [15]. The videos are aligned and cropped for interesting face regions as in Tzaban \u00a0[31]. We use 4 GPUs and Adam optimizer [12] with the learning rate of 1e-4. Total training steps are 1 million and 4 frames per video so the total of 16 frames for 4 videos are taken for a single training step.\n"]}
{"id": "2210.10759", "categories": "cs.LG math.OC", "paragraphs": ["We use Adam\u00a0 as our training optimizer with learning rate of \\(0.0001\\) . The loss function is taken as mean squared error. All the experiments are conducted on a Linux server with an Intel Xeon Platinum 8163 GPU and eight NVIDIA Tesla V100 GPUs.\n"]}
{"id": "2210.10692", "categories": "cs.CL", "paragraphs": ["The filtering models were trained to accept a pair of sentences from the source and target languages. During training, the [CLS] token hidden representation of the input sentence pairs is fed into a linear Layer and the model is optimized using binary cross entropy loss. However, at inference time, we add a sigmoid layer to the output to predict a number between \\(0.0\\)  and \\(1.0\\)  indicating the likelihood of the bitexts being translations of each other. We fine-tuned these models using each language's train split of positive and negative samples, then evaluated performance on the test set while optimizing on the development set.\n", "We fine-tuned the M2M-100 model based on the implementation within the Fairseqhttps://github.com/facebookresearch/fairseq toolkit [50]. We used batch sizes of \\(2,048\\)  tokens, a maximum sentence length of \\(1,024\\) , and a dropout of \\(0.3\\) . For optimization, we used Adam [36] with \\(\\beta _1\\)  = \\(0.9\\)  and \\(\\beta _2\\)  = \\(0.998\\) , a learning rate of \\(5e-5\\)  and a warmup of \\(2,500\\)  updates. The optimizer uses a label-smoothed cross-entropy loss function with a label-smoothing value of \\(0.2\\) . All models were trained for a maximum of \\(1,000,000\\)  update steps. We tokenized all data using the model's SentencePiece [42] tokenizer.\n"]}
{"id": "2205.11779", "categories": "cs.LG cs.AI cs.NI", "paragraphs": ["If \\(\\#(nbr(n))=1\\) , Eqn. REF  can be simplified into the following formula with the sole neighbor \\(k\\) :\n\\(\\theta ^{\\prime (n)}_{me} = \\big (1-\\frac{\\lambda }{2}\\big )\\theta ^{(n)}_{me} + \\frac{\\lambda }{2}\\theta ^{(k)}.\\) \n", "This means that the model parameters \\(\\theta ^{(n)}_{me}\\)  is shifted toward \\(\\theta ^{(k)}\\) . If \\(\\lambda =2\\) , it will be replaced with \\(\\theta ^{(k)}\\) . If \\(\\lambda =1\\) , it is shifted at the center position between \\(\\theta ^{(n)}_{me}\\)  and \\(\\theta ^{(k)}\\) . Practically, \\(\\lambda \\)  will be set to 1 or less: i.e., \\(0 < \\lambda \\le 1\\) .\n"]}
{"id": "2202.03954", "categories": "cs.CV cs.AI", "paragraphs": ["We set the training batch size to 128. The model was trained for 60 epochs using adam optimizer. The initial learning rate is 0.001, and changed to 0.0001 after 30 epochs. To avoid over-fitting, the L2 regularization is activate and the regularization parameter is set to 0.1. Meanwhile, the dropout probability \\(p_{drop}\\)  is set to 0.2 for all dropout layers. The hyperparameter \\(\\mathcal {H}\\)  of interaction catergory num is set to 2,4 and 6, respectively. The temperature hyperparameter \\(\\tau \\)  is set to be 0.1. The weights hyperparameter \\(\\lambda _z\\)  and \\(\\lambda _c\\)  in loss functions are set to be 0.005.\n"]}
{"id": "2203.10741", "categories": "cs.CL", "paragraphs": ["During training, we set the number of tokens in each batch to \\(10{,}240\\)  for QSGen-Hier, QSGen-ChildQ, and full summary generation on WikiBioSum.\nOn GovReport, each batch contains \\(16{,}384\\)  tokens.\nAs limited by the design of Longformer, the maximum output length for all tasks is set to \\(1{,}024\\) .\nWe use Adam\u00a0[26] as the optimizer, with a maximum learning rate of \\(5 \\times 10^{-5}\\) . The optimizer updates the model parameters every 8 batches.\nWe set the maximum numbers of update steps to 500, 700, \\(2{,}400\\) , and \\(5{,}000\\)  respectively for QSGen-Hier, QSGen-ChildQ, WikiBioSum, and GovReport.\nImportantly, we adopt gradient checkpointing\u00a0[10] to reduce the memory consumption of back propagation.\n"]}
{"id": "2210.04726", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["The input length for training KPs is normally short because our examples are masked serialized triples (concatenation of Subject/Object entity and a relation). Therefore, we set the input length to 64, which allows us to use very large batch sizes: between 4K and 8K, depending on the model size.\nNote that the objective when training KPs is to memorize the training data. Hence, we let KP training run for up to 200 epochs.\n"]}
{"id": "2212.09438", "categories": "cs.CV", "paragraphs": ["We trained our models with stochastic gradient descent (SGD) with learning rate\u00a02.5e-4, Nesterov momentum\u00a00.9 and weight decay\u00a05e-4.\nWe trained the model with 100,000 steps each including a minibatch of 16 samples from the Mapillary data set and 32 samples from the FGI autonomous steering data set.\nWe used more samples from the FGI autonomous steering data set due to the smaller size of the images and to have more supervision for the steering task, as it has only one scalar value for supervision on each image.\nWe chose the model with highest validation accuracy which was evaluated every 1000th step during training. The discriminators are trained with Adam optimizers using learning rate\u00a01e-4 and parameters\u00a0\\(\\beta _1=0.9\\) , \\(\\beta _2=0.99\\) .\n"]}
{"id": "2212.09400", "categories": "cs.CL", "paragraphs": ["Considering the size of train set is relatively small, we propose two different training settings, i.e., traditional deep learning training and cross-validation training [16].\nIn traditional setting, the train and dev sets are independent, and we train the model only train data and evaluate model on dev data, preserve the highest-accuracy model within at most 10 training epochs.\nIn cross-validation setting, we mix train and dev set into one set, then 9-fold cross-validation is utilized.\nIn order to avoid over-fitting training in cross-validation setting, we early stop training in chronological order and preserve three models for official evaluations.\nEach training epoch takes approximately 0.5 hour, and evaluating the model costs about 5 minutes.\n"]}
{"id": "2203.03583", "categories": "cs.CL cs.SD eess.AS", "paragraphs": ["We use Conformer-M\u00a0[28] as the ASR encoder.\nThe input frames are 80-dim log-Mel filterbanks extracted from 25ms window and 15ms overlap.\nWe train the model with CTC loss using AdamW optimizer and peak learning rate of \\(1e^{-3}\\) .\nDuring the training, we exploit SpecAugment\u00a0[29], dropout of 0.1, and weight decay of \\(1e^{-5}\\)  for the regularization.\nThe learning rate linearly increases for 5K iterations and is fixed at the peak for 295K iterations.\nThen, the learning rate decays following the inverse square root schedule\u00a0[30] for 700K iterations.\nThe batch size is set to 16 for each GPU and 4x RTX Titan(24GB) GPUs are used.\nGradients are accumulated for 4 batches, expanding the effective batch size to 256.\nWe follow additional training details from previous work that employed the same encoder\u00a0[31].\n", "LSTM-LM and Transformer-LM are a stack of LSTM and Transformer-XL\u00a0[1] layers, respectively.\nWe train these neural network-based LMs with and without SkipTC, sharing the same training configuration.\nLMs consist of 4 layers where the hidden dimension of each layer is 512.\nVocabulary size is set to 431, including 399 LC+V tokens, 27 TC tokens and 5 special tokens: <pad>, <sos>, <eos>, <unk> and <space>.\nWhen using SkipTC token, the <unk> token takes the role of SkipTC token.\nThe embedding weight and the final output weight are tied as the common practice\u00a0[32].\nWe train LSTM-LM using SGD with a momentum of 0.9, an initial learning rate of 0.1, weight decay of \\(1e^{-6}\\) , and exponential learning rate decay with a factor of 0.99.\nFor Transformer-LM, we use Adam optimizer with a learning rate of 0.001.\nThe training is conducted for 50 epochs with a batch size of 128.\n"]}
{"id": "2206.02428", "categories": "cs.CL", "paragraphs": ["All models were implemented with Pytorch and Hugging Face Transformers [17].\nFor models without pre-trained language backbones (e.g. Bi-DAF, R-Net), Glove embedding [18] was utilized, and out-of-vocabulary words were replaced with the \\(<\\) unk\\(>\\) token. Hidden size and embedding dimension were 300, and those of Transformer-based models were 768. We used Adam [7] with batch size 32, and gradient accumulation was applied. The initial learning rates were set at 2e-5, and dropout rate [22] was set to 0.2. During training, the validation-based early stop strategy was applied. During prediction, we selected answer spans using the maximum product of \\(p_{start}\\)  and \\(p_{end}\\) .\n"]}
{"id": "2206.10520", "categories": "cs.CV", "paragraphs": ["We trained StyleGAN2-ADA [13] on the CASIA-WebFace dataset under conditional settings, i.e. generating images of specific classes. The number of classes is set to 10,575 (number of identities in CASIA-WebFace), which is embedded into a 512-D vector and then concatenated with the latent vector (512-D) to generate class-related synthetic images.\nWe kept most of the implementation settings of StyleGAN2-ADA unchanged (as set in [13]), including the training setup, network architectures, optimizer, and training loss. We have only set the StyleGAN2-ADA model training epochs to 50, the batch-size to 128, and the learning rate to \\(0.0025\\) .\n", "The presented FR models in this paper utilize ResNet-50 [11] as a backbone architecture.\nWe train five instances of ResNet-50. The first instance is trained with CosFace loss [42] on the authentic CASIA-WebFace dataset [44] and is used for KT (model P). We followed the parameter selection in [42] for the CosFace loss margin value, and scale parameter to 0.35 and 64, respectively.\nThe other four instances are trained on the synthetic SFace dataset. The second instance is solely trained with CosFace loss, i.e. CLS, on the SFace dataset. The third instance is trained with only KT loss. The fourth and the fifth instances are trained with the CL loss with \\(\\alpha \\)  in Equation REF  of 1e-5 and 2e-5, respectively. The models trained on synthetic data are always noted with the training subset (SFace-10, SFace-20, SFace-40, or SFace-60) and the training strategy (CLS, KT, or CL).\n"]}
{"id": "2203.00242", "categories": "cs.CV cs.AI cs.CL cs.MM", "paragraphs": ["Our transformer architecture consists of 12 layers of transformer blocks, where each block has 768 hidden units and 12 self-attention heads.\nWe initialize the model from \\(\\text{BERT}_{base}\\)  and pre-train for 20 epochs on their respective pre-training datasets with a batch size of 480. The region features for images are obtained from the pre-trained VinVL object detectors [47]. We use Adam optimizer [18] with a linear warm-up for the first 10% of training steps, and set the peak learning rate as 6e-5. After warm up, a linear-decayed learning-rate scheduler gradually drops the learning rate for the rest of training steps.\nAll models were trained on 4 NVIDIA A100 GPUs, with 40GB of memory per GPU using MMF[35].\nThe pre-training takes 3 days.\nWe evaluate our pre-trained models on four downstream tasks: Visual Question Answering (VQA 2.0)[0], Natural Language for Visual reasoning[37] (\\(\\text{NLVR}^2\\) ), Visual Entailment[41] (VE), and Referring Expression[44] (RefCOCO+).\nDetailed training settings for each task can be found in our supplementary material.\n"]}
{"id": "2210.02798", "categories": "cs.CV", "paragraphs": ["We explore pre-training strategies on single objects (ShapeNet\u00a0[7]) and complex scenes with multiple objects (ScanNet\u00a0[12]) to evaluate the effectiveness of SoftClu. We implemented SoftClu in PyTorch and executed our experiments on two Tesla V100-PCI-E-32G GPUs. During pre-training, we set \\(J=64\\)  and \\(\\epsilon =1e-3\\) .\n", "ShapeNet\u00a0[7] is a collection of single-object CAD models and contains 57448 synthetic objects from 55 object categories.\nWe follow the experimental setup presented in [38], [20], we use PointNet\u00a0[34] and DGCNN\u00a0[47] as encoder networks.\nThe latent dimension of both encoders is 1024.\nFollowing\u00a0[20], each point cloud is randomly downsampled to 2048 points.\nWe use the official training split of ShapeNet for pre-training.\nOur pre-training involves 250 epochs by using the AdamW [26] optimizer, the batch size is equal to 32, and initial learning is equal to \\(0.001\\)  that decays by \\(0.7\\)  every 20 epochs.\n", "ScanNet\u00a0[12] is a dataset of indoor scenes with multiple objects and consists of 1513 reconstructed meshes for 707 unique scenes. Following\u00a0[48], we choose SR-UNet provided in PointContrast\u00a0[48] as backbone.\nFor pre-training, we use an SGD optimizer with a learning rate of 0.1 and a batch size of 32. The learning rate is decreased by a factor of 0.99 every 1K iteration.\nThe model is trained for 30K iterations.\n"]}
{"id": "2211.02536", "categories": "cs.CL cs.AI cs.SD eess.AS", "paragraphs": ["For the mppt of any iteration (biased or unbiased), around 50% of the input frames are masked by applying overlapping masks of length 20 frames (equivalent to 200\u00a0ms; [9] used masks length 10 for 50\u00a0Hz features i.e. also 200\u00a0ms). This is done by choosing 4% of frames as starting frames for the masks of length 20. The classification loss is computed only for the masked frames. The models are trained on 32 GPUs with a batch size of 87.5 seconds per GPU (for 250k updates, this is equivalent to 169 epochs). The learning rate ramps up linearly for the first 8% of updates and then decays linearly down to 0. The peak learning rate is 5e-4.\n"]}
{"id": "2211.12047", "categories": "cs.CV cs.LG", "paragraphs": ["\nwhere \\(\\lambda \\)  is modulation factor meant to control the time scale of the evolution of the error filters (and generally set to be less than one, e.g., \\(0.9\\) ) - note that this update rule is discarded if \\(\\mathbf {E}^\\ell _{ji} = (\\mathbf {W}^\\ell _{ij})^T\\)  as we do in this work.\nNote that the above local update rule is a generalization of the rule proposed in [38], [36] to the case of a (de)convolutional filter.\nAfter the update for any particular filter has been calculated and it has been used to adjust the current physical state of the kernel synapses, we further normalize/constrain each kernel such that its Euclidean norm does not exceed one (see Algorithm REF  for the specific re-projection step). This constraint ensures that Conv-NGC avoids the degenerate/trivial solution of simply increasing its synaptic kernel values while obtaining only small/near-zero latent activity values, much as is done in convolutional sparse coding [18] (this also means that one could alternatively view Conv-NGC as a sort of \u201cdeep\u201d convolutional sparse coding).\n"]}
{"id": "2209.10359", "categories": "cs.CV cs.AI", "paragraphs": ["If not otherwise specified, we set the momentum \\(\\alpha \\)  in Eq.\u00a0REF \nto \\(0.95\\)  and the length of the noise vector to 256. We train the\nstudent \\(\\mathtt {S}\\)  using SGD and Adam for the small and large datasets,\nrespectively. We train the generator \\(\\mathtt {G}\\)  using Adam for both the\nsmall and large datasets. To reduce the difficulty of training \\(\\mathtt {G}\\) \nfor the large datasets, we pretrain \\(\\mathtt {G}\\)  for some steps before\nthe main KD training by setting the coefficient of \\(\\mathcal {L}_{\\text{KD}}\\) \nto 0 and only optimizing the remaining losses in Eq.\u00a0REF .\nThe generator \\(\\mathtt {G}\\)  is class-conditional (as described in Section\u00a0REF )\nfor the large datasets, and unconditional for the small datasets.\nWe empirically found that this leads to better results. For further\ndetails about the training settings of \\(\\text{MAD}\\) , please refer to\nAppdx.\u00a0REF .\n"]}
{"id": "2210.00944", "categories": "cs.CV", "paragraphs": ["Datasets.  In our experiments, ImageNet-Subset\u00a0[48] is used for ablation study and to compare with other self-supervised knowledge distillation methods.\nThis dataset contains 100 classes and \\(\\approx \\) 130k images in high resolution (resized to 224\\(\\times \\) 224) \u00a0[46]. For comparison with SSL methods, we employ the ImageNet-1K dataset\u00a0[48].\n"]}
{"id": "2210.00960", "categories": "cs.LG", "paragraphs": ["We mainly consider the experiments on CIFAR-10 [28], CIFAR-100, and SVHN [35]. We also provide one experiment on ImageNet [12]. For the first three datasets, we conduct the experiments on training PreActResNet-18, which follows [41], For the experiment on ImageNet, we use ResNet-50 [21], following the experiment of [32]. For the inner problems, we adopt the \\(\\ell _\\infty \\)  PGD adversarial training in [32], the step size in the inner maximization is set to be \\(\\epsilon /4\\)  on CIFAR-10 and CIFAR100 and is set to be \\(\\epsilon /8\\)  on SVHN. Weight decay is set to be \\(5\\times 10^{-4}\\) . Additional experiments are provided in Appendix REF . https://github.com/JiancongXiao/Stability-of-Adversarial-Training\n{FIGURE}"]}
{"id": "2212.12326", "categories": "cs.CV cs.AI", "paragraphs": ["We fine-tuned a model trained on the Places2 dataset\u00a0[33] using our landscape dataset\u00a0[34]. The original pre-trained model has respectively trained 2 million iterations for edge inference and content inpainting, where content inpainting includes training taking ground truth as conditions and training taking generated edge maps as conditions. Additionally, we trained our edge inference model with 75,000 iterations, the content inpainting model taking ground truth edge maps as conditions with 75,000 iterations, and the content inpainting model taking generated edge maps as conditions with 75,000 iterations. All training is done on RTX-2080 Super graphics.\n{FIGURE}"]}
{"id": "2212.12937", "categories": "cs.CL cs.LG", "paragraphs": ["The model trainable parameters in \\({GAE}_{doc}\\) , \\({GAE}_{sent}\\) , GRU, and sentence scoring {\\(\\omega \\) , \\(W1\\) , \\(W2\\) } are described below.\nTo perform document summarization using\u00a0GAE-ISumm, we set the first convolution layer's embedding size as 128 for distributed word embeddings and 256 for remaining representations.\nThe input feature vectors are extracted from different language models with an input size of 300 for word embeddings and 768 for other representations.\nSince IndicBERT\u00a0[28] shows superior performance over other multilingual models, in this paper, we use IndicBERT for input feature extraction for all other experiments.\n", "We use Adam optimizer with an initial learning rate of 0.001 to train \\(GAE_{doc}\\) .\nWe use the scikit-learn package of Spectral Clustering to perform sentence graph clusterization.\nAfter experimenting with various values, the number of clusters is considered close to the average number of sentences in annotated summaries.\nThe joint loss function is optimized with an Adam optimizer and a learning rate of 0.0005.\nWe experimented with a range of values to determine the choice of \\(\\alpha \\)  and \\(\\beta \\) .\nThe model was effective when \\(\\alpha \\) =0.6 and \\(\\beta \\) =0.4.\nTo extract the summary, we chose the value of K (number of sentences in the predicted summary) dependent on the number of clusters.\nWe train each phase of the model to a maximum of 40 epochs.\nThe experiments were performed on a single V100 16GB RAM GPU machine.\n"]}
{"id": "2209.07562", "categories": "cs.CL", "paragraphs": ["Model Architecture.\nWe use the same Transformer architecture as BERT\u00a0[10] for our language model.\nWe adopt the XLM-R\u00a0[9] tokenizer, which offers good capacity and coverage in all languages.\nThe model has a vocabulary size of 250K.\nThe max sequence length is set to 128 tokens.\nWe use a \\([768, 768]\\)  dimensional projection head for contrastive loss.\nNote that although we have chosen this specific architecture, our social objective can be used in conjunction with a wide range of language model architectures.\n", "Training Procedure.\nOur training has two stages.\nIn the first stage, we train the model from scratch using the 6 billion Tweets without user engagement.\nThe model is trained for 500K steps on 16 Nvidia A100 GPUs (a2-megagpu-16g) with a total batch size of 6K.\nWe set the peak learning rate to 2e-4 and use the first 30K steps for warm-up.\nIn the second stage, the model is trained for another 500K steps on the 1 billion Tweets with the joint MLM and engagement loss.\nWe set the peak learning rate to 1e-4, contrastive loss temperature to 0.1, loss balancing factor \\(\\lambda \\)  to 0.1 for the second stage.\nWe follow RoBERTa\u00a0[22] for the rest of our hyperparameters.\nWe use mixed precision during training.\nOverall pre-training takes approximately five days.\n"]}
{"id": "2209.07522", "categories": "cs.CV cs.LG", "paragraphs": ["\u00a02)\u00a0 ViT probing: train \\(h\\)  only, with \\(f\\)  frozen.\nHere, \\(h\\)  is a ViT-Base, instead of a linear layer as used for linear probing.\nViT probing is much more lightweight than both fine-tuning and joint-training. It trains 3.5 times fewer parameters than even linear fine-tuning (86M vs. 306M).\nIt also obtains higher accuracy than linear fine-tuning without aggressive augmentations on the ImageNet validation set.\n"]}
{"id": "2209.10930", "categories": "cs.CV", "paragraphs": ["We used Resnet50 [8] with frozen batchnorm layer as Backbone for MGTR, the number of Encoder and Decoder layers are both set to 6, the same as in [27]. Both Backbone and Encoder-Decoder use the pretrained parameters from COCO [13] pretrained DETR [4]. The number of mutual gaze queries is set to 100. In the training phase, we choose the batch size to be 8, and we use AdamW [16] as an optimizer, with a constant learning rate of 1e-4 in Encoder-Decoder and 1e-5 in Backbone, we train the model until the performance on the test set no longer improves.\n"]}
{"id": "2206.07902", "categories": "cs.LG cs.CR stat.ML", "paragraphs": ["Hyperparameters.For all datasets and all methods, we set silos to train for 1 local epoch in every round (except Ditto\u00a0[54] which takes 2 local epochs).\nFor Vehicle, GLEAM, School, Rotated & Masked MNIST, and subsampled ADNI respectively, the local batch size across all silos are fixed with \\(B = 64, 64, 32, 100, 64\\) , and the clipping norm for per-example gradients are heuristically set to \\(c = 6, 6, 1, 1, 0.5\\) .\nVehicle uses \\(T=400\\)  rounds for most experiments (except fig:finetune-gap which trains for \\(T=200\\)  rounds), School and GLEAM use \\(T=200\\)  rounds, Rotated & Masked MNIST uses \\(T=200\\) , and ADNI uses \\(T=500\\) .\n", "For multi-task learning methods (\\(\\textsf {MR-MTL} \\) , Ditto\u00a0[54], Mocha\u00a0[86]), we sweep the regularization strength across a grid of \\(\\lambda \\in [0.0001,0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\\)  to find the best \\(\\lambda ^*\\)  wherever applicable (e.g. fig:pareto-vehicle-school,fig:mrmtl-bump,supp-fig:pareto-school-t200). To compensate for the change in the gradient magnitude, we also sweep different client learning rates across a grid of \\(\\eta \\in [0.001,0.003,0.01,0.03,0.1,0.3]\\) ; for fair comparison, the same grid of \\(\\eta \\)  is swept for methods that do not involve \\(\\lambda \\)  (e.g. IFCA, local finetuning).As discussed in Section\u00a0sec:discussion, releasing the results from repeated experiments (possibly with different hyperparameters) may technically compromise the privacy of the datasets\u00a0[59], [75]. In our case, we are primarily interested in understanding the behaviors and tradeoffs that emerge under silo-specific item-level DP, and thus for experimental control and ease of comparison we do not account for the privacy costs from hyperparameter tuning and repetitions. We also use only public datasets in our experiments.\nFor fig:finetune-gap and supp-fig:finetune-sweep, the learning rate is fixed to \\(\\eta = 0.01\\) . For all datasets, the chosen privacy parameter \\(\\delta \\)  satisfy \\(\\delta < n_k^{-1.1}\\)  where \\(n_k\\)  is the local training dataset size.\n"]}
{"id": "2207.12720", "categories": "cs.CV", "paragraphs": ["The first hyper-parameters are ordinary and are the subject of choice of most machine learning applications.\nThe last of them, the class weights, is less common, and is normally used for dealing with unbalanced classes, along with other sampling techniques [3], [6], [1], [16].\nIn our application the data were almost perfectly balanced, but we had the request of weighting the recall higher than the precision, i.e. to \"pay more attention\" to samples belonging to the TC class. This is the why we introduced this hyper-parameters to weight instances of the two classes differently when computing the loss function, in our case the binary crossentropy.\nThe parameter class weights has the form of a tuple, where the first element represents the weight of each sample of the class FC (False Contamination) in the loss function, while the second element represents the weight of each sample of the class TC (True Contamination) in the loss function. For example, class weights = \\((1,5)\\)  indicates that every instance of the TC class weights 5 times more than each instance of the FC class in the computation of the binary cross-entropy.\n"]}
{"id": "2201.09391", "categories": "cs.LG cs.AI", "paragraphs": ["We perform experiments over different popular GNN models, including a 2-layer GCN\u00a0[19] with 16 hidden neurons, a 2-layer GraphSAGE\u00a0[14] with 16 hidden neurons and a 2-layer GAT\u00a0[36] with 8 attention heads with 8 hidden neurons each. For Ogbn-Arxiv dataset, the number of hidden neurons is increased to 128. To train each model, we use an Adam optimizer with initial learning rate of \\(1\\times 10^{-2}\\)  and weight decay of \\(5\\times 10^{-4}\\) . As in the active learning setup, there should not enough labeled samples to be used as a validation set, we train the GNN model with fixed 300 epochs in all the experiments and evaluate over the full graph.\n"]}
{"id": "2205.11404", "categories": "cs.LG cs.NA math.NA stat.ML", "paragraphs": ["The FNO model for both Darcy Flow and the Navier-Stokes equations was trained using 12 modes and width 32.\nThe initial learning rate was set to 1e-3 and is halved every 100 epochs.\nWeight decay is set to 1e-8 and training finishes after 500 epochs. It is well-known that the training time per epoch for FNO can be significantly higher than that of DeepONet, however, FNO trains much faster i.e., with significantly fewer epochs [15], [21].\n"]}
{"id": "2205.11409", "categories": "cs.CL", "paragraphs": ["We set batch size to 8 for few-shot setting and 32 for full-data setting.For FewRel and TACRED datasets, we set batch size to 8 bacause of memory limitation for full-data setting. We use the learning rate 5e-5 for \\(\\text{BERT}_\\text{BASE}\\)  and 2e-5 for \\(\\text{RoBERTa}_\\text{BASE}\\) . We adopt the AdamW optimizer and constant learning rate scheduler.\n"]}
{"id": "2208.04529", "categories": "cs.LG", "paragraphs": ["The following configurations were applied to all GNN variants. Each baseline model contains two GNN convolutional layers followed by a readout function and then a 3-layer MLP to produce predictions. We used a batch size of 32 for the small dataset (500) and 512 for the large one (10,000). We used the Cross-Entropy loss to train all models and used Adam optimizer with default initial weights implemented in PyTorch. To prevent overfitting we used early stopping on the validation loss. We conducted grid search on learning rate, batch size and hidden dimension in GNNs. The hyperparameters were tuned as the following: (1) learning rate \\(\\in \\lbrace 0.1, 0.01, 0.001\\rbrace \\) ; (2) hidden dimension \\(\\in \\lbrace 32, 64\\rbrace \\) ; (3) readout function \\(\\in \\lbrace \\text{max}, \\text{average}\\rbrace \\) ; (4) edge weight normalization \\(\\in \\lbrace \\text{True}, \\text{False}\\rbrace \\) .\n"]}
{"id": "2204.03243", "categories": "cs.CL cs.LG", "paragraphs": ["We experiment with two standard pretraining settings, base and base++:\nBase is the BERT\\(_\\text{Base}\\)  training configuration\u00a0[8]: Pretraining on Wikipedia and BookCorpus\u00a0[54] (16 GB of texts) for 256 million samples on 512 token sequences (125K batches with 2048 batch size). We use the same corpus and \\(32,768\\)  uncased BPE vocabulary\u00a0[37] as with TUPE\u00a0[24] and COCO-LM\u00a0[29].\n", "Base++ trains the base size model with larger corpora and/or more training steps. We add in OpenWebText\u00a0[13], CC-News\u00a0[27] and STORIES\u00a0[43], to a total of 160 GB texts, and train for 4 billion (with 2048 batch size) samples, following recent research\u00a0[0], [27], [50]. We follow the prepossessing of UniLMV2\u00a0[0] and use \\(64,000\\)  cased BPE vocabulary.\n", "Downstream Tasks. We use the tasks included in GLUE\u00a0[44] and SQuAD 2.0 reading comprehension\u00a0[33].\nAll models are evaluated with the same standard fine-tuning protocols: Single task learning with vanilla fine-tuning and reporting the median of five random seeds in GLUE and SQuAD.\nPlease refer to Appendix\u00a0 for more details.\n"]}
{"id": "2202.06538", "categories": "cs.CL cs.AI", "paragraphs": ["We adopt the BART implementations from Huggingfacehttps://github.com/huggingface/transformers, and experiment based on both the BART-base and BART-large Seq2Seq fine-tuning settings. We run the experiments on single V100 with 16G memory. The maximum source sequence length is set to 512 and 200 respectively, for the full document input and supporting sentences input settings. The training batch size is 6 and 16 respectively for the QA4QG-base and QA4QG-large model, with gradient accumulation steps of 4. We train all model with maximum 5 epochs. The learning rate is 3e-5. During inference, we use beam search with beam size of 4, and we set the maximum target length to 32 and use the default value of the minimum target length, which is 12, with a length penalty of 1.0.\n"]}
{"id": "2211.10996", "categories": "cs.CV", "paragraphs": ["MINTIME was trained in two main versions, a) MINTIME-EF, which uses an EfficientNet-B0[41] as feature extractor and trained on DFDC dataset[16] as in [13], and b) MINTIME-XC, which uses an XceptionNet[38] as feature extractor (inspired by [28]) and trained on ForgeryNet images as in [25].\nThe two versions differ also in the training modality in fact the first was trained keeping fixed the whole convolutional backbone excluding the last 2 blocks, with a batch size of 8 and on an NVIDIA RTX 3060, while MINTIME-XC was trained End2End with a batch size of 32 in parallel on four NVIDIA A100 for 30 epochs.\nThe optimizer used is SGD with a learning rate of 0.01, which decays to 0.0001 using a cosine scheduler. The weight decay was set to 0.0001 as in [6].\nAll models were trained considering a maximum of two identities per input sequence, which does not limit the possibility of using more or fewer identities at inference time. The maximum number of faces we put in the input sequence was set to 16. Since the SlowFast[19] model trained in [25] was not available, we retrained it starting from the model pretrained on Kinetics 400[27] in order to see how it behaved in certain contexts not reported in the original paper. This is the only training conducted in which a single identity per video is considered in order to emulate what has been done in [25].\n"]}
{"id": "2208.10046", "categories": "cs.CV", "paragraphs": ["To pretrain the backbone, we use an Adam\u00a0[12] optimizer for the Conv-4 network and the stochastic gradient descent (SGD) optimizer for other backbone networks.\nThe backbone network, appended with a softmax layer, is trained with base data \\(\\mathcal {D}_b\\)  to classify all compositions in \\(\\mathcal {C}_b\\)  using the cross-entropy loss.\nStandard data augmentation including random crop, left-right flip, and color jitter, is applied.\nThe pretraining lasts for a maximum of 500 epochs with a batch size of 128.\nAnd the initial learning rate is set to \\(10^{-3}\\)  with a L2 penalty of \\(5 \\times 10^{-4}\\) .\n"]}
{"id": "2202.12109", "categories": "cs.CL cs.AI", "paragraphs": ["10pt\n[h]\n0pt\nX, Pt *[h]Context, Prompt tokens\n\\(Y=\\lbrace r_{0}:[[s_{0}^{0}, e_{0}^{0}], [s_{0}^{1}, e_{0}^{1}]]\\rbrace ,\\lbrace r_{1}:[[s_{1}^{0}, e_{1}^{0}]]\\rbrace  \\)\n\\(H_{enc},\\ H \\leftarrow \\text{BART}(X)\\) \n\\(\\hat{P} \\leftarrow \\text{BART-Decoder}(Pt, H_{enc})\\) \n\\(L \\leftarrow 0\\) *[h]Initialize datum loss\nrole in \\(Y\\) .keys()\nSet \\(\\hat{Y}_{role}\\)  to empty list\n\\(\\textup {EMB}_{slot}\\)  in \\(\\hat{P}\\) .get_next(role)\n\\(\\psi \\leftarrow \\textup {MeanPool}(\\text{EMB}_{slot})\\) \n\\(\\psi ^{(s)} \\leftarrow \\psi \\circ W^{(s)}\\) \n\\(\\psi ^{(e)} \\leftarrow \\psi \\circ W^{(e)}\\) \n\\(\\text{logit}^{(s)} \\leftarrow \\psi ^{(s)} H\\) *[h]cos-sim to H\n\\(\\text{logit}^{(e)} \\leftarrow \\psi ^{(e)} H\\) *[h]cos-sim to H\n\n\\(\\hat{Y}_{role}\\) .insert(   \\(\\operatornamewithlimits{arg\\,max}\\limits _{(i,j) \\in L^{2}, i<j}\\ \\text{logit}^{(s)}(i) + \\text{logit}^{(e)}(j)\\)  )\n\n\\(Y_{role},\\hat{Y}_{role} \\leftarrow \\textup {Hungarian}(Y_{role},\\hat{Y}_{role})\\) \n\\(L \\leftarrow L + CrossEntropy(Y_{role},\\hat{Y}_{role})\\)\n"]}
{"id": "2211.11172", "categories": "cs.LG cs.PF", "paragraphs": ["Adaptive-stopping elimination rate \\(\\rho \\): Similarly to the window size \\(\\lambda \\) , the elimination rate also faces the trade-off between the performance and the search time.\nWe tried 3 different values \\(0.25, 0.5, 0.75\\)  on elimination rate \\(\\rho \\) , and found that \\(\\rho =0.25\\)  eliminates less candidates but produces tiny performance improvements on \\(\\rho =0.5\\) , while \\(\\rho =0.75\\)  has a significant performance drop. For the best search time and performance consideration, we choose \\(\\rho =0.5\\)  as the default setting of the experiments.\n"]}
{"id": "2209.14383", "categories": "cs.CV", "paragraphs": ["We used the same U-Net architecture as backbone for all models due to its popularity for OD/OC segmentation\u00a0[17], [7], [8], [6].\nIts encoder consisted of 4 convolutional blocks with increasing output depths of 64, 128, 256 and 512, each followed by \\(2 \\times 2\\)  max-pooling layers for downsampling.\nThese blocks comprised two consecutive convolutional layers with \\(3 \\times 3\\)  filters, each followed by a batch normalization layer and a ReLU activation.\nAn additional block with 1024 filters was used at the bottleneck, followed by the decoder layers.\nThis part used 4 upsampling blocks with 512, 256, 128 and 64 filters each, where each block was based on a bilinear upsampling operation followed by a convolutional block. All models were trained using batches of 5 images, with input intensities standardized using each image mean and standard deviation. The same random, online data augmentation strategies were used for all models, consisting of vertical and horizontal flippings, gaussian blurring, brightness, contrast and saturation jitterings and random rotations and scalings. Their parameters and configurations were empirically selected based on the overall performance on a validation set. Adam optimization with an initial learning rate of 0.01 was used for 200 epochs to minimize the cross-entropy loss. When training using only DRISHTI data, 400 epochs were used instead. We also reduced the learning rate by a factor of 0.2 every time the performance plateaued for a maximum of 2 epochs. The best model through epochs according to the validation set was preserved and finally used for evaluation.\n"]}
{"id": "2203.01517", "categories": "cs.LG", "paragraphs": ["In Fig.\u00a0REF , we illustrate the two training stages of Correct-n-Contrast described in Sec.\u00a0. In Stage 1, we first train an ERM model with a cross-entropy loss. For consistency with Stage 2, we depict the output as a composition of the encoder and linear classifier layers. Then in Stage 2, we train a new model with the same architecture using contrastive batches sampled with the Stage 1 ERM model and a supervised contrastive loss (REF ) (which we compute after the depicted representations are first normalized) to update the encoder layers. Note that unlike prior work in contrastive learning [13], [28], as we have the class labels of the anchors, positives, and negatives, we also continue forward-passing the unnormalized representations (encoder layer outputs) and compute a cross-entropy loss to update the classifier layers while jointly training the encoder.\n{FIGURE}"]}
{"id": "2203.03761", "categories": "cs.LG stat.ML", "paragraphs": ["We run experiments on the full Federated EMNIST and Stack Overflow datasets\u00a0[19], two common benchmarks for FL tasks. F-EMNIST has 62 classes and \\(N=3400\\)  clients, with each user holding both a train and test set of examples. In total, there are \\(671,585\\)  training examples and \\(77,483\\)  test examples. Inputs are single-channel \\((28,28)\\)  images. We sample \\(n\\in [100,1000]\\)  clients per round for a total \\(R=1500\\)  rounds. The Stack Overflow (SO) dataset is a large-scale text dataset based on responses to questions asked on the site Stack Overflow. The are over \\(10^8\\)  data samples unevenly distributed across \\(N=342477\\)  clients. We focus on the next word prediction (NWP) task: given a sequence of words, predict the next words in the sequence. We sample use \\(n\\in [100,1000]\\)  and \\(R=1500\\) . On F-EMNIST, we experiment with a \\(\\approx 1\\)  million parameter (4 layer) Convolutional Neural Network (CNN) used by\u00a0[40]. On SONWP, we experiment with a \\(\\approx 4\\)  million parameter (4 layer) long-short term memory (LSTM) model, which is the same as prior work\u00a0[4], [40].\n", "On F-EMNIST, we use a server learning rate of \\(1.\\)  normalized by \\(n\\)  (the number of clients) and momentum of \\(0.9\\) \u00a0[51]; the client uses a learning rate of \\(0.01\\)  without momentum. On Stack Overflow, we use a server learning rate of \\(1.78\\)  normalized by \\(n\\)  and momentum of \\(0.9\\) ; the client uses a learning rate of \\(0.3\\) .\n"]}
{"id": "2206.11160", "categories": "cs.CL cs.CY", "paragraphs": ["Vocabularies of a maximum 500k \\(n\\) -grams (unigrams, bigrams, and trigrams only) are learned using Gensim's Phraser module, parameterized using a PMI threshold of 10 and minimum frequency of 5 [65]. For classification experiments, all posts sampled for a user from a given time period are tokenized using a Python implementation of Twokenizer [70], rephrased using the time period's specific Phraser model, concatenated together, and translated into a single document-term vector. Representations are transformed using TF-IDF weights learned at training time before being \\(\\ell _{2}\\) -normalized. As a classification architecture, we use \\(\\ell _{2}\\) -regularized logistic regression, optimizing parameters using limited-memory BFGS as implemented in Scikit-learn [73]. Inverse regularization strength \\(C\\)  is selected independently for each training sample such that F1 score is maximized within the development splits of a 10-fold cross validation run.\n"]}
{"id": "2209.02397", "categories": "cs.CV", "paragraphs": ["Our implementation was based on PyTorch. For TLPNet, we used the DecompST and the SCUT-EnsText datasets [59] to generate the training pair for network training. Therefore, the total number of training data pairs was approximately 7900. The input size of the TLPNet was 768 \\(\\times \\)  768, and the training batch size was 12 on an Nvidia GeForce RTX 3090 GPU. We used Adam to optimize the network with \\(\\beta \\)  = (0.5, 0.9), and the learning rate started at 0.0002 and decayed to nine-tenths after every 20 epochs in the training phase.\nFor TAANet, because we used the L1 loss in the training of CHM, which can strongly restrict the color of the output, we trained the two modules individually. The input size of TAANet was 768 \\(\\times \\)  768, and the training batch size was 20 and 10 in GTM and CHM, respectively, on a single Nvidia GeForce RTX 3090 GPU. The optimizer is the same as that in TLPNet, and the learning rate of the discriminators starts from 0.0004, which has the same decay rate as that in TLPNet.\n"]}
{"id": "2209.15475", "categories": "cs.CV eess.IV", "paragraphs": ["\\(T_1\\)  and \\(T_2\\)  in the similarity pooling. \\(T_1\\)  is set to be 0.001. \\(T_2\\)  is set to be \\(10^{-14}\\)  due to the small weight values after the process of normalization.\n"]}
{"id": "2206.02187", "categories": "cs.CV cs.SD eess.AS", "paragraphs": ["All experiments are carried out using a single NVIDIA GeForce RTX 3090 card. We adopt AdamW [16] as the optimizer with an initial learning rate 5e-4 with \\(L_2\\)  weight decay ranges between 5e-4 and 5e-5.\nDropout is used with a rate between 0.4 and 0.5. The number of encoder layers in each modality's encoder (i.e., \\(N_T, N_A, N_V\\) ) is tuned using a greedy scheme and set to 1 and 5 for MELD and IEMOCAP validation datasets, respectively. The number of multi-head attention fusion layers is set to 5 (i.e., \\(= m\\) ) for both dataset. The proposed M2FNet framework is trained using the categorical cross-entropy on each utterances softmax output for each of M dialogs and their k utterances. Section 2 of the supplementary material provides training and validation performance details.\n\\(Loss=-\\frac{1}{M \\cdot k} \\cdot \\sum _{i=1}^M\\sum _{j=1}^k\\sum _{t=1}^C y^{i,j,t}\\cdot \\log (y_{p}^{i,j,t}).\\) \n", "To extract deeper features from audio and visual contents, we introduce a new feature extractor model. Here, ResNet18 [8] is used as encoder module while the projector module consists a fully connected layer which projects the embeddings of encoder network to desired representations (i.e., \\(Z\\) ). Here we set the number of representations as \\(Z = 300\\) . For audio task, the extractor model is trained on Mel Spectrograms obtained from the corresponding audio signals while in case of visual feature extraction, it is trained on well-known CASIA webface database [34]. Here, the extractor model is trained using the loss function mentioned in Equation no. REF \nin which the weighting factors \\(\\lambda _1, \\lambda _2\\)  and \\(\\lambda _3\\)  are set to 20, 5 and 1, respectively. The proposed extractor model is trained upto 60 and 100 epochs for audio and visual task, respectively using Adam optimizer with learning rate of 1e-4 and decay rate of 1e-6.\n"]}
{"id": "2202.03077", "categories": "cs.LG cs.CR", "paragraphs": ["We conduct all experiments on Python 3.8 (PyTorch 1.1) with NVIDIA RTX A50000 GPUs. We run MMD-D, MMD-G, C2ST-S, C2ST-L, ME and SCF using the GitHub code provided by\u00a0[10] and implement MMD-RoD by ourselves. Following\u00a0[8], we use a deep neural network \\(f\\)  as the classifier in C2ST-S and C2ST-L, and train \\(f\\)  by minimizing cross-entropy loss. The neural network structure \\(\\phi \\)  in MMD-D and MMD-RoD has the same architecture with feature extractor in \\(f\\) , i.e., \\(f = g \\circ \\phi \\)  where \\(g\\)  is composed of two fully-connected layers and outputs the classification probabilities. For MNIST and CIFAR-10, we normalize the raw data into the scale \\([-1,1]\\) .\n", "For Blob, HDGM and Higgs, \\(\\phi \\)  is a five-layer fully-connected neural network. The number of neurons in hidden and output layers of \\(\\phi \\)  are set to 50 for Blob, \\(3 \\times d\\)  for HDGM and 20 for Higgs, where \\(d\\)  is the dimensionality of samples. For MNIST and CIFAR-10, \\(\\phi \\)  is a convolutional neural network (CNN) that contains four convolutional layers and one fully-connected layer. The structure of the CNN exactly follows\u00a0[10].\n", "We use Adam optimizer\u00a0[122] to optimize (1) parameters of \\(f\\)  in C2ST-S and C2ST-L, (2) parameters of \\(\\phi \\)  in MMD-D and MMD-RoD and (3) kernel lengthscale in MMD-G. We set drop-out rate to zero when training C2ST-S, C2ST-L, MMD-D and MMD-RoD on all datasets. We set the number of training samples \\(n_{\\mathrm {tr}}\\)  to 100 for Blob, 3, 000 for HDGM, 5, 000 for Higgs and CIFAR-10, 500 for MNIST.\n", "For ME and SCF, we follow\u00a0[5] and set \\(J=10\\)  for Higgs. For other datasets, we set \\(J=5\\) .\n", "For C2ST-S and C2ST-L, we set batchsize to 128 for Blob, HDGM and Higgs, and 100 for MNIST and CIFAR-10. We set the number of training epochs to \\(9000 \\times n^{te}/\\) batchsize for Blob, 1, 000 for HDGM and Higgs, 2, 000 for MNIST and CIFAR-10. We set learning rate to 0.001 for Blob, HDGM and Higgs, and 0.0002 for MNIST and CIFAR-10.\n", "For MMD-D, we use full batch (i.e., all samples) to train MMD-D and MMD-RoD for Blob, HDGM and Higgs. We use mini-batch (batchsize is 100) to train MMD-D and MMD-RoD for MNIST and CIFAR-10. We set the number of training epochs to 2, 000 for Blob, HDGM, Higgs and MNIST, and 1, 000 for CIFAR-10. We set learning rate to 0.0005 for Blob and Higgs, 0.00001 for HDGM, 0.001 for MNIST and 0.0002 for CIFAR-10.\n", "We conduct all experiments on Python 3.8 (PyTorch 1.1) with NVIDIA RTX A50000 GPUs. We run MMD-D, MMD-G, C2ST-S, C2ST-L, ME and SCF using the GitHub code provided by\u00a0[10] and implement MMD-RoD by ourselves. Following\u00a0[8], we use a deep neural network \\(f\\)  as the classifier in C2ST-S and C2ST-L, and train \\(f\\)  by minimizing cross-entropy loss. The neural network structure \\(\\phi \\)  in MMD-D and MMD-RoD has the same architecture with feature extractor in \\(f\\) , i.e., \\(f = g \\circ \\phi \\)  where \\(g\\)  is composed of two fully-connected layers and outputs the classification probabilities. For MNIST and CIFAR-10, we normalize the raw data into the scale \\([-1,1]\\) .\n", "For Blob, HDGM and Higgs, \\(\\phi \\)  is a five-layer fully-connected neural network. The number of neurons in hidden and output layers of \\(\\phi \\)  are set to 50 for Blob, \\(3 \\times d\\)  for HDGM and 20 for Higgs, where \\(d\\)  is the dimensionality of samples. For MNIST and CIFAR-10, \\(\\phi \\)  is a convolutional neural network (CNN) that contains four convolutional layers and one fully-connected layer. The structure of the CNN exactly follows\u00a0[10].\n", "We use Adam optimizer\u00a0[122] to optimize (1) parameters of \\(f\\)  in C2ST-S and C2ST-L, (2) parameters of \\(\\phi \\)  in MMD-D and MMD-RoD and (3) kernel lengthscale in MMD-G. We set drop-out rate to zero when training C2ST-S, C2ST-L, MMD-D and MMD-RoD on all datasets. We set the number of training samples \\(n_{\\mathrm {tr}}\\)  to 100 for Blob, 3, 000 for HDGM, 5, 000 for Higgs and CIFAR-10, 500 for MNIST.\n", "For ME and SCF, we follow\u00a0[5] and set \\(J=10\\)  for Higgs. For other datasets, we set \\(J=5\\) .\n", "For C2ST-S and C2ST-L, we set batchsize to 128 for Blob, HDGM and Higgs, and 100 for MNIST and CIFAR-10. We set the number of training epochs to \\(9000 \\times n^{te}/\\) batchsize for Blob, 1, 000 for HDGM and Higgs, 2, 000 for MNIST and CIFAR-10. We set learning rate to 0.001 for Blob, HDGM and Higgs, and 0.0002 for MNIST and CIFAR-10.\n", "For MMD-D, we use full batch (i.e., all samples) to train MMD-D and MMD-RoD for Blob, HDGM and Higgs. We use mini-batch (batchsize is 100) to train MMD-D and MMD-RoD for MNIST and CIFAR-10. We set the number of training epochs to 2, 000 for Blob, HDGM, Higgs and MNIST, and 1, 000 for CIFAR-10. We set learning rate to 0.0005 for Blob and Higgs, 0.00001 for HDGM, 0.001 for MNIST and 0.0002 for CIFAR-10.\n"]}
{"id": "2210.06391", "categories": "cs.LG cs.AI", "paragraphs": ["We use stratified sampling to randomly select 15% of the nodes as observed set, mask out the output labels of the rest 85% of the nodes for test prediction, and ensure that the nodes with the same label are split proportionally.\nFollowing , , we further divide the labeled set with three-fold cross-validation.\nThe bigger portions (10%) are used as training sets and the rest (5%) are used as validation sets.\nThe GNN models (GCN and GAT) are trained on the training set, then used to predict the masked-out test set.\nFigure\u00a0REF  illustrates the aforementioned data partition in our experiments.\nIn total, we use 5 random data splits, three-fold cross-validation for each split, and 5 random model initializations per data partition, resulting in 75 total runs for each experiment.\n{FIGURE}"]}
{"id": "2204.13792", "categories": "cs.LG", "paragraphs": ["Gaussian Processes. We use the model proposed in [3] and implemented using GPflow [10]. We use ADAM[4] to optimize the evidence lower bound with mini-batch size of 1000.\n", "Probabilistic Neural Network. We use Gaussian distribution in the output layer and ADAM[4] to optimize log-likelihood with learning rate \\(10^{-4}\\) . Our architecture consists of two hidden layers each with 256 nodes and L2 regularization factor of \\(0.01\\) .\n"]}
{"id": "2212.10714", "categories": "cs.CL", "paragraphs": ["Mini-batch training is employed using the Adam optimizer\u00a0[36]. L2 regularization is used to avoid over-fitting. The bias term \\(\\mathbf {b}^{(2)}_t\\)  is tuned for negative examples in the final softmax layer.\nPre-trained word embeddings trained by using the word2vec tool\u00a0[1] on the 2014 MEDLINE/PubMed baseline distribution are employed. The vocabulary size was 215,840.\nThe embedding of the drugs, i.e., DRUG1 and DRUG2 were initialized with the pre-trained embedding of the word drug.\nThe embeddings of training words that did not appear in the pre-trained embeddings were initialized with the average of all pre-trained word embeddings. Words that appeared only once in the training data were replaced with an UNK word during training, and the embedding of words in the test data set that did not appear in both training and pre-trained embeddings were set to the embedding of the UNK word. Word position embeddings are initialized with random values drawn from a uniform distribution.\nThe molecule-based vectors of unmatched entities are set to zero vectors.\n", "The random initialization method is prepared without textual information (No Text) as the baseline.\nIn this setting, embeddings of entities and relations are initialized with the random values drawn from a uniform distribution between \\(\\pm (\\gamma + \\frac{\\epsilon }{d})\\) , where \\(\\gamma =12\\) , \\(\\epsilon =2\\)  and \\(d\\)  is a dimension of KG embeddings.\n"]}
{"id": "2202.02925", "categories": "cs.CV", "paragraphs": ["We re-implement 14 SOD methods in our benchmark.\nThey are DHSNet [2], Amulet [3], NLDF [4], SRM [5], PicaNet [6], DSS [7], BASNet [8], CPD [9], PoolNet [10], EGNet [11], SCRN [12], GCPA [13], ITSD [14], MINet [15].\nSome early methods have some issues when using our settings, so we slightly modify their network structures in our re-implementations:\n", "\nDHSNet [2] requires a fixed input size because it flattens the topmost encoder feature and reshapes this feature to a fixed-size feature map after a fully connected layer.\nThis operation is not compatible with multi-scale training, so we directly reduce the channels of the topmost encoder feature to 1 using a convolutional layer.\nIn this way, we can train DHSNet with multi-scale inputs.\n\nPicaNet [6] uses a novel global attention module that reshapes the channel dimension to spatial size.\nSince the channel dimension is fixed, this method also requires a fixed input size.\nThus, we randomly crop input images and resize them to a fixed size in multi-scale training.\n\nAmulet [3] outputs foreground and background maps simultaneously.\nFor simplicity, we only output a saliency map as other methods.\n\nDHSNet www.github.com/xsxszab/DHSNet-Pytorch and SRM www.github.com/xsxszab/SRM-Pytorch are based on third-party implementations because no official code is available.\n\n"]}
{"id": "2210.15226", "categories": "cs.CL cs.SD eess.AS", "paragraphs": ["Based on the CTC recipe from the SpeechBrain toolkit [21], utterance audio length is limited from 4 to 10 seconds. Similarly, signals are ordered by length, using shorter ones in the first batches of the epoch, and longer clips at the end. Regarding data augmentation, the unique technique used is SpecAugment [22]. Furthermore, the model is optimized minimizing only a CTC loss and the learning rates (LR) are updated using the New Bob scheduler. Additionally, we manually restarted the LR at some points in training. Finally, the best checkpoint in terms of WER is stored. The ASR has been trained using a batch size of 3, setting the starting LRs for the linear layers and Wav2Vec2 of 1.0 and \\(10^{-5}\\) , respectively.\n"]}
{"id": "2208.03791", "categories": "cs.CV", "paragraphs": ["ScanNet semantic segmentation.\nWe train all networks using the AdamW optimizer\u00a0[28] and a one-cycle policy\u00a0[45] for 600 epochs with batch size 8.\nFor the MinkowskiEngine baseline, we use a learning rate of 1\\(e\\) -2 with 0.01 weight decay, and for the GHA-augmented network, we use a learning rate of 1\\(e\\) -3 with 0.05 weight decay, which performed better in the respective settings.\nFor augmentation, we use random rotation, scaling, and random cropping\u00a0[68].\n", "KITTI object detection.\nWe use the original setting from\u00a0[9], namely, we train the network for 40 epochs, with batch size 6, AdamW optimizer\u00a0[28], 0.018 learning rate, and a one cycle learning rate policy\u00a0[45].\n", "nuScenes object detection.\nWe use a batch size of 8 for 10\u00a0cm voxels, and 6 for 7.5\u00a0cm voxels, the AdamW optimizer\u00a0[28], and a learning rate of \\(1e-5\\)  with a one-cycle policy\u00a0[45] with 0.05 weight decay.\n"]}
{"id": "2206.03271", "categories": "cs.LG cs.AI cs.CV cs.RO", "paragraphs": ["Reptile-PPO incorporates the inner-outer loop in Reptile\u00a0[10] with PPO. For each iteration in the inner loop, the environment is fixed to one randomly sampled task level, where rollouts are collected and used to perform 3 iterations of batched gradient updates. Following that, the outer loop takes the updated policy parameters, and performs a soft parameter update. This process is repeated iteratively.\n"]}
{"id": "2212.01747", "categories": "cs.CV", "paragraphs": ["We adopt the PVCNN\u00a0[21] styled U-Net\u00a0[30] proposed in PVD\u00a0[44] as our velocity filed model \\(v_{\\theta }\\) .\nAs we summarize in Algorithm\u00a0REF , there are three phases,\nincluding 1) training the velocity flow model, 2) improving straightness via reflow, and 3) flow distillation.\nFor step 1), we mainly follow the setting in DDPM\u00a0[9] and use a batch size of 256 and a learning rate of \\(2e^{-4}\\) . We train the model for 200k steps and apply an exponential moving average (EMA) at a rate of 0.9999.\nFor step 2), we randomly sample 50k data pairs \\((X^{\\prime }_0, X^{\\prime }_1)\\)  using the pretrained network \\(v_{\\theta }\\)  and fine-tune it for 10k steps by minimizing Eqn.\u00a0REF .\nHere \\(X^{\\prime }_1\\)  is sampled by setting \\(N=1000\\)  in Eqn.\u00a0REF  for the best quality.\nWe use a fixed learning rate of \\(2e^{-5}\\)  in this step.\nFor step 3),\nwe use the samples \\((X^{\\prime }_0, X^{\\prime }_1)\\)  generated in step 2) and\nfinetune \\(v_\\theta \\)  for another 10k steps with learning rate \\(2e^{-5}\\) .\n"]}
{"id": "2203.13064", "categories": "cs.CL", "paragraphs": ["Our encoder is loaded with its default pretrained weights; the linear layers' weights are initialized with random numbers. Our models are trained by Adam optimizer [14] with default hyperparameters. We use a multi-class categorical cross-entropy loss function. The early stopping technique is used: Stopping criteria is 3 epochs without improving the loss function on the dev set, which is a random 2% sample from the same source as training data and is different for each stage.\n"]}
{"id": "2211.03044", "categories": "cs.CL cs.LG", "paragraphs": ["FewGen is a training data generation method and can be used with any fine-tuning method on any classification model.\nWe use moderate-sized PLMs to ensure our results are reproducible on typical research hardware:\nCTRL (\\(1.6\\) B parameters)\u00a0[27] as the generator \\(G_{\\theta }\\)  and RoBERTa\\(_{\\text{Large}}\\)  (356M parameters)\u00a0[42] as the classifier \\(C_{\\phi }\\) .\nWe use prefix-tuning for training \\(G_{\\theta }\\)  and prompt-based fine-tuning for training \\(C_{\\phi }\\) .\nFor simplicity, we use the most basic manual prompt version of LM-BFF\u00a0[17].\nThe only exception is CoLA for which we use the standard fine-tuning since the input data might be out of the distribution of \\(C_{\\phi }\\) \u00a0[17].\nThe hyperparameter tuning is performed on \\(\\mathcal {D}_{\\text{dev}}\\) . More details are in Appendix\u00a0.\n"]}
{"id": "2207.10959", "categories": "cs.CV", "paragraphs": ["The proposed framework is implemented with PyTorch-1.7.\nQueryProp utilizes AdamW\u00a0[28] optimizer with weight decay 0.0001.\nThe whole framework is trained with 8 GPUs and each GPU holds one mini-batch.\nThe framework is trained in two stages.\nWe first train the backbone and the detection heads on both ImageNet DET and VID.\nGiven a key frame \\(I_k\\)  and a non-key frame \\(I_{i}\\) ,\nwe randomly sample two frames from \\(\\lbrace I_t\\rbrace _{t=k-3\\tau }^{k-\\tau }\\)  for short-term memory generation,\nand two frames from \\(\\lbrace I_t\\rbrace _{t=0}^{k}\\)  for long-term memory generation.\nIf they are sampled from DET, all frames within the same mini-batch are the same since DET only has images.\nWe use the parameters pre-trained on COCO\u00a0[26] for model initialization.\nThe training iteration is set to 90k and the initial learning rate is set to \\(2.5\\times 10^{-5}\\) ,\ndivided by 10 at iteration 65k and 80k, respectively.\nAfter finishing the first training stage,\nwe start training the APG on ImageNet VID.\nFor each non-key frame, we randomly select \\(m\\)  (10 by default) adjacent frames as key frames to form a training batch.\nThe gate is optimized in a self-supervised manner.\nThe initial learning rate is set to \\(10^{-4}\\)  and the total training iteration is 16k,\nand the learning rate is dropped after iteration 8k and 12k.\nThe number of queries and boxes in the detection heads is 100 by default.\n"]}
{"id": "2202.05263", "categories": "cs.CV cs.GR", "paragraphs": ["Our network follows the mip-NeRF structure. The network \\(f_\\sigma \\)  is composed of 8 layers with width 512 (Mission Bay experiments) or 1024 (all other experiments). \\(f_c\\)  has 3 layers with width 128 and \\(f_v\\)  has 4 layers with width 128. The appearance embeddings are 32 dimensional. We train each Block-NeRF using the Adam\u00a0[28] optimizer for 300K iterations with a batch size of 16384. Similar to mip-NeRF, the learning rate is an annealed logarithmically from \\(2 \\cdot 10^{-3}\\)  to \\(2 \\cdot 10^{-5}\\) , with a warm up phase during the first 1024 iterations. The coarse and fine networks are sampled 256 times during training and 512 times when rendering the videos. The visibility is supervised with MSE loss and is scaled by \\(10^{-6}\\) . The learned pose correction consists of a position offset and a \\(3\\times 3\\)  residual rotation matrix, which is added to the identity matrix and normalized before being applied to ensure it is orthogonal. The pose corrections are initialized to 0 and their element-wise \\(\\ell 2\\)  norm is regularized during training. This regularization is scaled by \\(10^{5}\\)  at the start of training and linearly decays to \\(10^{-1}\\)  after 5000 iterations. This allows the network to learn initial geometry prior to applying pose offsets.\n"]}
{"id": "2210.16058", "categories": "cs.LG cs.AI cs.RO", "paragraphs": ["i) PointMaze: We generate 20 small \\(5\\! \\times \\!5\\)  small mazes as pre-training environments, which include various topographies.\nIn each episode, the agent is initialized in a random position of the central grid.\nIn Fig. REF (a), we exemplify four typical pre-training environments. The observation for the skill policy is the relative position with regard to the gird where the agent is located. We pre-train the skills of horizon two over those mazes via maximizing the average cumulative return averaging over the learning objective.\nii) AntMaze: As shown in Fig. REF (b), we keep the same 3-D four-legged robots in an open environment and set the skill horizon as 100.\niii) FetchPickAndPlace and FetchStack2: As skills will be triggered only when the agent reaches the selected sub-goal and a target box is grasped, moving the target box around can potentially explore more novel goals. Thus, we handcraft skills as moving the gripper along a random direction and keeping the grasping action.\n"]}
{"id": "2203.06604", "categories": "cs.CV", "paragraphs": ["ShapeNet\u00a0[4] consists of about 51,300 clean 3D models, covering 55 common object categories. We split the dataset into a training set and a validation set but only conduct pre-training on the training set. For each instance, we sample 1024 points via FPS as input point cloud. Note that we only apply standard random scaling and random translation for data augmentation during pre-training. For pre-training details, we use an AdamW optimizer\u00a0[26] and cosine learning rate decay\u00a0[25]. The initial learning rate is set to 0.001, with a weight decay of 0.05. We pre-train our model for 300 epochs, with a batch size of 128.\n{FIGURE}", "To demonstrate the effectiveness of our method, we visualize reconstruction results on ShapeNet validation set in Figure REF . The model is pre-trained with a masking ratio of 60%, but it is able to reconstruct inputs with different masking ratios. This high generalization capability can be expected, as our model learns high-level latent features well. Furthermore, our method speeds up pre-training by 1.7\\(\\times \\)  compared to Point-BERT\u00a0[53].\n"]}
{"id": "2210.06177", "categories": "cs.CV cs.CL cs.SD eess.AS", "paragraphs": ["For audio-visual extraction module and audio-contextual extraction module, adam is used as an optimizer. And the initial learning rate is set to 0.001. Besides, the learning rate is halved if the validation loss increases consecutively for 3 epochs. The training process stops when validation loss increases consecutively for 6 epochs.\n"]}
{"id": "2205.08576", "categories": "cs.CV cs.LG", "paragraphs": ["All methods are implemented with Pytorch and deployed in a distributed training system using DistributedDataParallel (DDP).\nFor Fed-BEiT and Fed-MAE, we adapt the official implementations of BEiT and MAE to a FL setting as shown in Fig.\u00a0REF .\nViT-B/16 [16] is selected as the backbone for the proposed models. Following the setup in BEiT[24] and MAE[25], the input is split into \\(14\\times 14\\)  image patches and the same number of visual tokens for BEiT and \\(16\\times 16\\)  patches for MAE.\nWe randomly mask at most \\(40\\%\\)  of total image patches for BEiT and \\(60\\%\\)  of those for MAE in our main experiment.\nAdamW with \\(\\beta 1 = 0.9\\) , \\(\\beta 2 = 0.999\\)  is employed for optimization.\n"]}
{"id": "2204.08109", "categories": "cs.CL", "paragraphs": ["For ArcaneQA, we are only able to train our model with batch size 1 due to the memory consumption, so we choose a workaround to set the number of gradient accumulation to be 16. We use Adam optimizer with an intial learning rate of 0.001 to update our own parameters in BERT-based models. For BERT's parameters, we fine-tune them with a learning rate of 2e-5.\nFor ArcaneQA w/o BERT, we train it with batch size 32 and an initial learning rate of 0.001 using Adam optimizer.\nFor both models, the hidden sizes of both encoder and decoder are set to be 768, and the dropout rate is set to 0.5.\nAll hyper-parameters are manually tuned according to the validation accuracy on development set. specifically, we do manual hyper-parameter search from [1e-5, 2e-5, 3e-5], [8, 16, 32], [0.0, 0.2, 0.5] to tune the learning rate of fine-tuning BERT, steps of gradient accumulation and dropout rate respectively.\n"]}
{"id": "2211.08712", "categories": "cs.CV", "paragraphs": ["The number of blocks of U-Net and V-Net \\(L_p\\)  and E-Net \\(L_e\\)  is set to 5 and 18, respectively. The model is implemented in PyTorch [56]. We use MegaDepth dataset [57], which includes 196 different locations. Each location provides an SfM model reconstructed by COLMAP [58] using SIFT. We select four locations, which include about 12k images. We fix all image poses and use HLoc toolbox[59], [21] to reconstruct the selected 4 locations with SuperPoint. We use the reconstructed SfM models to construct our training set. If not otherwise specified, all learned methods used in the following experiments are trained under this training set. BMNet used in GAM is trained using an SGD optimizer with an initial learning rate \\(0.001\\)  and batch size 1. It converges after 140 epochs of training on one GTX1080Ti GPU. The following experiments, if not otherwise specified, use GAM with \\(k=3\\)  and \\(ratio=0.7\\)  as the default configuration.\n"]}
{"id": "2209.12793", "categories": "cs.LG cs.CV", "paragraphs": ["For the GNN training process, we adopted the Adam optimizer [57] with the learning rate initialized to be 0.001 and a cosine annealing scheduler [58] with the maximum iteration set to the number of epochs to tune the learning rate for minimizing the loss. After obtaining the embedding from each GNN layer, we apply the Leaky Rectified Linear Unit with a negative slope of 0.2 for non-linear activation.\n", "The MVCNN is trained using a PyTorch implementation and uses the ResNet architecture with a supervised learning regime. A patience factor is used to stop the training process after 20 epochs that see an increase in the validation accuracy, resulting in around 30 training epochs. The models are trained with a batch size of 8, 512 embedding dimensions, 12 views, and \\(1\\mathrm {e}{-4}\\)  learning rate.\n"]}
{"id": "2206.00133", "categories": "cs.LG q-bio.BM stat.ML", "paragraphs": ["\nPCQM4Mv2. The main dataset we use for pre-training is PCQM4Mv2 [41] (license: CC BY 4.0), which contains 3,378,606 organic molecules, specified by their 3D structures at equilibrium (atom types and coordinates) calculated using DFT. An earlier version of this dataset without any 3D structures, called PCQM4M, was used for supervised pre-training [77], but to our knowledge, this is the first time the 3D structures from v2 have been used and in a self-supervised manner. Molecules in PCQM4Mv2 have around 30 atoms on average and vary in terms of their composition, with the dataset containing 22 unique elements in total. The molecules in PCQM4Mv2 only contain one label, unlike e.g. QM9, which contains 12 labels per molecule, however we do not use these labels as denoising only requires the structures. The large scale and diversity of PCQM4Mv2 hence makes it well-suited for pre-training via denoising.\n", "QM9. Widely used as a molecular property prediction benchmark [31], [20], [54], [51], [19], [28], [37], [55], [65], [22], QM9 is a dataset [45] (license: CCBY 4.0) with approximately 130,000 small organic molecules containing up to nine heavy C, N, O, F atoms, specified by their structures. Each molecule has 12 different labels corresponding to different molecular properties, such as highest occupied molecular orbital (HOMO) energy and internal energy, which we use for fine-tuning.\n", "DES15K. DES15K [17] (license: CC0 1.0) is a small dataset containing around 15,000 interacting molecule pairs, specifically dimer geometries with non-covalent molecular interactions. Each pair is labelled with the associated interaction energy computed using the coupled-cluster method with single, double, and perturbative triple excitations (CCSD(T)) [2], which is widely regarded as the gold-standard method in electronic structure theory. Note that CCSD(T) is usually more expensive and accurate than DFT, which is used for all aforementioned datasets. Pre-trained models are fine-tuned to predict the interaction energies in DES15K.\n"]}
{"id": "2210.15184", "categories": "cs.CL", "paragraphs": ["Hyperparameters: We use the transformer and mT5 as our model classes as described previously in Section\u00a0. The hyperparameters for our transformer model was optimized for fine-tuning of Odia, trained on \u00a01M sentence pairs. For fine-tuning, we use the Adafactor optimizer [42], with a linearly decaying learning rate of 1e-3. Since training with smaller batches is known to be more effective for extremely low-resource language training [0], we tuned the training batch size for every language - varying from 32 to 256 (with gradient accumulation as 2) though we did not see very significant variation in the performance on the basis of this tuning. For our stopping criteria: we fine-tuned all models for 60 epochs (which concluded with considerably overfit models) and then selected models by we picking the checkpoint which had the best validation performance on BLEU (with only the 13a tokenizer which mimics the mteval-v13a script from Moses) [38].\n"]}
{"id": "2201.01683", "categories": "cs.CV", "paragraphs": ["We basically refer to [36] for the training setting. We use the single-level NeRF and sample 64 points along each camera ray. For points that are far from the predicted SMPL surface, we do not feed them into NeRF for faster training. Specifically, for points with signed distance \\(h > h_0\\) , our model returns the zero density and color directly. We set \\(h_0 = 0.2m\\)  in our experiments. We conduct the training on a single Nvidia V100 GPU. Learning rate decreases exponentially from \\(5e^{-4}\\)  to \\(5e^{-5}\\)  in training. The training typically converges in about 200k iterations, which takes about 14 hours.\n"]}
{"id": "2210.02018", "categories": "cs.CV", "paragraphs": ["We use the Resnet100 [7] network architecture to train our InterFace. This was motivated by the wide use of this architecture in the state-of-the-art face recognition solutions [23], [4], [1]. We follow the common settings and set the scale parameter s to 64, Stochastic Gradient Descent(SGD) optimizer with an initial learning rate of 1e-1, set momentum parameter to 0.9, and weight decay parameter to 5e-4. The learning rate is divided by 10 at 80k, 140k, 210k, and 280k training iterations. The total number of training iterations is 295K. The model is implemented using the PyTorch framework. We set batch-size to 512 and train our model on Linux device (CentOS Linux release 7.9.2009) with Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz, 128RAM and 2 Nvidia GeForce RTX 6000 GPUs. The network training and subsequent evaluation generate 512-d embedded features on images of size 3*112*112. During training, the images were randomly flipped horizontally with a probability of 0.5, and the data was normalized between -1 and 1.\n"]}
{"id": "2209.15589", "categories": "cs.CV cs.LG", "paragraphs": ["For CLIP and softmax classification methods, we set the maximum number of allowable labels in JFT to 8. For images with more labels than this, the label list will be truncated; for images with fewer labels, it will be padded.\n", "For CLIP, we re-use the same training crop and rescaling augmentations as [26] and the contrastive pre-training for [0]. For all datasets, we use AdamW with weight decay of 2e-2, b1=0.9, b2=0.95, and a gradient clipping thresholding the global norm to 1.0. We use warm-up cosine decay learning rate schedule, with base learning rate of 1e-3, starting learning rate of 1e-7, and 10K warmup steps. The joint embedding space has dimension 768 and the language encoder is fed a maximum caption length of 55 characters.\n", "For BYOL, we re-use the same training crop and rescaling augmentations as [11] and the official I1K implementation [4]. For JFT and ALIGN, we use a target EMA of 0.99, base learning rate of 3e-4, and weight decay of 1e-1. For I1K and I21K, we use a target EMA of .996 and weight decay of 1e-1. For COCO, we use a target EMA of .97, weight decay of 1e-6, and learning rate of 3e-4. For all datasets, we use the Adam optimizer with b1=0.9, b2=0.999. We use a warmup cosine schedule, with warmup steps equal to 5% of the total steps in the entire schedule. Our SimCLR implementation uses these same optimization settings (but no target EMA).\n", "For classification-based supervised ViT training, we use the standard crop-resize augmentation as is standard in I1K training and drop path rate of 0.1. We use a cosine schedule for the learning rate, with a base value LR of 1e-4, and 6K warmup steps. We use a weight decay of 0.3. We use the Adam optimizer with b1=0.9, b2=0.95. These settings are the same across datasets. We use gradient clipping thresholding the global norm at 1.0. Importantly, we use a common initialization trick of initializing values to keep the initial range within 0.02, and initializing the head bias to -10 [9], [32].\n", "For all datasets, we use AdamW with weight decay of 2e-2, b1=0.9, b2=0.95, and a gradient clipping thresholding the global norm to 1.0. We use warm-up cosine decay learning rate schedule, with base learning rate of 1e-3, starting learning rate of 1e-7, and 10K warmup steps. The joint embedding space has dimension 768 and the language encoder is fed a maximum caption length of 55 characters.\n"]}
{"id": "2204.12672", "categories": "cs.CL", "paragraphs": ["For the 512-dimensional models on De \\(\\rightarrow \\)  En, they are trained on 10 \\(\\times \\)  1080Ti GPUs. It consists of a 6-layer encoder and a 6-layer decoder. The model dimension is set to 512, the intermediate dimension is 2048 and the number of attention heads is 8. The number of max tokens is 6144. The optimizer is Adam with betas set to (0.9, 0.98). The learning rate is 0.00015 and the warm-up steps are 2000. We also reduce the learning rate on plateaus with a patience of 4. Weight decay is 0.000001. Label smoothing is 0.1 and dropout rate is 0.3.\n", "For the 1024-dimensional models on De \\(\\rightarrow \\)  En, they are trained on 8 \\(\\times \\)  TitanX GPUs. It consists of a 6-layer encoder and a 6-layer decoder. The model dimension is set to 1024, the intermediate dimension is 4096 and the number of attention heads is 16. The training configurations follow [5]. The number of max tokens is 3584. The optimizer is Adam with betas set to (0.9, 0.98). The learning rate is 0.0005 and the warm-up steps are 6000. The learning rate scheduler is inverse_sqrt. The warm-up initial learning rate is 1e-7. Label smoothing is 0.1 and dropout rate is 0.3.\n", "For the 512-dimensional models on Zh \\(\\rightarrow \\)  En, they are trained on 10 \\(\\times \\)  1080Ti GPUs. It consists of a 6-layer encoder and a 6-layer decoder. The model dimension is set to 512, the intermediate dimension is 2048 and the number of attention heads is 8. The number of max tokens is 10240. The optimizer is Adam with betas set to (0.9, 0.98). The learning rate is 0.00025 and the warm-up steps are 500. We also reduce the learning rate on plateaus with a patience of 4. Weight decay is 0.000001. Label smoothing is 0.1 and dropout rate is 0.3.\n", "For the adaptive prefix generation part, our proposed MonoLSTM is based on a 2-layer, 512-dimensional unidirectional LSTM. It is also trained on 10 \\(\\times \\)  1080Ti GPUs. The number of max tokens is 20480. The optimizer is Adam with betas set to (0.9, 0.997) and epsilon set to 1e-9. The learning rate is 0.005 and the warm-up steps are 4000. We also reduce the learning rate on plateaus with a patience of 4. Weight decay is 0.000001. Label smoothing is 0.1 and dropout rate is 0.2.\n"]}
{"id": "2212.05861", "categories": "cs.CV", "paragraphs": ["In (REF ), \\(w_1\\) , \\(w_2\\)  and \\(w_3\\)  are trainable parameters to automatically balance the detection, counting and reID tasks. For initialization, we set \\(w_1\\) , \\(w_2\\)  and \\(w_3\\)  to \\(-2\\) , \\(-1\\)  and \\(-1\\) , respectively. In Fig.REF , we show the training curves for the three parameters on the training set, and they coverage at the 20th epoch. In Fig.\u00a0REF , the corresponding detection, counting and reID losses also converge at the same epoch. Meanwhile, we show the loss curves for the three tasks (dotted lines in Fig.\u00a0REF ) when the training parameters are fixed (\\(w_1=-2\\) , \\(w_2=-1\\)  and \\(w_3=-1\\) ). As observed, the dotted lines converge at relatively high loss values, which indicates that the uncertainty weights are effective for multi-task training. On the validation set, automatically adjusting the three parameters also performs better than using fixed training parameters (MOTA of 71.8 vs 70.2, IDF1 of 76.3 vs 72.3).\n"]}
{"id": "2211.12827", "categories": "cs.CV", "paragraphs": ["We train SSIS-Track by following the training strategies of CondInst\u00a0[23] and AdelaiDet\u00a0[22].\nFor images, we adopt the weights of CondInst trained on COCO\u00a0[16] to initialize the weights\nof the backbone network, and then use the same parameters in SSIS\u00a0[26], [27] to train SSIS-Track on the SOBA dataset\u00a0[28].\nFor videos, we adopt the weights of SSIS-Track trained on images to initialize the the\nmodel, set the mini-batch size as four, and optimize SSIS-Track on an NVIDIA RTX 3090 GPU for 5,000 iterations.\nWe set the initial learning rate as \\(1e-5\\)  and reduce it to \\(1e-6\\)  at 4,500 iterations.\nAlso, we re-scale the input images without changing the aspect ratio, such that the shorter side is no less than 672 pixels.\n"]}
{"id": "2206.07011", "categories": "cs.CV", "paragraphs": ["We implement our model based on IFC\u00a0[12] with Apache 2.0 license and Mask2Former\u00a0[7] with Creative Commons Attribution-NonCommercial 4.0 International License, using Detectron2 framework\u00a0[29] under Apache License 2.0.\nAll models are trained with a node of 8 NVIDIA V100 GPUs.\nFollowing the common recipe in\u00a0[12], [6], we initialize our model with weights pre-trained on COCO dataset because both YouTubeVIS2019 and YouTubeVIS2021 datasets are relatively small-scale.\nSince pretraining the inter-frame recurrent attention modules is not possible with an image instance segmentation dataset, we randomly initialize these modules.\nDuring training, we randomly sample clips\u00a0[12], [6] from each video.\nThe number of frames for each video is set to 2, and the shorter side of the training clip is resized to 360 or 480, following \u00a0[7].\nThe learning rate for the backbone is \\(1/10\\)  of the learning rate of other parts of the network.\nWe follow the same setting in\u00a0[7]: \\(\\lambda _c = 2\\) , \\(\\lambda _m = 5\\)  and \\(\\lambda _{D} = 5\\)  in Eq.(REF ).\n\\(\\lambda _e = 0.3\\)  in Eq.(REF ).\nDuring inference, the model takes the whole video and predicts the video instance segmentation for all the frames.\n"]}
{"id": "2209.05840", "categories": "cs.CL cs.AI", "paragraphs": ["We used a 1-layer 256-dimensional BiLSTM to encode words. We set the dimensions as \\((d_v, d_t, d_i, d_e) = (496, 512, 2048, 128)\\) . We used ResNet-152\u00a0[6], which was pre-trained on ImageNet\u00a0[19], to extract a feature vector of 2048 dimensions from an image.\n"]}
{"id": "2209.00351", "categories": "cs.CL cs.LG", "paragraphs": ["We train the case-preserving version of BERT\\(_{\\text{\\scriptsize {BASE}}}\\) with 2M sentences containing funding information to obtain the BERT\\(_{\\text{\\scriptsize {TAPT}}}\\) \u00a0 model.\nUnless indicated otherwise, the hyper-parameters recommended by\u00a0[11] are used for training.\nThe training is done on an NVIDIA Tesla K80 GPU with 12 GB of memory with a batch size of 2048 through gradient accumulation and for one epoch (1000 steps).\nWe further fine-tune BERT\\(_{\\text{\\scriptsize {TAPT}}}\\) for the mention detection task on the ELFund dataset. The fine-tuning process is done for 3 epochs with batch size of 8. We refer to this model as BERT\\(^{\\text{\\scriptsize {MD}}}_{\\text{\\scriptsize {TAPT}}}\\) .\nFor disambiguation, the bi-encoder model is trained on the EDFund dataset with a learning rate of \\(2\\times 10^{-5}\\)  and batch size of 16. The training is performed in 4 rounds, each round consisting of 2 epochs. In the first round, 3 random negatives are used for each mention. The score threshold \\(\\tau \\)  is set to 0.042 using grid search. Following\u00a0[22], the mention and entity representations are limited to 64 and 256 tokens, respectively.\n"]}
{"id": "2204.09081", "categories": "cs.CL", "paragraphs": ["\nBatch size: 32\n\nOptimizer: Adam [2] with \\(lr=5e^{-5}\\)  and \\(\\varepsilon =1e^{-8}\\)  without weight decay.\n\n10 epochs, model with best F1-score on the dev dataset was selected.\n\nTrain-dev-test split: 80%-10%-10% with corresponding Wikipedia and CoNLL datasets merged and shuffled.Since the CoNLL datasets could potentially contain mentions of Food and Drugs, the alias dictionary for each dataset was used to exclude those from training for the weighted models.\n\n"]}
{"id": "2211.01874", "categories": "cs.CL", "paragraphs": ["We use the same set of hyperparameters for all model setups.\nThe only hyperparameter we tune for INJECT\u00a0is the layer for context integration.\nWe tested layer 3, 6, 9 and 12 and found 12 to perform the best according to the average benchmark-performance on the development set using three random seeds.\nWe use context integration on layer 12 for all reported results.\nMore details can be found in the Appendix\u00a0REF .\n"]}
{"id": "2208.09394", "categories": "cs.CV", "paragraphs": ["Our experiments are implemented on PyTorch\u00a0[13] with CUDA acceleration. If not otherwise specified, we use ResNet-50\u00a0[3] as our backbone, and a SECOND FPN\u00a0[24] is adopted to merge multi-level image features. Following BEV detectors that report performances on nuScenes\u00a0[5], [10], most of our experiments were performed under \\(256\\times 704\\)  resolution with batchsize 8 (\\(8\\times 6\\)  images for six images per sample). Data augmentation, including RandomFlip, Random Rotate, Random Scaling, and Random Cropping as in BEVDepth\u00a0[10] are used. We train our models on the nuScenes dataset for 24 epochs, using learning rate 2e-4, depth weight 3, and EMA strategy. When comparing with other methods on nuScenes, CBGS\u00a0[27] is adopted.\n"]}
{"id": "2211.15891", "categories": "cs.LG cs.AI", "paragraphs": ["For reservoir 4009, we set \\(M\\) to 4 and \\(\\epsilon \\) to \\(1.8\\) . For all other reservoirs, \\(M=3\\)  and \\(\\epsilon =1.5\\) . For each reservoir, we tested models with 4 or 6 LSTM layers, and 5 reservoirs use 6 LSTM layers while the rest use 4. We also tested LSTM layer widths of 512 and 1024 nodes and found 1024 node layers were better suited for the N and C models, while E models performed better with 512 nodes across all reservoirs. While \\(f=72\\)  (3 days) was set by our problem definition, we tested \\(h \\in {72, 168, 360, 720}\\) , i.e., \\({3, 7, 15, 30}\\)  days, and found \\(h=360\\)  to work the best for all reservoirs.\n", "All models were trained using PyTorch 1.9.1+cu102 on a Linux server running CentOS 7.9.2009 equipped with 2x 20-core Intel(R) Xeon(R) Gold 6148 CPUs, 768 GB RAM, and 3 NVIDIA V100 GPUs. Finally, the LSTM layers were trained using an SGD optimizer with learning rate 1E-3, while the fully connected layers were trained using an Adam optimizer with learning rate 5E-4.\n"]}
{"id": "2207.11562", "categories": "cs.CL", "paragraphs": ["For an optimizer, AdamW [20] (lr=0.001, batch size=128, weight decay={0.1, 0.001, 0.00001}, epochs={1, 2}). The values within the curly brackets indicate that the best parameter among them is used to report the result. For a learning rate shceduler, a cosine learning rate decay is used [21].\n"]}
{"id": "2205.12128", "categories": "cs.CV", "paragraphs": ["The parameters of all the networks are randomly initialized. The input of the SAC algorithm is the ResNet-50 extracted features from 768x144 raw image. All experiments are trained on a NVIDIA 3070 GPU with Adam optimizer. While training the networks, we use learning rate \\(\\eta = 10^{-3}\\)  and the discount factor \\(\\gamma = 0.99\\) . For RL system, we train 3 SAC agents simultaneously. The episode terminates once the ego-vehicle deviates from desired position and rotation more than 5 meters in total.\n"]}
{"id": "2212.07283", "categories": "cs.LG cs.CV", "paragraphs": ["We evaluate the generative classifier on the CIFAR-10 dataset (ten classes problem), a widely used benchmark for robust classification\u00a0[2].\nThe proposed generative classifier consists of \\(K\\)  binary classifiers in a \\(K\\)  class classification problem. In order to have fair comparison with the discriminative approach which typically uses a single model, we limit the capacity of the binary classifier so that the overall capacities of these two approaches are similar.\nSpecifically, we use a customized model \u201cResNet18Thinner\u201d (see also in fig:capacityablation) to train the binary classifiers. The ResNet18Thinner model is an ResNet18 architecture with a width multiplier of 0.5, and has 10M parameters. The baseline discriminative robust classifier\u00a0[6] is based on the ResNet50 architecture which has 90MB parameters.\n", "All the binary classifier are trained with the SGD optimizer using a batch size of 128 and weight decay \\(10^{-4}\\)  for 2000 epochs. For classes 2, 3, 4, 5, 7, 8, 9 we use a starting learning rate of 0.1, and for classes 0, 1, 6 we find the 0.1 learning rate being too high and instead use a starting learning rate of 0.05. The learning rate is reduced to 0.01 after epoch 1500. The training perturbation size is 0.3 (\\(L_2\\)  norm), in contrast to the perturbation size of 0.5 used in training the softmax robust classifier. Following the common practice in the adversarial machine learning literature\u00a0[15], we use early stopping on the test set to select the model.\n"]}
{"id": "2206.11959", "categories": "cs.LG cs.AI", "paragraphs": ["For the fair comparison with other baselines, We use the Graph Isomorphism Network (GIN)\u00a0[39] with 5 layers and 300 hidden units each layer as our backbones for models pre-trained on all those datasets mentioned above except for AP_NF(whose settings are kept the same with GCC\u00a0[25]), and mean-pooling to get graph-level representations following\u00a0[14].\n"]}
{"id": "2210.06983", "categories": "cs.CV cs.LG", "paragraphs": ["Following\u00a0[14], [37], we use ImageNet-1k as the pre-training corpus which contains 1.28 million images. All images are resized to a fixed resolution of \\(224\\times 224\\) . We utilize two vision Transformer variants as the encoder, the Base model (ViT-B) and the Large model (ViT-L) with \\(16\\times 16\\)  input patch size\u00a0[21]. The ViT-B encoder consists of 12 Transfomer blocks with embedding dimension 768, while the ViT-L encoder consists of 16 blocks with embedding dimension 1024. For both settings, the decoder uses 8 Transformer blocks with embedding dimension 512 and a linear projection whose number of output channels equals the number of pixel values in a patch. All the Transformer blocks have 16 attention heads. The ViT-B/ViT-L encoder have roughly 87M and 304M parameters, respectively.\n", "For the pre-training of the two DMAE models, we set the masking ratio to 0.75 following\u00a0[14]. The noise level \\(\\sigma \\)  is set to 0.25. Random resizing and cropping are used as data augmentation to avoid overfitting. The ViT-B and ViT-L models are pre-trained for 1100 and 1600 epochs, where the batch size is set to 4096. We use the AdamW optimizer with \\(\\beta _1,\\beta _2=0.9,0.95\\) , and adjust the learning rate to \\(1.5\\times 10^{-4}\\) . The weight decay factor is set to \\(0.05\\) . After pre-training, we also visualize the model performance of DMAE ViT-L in Fig.\u00a0REF . From the figure, we can see that the trained model can recover the masked patches and purify the noisy unmasked patches, which demonstrates its capability of accomplishing both tasks simultaneously.\n{FIGURE}"]}
{"id": "2204.10437", "categories": "cs.CV eess.IV", "paragraphs": ["where \\(z_1=h_\\theta (f_\\theta (x_1))\\)  and \\(z_2=h_\\xi (f_\\xi (x_2))\\) , \\(\\tau \\)  is a temperature hyperparameter, and \\(N\\)  is the queue size.\nFollowing\u00a0[13], \\(f_\\theta \\)  is a standard ResNet-50 and \\(h_\\theta \\)  is a two-layer MLP head (hidden layer 2048-d, with ReLU). Moreover, when adopting MoCo-v2 in DiRA, \\(f_\\theta \\) , \\(h_\\theta \\) , and \\(g_\\theta \\)  are optimized using SGD with an initial learning rate of 0.03, weight decay 0.0001, and the SGD momentum 0.9.\n", "where stopgrad means that \\(y_2\\)  is treated as a constant in this term. Following\u00a0[14], \\(f_\\theta \\)  is a standard ResNet-50 and \\(h_\\theta \\)  is a three-layer projection MLP head (hidden layer 2048-d), followed by a two-layer predictor MLP head. Moreover, when adopting SimSiam in DiRA, \\(f_\\theta \\) , \\(h_\\theta \\) , and \\(g_\\theta \\)  are optimized using SGD with a linear scaling learning rate (lr\\(\\times \\) BatchSize/256). The initial learning rate is 0.05, weight decay is 0.0001, and the SGD momentum is 0.9.\n", "Barlow Twins\u00a0[52]:\nWe adopt Barlow Twins\u2014 a popular representative of redundancy reduction instance discrimination learning methods, into our framework.\nBarlow Twins makes the cross-correlation matrix computed from two siamese branches close to the identity matrix. By equating the diagonal elements of the cross-correlation matrix to 1, the representation will be invariant to the distortions applied to the samples. By equating the off-diagonal elements of the cross-correlation matrix to 0, the different vector components of the representation will be decorrelated, so that the output units contain non-redundant information about the sample. The discrimination loss is defined as follows:\n\\(\\mathcal {L}_{dis} = \\sum \\limits _{i} (1 - \\mathcal {C}_{ii})^2 + \\lambda \\sum \\limits _{i} \\sum \\limits _{i\\ne j} \\mathcal {C}_{ij}^2\\) \n", "Joint training process: Following\u00a0[8], [60], we perform the overall pre-training with the discrimination, restoration, and adversarial losses in a gradual evolutionary manner. First, the encoder \\(f_\\theta \\)  along with projector \\(h_\\theta \\)  are optimized using the discrimination loss \\(\\mathcal {L}_{dis}\\)  according to the learning schedule of the original discriminative methods\u00a0[13], [14], [52], [25], empowering the model with an initial discrimination ability. Then, the restoration and adversarial losses are further fused into the training process incrementally. To stabilize the adversarial training process and reduce the noise from imperfect restoration at initial epochs\u00a0[60], we first warm up the \\(f_\\theta \\)  and \\(g_\\theta \\)  using the \\(\\mathcal {L}_{dis}+ \\mathcal {L}_{res}\\) , and then add the adversarial loss \\(\\mathcal {L}_{adv}\\)  to jointly train the whole framework; the optimization of the framework by incorporation of \\(\\mathcal {L}_{res}\\)  and \\(\\mathcal {L}_{adv}\\)  takes up to 800 epochs. Following \u00a0[54], we use the early-stop technique on the validation set, and the checkpoints with the lowest validation loss are used for fine-tuning.\n"]}
{"id": "2204.10555", "categories": "cs.CL", "paragraphs": ["In this paragraph, we describe the experimental settings of adaptive pre-training baselines, namely TAPT, TAPT (+ RecAdam), and DAPT. For QA tasks, we further pre-train the PLM for {1,3,5,10} epochs and then report the best performance among them. Specifically, reported TAPT result on NewsQA, Relation, and Medication are obtained by 1 epoch of further pre-training. We use the weight decay of 0.01, learning rate of 5e-5, maximum sequence length of 384, batch size of 12, and linear learning rate decay of 0.06 warmup rate, with a half-precision. Also, the masking ratio for the pre-training objective is set to 0.15, following the existing strategy introduced in the original BERT paper\u00a0[9].\n", "For NER tasks, we further pre-train the PLM for 3 epochs across all datasets. In particular, the learning rate is set to 5e-5, batch size is set to 32, and the maximum sequence length is set to 128.\nWe also use AdamW\u00a0[26] as the optimizer for all experiments.\n", "Regarding the setting of TAPT (+ RecAdam) on all tasks, we follow the best setting in the original paper\u00a0[6] \u2013 sigmoid as an annealing function with annealing parameters: \\(k = 0.5\\) , \\(t_0 = 250\\) , and the pretraining coefficient of 5000.\n", "For training with DAPT, we need an external corpus having a large amount of data for adaptive pre-training. Thus, we first choose the datasets of two domains \u2013 News and Medical. Specifically, as the source of corpus for the News domain, we use the sampled set of 10 million News from the RealNews dataset used in\u00a0[14]. As the source of corpus for the Medical domain, we use the set of approximately 100k passages from the Medical textbook provided in\u00a0[22]. The size of pre-training data used in DAPT is much larger than TAPT. In other words, for experiments on NewsQA, TAPT only uses fine-tuning contexts containing 5.8 million words from the NewsQA training dataset, while DAPT uses more than a hundred times larger data \u2013 enormous contexts containing about 618 million words from the RealNews database. For both News and Medical domains, we further pre-train the BERT-base model for 50 epochs with the batch size of 64, to match the similar computational cost used in\u00a0[15]. Other experimental details are the same as TAPT described above.\n"]}
{"id": "2211.16749", "categories": "cs.LG cs.AI cs.AR", "paragraphs": ["With the searched optimal tensorization shape \\({\\mathbf {s}}^*\\) , we construct a Rank SuperNet with maximum ranks following a \\(\\sim \\) 60% target compression ratio.\nWe use the original fine-tuned BERT-base as the teacher model and launch the 10-epoch logit distillation flow to train the Rank SuperNet.\nWe use Adam optimizer with a learning rate of 3e-5 and a linear decay schedule.\nIn the limited difference technique, we restrict the maximum allowed rank change across iterations to 3.\nWe use a sandwich rule with one largest SubNet, one smallest SubNet, and two randomly sampled SubNets.\n"]}
{"id": "2207.02536", "categories": "cs.LG cs.AI", "paragraphs": ["W=D+FW+RW\n\nwhere \\(F_H\\) , \\(F_W\\)  are two fixed scalars, \\(R_H\\) , \\(R_W\\)  are two uniform random variables. \\(F_H\\)  and \\(F_W\\)  are \u201coversizes\u201d that leave the model space to work and eventually produce longer outputs. \\(R_H\\)  and \\(R_W\\)  are \u201cregularizers\u201d that help avoiding overfitting on a fixed grid size. To lower the computational cost of the model, we chose some small values: \\(F_H=0, F_W=2, R_H\\in [0,3], R_W\\in [0,3]\\) , and divided samples from the dataset into groups based on the number of terms in order to lower \\(H,W\\)  per batch. This choice won't affect training as the gradients are computed on all losses from each group reduced together. We chose two groups of \\([1,2]\\)  and \\([3,4]\\)  number of terms, and same \\([1,10]\\)  number of digits.\n"]}
{"id": "2207.02518", "categories": "cs.CL cs.LG", "paragraphs": ["\\(S(s, g)\\)  is trained for 200,000 steps, using a learning rate of \\(10^{-5}\\) , a batch size of 1024 and\n16-bit mixed precision used for the model weights and embeddings. During training, models were evaluated both \\(\\mathcal {D}_{\\text{v\\_ID}}\\) \nand \\(\\mathcal {D}_{\\text{v\\_OOD}}\\)  every 20 training steps. The top-10 performing model checkpoints\nby \\(F_1\\)  score on \\(\\mathcal {D}_{\\text{v\\_ID}}\\)  were stored, along with their \\(F_1\\)  score on \\(\\mathcal {D}_{\\text{v\\_OOD}}\\) .\n"]}
{"id": "2206.01127", "categories": "cs.CV cs.CL", "paragraphs": ["Following previous work\u00a0[8], [1], [40], we adopt the same base-size network architecture which consists of 12-layer Transformer blocks with 768 hidden size and 12 attention heads.\nWe follow the parameter initialization method used in BEiT\u00a0[1].\nThe image resolution used for pretraining is \\(224 \\times 224\\) , and the image patch size is \\(16 \\times 16\\) .\nWe mix the data and pretrain the model from scratch with a total batch size of \\(6,144\\)  for 480k steps (i.e., 100 epochs of the image-text pairs).\nEach batch contains \\(2,048\\)  images, \\(2,048\\)  text and \\(2,048\\)  image-text pairs.\nFor the ablation experiments, we train the model for 40 epochs.\nFollowing BEiT, we use random resized cropping, horizontal flipping, and color jittering\u00a0[42] to perform image augmentation.\nWe use a SentencePiece tokenizer\u00a0[18] with 64k vocab size to tokenize the text data.\nAdam\u00a0[16] optimizer with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.999\\)  is utilized to optimize the model.\nThe peak learning rate is 2e-3, with linear warmup over the first \\(10,000\\)  steps and cosine learning rate decay.\nThe weight decay is 0.05.\nWe disable dropout, and use stochastic depth\u00a0[11] with a rate of 0.1.\n"]}
{"id": "2206.04790", "categories": "cs.CV", "paragraphs": ["We use mini-batch stochastic gradient descent, with momentum of 0.9 and weight decay 0.001. For each video, we use an 8-frame clip, where the frames are uniformly sampled. We use batch size of 8. For UCF101 and Kinetics100 in the SSL setting, we train the model for 400 epochs and for HMDB51, we train for 500 epochs. The initial learning rate is set to 0.1 and then decayed using cosine annealing policy. For the SSL setting, we use the data split proposed in VideoSSL [17]. For the few-shot setting, we use the default hyperparameters of TRX\u00a0[28], ARN\u00a0[39] and C3D-PN\u00a0[30], respectively. In the fully supervised setting, we train R(2+1)D for 100 epochs on UCF101, HMDB51 and 50 epochs on Kinetics-400.\n"]}
{"id": "2206.04838", "categories": "cs.LG cs.AI", "paragraphs": ["For the hyper-parameters of DACS, the reduced dimension in the auxiliary classifier is set to 16, and we set the number of buckets (\\(k\\)  in Eq. REF ) and the number of breaks (\\(h\\)  in Eq. REF ) to 100 and 4, respectively. The temperature is set to 0.25 (\\(\\tau \\)  in Eq. REF ). The above parameters are chosen by validation on CIFAR-10 dataset, and we found that such parameters work fairly well in different datasets in this paper.\n"]}
{"id": "2206.04756", "categories": "cs.LG cs.AI cs.CV", "paragraphs": ["\nRandomApply(transforms.ColorJitter(0.8, 0.8, 0.8, 0.2), p=0.3)\n\nRandomHorizontalFlip()\n\nRandomApply(transforms.GaussianBlur((3,3), (1.0, 2.0)), p=0.2)\n\nRandomResizeCrop(size=(64, 64), scale=(0.6,1.0))\n\nnormalization.\n\n", "During training, we use Adam optimizer by default, whose learning rate is \\(3e-4\\)  without weight decay. The batch size is set to be 512 without exceptional notation. For evaluation on dSprites, Shapes3D, and CelebA, we select the weights after training for 15 epochs for evaluation. We select the weights after training for 140 epochs for evaluation on Cars3D and the weights of the 200th epoch on SmallNORB considering the small scale of these two datasets.\n"]}
{"id": "2206.04843", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["For each model we used the following hyper parameters, initially optimized so that each model has the same number of total parameters that of approximately \\(18,000\\) .\n"]}
{"id": "2203.04668", "categories": "cs.CV", "paragraphs": ["In pre-training, we borrow the official PyTorch implementation for ImageNet training. The total number training epochs is set to 90. Stochastic gradient decent with momentum 0.9 is used to update the model parameters. The initial learning rate is 0.1 and are multiplied by 0.1 every 30 epochs. The weight decay is 1e-4. In this paper, we conduct most of the experiments with ResNet-50[14] to validate our claim.\n"]}
{"id": "2207.09295", "categories": "cs.CV cs.LG", "paragraphs": ["We fine-tuned a Faster R-CNN model pretrained on COCO using the default training settings from Detectron2 v0.6 [135] with the following modifications: we trained with a batch size of 8 and a learning rate of 0.0025 on two NVIDIA RTX A5000 GPUs for 18 epochs, reducing the learning rate by a factor of 10 at epochs 12 and 16, and selected the best model checkpoint based on validation AP50.\n", "We fine-tuned a ScaledYOLOv4 CSP model pretrained on COCO using the default training settings from the official implementation with the following modifications: we resized all inputs to 896px on their longest side and selected the best model checkpoint based on validation AP50. We used a batch size of 32 and trained on two NVIDIA RTX A5000 GPUs.\n", "We fine-tuned a YOLOv5m model pretrained on COCO using the default training settings from v6.0 release with the following modifications: we resized all inputs to 896px on their longest side, trained the detector for 150 epochs, and selected the best model checkpoint based on validation AP50. We used a batch size of 64 and trained on two NVIDIA RTX A5000 GPUs.\n"]}
{"id": "2205.07633", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["We make use of the Long-Short Term Memory [11] with a dot-product attention mechanism for the decoder architecture\nand Gated Recurrent Unit [4] for the encoder architecture.\nIn both cases, we use a hidden size of 300 and a latent embedding vector of size 200.\nNote that in the case of prior-posterior shuffling, there are shared parameters for \\({p_\\phi }\\)  and \\({q^p_\\zeta }\\)  through the encoder, the hidden state\nis mapped to the appropriate mean and variance by independent neural networks.\nUnder the context \\({\\textnormal {c}}\\)  we assume access to the ground-truth dialogue state, this can be easily replaced by the predictions of a dialogue state-tracker [19], [17].\n"]}
{"id": "2206.07359", "categories": "cs.CL cs.AI", "paragraphs": ["We use the pre-trained model from the huggingface library\u00a0https://github.com/huggingface/transformers. The optimizer is AdamW and the learning rate is 1e-6 as an initial value. The learning rate scheduler used for training is get_linear_schedule_with_warmup, and the maximum value of 10 is used for the gradient clipping. We select the model with the best performance on the validation set. All experiments are conducted on one A100 GPU.\n"]}
{"id": "2205.13943", "categories": "cs.CV cs.AI", "paragraphs": ["We adopt ResNet-50\u00a0[24] and Vision Transformer\u00a0[15] (ViT-S/16 and ViT-B/16) as the backbone. We pre-train on ImageNet-1K (IN-1K) training set with AdamW\u00a0[31] optimizer with the basic learning rate \\(1.5e^{-4}\\)  adjusted by a cosine learning rate scheduler and a batch size of 2048. The input image size is \\(224\\times 224\\)  with a patch size of \\(32\\times 32\\) . We use a random masking ratio of 60%. By default, the learnable mask tokens are placed at stage-3 in ResNet-50 and layer-5/layer-8 in ViT-S/ViT-B, respectively. We adopt a linear prediction head as the decoder. Our experiments are implemented by Pytorch and conducted on a workstation with NVIDIA V100 GPUs. We report the average results of 3 trials for all experiments, and use bold and underline to indicate the best and the second-best performance. See Appendix\u00a0 for detailed pre-training settings.\n{FIGURE}"]}
{"id": "2205.13927", "categories": "cs.LG", "paragraphs": ["\\(\\kappa \\)  was set to a constant in the original work of [21], but this is problematic since it is a sensitive hyperparameter due to the training dynamics.\nSpecifically, if \\(\\kappa \\)  is chosen too small, possible failures modes include that \\(\\mathcal {L}_{rec}\\)  never reaches it (with the pressure staying only on the reconstruction loss), that \\(\\kappa \\)  will be reached by over-fitting, or that the posterior distribution collapses which leads to high \\(D_{\\text{KL}}(Q_\\psi \\parallel P_\\theta ) \\)  and destabilizes the training.\nOn the other hand, if \\(\\kappa \\)  is chosen too large, the model can underfit or the reconstruction loss drops below \\(\\kappa \\) , leading to a negative loss value and harming the training success significantly.\nTo address this issue we introduce kappa annealing and adjust \\(\\kappa \\)  during training. Let \\(\\mathcal {\\textbf {L}}_{c} = \\frac{1}{K} \\sum _{k=0}^K \\mathcal {L}_{rec}(X_k,Y_k) - \\kappa \\)  be the mean constraint difference for \\(K\\)  training samples in one epoch of training or a defined number of update steps. In our experiments, we find that initializing \\(\\kappa \\)  slightly too large\nand updating it every epoch with\n\\(\\kappa ={\\left\\lbrace \\begin{array}{ll}\\kappa + \\mathcal {\\textbf {L}}_{c} & \\quad \\text{if } \\mathcal {\\textbf {L}}_{c} < 0 \\text{ and } \\lambda \\le 1 \\\\\\kappa & \\quad \\text{otherwise}\\end{array}\\right.}\\) \n"]}
{"id": "2205.13947", "categories": "cs.LG cs.AI", "paragraphs": ["In the experiment, we use \\(T=12\\)  historical time steps to predict the traffic speed of the next \\(M=6\\)  time steps. Due to different time resolutions, Didi-Shenzhen and Didi-Chengdu datasets will predict the results in the next 60 minutes, while METR-LA and PEMS-BAY datasets will predict the next 30 minutes\u2019.\nThe ST-GFSL framework is trained by Adam optimizer with learning rate decay in both inner loop and outer loop. In ST-Meta Learner, the number of GAT is set to 2, and the the GRU layer is set to 1.\nTotally, there are several important hyperparameters in our model, and we set them as: the dimension of meta knowledge \\(d_{MK} = 16\\) , task learning rate \\(\\alpha = 0.01\\) , meta-training rate \\(\\beta = 0.001\\) , task batch number \\(\\Vert \\mathcal {T}\\Vert  = 5\\) , and sum scale factor of two loss function \\(\\lambda = 1.5\\) .\n", "[left=1em]\nTCN\u00a0[22]: 1D dilated convolution network-based temporal convolution network. We used the addition of two TCN models as the feature extractor. The size of the dilated convolution kernel is set to 2 and 3 respectively.\n\nSTGCN\u00a0[20]: Spatial temporal graph convolution network, which combines graph convolution with 1D causal convolution. The layer of STGCN block is set to 2, and use one-layer FC as the multi-step predictor.\n\nGWN\u00a0[0]: A convolution network structure combines graph convolution with dilated casual convolution, which introduces a self-adaptive graph to capture the hidden spatial dependency. The block of GWN is set to 4, and the layer of GWN blocks is set to 2.\n\n"]}
{"id": "2203.07847", "categories": "cs.CL cs.LG", "paragraphs": ["Training is started from a pre-trained transformer LM. Specifically, we employ the Hugging Face\u00a0[38] implementation of BERT and RoBERTa.\nFor sentence representation, we take the embedding of the [CLS] token. Then similar to\u00a0[16], we train the model in an unsupervised fashion on \\(10^6\\)  randomly samples sentences from Wikipedia. The LM is trained with a learning rate of \\(3.0\\mathrm {e}{-5}\\)  for 1 epoch at batch-size of 192. The projector MLP \\(q\\)  has three linear layers, each with 4096 output units in conjunction with ReLU and BatchNorm in between.\nFor BERT hyperparameters are \\(\\alpha =0.005\\) , \\(\\lambda =0.013\\) , and dropout rates are \\(r_A=5.0\\%\\)  and \\(r_B=15.0\\%\\) . For RoBERTa hyperparameters are \\(\\alpha =0.0033\\) , \\(\\lambda =0.028\\) , and dropout rates are \\(r_A=6.5\\%\\)  and \\(r_B=24.0\\%\\) .\nThe values were obtained by grid-search. First a coarse-grid was put in place with a step-size of \\(0.1\\) \nfor \\(\\alpha \\) , \\(10\\%\\)  for the dropout rates \\(r_A, r_B\\) . For \\(\\lambda \\)  the coarse-grid consisted of different magnitudes \\(\\lbrace 0.1,0.01,0.001\\rbrace \\) . Second, on a fine-grid with step-size of \\(0.01\\)  and \\(1\\%\\) , respectively.\n"]}
{"id": "2203.07861", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["Gradient computes the derivative of the target class score with respect to the input sample and returns the saliency map of the input sample at the end. Integrated Gradient uses the linear path method to compute gradient along the path from a baseline \\(x^{\\prime }\\) . We use a zero vector for the baseline. As suggested by [47], the number of steps for the path should be selected between 20 and 300. Hence, we use steps \\(N = 60\\) , meaning that it takes 60 steps from baseline \\(x^{\\prime }\\)  to the original input sample \\(x\\) , according to \\(x = x^{\\prime } + \\frac{(x - x^{\\prime })}{N} \\cdot n\\) , \\(n\\)  is the current step.\n", "SmoothGrad also computes the derivative of the target class score with respect to the input sample. However, it adds Gaussian noise \\(\\mathcal {N}(0, \\sigma ^2)\\)  to the input sample multiple times and computes the gradients from the perturbed samples \\((x + \\mathcal {N}(0, \\sigma ^2))\\) . The number of iteration of adding noise is chosen \\(N = 60\\)  and the standard deviation of Gaussian noise is chosen \\(\\sigma = 0.2\\) .\n", "For LRP, we select the \\(\\epsilon \\) -propagation rule for every DL model with \\(\\epsilon = 1e-9\\) . Due to the residual block in TCN model the propagated relevances should therefore be added together (\\(R = R_1 + R_2\\) ).\n", "For LIME, we use the cosine distance function as the kernel function with width \\(w = 5.0\\)  to weight the perturbed samples, and perform 1000 iterations.\nFor the perturbation, we consider that neighbors along time dimension should have similar relevance to reduce the computational time, so we set the number of features along time dimension 50 for GunPointAgeSpan. This means that saliency maps for samples with length 150 in GunPointAgeSpan have same relevance for every 3 neighbors and saliency maps for dense labeling samples in tool tracking have same relevance for every 4 neighbors.\n", "For Kernel SHAP, we use 1000 iterations and set the number of features along time dimension to 50.\nFurthermore, the sampling of feature perturbation in Kernel SHAP is based on the distribution \\(p(f) = \\frac{(F - 1)}{(f \\cdot (F - f))}\\) , where \\(f\\)  is the number of selected features and \\(F\\)  is the total number of features in interpretation space.\n"]}
{"id": "2211.10253", "categories": "cs.CV", "paragraphs": ["We train TISS with SGD optimizer and the same learning rate policy, momentum and weight decay. We set the initial learning rate to \\(10^{-2}\\)  for the first learning step and decrease it to \\(10^{-3}\\)  for the following steps as done in [4]. The learning rate is decreased with a polynomial strategy with power \\(0.9\\) . The batch size is 12 with 30 epochs of training for Pascal-VOC 2012. For ADE20K, batch size is set to 8 and epoch is set to 60. We apply the same data augmentation of [5] and crop the images to \\(512 \\times 512\\)  during both training and validation. The hyper-parameters of each method are set refer to the protocol of incremental learning defined in [11], using \\(20 \\%\\)  of the training set as validation. The final results are reported on the standard validation set of the datasets. We set the \\(w^t_{unce}\\) , \\(w^t_{ct}\\)  and \\(w^t_{cd}\\)  to \\(1.0\\) , \\(0.1\\)  and \\(0.1\\)  for each step at all tasks, respectively. Then we set \\(w^t_{unkd}\\)  to \\(10.0\\)  for the tasks of which the new classes are added at once and to \\(30.0\\)  for the tasks of which the new classes are added sequentially.\n"]}
{"id": "2206.00637", "categories": "cs.LG", "paragraphs": ["Unless otherwise stated, we use GIN as our standard WLGNN. We use 5 layers by default with hidden dimension 64. We use weighted cross-entropy loss, Adam optimizer with initial learning rate 0.01 and decay of rate \\(0.5\\)  after every 50 epochs. We use dropout on the final layer and we train for 300 epochs by default with a batch size of 32.\n"]}
{"id": "2203.12707", "categories": "cs.CV eess.IV", "paragraphs": ["We unify the model training configuration in this section. We Compare our MSPC, the modified virtual adversarial training (VAT), and the modified mean teacher (MT) models with the recently proposed, popular CycleGAN, GCGAN and CUTGAN, where \u201cmodified\" means transferred from semi-supervised framework to I2I. Please refer to the Section 1 in supplementary for the detailed implementation of modified VAT and MT. We choose the 9-layers of ResNet-Generator with encoder-decoder style [52] and the PatchGAN-Discriminator[24] for all of the models. Besides, we choose the Resnet-19 as our \\(T\\)  network structure. For all of the model optimization, we set the batch-size to 4 and optimizer to Adam with learning rate \\(2\\times 10^{-4}\\)  and \\(\\beta =[0.5,0.999]\\) . On all of the dataset, to be fair, we train each model with 200 epoches and we report the performance of the model from the last epoch because of no validation is provided.\n", "Additionally, for our MSPC model, we have three mini-max game between \\(G,T,D,D_{pert}\\) . Thus, we separate the model training procedure into two steps, \\(\\lbrace D,D_{pert},T\\rbrace -step\\)  and \\(G-step\\) . In each step, we only optimize the corresponding networks and fix others. The size of the spatial transformation grid is \\(2\\times 2\\) . For all the experiments, we set the maximum scale of perturbation to be \\(a=\\frac{1}{3}, b=3\\)  and the translation factors to be \\(c=-0.25, d=0.25\\) .\n{FIGURE}"]}
{"id": "2203.12748", "categories": "cs.LG cs.AI cs.CY stat.ML", "paragraphs": ["For CUB200 and CARS196, we did not perform hyperparameter search but followed reported hyperparameters from\u00a0[68] for best performance with an ImageNet [13] pretrained ResNet50 \u00a0[25] and frozen batch normalization layers. As detailed in\u00a0[68], we train for 150 epochs with embedding dimension 128, learning rate \\(0.00001\\)  with no scheduler, and weight decay \\(0.0004\\) . We train with a batch size of 128, with the Adam optimizer\u00a0[36] over five seeds inclusive for the balance control datasets, and for CUB200 color experiments; and seeds \\(0-9\\)  for the manually class imbalanced experiments. For training transforms, we normalize each image using color channel means \\( (0.485, 0.456, 0.406) \\)  and standard deviations \\((0.229, 0.224, 0.225)\\) , randomly crop the image and re-size to \\(224 \\times 224\\)  and horizontally flip with probability \\(0.5\\) . For testing transforms, we normalize each image with the aforementioned color channel means and standard deviations, resize to \\(256 \\times 256\\) , and center crop to \\(224 \\times 224\\) .\n", "For CelebA and LFW, we performed hyperparameter search over the following hyperparameters: architectures: ResNet50\u00a0[25], and SE-Net50 (both with and without frozen batch normalization layers); number of training epochs; learning rates; last linear layer learning rate (differ from other layer learning rates); learning rate schedulers; embedding dimensions: 64, 128, 256; pre-training; image augmentations. We evaluated hyperparameter sets on a validation set we cut from the typical training set (20% of training set), and chose the set of hyperparameters with best recall@k score for CelebA and best NMI score for LFW. NMI is used for LFW due to the high number of singleton classes present in the dataset (recall@1 is meaningless for singleton classes).\n", "For CelebA, we train on the ResNet50\u00a0[25] architecture with frozen batch normalization layers, for 125 epochs with learning rate \\(0.00001\\) , and no scheduler, weight decay \\(0.0004\\) , Adam\u00a0[36] optimizer, and batch size of 128. For training transforms, we normalize each image using color channel means \\( (0.5, 0.5, 0.5) \\)  and standard deviations \\((0.5, 0.5, 0.5)\\) , resize to \\(256 \\times 256\\) , center crop to \\(224 \\times 224\\)  and horizontally flip with probability \\(0.5\\) . For testing transforms, we normalize each image with the aforementioned color channel means and standard deviations, resize to \\(256 \\times 256\\) , and center crop to \\(224 \\times 224\\) . We average over runs with seeds \\(0-2\\) , inclusive.\n", "For LFW, we train on the ResNet50\u00a0[25] architecture with frozen batch normalization layers, for 125 epochs with initial learning rate \\(0.00001\\)  for all model parameters except the last linear layer, which has initial learning rate \\(0.0001\\) , and a multi-step learning rate scheduler which reduces the learning rate by a factor of \\(0.3\\)  at epochs 50 and 100, weight decay \\(0.0004\\) , Adam\u00a0[36] optimizer, and batch size of 64. For training transforms, we normalize each image using color channel means \\( (0.5, 0.5, 0.5) \\)  and standard deviations \\((0.5, 0.5, 0.5)\\) , resize to \\(256 \\times 256\\) , center crop to \\(224 \\times 224\\)  and horizontally flip with probability \\(0.5\\) . For testing transforms, we normalize each image with the aforementioned color channel means and standard deviations, resize to \\(256 \\times 256\\) , and center crop to \\(224 \\times 224\\) . We average over runs with seeds \\(0-2\\) , inclusive.\n"]}
{"id": "2207.09519", "categories": "cs.CV cs.AI cs.CL", "paragraphs": ["We conduct experiments for Tip-Adapter and Tip-Adapter-F on 11 widely-used image classification datasets: ImageNet [9], StandfordCars [33], UCF101 [53], Caltech101 [13], Flowers102 [43], SUN397 [62], DTD [7], EuroSAT [22], FGVCAircraft [40], OxfordPets [45], and Food101 [2]. For few-shot learning, we compare the performance of 1, 2, 4, 8, 16 few-shot training sets, and test on the full test sets. For the CLIP backbone, we utilize ResNet-50 [21] as the visual encoder and a transformer [12] as the textual encoder. We obtain the pre-trained weights of both encoders from\u00a0[47] and freeze them during training. We follow the data preprocessing protocol in CLIP\u00a0[47], which is composed of random cropping, resizing, and random horizontal flip.\nOther than the learnable prompts in CoOp, we follow CLIP to adopt prompt ensembling especially on ImageNet and use single handcrafted prompt on other 10 datasets. The Tip-Adapter-F is fine-tuned using batch size 256, learning rate \\(0.001\\) , and the AdamW\u00a0[31] optimizer with a cosine scheduler. We set 100-epoch training for EuroSAT dataset and only 20-epoch training for other 10 datasets.\n"]}
{"id": "2207.09531", "categories": "cs.CV", "paragraphs": ["Models were trained on identical input sizes, each image is \\(35 \\times 35 \\times 1\\) . It is imperative to note that all tests are run with the same settings and using the same hardware. An NVIDIA Tesla T4 GPU was used to train the models, with a batch size of 256. In order to save time and prevent overfitting, we define a callback function that stops the training after 30 unchanged epochs on validation loss.\n"]}
{"id": "2203.13166", "categories": "cs.CV", "paragraphs": ["Following the optimisation process in Algorithm REF , there are essentially two sets of optimisation settings in our system, one for optimising the transformer parameters \\(\\theta \\)  and another for updating video centres. For the former, we utilise SGD with momentum as the optimiser with a weight decay of \\(1\\mathrm {e}{-5}\\) ,\nand leverage the OneCycle learning rate policy [76] to implement \\(\\xi \\) . As for the latter, we directly use Eq. REF  and REF  to update the video centres, and its learning rate is defined as \\(\\eta =p\\xi \\)  where \\(p > 0\\)  is a constant number. The intuition is that we should keep the step size of updating video centres to be proportional to that of updating network parameters. There are other ways of designing the video learning rate \\(\\eta \\) , but we find this way can work the best in practice.\n", "We train a total of 900 epochs in each training session, with the first 400 epochs as the warm-up stage. The maximum learning rate in the OneCycle scheduler is set to \\(5.1\\mathrm {e}{-4}\\) .\nA batch size of 512 is used, and a maximum length of 90 is posed on the sampled clip to ensure reasonable memory usages.\nFor each training epoch, we sampled 10 clips out of each face track to be attracted to its own centre, while 16 clips are sampled per track to be repelled to one of its negative video centres. The margin value \\(g\\)  in Eq. REF  is empirically set to be \\(1.0\\) . Besides, we use Eq. REF  to initialise the video centres at the beginning of each training session, while for every 50 epochs we re-compute video centres with Eq. REF .\n"]}
{"id": "2205.00283", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["We used the pretrained 'roberta-base' model from the Huggingface Transformers\nhttps://huggingface.co/transformers/ library. All other modules used in our methodology were built using PyTorch. As observed by [5], we also found that the Hyperbolic Tangent(Tanh) activation function worked better than ReLU, and hence we used the Tanh activation for all layers in our model. The model was trained using an AdamW optimizer [9] with a learning rate of \\(0.001\\)  and beta values set to \\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.99\\)  and the loss used was cross entropy loss. Additionally, early stopping was used if the validation loss does not decrease after 10 successive epochs. The batch size was set to 64 for both Baseline models as well as the proposed model. A single Nvidia P100-16GB GPU provided by Google Colab was used to train all models.\n"]}
{"id": "2212.04097", "categories": "cs.CV cs.LG", "paragraphs": ["Meta-USCL proposed in this work is implemented with PyTorch 1.8.0. The model training can be split into two parts: pre-training and fine-tuning. In the pre-training phase, 80% of data are used for training, and 20% are used for validation. The batch size of 32 is adopted to conduct the training. We conduct contrastive learning with \\(224\\times 224\\)  images for the POCUS pneumonia detection and the UDIAT-B breast tumor segmentation tasks. For BUI breast cancer diagnosis, we use \\(64\\times 64\\)  images. The key reason for using a small image size for transferring to this classification task is that the BUI dataset mainly contains small images, which will be elaborated on in Sec. REF . For the experiments without meta-learning, the optimizer is Adam, with a learning rate \\(\\alpha _1=10^{-3}\\)  and an \\(L2\\)  regularization (weight decay) strength of \\(10^{-4}\\) . The temperature parameter \\(\\tau \\)  equals 0.5 as in [13]. For the experiments with meta-learning, the optimizer for the main model is replaced with SGD with a learning rate of \\(\\alpha _1=0.1\\) . The learning rate for CMW-Net is \\(\\alpha _2=6\\times 10^{-5}\\)  as in the [23]. In each case, the images are augmented with random cropping followed by transformation to the initial size, random flipping, and random color jittering to make the model robust to different image appearances from different imaging systems. As important hyper-parameters, the batch size and the strengths for data augmentations are discussed in detail (see Sec. REF ).\n", "As for fine-tuning, we consider downstream classification and segmentation tasks. For classification tasks, we use a consistent batch size of 128 with the same image sizes as pre-training. The optimization method is set to be Adam with a learning rate of 0.01, training 30 epochs. Images are augmented with randomly cropping, resizing, and flipping. Please note that only the last three layers of pre-trained models are tuned in the fine-tuning phase of classification. For the segmentation task, we discard the final classification layer and use the pre-trained DNN backbone to construct a Mask-RCNN\u00a0[49] model, and fine-tune it on the target dataset. The segmentation is trained with the batch size of 2 in the original image size. SGD method with learning rate \\(5\\times 10^{-3}\\) , momentum 0.9, \\(L2\\)  regularization strength \\(10^{-4}\\)  is used for optimization. Only random flipping is used for data augmentation. All fine-tuning experiments are repeated 5 times. The mean and standard deviation of the results are reported to ensure the reliability of the results.\n"]}
{"id": "2206.00059", "categories": "cs.CL cs.AI", "paragraphs": ["In this section, the model parameters are described for MoE-1, 2, 3 and 4. All of these models represent Mixture of Experts and are based on transformer [50]. Transformer is an encoder-decoder based model that uses self-attention to capture relationships between the elements of the sequence. In our implementation, we used multi-head attention with implementation similar to https://www.tensorflow.org/text/tutorials/transformer#point_wise_feed_forward_network.\n"]}
{"id": "2202.13174", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["As described in section , our BioADAPT-MRC framework consists of three main components: feature extractor, MRC-module, and discriminator.\nIn the implementation of the framework with PyTorch [35], we initialize the feature extractor with the parameters from the pre-trained BioELECTRA model using the huggingface API [53].\nFor the parameters in the MRC-module and the discriminator, we perform random initialization.\nFor tokenization, we set the maximum query length to 64, the maximum answer length to 30, the maximum sequence length to 384, and the document stride to 128, as suggested in [41], [12], [28].\nWe empirically determine that learning rate 5e-5 and batch size 32 are the best choices for our experiments.\nFor each step in the training epoch, we randomly select two samples from the source domain dataset SQuAD and one sample from the target domain dataset BioASQ-7b/8b/9b.\nAfter trial and error, we set the regularization parameter \\(\\lambda \\)  to 0 and then increase it by 0.01 at every 10 epochs up to 0.04.\nWe run all our experiments on a Linux server with Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz and a single Tesla V100-SXM2-16GB GPU.\n"]}
{"id": "2203.02225", "categories": "cs.CL", "paragraphs": ["Instead of learning from scratch, we perform continual pre-training from BART-large [22] due to limited computation resources.\nThe batch size and number of training steps are 1152 and 160k.\nThe model is trained by Adam [21] w/ learning rate of 1e-5 and warmup proportion of 0.03.\nThe gradient clip, dropout rate and weight decay are 1.0, 0.1 and 0.01.\nNotably, (i) BookCorpus has already been used by BART pre-training and our data processing is based on heuristics without human-curated resources;\n(ii) Our continual pre-training only needs 90 GPU hours on 200M tokens, i.e., 0.13% of BART that consumes 70K hours on 2.2T tokens (see Appendix\u00a0REF ).\nHence, ClarET with zero newly introduced corpus and relatively negligible computing overhead makes great lifts and preserves fair comparisons with baselines.\n"]}
{"id": "2205.05912", "categories": "cs.CV", "paragraphs": ["In all the experiments except those in sec:ablation study, we use the loss function defined in eq:finalloss with \\(\\alpha =1/9\\)  and the fusion strategy in eq:infer with \\(T=0.5\\) .\nWe refer the reader to the the supplementary material for more details of the datasets with specific settings and the training configurations.\n", "We use Adam [12] as the optimizer with learning rate 2e-4 and weight decay 1e-4. The data augmentation strategies we use include random color jittering, random horizontal flipping, and random scaling. We use the overall loss function defined in eq:finalloss with \\(\\alpha =1/9\\)  and \\(T=0.5\\) . We use the batch size of 4 and maximum input size of 1088\\(\\times \\) 1088 during training.\n", "For the backbone, we select the ResNet-50 [10] that is already pretrained on the ImageNet dataset[4], which is the same as the setting applied in [19] and [17]. Same as the setting in DeepLabV3[1], we remove the strides of the last two stages (stage 4 and stage 5). Thus, the output stride of the last feature map is 8, and this feature map is subsequently used for semantic segmentation using the vanilla DeepLabV3 segmentation classifier.\nAs for the bbox detection, following Faster R-CNN[24], the feature maps from stage 2 and stage 5 followed by the Feature Pyramid Network[16] are extracted for the generalized bbox detection.\n"]}
{"id": "2210.02665", "categories": "cs.CV cs.AI", "paragraphs": ["Training data: Using the hardware setup introduced earlier, we collected a total of 322 images as training data, validation data and test data. Each image contains a variable number of rice kernels, mostly between 40 and 80. All data are annotated by professionals with many years of experience. They are used for the training and testing of our rice detection and classification model.\n", "Training details: Our detection and classification networks are implemented with Pytorch\u00a0[25]. Both the detection model Yolo-1 and Yolo-2 are trained for 300 epochs from the pretrained yolov5x model using SGD optimizer\u00a0[26], with learning rate of 0.01 and batch size of 16. Then the trained models are fine-tuned for 300 epochs with a small learning rate 0.0032. The size of the input image is 640*640. The classification network ResNeXt is trained for 100 epochs from a pretrained model using Adam optimizer\u00a0[27], with learning rate of 0.001 and batch size of 32. The size of the input image is 224*224.\n"]}
{"id": "2210.02577", "categories": "cs.LG cs.CR", "paragraphs": ["We take a number of steps to improve the reproducibility of our results and comparability to prior work. Firstly, we use the WRN-34-10 model for the CIFAR-10 experiments and the SmallCNN model for the MNIST experiments, as is done in TRADES, and to train our family of TRADES defenses we utilize the default hyperparameters included in the author's GitHub repository. see \"train_trades_cifar10.py\" and \"train_trades_mnist.py\" at https://github.com/yaodongyu/TRADES Secondly, we use the same seed for each experiment to obtain the same random weight initializations and dataset shuffles. Thirdly, to attack our defense models, we use the default code provided by Liu et al. [8] for their AAA implementation and we borrow more code from the TRADES repository for PGD attacks, where PGD attacks on CIFAR-10 are run for 20 iterations with \\(\\epsilon = 0.031\\) , while attacks on MNIST are run for 40 iterations with \\(\\epsilon = 0.3\\)  (the same \\(\\epsilon \\)  values are used for AAA). Lastly, in the RT-threat model, we set \\(\\theta ^{\\text{max}} = 30, \\delta _x^{\\text{max}} = 3\\text{px}, \\delta _y^{\\text{max}} = 3\\text{px}\\) , which is consistent with prior work [3].\n"]}
{"id": "2209.14698", "categories": "cs.CV cs.LG cs.NI cs.SI", "paragraphs": ["While searching for the optimal hyper-parameters, we temporarily capped the maximum number of epochs to 50. We also used only 20 clips from a single speaker data set initially to further reduce training time. Once the model began to overfit, we quadrupled the size of our data set to include more speakers.\n"]}
{"id": "2207.03456", "categories": "cs.LG cs.SY eess.SY", "paragraphs": ["Test case 2 (Spatially correlated smooth permeability distribution): We use test case 2 to represent uncertainty distribution of a smoother permeability field. Figure REF b illustrates reservoir domain for this case.\nIt comprises of four producers located at four corners of the domain and an injector located at the center of the domain.\nThe permeability distribution for this case is considered as a log normal distribution which is constrained with fixed values at well locations.\nAs a result, log permeability \\(g\\)  is sampled from the normal distribution \\(\\mathcal {G}_2\\) :\n\\(g & \\sim \\mathcal {G}_2(\\mu _g, \\Sigma _g), \\textup { where,}  \\\\\\mu _g &= 2.41,  \\\\\\Sigma _g &= C(x,x) - C(x,x^{\\prime }){C(x^{\\prime }, x^{\\prime })}^{-1}C(x, x^{\\prime }), \\) \n"]}
{"id": "2201.13178", "categories": "cs.CV cs.AI cs.CR cs.LG", "paragraphs": ["Settings for SiamFC.\nWe conduct experiments based on the open-sourced codeshttps://github.com/huanglianghua/siamfc-pytorch. We adopt the same training strategy, training data, and parameters adopted in the codes. Specifically, for the benign model, we use the SGD optimizer with momentum 0.9, weight decay of \\(5 \\times 10^{-4}\\) , and an initial learning rate of 0.01. An exponential learning rate scheduler is adopted with a final learning rate of \\(10^{-5}\\) . We train the model for 50 epochs with a batch size of 8 and a backbone of AlexNet-v1 [12] on a single NVIDIA 2080Ti; For BOBA, we sample 10% training samples to generate poisoned samples by adding triggers with a modification rate \\(\\psi \\)  of 1%. Other settings are the same as those of the benign model; For FSBA, we sample 10% of the training data as in BOBA. When computing the \\(L_{f}\\)  described in Section REF , we decay the learning rate as 0.25 of the original one. Other settings are the same as those of the benign model.\n", "Settings for SiamRPN++.\nWe conduct experiments based on its open-sourced codeshttps://github.com/STVIR/pysot. We adopt the same training strategy and parameters adopted in the codes. Due to the limitation of computational resources, we train the SiamRPN++ with a backbone of ResNet-50 [8] only on COCO [18], ILSVRC-DET [27], and ILSVRC-VID [27] datasets with four NVIDIA V100 GPUs. Specifically, for the benign model, we train the model for 20 epochs with a batch size of 28. An SGD optimizer with momentum 0.9, weight decay of \\(5 \\times 10^{-4}\\) , and an initial learning rate of 0.005 is adopted. A log learning rate scheduler with a final learning rate of 0.0005 is used. There is also a learning rate warm-up strategy for the first 5 epochs; For BOBA, we sample 10% of the training samples to generate poisoned samples by adding triggers with a modification rate \\(\\psi \\)  of 1%. Other settings are the same as those of the benign model; For FSBA, we sample 10% training samples to generate poisoned samples by adding triggers with a modification rate \\(\\psi \\)  of 1% as well. When computing \\(L_{f}\\) , the learning rate is decayed as 0.1 of the original one. Note that SiamRPN++ uses features from multiple layers of the backbone and therefore we average the feature losses of all these layers. Other settings are the same as those used for training the benign model.\n{FIGURE}", "Settings for SiamFC++.\nWe conduct the experiments based on the open-sourced codeshttps://github.com/MegviiDetection/video_analyst. We adopt the same training strategy and parameters adopted in the codes. Due to the limitation of computational resources, we train the SiamFC++ with a backbone of Inception v3 [29] only on COCO [18] and ILSVRC-VID [27] datasets with four NVIDIA V100 GPUs. Specifically, for the benign model, we train the model for 20 epochs with a batch size of 64. An SGD optimizer with momentum 0.9, weight decay of \\(5 \\times 10^{-4}\\) , and an initial learning rate of 0.04 is adopted. A cosine scheduler is used with a final learning rate of \\(10^{-6}\\) . There is also a learning rate warm-up strategy for the first epoch. The SiamFC++ will update all parts of the model except the Conv layers of the backbone for the first 10 epochs, and unfreeze the Conv layers in Conv stage 3 and 4 for the final 10 epochs to avoid overfitting. Other details can be found in their codes; For BOBA, we sample 10% training samples to generate poisoned samples by adding triggers with a modification rate \\(\\psi \\)  of 1%. Other settings are the same as those of the benign model; For FSBA, we sample 10% of the training data with a modification rate \\(\\psi \\)  of 1%. When computing the \\(L_{f}\\) , we decay the learning rate as half of the original one. Other settings are the same as those of the benign model.\n"]}
{"id": "2208.12259", "categories": "cs.CV", "paragraphs": ["Finetuning setup.\nFor segmentation task in S3DIS, Pix4Point is trained using Cross-Entropy loss with \\(0.2\\)  label smoothing, AdamW optimizer [28] with a learning rate of 5e-4, a cosine learning rate scheduler, warmup epochs of 10, a weight decay of 1e-4, a batch size of 32, for 100 epochs.\nData augmentations include rotation, scaling, jittering, color auto-contrast, and color dropping.\nFor part segmentation in ShapeNetPart, Pix4Point is trained similarly as in S3DIS using 2048 points as input. The difference lies in: the models are trained for 500 epochs with a batch size of 64 and we drop normals instead of colors and do not perform color auto-contrast since there is no color information.\nFor classification in ScanObjectNN, Pix4Point is trained similarly as ShapeNetPart. A weight decay of \\(0.05\\)  and 200 epochs are used. The number of input points is set to \\(1,024\\) , where the points are randomly sampled during training and uniformly sampled during testing.\nIn all experiments, the best model in the validation set is selected for testing.\n"]}
{"id": "2211.15066", "categories": "cs.CV", "paragraphs": ["We adopt the semi-supervised settings to conduct the task of semantic segmentation. In our settings, merely 3.5% of all the images in the training set are labeled and the other images are regarded as the unlabeled data in our semi-supervised learning settings.\n"]}
{"id": "2212.03498", "categories": "cs.CV", "paragraphs": ["Five widely used polyp segmentation datasets are adopted to evaluate the model performance, including Kvasir\u00a0[9], CVC-ClinicDB\u00a0[1], CVC-ColonDB\u00a0[2], EndoScene\u00a0[20] and ETIS\u00a0[18]. For the comparability, we follow the same dataset partition as\u00a0[6]. Besides, nine state-of-the-art methods are used for comparison, namely U-Net\u00a0[16], U-Net++\u00a0[28], ResUNet\u00a0[25], ResUNet++\u00a0[10], SFA\u00a0[7], PraNet\u00a0[6], SANet\u00a0[22], MSNet\u00a0[26] and Polyp-Pvt\u00a0[5]. Pytorch is used to implement our BoxPolyp model. All input images are uniformly resized to 352\u00d7352. For data augmentation, random flip, random rotation and multi-scale training are adopted. The whole network is trained in an end-to-end way with a AdamW optimizer. Initial learning rate and batch size are set to 1e-4 and 16, respectively. We train the entire model for 80 epochs.\n"]}
{"id": "2204.02687", "categories": "cs.LG cs.AI cs.CY", "paragraphs": ["We use embedding dimension \\(\\epsilon \\) =64, hidden state dimension \\(d\\) =512 for base GRU model and RETAIN. Hidden states dimension \\(d^{\\prime }\\)  for each GRU in R-MoE is determined by the internal cross-validation set (range: 32, 64, 128, 256, 512). The number of experts for R-MoE is also determined by internal cross-validation set (range:1, 5, 10, 20, 50, 100). For the SGD optimizer, we use Adam [15]. For learning rate of GRU, RETAIN, CNN, and LR we use \\(0.005\\)  and for R-MoE we use \\(0.0005\\) . To prevent over-fitting, we use L2 weight decay regularization during the training of all models and weight \\(\\lambda \\)  is determined by the internal cross-validation set. Range of \\(\\lambda \\)  for GRU, RETAIN, CNN, and LR is set as (1e-04, 1e-05, 1e-06, 1e-07). For R-MoE, after observing it requires a much larger \\(\\lambda \\) , we set the range of \\(\\lambda \\)  for R-MoE as (0.75, 1.0, 1.25, 1.5). We also use early stopping to prevent over-fitting. That is, we stop the training when the internal validation set's loss does not improve during the last \\(K\\)  epochs (\\(K\\) =5).\n"]}
{"id": "2201.06227", "categories": "cs.LG cs.AI cs.DC", "paragraphs": ["We follow the popular batch size setting of each model\u00a0[71], [62],\nand use the recommended learning rates and learning rate schedulers: 0.1 for image classification, 0.01 for semantic segmentation, 0.001 for Transformer, and 3e-5 for fine-tuning BERT; step decay learning rate schedule for CV training, inverse square root schedule for Transformer training, and linear schedule for fine-tuning BERT.\nWe use the all-reduce parameter synchronization scheme for data parallel distributed training with multiple GPUs or machines and allocate one GPU per process.\n"]}
{"id": "2203.06378", "categories": "cs.CL", "paragraphs": ["We train the model with a maximum sequence length of 512 for the entire training time.\nWith the markers inserted, the actual maximum sequence length is smaller but we maintain the length as 512 to keep coordinated with previous pre-trained models.\nWe use the ADAM optimizer [8] used in BERT with a batch size 8,192 on 64x Tesla V100 GPUs.\nWe set the learning rate to 1e-4 with a linear warmup scheduler.\nWe run the warmup process for 10k steps and train 100k steps in total.\n"]}
{"id": "2210.04303", "categories": "cs.CV cs.AI cs.LG cs.NE cs.RO", "paragraphs": ["\nWe conduct a systematic hyperparameter tuning process (described in detail in the next subsection) for each of the 21 tested advanced deep models individually. In particular, We ran a grid search over the two most influential hyperparameters, the learning rate and the weight decay rate.\n\nWe do not perform any early stopping but train a substantial number of optimization steps, which has been shown to be vital for generalization, especially on smaller datasets [64], [25], [82], [44].\n\nWe deploy a custom staircase learning rate decay schedule that decreases the learning rate over the training process by dividing the learning rate by four at 60%, 80%, and 93% of the training epochs.\n\nWe warm up the training by running the first epochs with 1/10th of the initial learning rate in order to have the moments' estimates in Adam [38], Batch-Normalization [34], and Layer-Normalization [4] modules initialized properly.\n\nWe replace the standard Adam optimizer with AdamW [55], which decouples the weight decay rate from the loss function, thus avoiding biasing the moments' estimators of Adam.\n\nWe apply a rich set of data augmentation techniques, including random brightness, contrast, and saturation modifications, guided policy learning [50], and add noise to the expert's actions during exploration [26].\n\n"]}
{"id": "2208.10442", "categories": "cs.CV cs.CL", "paragraphs": ["We pretrain BEiT-3 for 1M steps.\nEach batch contains 6144 samples in total, including 2048 images, 2048 texts and 2048 image-text pairs.\nThe batch size is much smaller than contrastive models\u00a0[42], [22], [61].\nBEiT-3 uses \\(14 \\times 14\\)  patch size and is pretrained at resolution \\(224 \\times 224\\) .\nWe use the same image augmentation as in BEiT\u00a0[2], including random resized cropping, horizontal flipping, and color jittering\u00a0[55].\nA SentencePiece tokenizer\u00a0[24] with 64k vocab size is employed to tokenize the text data.\nWe use the AdamW\u00a0[27] optimizer with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.98\\)  and \\(\\epsilon =\\) 1e-6 for optimization.\nWe use a cosine learning rate decay scheduler with a peak learning rate of 1e-3 and a linear warmup of 10k steps.\nThe weight decay is \\(0.05\\) .\nStochastic depth\u00a0[20] with a rate of \\(0.1\\)  is used.\nThe BEiT initialization algorithmWe first randomly initialize the parameters within a small range, e.g., \\([-0.02, 0.02]\\) . Next, we rescale the \\(l\\) -th Transformer layer's output matrices (i.e., the last linear projection within each sublayer) of self-attention and FFN by \\(\\frac{1}{\\sqrt{2l}}\\) .\u00a0[2] is used to stabilize Transformer training.\n"]}
{"id": "2209.11128", "categories": "cs.CL", "paragraphs": ["The model is trained with gradient descent, using ADAM optimizer.\nWe set the hyperparameters according to the BLEU and perplexity results of a grid search on the development set.\nUtterance encoder and decoder hidden sizes are 250, the context-LSTM hidden size is 100.\nThe latent variables are 20-dimensional vectors, their number differs across experiments and is given in the main text.\nFor the RNN components, we use a dropout probability of \\(0.3\\) .\nThe total model size is 7,047,529 parameters.\nThe training time is 3-8 hours using one GPU, depending on dataset.\n"]}
{"id": "2212.08653", "categories": "cs.CV eess.IV", "paragraphs": ["The AdamW optimizer with a learning rate of 5e-4 and a weight decay of 0.5 is used, with a batch size of 4,096. The built-in automatic mixed precision library in PyTorch is adopted for training in all experiments.\nFor the exponential moving average (EMA) model which generates the attentive mask, its momentum starts from 0.996 and gradually increases to 1 using a cosine scheduler during training, following [11]. For the A-CLIP-eff variant, we use a halved resolution image as the input of the EMA model, with a bi-cubic interpolation method to get the new position encodings for the lower-resolution images.\n"]}
{"id": "2202.13402", "categories": "cs.CV", "paragraphs": ["Within the concept model, LSTM dimension was set to 64 and LSTM sequence length to 8. During training, the learning rate was \\(2 \\times 10^{-4}\\) , and the model was trained for 25, 20, 10 epochs for PGS, and CVS tasks separately. The Cross-entropy loss was applied to all the three problems.\n"]}
{"id": "2203.09771", "categories": "cs.CV", "paragraphs": ["\nThere are several datasets publicly available for training and evaluating VFI models, including Middlebury [2], UCF101 [55], Vimeo90K [67] and Adobe240-fps [56]. The Middlebury dataset contains two subsets, i.e., Other and Evaluation. The former provides ground-truth middle frames, while the later hides the ground-truth, and the users are asked to upload their results to the benchmark website for evaluation. The UCF101 dataset [55] contains 379 triplets of human action videos, which can be used for testing VFI algorithms. The frame resolution of the above two datasets is \\(256\\times 256\\) .\n", "We combine the training subsets in Adobe240-fps and Vimeo90K to train our SDL model. The Vimeo90K dataset [67] has \\(51,312\\)  (\\(3,782\\) ) triplets for training (testing), where each triplet contains 3 consecutive video frames of resolution \\(256\\times 448\\) . This implicitly sets the value of \\(t\\)  to \\(0.5\\) , and hence it is insufficient to train our SDL model for finer time intervals. We further resort to the Adobe240-fps dataset [56], which is composed of high frame-rate videos, for model training. We first extract the frames of all video clips, and then group the extracted frames with 12 frames per group. There is no overlap between any two groups. During training, we randomly select 3 frames \\(I_a, I_b, I_c\\)  from a group as a triplet, where \\(\\lbrace a,b,c\\rbrace \\in \\lbrace 0,1,...,11\\rbrace \\)  and \\(a<b<c\\) . The corresponding value of \\(t\\)  can be calculated as \\((b-a)/(c-a)\\) . We also randomly reverse the direction of the sequence for data augmentation (\\(t\\)  is accordingly changed to \\(1-t\\) ). Each video frame is resized to have a shorter spatial dimension of 360 and a random crop of \\(256\\times 256\\) . Horizontal flip is performed for data augmentation. Following SuperSloMo [25], we use 112 video clips for training and the rest 6 for validation.\n", "During model updating, we adopt the Adam [29] optimizer with a batch size of 48. The initial learning rate is set as \\(2\\times 10^{-4}\\) , and it decays by a factor of \\(0.8\\)  for every 100K iterations. The model is updated for 600K iterations.\n"]}
{"id": "2202.09146", "categories": "cs.CV cs.AI cs.IR cs.RO", "paragraphs": ["For all our proposed method variants and the re-implemented baselines, we use the same training parameters as the original work\u00a0[9] using a PyTorch re-implementationhttps://github.com/Nanne/pytorch-NetVlad and as listed here: margin \\(g=0.1\\) , clusters centers (vocabulary size) \\(V=64\\) , total training epochs 35, optimized using SGD with \\(0.9\\)  momentum and \\(0.001\\)  weight decay, and \\(0.0001\\)  learning rate decayed by \\(0.5\\)  every 5 epochs. For triplet set mining, we have used the same methodology of NetVLAD: training and selection of the query-positive-negative triplets are carried using weakly supervised GPS data; for a single query image \\(q\\) , 1 positive (within 10m) and 10 negatives (far away than 25m) are selected from a pool of randomly sampled 1000 negatives; finally, hard negatives are tracked over epochs and used along with new hard negatives for training stability.\n"]}
{"id": "2209.13130", "categories": "cs.CV cs.AI", "paragraphs": ["The proposed algorithm is written in Python and PyTorch and runs on Linux. On a single NVIDIA TITAN RTX GPU, we train for 40 epochs. The initial learning rate is set to 0.0001 and the learning rate decreases by 50% every five training epochs. The batch size is set to 4. The generated pseudo-LiDAR is randomly sampled to 4096 points as input of the scene flow estimator. With the same parameter settings as PointPWC-Net, there are four levels of the feature pyramid in the scene flow estimator in this paper. In Eq. (REF ), the first level weight \\(\\Lambda _{l_0}\\)  to the fourth level weight \\(\\Lambda _{l_3}\\)  are 0.02, 0.04, 0.08, and 0.16. The self-supervised loss weights are \\(\\lambda _1 = 1.0\\) , \\(\\lambda _2 = 0.2\\) , \\(\\lambda _3 = 0.2\\) , and \\(\\lambda _4 = 1.0\\) , respectively.\n"]}
{"id": "2208.09023", "categories": "cs.CV", "paragraphs": ["Specifically, AdamW\u00a0[34] optimizer and the step learning rate schedule are applied to optimize our model. An initial learning rate of 0.0001 and a weight decay of 0.05 are utilized for all backbones. We set a learning rate multiplier of the backbone to 0.1 and we decay the learning rate at 0.9 and 0.95 fractions of the total number of training steps by a factor of 10. For data augmentation, we use the large-scale jittering (LSJ) augmentation with a random scale sampled from range 0.1 to 2.0 followed by a fixed size crop to 1024\\(\\times \\) 1024 on COCO dataset and 640\\(\\times \\) 640 on UVO dataset. Besides, a Cutout\u00a0[26] strategy that randomly cuts out a region of size [1/8\\(\\cdot \\) w, 1/8\\(\\cdot \\) h] to [1/3\\(\\cdot \\) w, 1/3\\(\\cdot \\) h] is introduced during training. On COCO dataset, we train our models for \\(38\\times 10^4\\)  iterations with a batch size of 16, while on UVO dataset, we train our models for \\(12\\times 10^4\\)  iterations with the same batch size.\n"]}
{"id": "2212.07850", "categories": "cs.CL", "paragraphs": ["The implementation of all our models is based on Fairseq-ST [67].\nWe use 512 as embedding size and 2,048 hidden neurons in the feed-forward layers both in the encoder and in the decoder.\nWe set dropout at 0.1 for feed-forward, attention, and convolution layers. Also, in the convolution layer, we set 31 as kernel size for the point- and depth-wise convolutions.\nThe vocabularies are based on SentencePiece [61] with dimension of 8,000 [20] for the target side (de, es) and of 5,000 [67] for the source side (en).\nWe optimize with Adam [37] by using the label-smoothed cross-entropy loss with 0.1 as smoothing factor [62]. We employ Connectionist Temporal Classification \u2013 or CTC \u2013 [28] as auxiliary loss to avoid pre-training and also to compress the input audio, reducing RAM consumption and speeding up inference [23].\nThe learning rate is set to \\(5\\cdot 10^{-3}\\)  with Noam scheduler [65] and warm-up steps of 20k.\nWe stop the training after 15 epochs without loss decrease on the dev set and average 7 checkpoints around the best (best, three preceding, and three succeeding).\nTrainings are performed on 4 NVIDIA A40 GPUs with 40GB RAM. We set 40k as the maximum number of tokens per mini-batch, 2 as\nupdate frequency, and 100,000 as maximum updates (\\(\\sim \\) 23 hours).\n"]}
{"id": "2201.12725", "categories": "cs.CV", "paragraphs": ["For NAS-Bench-101, the AdamW optimizer is set with \\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.982\\) , weight decay term is \\(5\\times 10^{-4}\\)  and \\(\\epsilon = 10^{-9}\\) . The batch size is set to 256 and the NAR is trained for 35 epochs with 50 iterations as warm-up.\nFor NAS-Bench-201, the AdamW optimizer is set with \\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.99\\) , weight decay term is \\(1\\times 10^{-2}\\)  and \\(\\epsilon = 10^{-9}\\) . The batch size is set to 128 and the NAR is trained for 55 epochs with 30 iterations as warm-up.\n"]}
{"id": "2212.08486", "categories": "cs.CL", "paragraphs": ["For \\(\\textsc {blaser}_\\text{s}\\) , the regressor has two hidden layers of sizes 3072 and 1536, similar to comet . We keep the laser encoders fixed during training. We use a learning rate of 5e-5 and employ learning rate annealing with a linear schedule. When training comet, we follow the official implementation and fine-tune the entire model from the xlm-r-large model checkpoint [14]. For both \\(\\textsc {blaser}_\\text{s}\\)  and comet, we train them for 20 epochs. We standardize the human ratings in our training set by subtracting them with a mean and a variance computed based on the entire training set.\n"]}
{"id": "2211.12561", "categories": "cs.CV cs.CL cs.LG", "paragraphs": ["paragraph40.001ex plus 0.001ex minus .001ex-1emImplementation.\nIn our retrieval module \\(R\\) , we use the off-the-shelf CLIP model (ViT-L/14) [34] for both the query and memory encoders \\(E_Q\\)  and \\(E_M\\) .\nWe use FAISShttps://github.com/facebookresearch/faiss [22] to index the external memory \\(\\mathcal {M}\\)  (Flat Index) and perform MIPS-based retrieval.\n", "For our generator \\(G\\) , we use a Transformer [40] of 2.7B parameters. The sequence length is 4096, which can take up to 3 documents. For each input document \\(x\\) , we retrieve \\(K \\sim \\operatorname{Uniform}(\\lbrace 0,1,2\\rbrace )\\)  documents and prepend them to \\(x\\) . At inference time, we may also retrieve and add \\(K\\!>\\!2\\)  documents via ensemble (see \u00a7REF ).\n", "The model is trained for five days on 256 A100 GPUs.\nOur implementation is in PyTorch [33] using Metaseqhttps://github.com/facebookresearch/metaseq [48]. We use model parallelism over 4 GPUs and a batch size of 16 sequences per GPU. The optimization uses a linear learning rate decay with 1500 warmup steps, a peak learning rate of 1e-4, a gradient clipping of 1.0, and the Adam optimizer with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.98\\)  [26].\n", "paragraph40.001ex plus 0.001ex minus .001ex-1emBaseline.\nFor our baseline, we train a vanilla CM3 with no retrieval augmentation, using the same model architecture, training data, and amount of compute, for fair comparison. Since RA-CM3's external memory consists of the same training data, the total information accessible to RA-CM3 and vanilla CM3 are controlled to be the same.\n"]}
{"id": "2202.07901", "categories": "cs.LG cs.CV", "paragraphs": ["For all experiments we use Nvidia Tesla V100-SXM2 GPUs with 32 GB VRAM equipped with Core Xeon CPUs and 192 GB RAM. We use the vanilla Adam optimizer with a learning rate of \\(10^{-4}\\) .\n"]}
{"id": "2202.07959", "categories": "cs.CL", "paragraphs": ["For MT, we follow the setting of [21] to train the model on the WMT14 datasets. For En-De, the training set contains 4.5M parallel sentence pairs. We use newstest2013 as our dev set. For En-Fr, there are 36M parallel sentence pairs for training, and we use newstest2012+2013 as the dev set.\n", "For GEC, we use the public Lang-8\u00a0[18], NUCLE\u00a0[4], FCE\u00a0[31] and W&I+LOCNESS datasets\u00a0[2]. After de-duplicating, we have around 900K sentence pairs for training. Both the dev (CoNLL-13) and test (CoNLL-14) have 1.3K sampales.\n"]}
{"id": "2210.16606", "categories": "cs.LG cs.AI cs.SC", "paragraphs": ["The fixed batch sizes lied in \\(\\left[4, 16\\right]\\) , learning rates in \\(\\left(0.1, 0.6\\right)\\) , learning rate exponential decay factors in \\(\\left(0.9, 1.0\\right)\\) .\nWe used the Adam optimiser.\n"]}
{"id": "2206.13433", "categories": "cs.LG cs.AI", "paragraphs": ["The summary of the DL framework within the RL architectures are as follows: (a) Deep Neural Network (DNN) consisting of a total of \u00a037,000 training parameters and fully-connected (dense) layers with 2 hidden layers having 128 and 256 neurons, respectively, with ReLU activation. (b) Recurrent Neural Network (RNN) consists of a total of \u00a0468,000 training parameters and fully connected (LSTM) layers with 2 hidden layers having 128 and 256 neurons, respectively. The output layer consists of the number of actions the agent can decide for decision-making with linear activation. The parameters of the DRL agent are as follows: discount rate = 0.95, learning rate = 1e-4, and the epsilon decay rate = 0.99 is selected with the initial epsilon = 0.5.\n"]}
{"id": "2211.01200", "categories": "cs.CL", "paragraphs": ["\u00a0\u00a0During the training procedure, we optimize the student network by AdamW with 1e-2 weight decay and schedule the learning rate with a linear decay peaking at 2e-5 after 10% warm-up steps. We set 128 tokens as the maximum length of each sequence and use a batch size of 256. The training procedure takes 3 days for 15 epochs on 8 40GB Nvidia A100 GPUs. For the evaluation procedure, we fine-tune the student encoder for few epochs with a batch size of 32 on English training data, and evaluate on target languages.\n"]}
{"id": "2210.01202", "categories": "cs.CV", "paragraphs": ["GRAF [22]: On each dataset, we train more than 4500 epochs until convergence. The patch size used during training is set to 32. Due to the patch based training process, it supports to be trained at \\(320\\times 320\\)  resolution and convergences slower.\n", "pi-GAN [0]: We train 685 epochs on each dataset and adopt the progressive training strategy as in [0]. Specifically, we train 150 epochs with batch size \\(=30\\)  at image resolution \\(32\\times 32\\) , 270 epochs with batch size \\(=12\\)  at image resolution \\(64\\times 64\\)  and 265 epochs with batch size \\(=8\\)  at image resolution \\(128\\times 128\\) .\n", "GIRAFFE [19]: We set the parameters according to the instructions provided in the official implementation https://github.com/autonomousvision/giraffe. Specifically, we set the number of object field to 1 and use a background field. For training on our datasets, we separately set the parameters for the foreground field in the following manner:\n", "\nStonehenge: The scaling range is set to \\([0.45, 0.55]\\)  and the translation range is set to \\([0.0, 0.0]\\)  for x-axis, y-axis and z-axis.\n\nGrass and flowers: The scaling range is set to \\([0.7, 0.8]\\)  and the translation range is set to \\([0.0, 0.0]\\)  for x-axis, y-axis and z-axis.\n\nisland: The scaling range is set to \\([0.45, 0.55]\\)  for x-axis, y-axis and z-axis. And the translation range for x-/y- axis is set to \\([-0.15, 0.15]\\) , while the translation range for z-axis is set to \\([0.0,0.0]\\) .\n\n"]}
{"id": "2210.14624", "categories": "cs.CV", "paragraphs": ["As part of the ResNet-50 training process, we used RapidAI4EO dataset, which covers 500,000 patches across the European Economic Area (EEA39), with a 70:20:10% split for training, validation, and testing respectively. These 500,000 locations comprise 1400 tiles where each tile covers an area of 8000 x 8000 meters. We further divide this 8000 x 8000 m area into 1600 small patches, on which we ran our ResNet-50 model. The ResNet-50 model is pretrained on ImageNet. Four-channel (RGB-N) images were taken as input for our model. The images were normalized using the mean and standard deviation calculated over all training images. The model is trained for 20 epochs with a learning rate of 0.0001. After every epoch, the learning rate is reduced by a factor of 0.1. We experimented with different loss functions such as Focal Loss, BCE loss and KL Divergence loss for training our model concluding KL Divergence loss to be the best. To incorporate the multi-temporal approach, we trained our LSTM model for 20 epochs with a learning rate of 0.00001 using KL Divergence loss.\n"]}
{"id": "2207.12654", "categories": "cs.CV", "paragraphs": ["Datasets. We adopt the common experimental protocol of SSL, i.e., first pre-training a backbone network with large-scale unlabeled data and then fine-tuning it on downstream tasks with much fewer labeled data. Some previous 3D SSL methods make use of ShapeNet\u00a0 and ScanNet\u00a0 datasets to pre-train the 3D backbones, thus they only focus on the indoor setting and suffer from large domain gap when transferring to the self-driving setting. In our experiments, we adopt Waymo Open Dataset\u00a0 for the self-supervised pre-training. The Waymo dataset contains 798 scenes (158,361 frames) for training and 202 scenes (40,077 frames) for validation; it is 20\\(\\times \\)  larger than KITTI\u00a0. We adopt the whole training set to pre-train various 3D backbones without using the labels.\n", "Network Architectures. For thorough examination of the efficacy and versatility of our approach, we investigate the performance of ProposalContrast on diverse 3D backbone architectures, including grid-based, i.e., VoxelNet\u00a0 and PointPillars\u00a0, as well as point-based, i.e., PointNet++\u00a0. The projection layer, \\(f_\\text{Proj}(\\cdot )\\) , in Sec.\u00a0REF  is implemented as two linear layers, with the first layer followed by a batch normalization (BN) layer and a ReLU. The channel dimension of the output is set as \\(C\\!=\\!128\\) . The predictor \\(f_\\text{Pred}(\\cdot )\\) , implemented as a linear layer, outputs a 128-\\(d\\)  vector as the pseudo-class embedding, i.e., \\(O\\!=\\!128\\) . All the functions in the proposal encoding module are instantiated by linear layers with 128 channels. The attention linear head \\(h(\\cdot )\\)  transforms the attended features to the original backbone channels.\n", "Implementation Details. We empirically consider four types of data augmentations to generate different views, including random rotation (\\([-180^\\circ , 180^\\circ ]\\) ), random scaling ([0.8, 1.2]), random flipping along X-axis or Y-axis, and random point drop out. For random point drop out, we sample 100k points from the original point cloud for each of the two augmented views.\n20k points are chosen from the same indexes to ensure a 20% overlap for the two augmented views,\nwhile the other 80k points are randomly sampled from the remained point clouds.\nWe sample \\(N\\!=\\!2048\\)  spherical proposals for every point cloud frame; each proposal contains \\(K\\!=\\!16\\)  points within \\(r\\!=\\!1.0\\)  m radius.\nThe parameters for the VoxelNet\u00a0 backbone are the same as the corresponding 3D object detectors. The temperature parameter \\(\\tau \\)  in the IPD loss \\(\\mathcal {L}_\\text{IPD}\\)  (Eq.\u00a0REF ) is set to 0.1. The coefficients \\(\\alpha \\)  and \\(\\gamma \\)  in Eq.\u00a0REF  are both set to 1 empirically. We pre-train the models for 36 epochs, and use Adam optimizer\u00a0 to optimize the network. Cosine learning rate schedule\u00a0 is adopted with warmup strategy in the first 5 epochs. The maximum learning rate is set to 0.003.\n"]}
{"id": "2205.12374", "categories": "cs.CL cs.LG", "paragraphs": ["\nWe train our models using the Transformer implementation using HuggingFace [33]. We tokenize data using SentencePiece [11], using the same vocabulary used in [15] for natural language, and a custom 10k vocabulary for code. We use the Transformer architecture with a hidden dimension of 768, feed-forward size of 3072, and 6 layers for both the encoder and decoder. We initialize all natural language models with BART [15], and code models randomly. We set the maximum sequence length \\(=\\) 2048, using a batch size of 65K tokens distributed over 8 A100 GPUs.\n"]}
{"id": "2206.14318", "categories": "cs.CL cs.LG eess.AS", "paragraphs": ["The proposed compact SLU system is trained with the Adam optimizer with \\(\\beta _1=0.9,\\beta _2=0.98\\) , \\(\\epsilon =10^{-16}\\)  and warmup [23]. The final model is constructed by averaging the model parameters of the last 10 training steps. To regularize during training, we apply dropout with a rate of \\(P_{drop=0.1}\\)  to each sub-layer, including the content and position embeddings.\n"]}
{"id": "2209.08546", "categories": "cs.CV", "paragraphs": ["In our experiments, we follow the settings in NeRF, and sample 64, 128 points for coarse and fine models respectively. We use the Adam optimizer with an initial learning rate at \\(5e^{-4}\\)  which decays exponentially to \\(5e^{-5}\\)  during optimization. We use a batch size of 1024 rays and train our model on a single RTX2080Ti GPU.\n"]}
{"id": "2209.10658", "categories": "cs.LG cs.CE", "paragraphs": ["We train every model for a maximum of 5000 epochs with a mini-batch of size 128 and use the Adam optimizer [23] with \\(\\beta _{1}=0.9\\) , \\(\\beta _{2}=0.999\\)  in combination with a cosine learning rate scheduler. The parameters of the encoder and decoder are randomly initiated as described in [14].\n"]}
{"id": "2202.04736", "categories": "cs.LG cs.AI", "paragraphs": ["\nFor VGG-16(+), we increase the number of training epochs to 240, and decay the learning rate at 150-th, 180-th, and 210-th epoch.\n\nFor WRN-32-2(+), we do not split the official training set into the a training and a validation set as our other experiments did. We also report the best validation accuracy instead of the best test accuracy. The number of training epochs is increased to 240 and the learning rate is decayed at 150-th, 180-th, and 210-th epoch.\n\nFor RN-50(+), we replace the first convolution layer to be of kernel size 3, padding size 1, and strides 1.\n\n"]}
{"id": "2211.15755", "categories": "cs.LG math.OC", "paragraphs": ["The proposed GNN-based architecture and other neural network baselines\nwere implemented and trained using PyTorch. The minibatch size was\nset to 8 for training. The AdamW optimizer\n[65] with a learning rate of\n\\(1\\mathrm {e}{-4}\\)  and a weight decay of \\(1\\mathrm {e}{-6}\\)  was used for\nupdating the trainable parameters. Early stopping was applied with a\npatience of 5, while the learning rate was decayed by \\(0.95\\)  when the\nvalidation loss was worse than the best validation loss at every\nepoch. The maximum epoch was set to 20. The training and testing\nprocess was performed using Tesla V100 GPU on machines with Intel CPU\ncores at 2.7GHz. The architectural design of the proposed GNN-based\nencoder-decoder structure is detailed in\nAppendix\u00a0. Other details on the training\nprocess are elaborated in Appendix\u00a0.\n"]}
{"id": "2204.13650", "categories": "cs.LG cs.CR cs.CV stat.ML", "paragraphs": ["Scaling the depth of neural networks has been crucial in achieving good performance with non-private training [40], [89], [14]. In this paper, we were interested in exploring whether we could leverage the same benefits of deep models when training with DP-SGD. However, in our initial experiments on WRN models for CIFAR-10 classification without extra data under \\((8, 10^{-5})\\) -DP,\nwe found that both training and validation set performance degraded for networks deeper than the WRN-16-4.\n", "To understand this, in figure:computedepthscaling, we train a range of Wide-ResNets at different depths (from 16 to 100 layers)\nusing a fixed noise parameter \\(\\sigma = 2\\)  and a batch size of 4096 until exhausting a very large privacy budget of \\((64, 10^{-5})\\) -DP, and track the performance of these models during training. We emphasize that \\(\\varepsilon = 64\\)  is too large to provide a meaningful notion of privacy, but our goal here is to gain an understanding for the training dynamics.\nIn our initial experiment, we train without data augmentation. We find that the deeper models start converging slower than the shallower models, likely due to the added noise in DP-SGD. However, when trained for long enough (i.e., for a sufficiently large privacy budget), the deeper models outperform the shallower models. This is illustrated in fig:deptha, where we show that at low \\(\\varepsilon \\)  values (which corresponds to early in training for this experiment), the WRN-16-4 outperforms all deeper models, while at high \\(\\varepsilon \\)  values (i.e., late in training), some of the deeper models start to outperform\nthe shallower models.\n", "Interestingly, we find that using data augmentation with large augmentation multiplicities significantly speeds up convergence of the deeper models. Performing the same experiment as before\nusing augmentation multiplicity \\(K=16\\) , we see in fig:depthc that even early in training at \\(\\varepsilon =8\\) , the WRN-40-4 significantly outperforms the WRN-16-4. Late in training, at larger values of \\(\\varepsilon \\) , a much deeper network is optimal compared to when not using augmentation multiplicity. We note this effect of speeding up convergence of deeper models is not simply due to using data augmentation (See fig:depthb using augmentation multiplicity 1).\n"]}
{"id": "2204.13661", "categories": "cs.LG cs.AI cs.RO", "paragraphs": ["(Data)\nIn generating training and evaluation dataset, we guarantee that (1) the combinations of objects in training dataset are different from those of evaluation dataset, and (2) training data contains all \\(N\\)  objects in the library.\n In practice, we found it is critical to keep all individual objects seen in training, or at evaluation the model would very likely mess up objects' colors or shapes, as further detailed in Appendix .\n We use the same object library for all experiments for lower randomness.\nWe use 1k episodes in training and 10 episodes of length 100 for 100 scenes, and 10k episodes of length 10 for evaluation.\nAdditionally, we make sure that \\(90\\%\\)  of the transitions have objects moving (filtering collisions), so there is denser signals for relating actions and moved objects.\n"]}
{"id": "2208.04278", "categories": "cs.CV cs.GR cs.LG", "paragraphs": ["Data: For experimentation, we use the Human Body Segmentation dataset introduced by [13], which contains 381 meshes for training and 18 for testing. Each mesh comes with a semantic segmentation label containing 8 classes. For contrastive learning, we utilize the entire training set without the segmentation labels. For downstream segmentation, we use varying quantities of samples.\n", "Training: For contrastive pre-training, we utilize all 381 meshes for training. For downstream segmentation, we conduct experiments by randomly sampling varying proportions of data: 5%, 10%, 25%, 33%, 50%, 67%, 75%, and 100%. For example, when using 10% of the data, we train on only 38 randomly selected meshes and their corresponding segmentation labels. All segmentation experiments are performed with a standard Mesh-UNet. Each experiment is repeated 3 times and the means are reported. We implemented the models using PyTorch and trained using an NVIDIA GeForce GTX 1080.\n", "Hyperparameters: We perform pre-training with minibatch size \\(M_1\\)  = 32 and downstream training with \\(M_2\\)  = 12. We pre-train for epochs \\(N_1\\)  = 100 and downstream train for epochs \\(N_2\\)  = 30. These training hyperparameters were experimentally tuned. We use the Adam optimizer with a learning rate of 0.0002 and group norm of 16. For our loss, we set \\(\\tau \\) , the temperature parameter, to 0.7. The optimizer and loss hyperparameters were chosen based on previous works.\n", "Augmentations: For anisotropic scaling, we randomly sample each scaling value from a normal distribution with \\(\\mu = 1.0\\)  and \\(\\sigma \\)  = 0.1. We perform vertex shifting with a probability of 0.2 and edge flipping with a probability of 0.05.\n"]}
{"id": "2206.05184", "categories": "cs.CV", "paragraphs": ["To compare with existing self-supervised learning methods for ViTs,\nwe adopt the ViT-S/16 as the backbone network.\nThe DINO\u00a0[6] is selected as our major baseline method.\nDuring pretraining,\nthe model is trained by an AdamW\u00a0[32] optimizer\nwith the batch size of 512 on 8 GPUs and learning rate of 0.001.\nFollowing\u00a0[6],\nwe apply the multi-crop training scheme where 2 global crops with the resolution of 224\\(\\times \\) 224 and 4 local crops with the resolution of 96\\(\\times \\) 96 are adopted.\nThe global crops are cropped with a ratio between 0.35 and 1.0,\nand the local crops are cropped with a ratio between 0.05 and 0.35.\nThe number of heads for modeling the pixel-level self-relation\nis set to 6 by default.\nThe temperature terms for the pixel-level and channel-level self-relation\nare set to 0.5 and 0.1, respectively.\nThe temperature terms are only applied to the momentum\nencoder to generate a sharper distribution, speeding up the encoder training.\nWhen sampling overlapping regions for pixel-level self-relation,\nthe overlapping regions for global and local crops are set to\n13\\(\\times \\) 13 and 6\\(\\times \\) 6, respectively.\nFor performance comparison, we pre-train models on the ImageNet-1k\u00a0[38] dataset.\nFor ablation, the ImageNet-S\\(_{300}\\)  dataset\u00a0[15]\nis used to save training cost.\n"]}
{"id": "2206.05252", "categories": "cs.CV", "paragraphs": ["Action Recognition. We train 3D-ResNet18 on HMDB51 following the setup of\u00a0[13]. For each video clip, we randomly select a contiguous 16-frame segment. Then, all images are first channel-normalized with respect to the ActivityNet\u00a0[2] mean. Following normalization, we apply a multiscale random crop with aspect ratio one at scales \\(\\lbrace 1, 1/2^{1/4}, 1/\\sqrt{2}, 1/2^{3/4}, 1/2\\rbrace \\) , choosing one of the four corners + center of the video to crop (as in a five-crop). We then resize all videos to have size \\(112 \\times 112\\)  pixels. We then train for 50 epochs using a starting learning rate of \\(0.001\\)  with weight decay \\(10^{-5}\\) , optimizing using stochastic gradient descent. When validation loss saturates for 10 epochs, we decrease learning rate by a factor of 10. For more details, consult the method of\u00a0[13].\n"]}
{"id": "2210.02549", "categories": "cs.LG", "paragraphs": ["For each experiment, we generate 1200 random examples from the task generator.\nWe split this set randomly into a training set with 80% of the data \u2014 960\nexamples \u2014 and a test set with the remaining 240 examples. The reservoir is run\non each training example for the reservoir-based models, which creates the\ninput features for training the decoder. The sequential supervised models use\nbatches of single sequences with the Adam algorithm\n[20]. They are trained for ten epochs in total. With\nreservoir models, only the last linear layer (the decoder) is trained. We minimize the\ncross-entropy loss with stochastic gradient descent (SGD), doing only a single\npass over the 960 training examples.\n"]}
{"id": "2210.05793", "categories": "cs.LG cs.CL cs.SD eess.AS", "paragraphs": ["Hard target distillation is sensitive for learning hyper parameters such as learning rate and learning schedule. We use different learning rate and schedule for the encoder and the decoder, as Wav2vec 2.0 paper\u00a0[21] did, because the encoder is pre-trained. Otherwise, the training is easily diverged. On the other hands, soft target distillation is forgiving over wide range of learning rate and schedule. It allows us to have higher learning rate.\n"]}
{"id": "2210.09549", "categories": "cs.CV cs.LG", "paragraphs": ["We apply an Imagen-like training strategy, i.e., training the base model and then the super-resolution model twice. The Adam optimiser is adopted, having a learning rate of 1e-4. We give 10,000 linear warm-up steps with a batch size of 8 and training epochs of 1,000. The loss function is Mean Squared Error (MSE), formulated as follows.\n\\(MSE(I, K) = \\frac{1}{M \\times N} \\sum _{i=0}^{M-1} \\sum _{j=0}^{N-1} [I(i, j) - K(i, j)]^2 \\quad ,\\) \n"]}
{"id": "2207.08806", "categories": "cs.LG cs.AI", "paragraphs": ["The pre-trained model has 12 layers, and the hidden dimension of each block is 256. The masked ratio \\(p\\)  is \\(0.25\\) . The model is pre-trained for 100 epochs using Adam optimizer with initial learning rate \\(2\\times 10^{-4}\\) , batch size 128 and is trained on four P40 GPUs.\n", "For the molecular property prediction tasks with 2D information only, all the coordinates are randomly sampled from uniform distribution \\([-1,1]\\) . For the tasks on MoleculeNet, we use grid search to determine the learning rate, dropout and batch size, which are summarized in Appendix . For ogb-molpcba, the learning rate is fixed as \\(10^{-4}\\)  and we train the model for 100 epochs.\n"]}
{"id": "2207.03145", "categories": "cs.CL", "paragraphs": ["\nBaseline.\nThe model is trained on the 4-label classification task on CANARD/ GECOR.\n\nFine tuning only.\nThe model is trained on the 4-label classification task on the training set of ConvQuestions.\n\nBaseline + AL.\nThe model is trained on the 4-label classification task on CANARD/GECOR, but labeled instances of CANARD are added via active learning. Each round of active learning adds 50 instances that are labeled for either coreference or ellipsis. We evaluate several versions of this variant: three versions use instances that were annotated for coreference via, respectively, 1, 2, and 3 rounds of active learning. Three others versions use instances that were annotated for ellipsis via 1, 2, and 3 rounds.\n\nBaseline + all AL.\nIdentical to baseline + AL, but using all annotations produced for coreference and ellipsis (3 rounds for each).\n\nBaseline + all AL + fine tuning.\nIdentical to Baseline + all AL., but training on CANARD/GECOR is followed by a fine-tuning step on the training set of ConvQuestions.\n\n2-label variants.\nWe evaluate three of them. They are respectively identical to  baseline, to baseline + all AL, and to baseline + all AL + fine tuning, with the difference that the model is trained on the 2-labels classification task.\n\n", "We use the \u201cdistilbert-base-uncased\u201d pretrained HuggingFace model https://huggingface.co/distilbert-base-uncased.\nThe tokenization of the concatenated string is done by HuggingFace's DistilBertTokenizerFast.\nThe same hyperparameters are used in all variants.\nThe hidden layer is made of 768 units with ReLU activation function.\nThe output layer uses sigmoid activation function.\nFor all models, training is done in 10 epochs, with a batch size of size 16, a learning rate of \\(0.0001\\) , and a dropout probability of \\(0.1\\) .\nDuring training, weight are updated by retropropagating the Mean Squared Error of each output unit.\nWe compensate class imbalance by using class weights, in such way that the cumulative weight of negative and positive classes are equal (for ellipsis and coreference, respectively).\n"]}
{"id": "2212.12641", "categories": "cs.LG cs.CV", "paragraphs": ["We trained 45100 iterations for C-10 and TIN and 70100 iterations for ImaneNet, using the Adam optimizer [75] with \\(\\beta _{1} = 0.9\\)  and \\(\\beta _{2} = 0.999\\) . The batch size is 256 for C-10, 16 for TIN, and 4 for ILSVRC2012.\nThe learning rate for the first 5100 iterations is set to 1e-4, and 5e-5 for the rest.\nWe applied the data augmentation of random \\(2 \\times 2\\)  translation and horizontal flip on C-10 and TIN.\nFor ILSVRC2012, the image is first resized to be 256 in height or width, whichever is shorter, and cropped to \\(224 \\times 224\\)  at random.\n"]}
{"id": "2212.12735", "categories": "cs.LG", "paragraphs": ["We train PPO with Adam optimizer with learning rate 7e-4, max gradient norm 0.5, clip parameters 0.1, value loss coefficient 0.5, and entropy coefficient 0.01.\nWe use different training schedule between the VAE part (encoder & decoder) and the policy part. We use two Adam optimizers with different learning rates. The VAE optimizer uses a learning rate of 1e-3, and the policy optimizer uses a learning rate of 7e-4.\nFor the full details of hyper-parameter settings, please refer to the code repository that we will publish soon.\n"]}
{"id": "2212.08974", "categories": "cs.CV", "paragraphs": ["We describe the pre-training setting of our backbone.\nFor the point cloud backbone, we adopt the standard transformer encoder with 12 transformer blocks. The hidden dimension of the transformer blocks is 384, and the number of heads is set to 6 for each block. For the 2D image branch, we leverage the ClipCaption\u00a0[22] model.\n"]}
{"id": "2202.07308", "categories": "cs.CV", "paragraphs": ["For training the global alignment network, we perform data augmentation by sampling camera views from a 3-frequency subdivision icosahedron. This gives us 92 additional training samples per example. Since there are 400 examples in HAA4D that are provided with globally aligned skeletons, with the help of data augmentation, we have 36,800 examples of training our global alignment network. We split all the data into training and validated with a ratio of 0.8 under two settings: cross-views and cross-actions. We select 73 views on the icosahedron sphere for cross-views and test the rest 19 views to ensure that our network generates predictions decently while encountering unseen views. We also trained our network on cross-views, i.e., 32 out of the 40 classes, to secure that the model is used to generalize different actions. Here are our training environment and configurations in more details:\n"]}
{"id": "2206.14009", "categories": "cs.CV cs.SD eess.AS eess.IV", "paragraphs": ["All experiments were implemented with PyTorch[27]. We use the adam optimizer[18] with initial learning rate set as 0.001 for training all experiments. We have developed a prototyping framework for models, where any change to the model, will generate appropriate model/validation/training logs automatically. We extensively use this to log our experiments.\n", "The Lip2Speech network is trained for 300 epochs on both LRW and YLD, with early stopping when the validation accuracy does not improve for 10 epochs. We use a batch size of 84 and teacher force the decoder inputs. The teacher forcing is annealed for every 10 epochs, which slowly shifts the decoder to test sequence generation. We also augment the train data with random horizontal flips for the lip sequence.\n"]}
{"id": "2210.12739", "categories": "cs.LG cs.AI cs.CV", "paragraphs": ["For ESBN, Transformer, RelationNet and PrediNet, we follow the same\nsettings as [32], where all given images (including\nexamples and answer candidates) are treated as a sequence and passed\nthrough a context normalization layer before being fed to the model.\nFor HyperNetwork, we also use the NICE backbone for fair comparisons\nand maintain the key memories (but not the value memories) to compute\nthe weights; at each layer of the backbone, the attention weights\nare computed as the output of an LSTM cell, where the input for LSTM\nis the concatenation of the input and (pseudo-)output of current layer,\nand the hidden states are taken from the LSTM cell of previous layer.\nFor FINE with NICE backbone, we use 4 NICE layers while using 2-layer\nMLP for FINE with MLP backbone. We use 8-32 basis weight matrices\nin the experiments.\n", "We use the Adam optimizer with no weight decay along with gradient\nclipping with threshold 10 in all experiments. All tasks are trained\nwith 200-300 epochs. The training and testing batch sizes are 32 and\n100, respectively. Feature vectors of images are of size 128.\n"]}
{"id": "2211.17078", "categories": "cs.LG cs.AI math.OC", "paragraphs": ["When training the general VRP agent, we used\nand encoding dimension \\(d=128\\)  and three encoder layers\nand 8 attention heads for all layers except for\nthe final policy output layer.\nFor feedforward layers we used 64 dimensional hidden layers.\nWe used dynamical masking as described\nin section REF \nas we found that the memory limitations were\ntoo prohibitive for full tensor demand structure.\nFor dynamical masking in the decoder, we used\none source \\(D_{ij}\\)  given by\n\\(D_{ij} = \\sum _k D^\\mathrm {total}_{ijk} + D^\\mathrm {total}_{ij}\\) \n"]}
{"id": "2207.00587", "categories": "cs.CV cs.AI", "paragraphs": ["In the NIST SD14 database, the two fingerprint impressions of each finger are stored with labels `f' and `s'. A positive pair of fingerprints is formed by the latent fingerprint derived from `f' and the rolled fingerprint `s', or the latent fingerprint derived from `s' and the rolled fingerprint `f'. Thus, there is a total of 54,000 positive pairs of fingerprints for the training. The negative pairs of fingerprints are formed by randomly selecting a non-mated rolled fingerprint for each of the synthesized latent ones, and thus the total number of the negative pairs is also 54,000. We randomly choose 80% of these pairs of fingerprints to train the set of CNNs and use the remaining 20% pairs to train the RBM and tune the entire network.\n"]}
{"id": "2206.00343", "categories": "cs.CV cs.AI", "paragraphs": ["These parameters have been the following: a learning rate of \\(3\\times 10^{-4}\\) , a batch size of 5, Adam has been used as the optimizer, and the MSE has been used as the loss function. The network has been trained using the early-stopping technique, with a patience of 7, and the velocity data has been normalized between -1 and 1. In this way, the new training can be perfectly compared with the one performed in [11].\n"]}
{"id": "2212.01428", "categories": "cs.LG physics.flu-dyn", "paragraphs": ["The simulation was done with unitless equations.\nViscosity was set to 0.001, with density of 1, which gives us a Reynolds number of 1800.\nThe timestep was set to 0.001, for 5000 timesteps, for a total of a five second simulation.\nThe discount factor \\(\\gamma \\)  was set to 1.\nThe Adam optimizer[33] was used with a learning rate of 0.0005 and no weight decay.\nThe weights and biases of each layer were initialized with Xavier normal initializer[34] using a gain of 0.9.\nA convolutional width of 128 was used for each GraphSAGE and GCN layer.\nTwo GraphSAGE and GCN layers were used.\nAdditionally, training was parallelized using Ray.\nFive evenly spaces snapshots were used at timestep 1000, 2000, 3000, 4000, and 5000.\n"]}
{"id": "2211.14730", "categories": "cs.LG cs.AI", "paragraphs": ["By default, PatchTST contains 3 encoder layers with head number \\(H=16\\)  and dimension of latent space \\(D=128\\) . The feed forward network in Transformer encoder block consists of 2 linear layers with GELU [15] activation function: one projecting the hidden representation \\(D=128\\)  to a new dimension \\(F=256\\) , and another layer that project it back to \\(D=128\\) . For very small datasets (ILI, ETTh1, ETTh2), a reduced size of parameters is used (\\(H=4, D=16\\)  and \\(F=128\\) ) to mitigate the possible overfitting. Dropout with probability \\(0.2\\)  is applied in the encoders for all experiments. The code will be publicly available.\n", "To see whether PatchTST is sensitive to the choice of Transformer settings, we perform another experiments with varying model parameters. We vary the number of Transformer layers \\(L=\\lbrace 3,4,5\\rbrace \\)  and select the model dimension \\(D=\\lbrace 128, 256\\rbrace \\)  while the inner-layer of the feed forward network is \\(F = 2D\\) . In total, there are 6 different sets of model hyper-parameters to examine. Figure REF  shows the MSE scores of these combinations on different datasets. Except ILI dataset reveals high variance with different hyper-parameter settings, other datasets are robust to the choice of model hyper-parameters.\n{FIGURE}"]}
{"id": "2206.03322", "categories": "cs.LG cs.AI", "paragraphs": ["We divide the collected data(11311 data points) randomly into a training sample containing 8000 FEA evaluations and a testing sample containing the remaining 3311 evaluations. Due to the unavailability of large data, to increase the performance of prediction, we trained a family of base deep learning networks as it has been seen to be more accurate than individual classifiers[4]. For this purpose, we used 5-fold cross-validation and trained one base deep neural network on each fold dataset. We further divided each fold dataset into the training dataset (90%) and the validation dataset(10%). The prediction of the individual trained networks was then averaged out to obtain the final values of the maximum Von-mises stress on a given design(refer figure REF ). After experimenting with different loss functions and learning rates, we found that L1 loss performed better than other losses with a learning rate of 0.001. We used Adam[5] optimizer with Xavier initialization for training the model. Code of training and data can be found at https://github.com/vardhah/FEAsurrogate.\n{FIGURE}"]}
{"id": "2206.15472", "categories": "cs.CV", "paragraphs": ["We used SGD optimizer+QAS for training. We set weight decay as 0 since we observed no over-fitting during experiments. This is also a common choice in transfer learning\u00a0[37].\nWe find the initial learning rate significantly affects the accuracy, so we extensively tuned the learning rate for each run to report the best accuracy.\nWe used cosine learning rate decay and performed warm-up\u00a0[27] for 1 epoch on VWW and 5 epochs on other datasets.\nWe used Ray\u00a0[52] for experiment launching and hyper-parameter tuning.\n"]}
{"id": "2210.14319", "categories": "cs.CV", "paragraphs": ["Data augmentation methods we used include CutMix [40] and Mixup [41] as in [19]. The initial learning rate is set to \\(5e^{-4}\\)  and 20 epochs warmup. We report the best top-1 accuracy after 100 epochs. The batch size is 128 per GPU and four A100 GPUs are used in total. Following [19], we resize all images to 3\\(\\times \\) 224\\(\\times \\) 224 on all datasets to get our baselines and resize all images to 3\\(\\times \\) 448\\(\\times \\) 448 unless otherwise specified. For a fair comparison, all vision Transformer backbones we selected have a comparable amount of parameters with ResNet50.\n"]}
{"id": "2207.14671", "categories": "cs.CV eess.IV", "paragraphs": ["We denote here by \\(\\theta \\)  all the learnable parameters of our methods, including those of the CNNs and the scalar parameters involved in the HQS optimization procedure introduced above (e.g.,\u00a0, \\(\\delta \\) , \\(\\eta \\) , ...).\nWe use triplets of the form \\((x^{(i)}, Y^{(i)}, \\Delta ^{(i)})\\)  (\\(i=1,\\ldots ,n\\) ) of training data to supervise the learning\nprocedure. In our setting where ground-truth HDR/HR images are\nnormally not available for real image bursts, the training data\nis necessarily semi-synthetic, that is, obtained by applying various\ntransformations to real images. Obtaining robust inference with real raw bursts is thus challenging. The hybrid nature of our algorithm, which exploits both a learning-free inverse problem formulation and data-driven priors, appears to be a key to achieving good generalization on real raw data acquired in various conditions that do not necessarily occur in the training dataset.\n{FIGURE}{FIGURE}"]}
{"id": "2206.15369", "categories": "cs.CV cs.LG", "paragraphs": ["Given an input image, we define two scale parameters, for global and local crops.\nGlobal crops follow the data augmentation protocol described in Section\u00a0REF , while local crops are resized to \\(96\\times 96\\)  instead.\nSimilar to\u00a0[5], [6], we extract multiple global and local crops, respectively \\(M_g\\)  and \\(M_l\\) .\nfig:training illustrates one global \\(M_g = 1\\)  and four local \\(M_l = 4\\)  crops.\nIn\u00a0sec:exp, we explore the use of multi-crop for supervised learning, and study the effect of different hyper-parameters under that setting.\n", "Training with cosine softmax cross-entropy loss.\nThe complete training pipeline, using the components described so far, is illustrated in\u00a0fig:training.\nIt uses multi-crop data augmentation on each input image \\(I\\)  and produces \\(M = M_g + M_l\\)  crops \\(I_j\\) , \\(j=1,\\ldots ,M\\) .\nEach crop is individually input to the network composed of the encoder followed by the projector, and produces representation \\(x_j = f_\\theta (I_j)\\) .\nWe adapt the cosine softmax cross-entropy loss defined for image \\(I\\)  in\u00a0eq:cecos, such that we average the loss over all (local and global) crops of an image:\n\\(\\mathcal {L}_{\\text{CE}}^*= - \\frac{1}{M} \\sum _{j=1}^{M} \\sum _{c=1}^C y_{[c]} \\log \\frac{\\exp (g_\\phi (x_j)^\\top \\bar{\\omega }_c / \\tau ) }{\\sum _{k=1}^C \\exp (g_\\phi (x_j)^\\top \\bar{\\omega }_k / \\tau )}.\\) \n"]}
{"id": "2206.15241", "categories": "cs.LG physics.ao-ph", "paragraphs": ["We use the Adam optimizer with the default learning rate of 0.001, and beta parameters of 0.9 and 0.999. We train the models for 20 epochs and select the best epoch based on the highest CSI performance on the validation set. Reported performances are based on the test set, unless otherwise specified. We use a window size of 3 and lead times ranging from 6 to 87 hours. We use two Pres type variables (T, rh_liq) at three isobaric planes,{500hPa, 700hPa, 850hPa}; and six variables of Unis type (rain, q2m, rh2m, t2m, tsfc, ps). These baseline settings are used in all experiments, unless otherwise stated.\n"]}
{"id": "2205.14589", "categories": "cs.CV cs.LG", "paragraphs": ["Loss weights: We set MasKD loss weight \\(\\lambda _1 = 1\\)  and regression loss weight \\(\\lambda _2 = 1\\)  on Faster RCNN students. For other detection frameworks, we simply adjust the loss weight \\(\\lambda _1\\)  of \\(\\mathcal {L}_\\mathrm {MasKD}\\)  to keep a similar amount of loss value as Faster RCNN. Concretely, the loss weights \\(\\lambda _1\\)  on RetinaNet, FCOS, and RepPoints are 5, 10, and 10, respectively.\n"]}
{"id": "2206.13676", "categories": "cs.LG", "paragraphs": ["where \\(L_{adv}\\)  is the adversarial loss to determine how well \\(D\\)  can distinguish real and fake signals. \\(L_{cls}\\)  is the classification loss to determine how well \\(D\\)  can classify the input signal to its proper categorical label.\n\\(\\lambda \\)  is a hyper-parameter that controls the relative importance of categorical classification loss and adversarial loss. We use \\(\\lambda = 1\\)  in all of our experiments.\n"]}
{"id": "2206.13737", "categories": "cs.CV", "paragraphs": ["We use Efficient-b2\u00a0[18] as the backbone for the segmentation network \\(S\\)  and modify it with UNet-style skip connection. The synthesizer network \\(T\\)  has 4 convolutional blocks, each block consisting of \\(3\\times 3\\)  convolutional kernel with the channel size of 2, Leaky-ReLU and AdaIN. For the MIM model, we use the encoder part of the generator with a fully connected layer as the feature extractor \\(F\\)  from\u00a0[15]. We choose Adam optimizer\u00a0[9] for all of the models with initial learning rate \\(3\\times 10^{-4}\\)  and \\(\\beta =[0.5,0.999]\\) . The learning rate will linearly decay to zero at the end of the training. The total training epochs are 2,000. We compare our method to Cutout, AdvBias, RandConv and GIN\u00a0[14] under the same settings.\n"]}
{"id": "2208.13504", "categories": "cs.CV cs.LG", "paragraphs": ["We use Sentinel-2 RGB bands to create images of size \\(10980 \\times 10980\\)  with spatial resolution of 10 meters per pixel. These three bands, along with the near infrared, are the only bands provided by Sentinel-2 with this resolution. The remaining bands are given at 20m and 60m. Since this work stresses the use of MTSs and part of the experiments rely on visual inspection, we choose to deal with images of bounded complexity in terms of band composition to facilitate the interpretation of the results.\n", "The whole area considered for embedding training is covered by 4 satellite images (see Figure REF  on the right). Note that this scheme can be extrapolated directly to any other region of interest. To maintain a balance between complexity and soundness of the results, the final analysis focuses on the sequence of images, \\((X^1, ..., X^T)\\) , corresponding to the area marked in red in Figure REF . The other three areas will be used to conform \\((Y^1, ..., Y^T)\\) , the sequence of images from which the distant tiles are sampled.\nWe get images of each season of the year during the last five years (2017-2021). Therefore, we use a total of \\(4 \\mbox{ (regions) } \\times 5 \\mbox{ (years) }\\times 4 \\mbox{ (seasons) } = 60\\)  Sentinel-2 images to train the embedding \\(f^g\\) .\n{FIGURE}", "We firstly train the geographic-based embedding, \\(f^g\\) , following the procedure proposed in Section REF  and the experiments carried out in [5] are taken into consideration to fix the values of the learning parameters. Thus, \\(N=100000\\)  triplets sampled from the Sentinel-2 images, 5000 triplets from each timestamp. The size of the tiles is \\(100\\times 100\\)  pixels (covering 1 km\\(^2\\) ), the geographical neighborhood is given by a ball of radius \\(r=50\\) , and the distant tile is always chosen from a different region with the same timestamp to amplify the differences between neighbors and distant tiles. The training process is iterated 50 epochs, with a batch size of 50 and a margin of \\(\\delta = 50\\) . The last layer of the network has \\(d=512\\)  features, which correspond to the number of dimensions of the embedding space for the tiles. For the clustering-based embedding \\(f^c\\) , we continue the learning of the model using the procedure described in Section REF , with \\(M=20000\\)  triplets taken from \\((X^1, ..., X^T)\\)  (the red region). The neighborhood is given by a partitional clustering of size \\(K=5\\) . The training process is iterated 25 epochs, with a batch size of 50 and a margin of \\(\\delta = 50\\) . We reduce the number of triplets and epochs due to the more specific nature of this refinement.\n"]}
{"id": "2211.07625", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["Replace or keep the classification head?\nAt the beginning of stage (b), we discard the rotation prediction classification head of stage (a) and replace it with a new 2-way linear classification head. We do it to minimize the effects of the classification head, as our goal is to focus on the previous structure of machines instead of this linear layer. We keep the last linear layer of stage (a) as a variant. The correlation between this variant and the default setting is very weak (\\(\\rho \\)  = \\(0.08\\) ).\n"]}
{"id": "2211.06007", "categories": "cs.LG cs.SD eess.AS stat.ML", "paragraphs": ["We closely follow slimIPL experiments\u00a0[6] with the same hyper-parameters, including transformer model, CTC loss and character tokens (see Appendix\u00a0).\nWe make only few changes: a) to speed up training and decrease memory usage we get rid of relative positional embedding\u00a0[27] and use CAPE\u00a0[28] instead; b) we train with larger batch per GPU but on 8 GPUs.\nIn our experiments we only vary \\(C\\) , \\(p\\)  and the type of PL (soft-labels vs hard-alignment), while keeping everything else fixed.\nWe run experiments with 3 seeds and we report mean and standard deviation as the final performance.\n"]}
{"id": "2203.05272", "categories": "cs.CV", "paragraphs": ["For the ConvNet baseline, we use the SGD optimizer to train for 600 epoch, with a weight decay of \\(0.001\\) . We set the initial learning rate to \\(0.01\\)  and use a momentum of \\(0.98\\)  with a decay rate of \\(0.1^{1/200}\\) . It roughly takes 24 hours to train on 4 Nividia v100 GPUs, and we does not observe obvious increase in training time after applying the CBL.\n"]}
{"id": "2204.02090", "categories": "cs.CV cs.IR cs.SD eess.AS", "paragraphs": ["We train our model for estimating the audio-visual correspondence score for a given audio-visual pair in an end-to-end manner. The positive examples correspond to the synchronised pairs in which the audio corresponds to the visual. The negative examples are obtained by introducing random temporal misalignment between the in-sync audio-visual pairs. This allows us to follow a self-supervised training pipeline. During the training, the positive and the negative examples are sampled equally. The audio-visual correspondence score needs to be maximised for the positive examples and minimised for the negative examples. The binary cross-entropy loss criterion is used to optimise the model parameters during the training. Unless stated otherwise, we train our model with audio-visual input corresponding to a sequence length of 5 visual frames (0.2s) sampled at 25 fps. We use this training setup in sections 4.1 and 4.2.\n"]}
{"id": "2206.06054", "categories": "cs.LG cs.SE", "paragraphs": ["For the COMPAS dataset, we trained a fully connected neural network and a decision tree classifier. The neural network is composed of 3 hidden layers of size 12, 9, and 9. We use the RMSprop algorithm for optimization. For decision-tree training, we set the \\(\\mathit {max\\_depth}\\)  parameter to 8. For training, we shuffle the data and use 67% of it.\n", "For the GermanCredit dataset, we trained a fully connected neural network and a decision tree classifier. The neural network is composed of 1 hidden layer of size 10, and we use the Adam optimizer. In decision-tree training, we set the \\(\\mathit {max\\_depth}\\)  parameter to 6. For training, we shuffle the data and use 67% of it.\n", "For the SpeechCommand dataset, we apply a number of pre-processing steps and infer a spectrogram image for each audio file. We use 80% of the spectogram inputs for training a convolutional neural network consisting of 2 convolutional layers with kernels (32x32x3) and (64x64x3), and a fully connected layer of size 128. We use dropout for regularization and Adam for optimization.\n", "The HotelReview dataset consists of over 515k reviews, and only ca. 85k of them are scored above 6 (out of 10)\u2014labeled as positive in our evaluation. We sample the same number of inputs from the ones that are labeled as negative to form a new dataset consisting of around 170k inputs. We use 90% of them as training set. We use the USE model from Tensorflow Hubhttps://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3. The USE-encoded reviews are used to train a fully connected neural network with 2 hidden layers (256 and 128 neurons, respectively). We use dropout for regularization and Adam for optimization.\n"]}
{"id": "2206.06100", "categories": "cs.CV cs.AI cs.GR cs.LG eess.IV", "paragraphs": ["We generated training data (i.e., pairs of all-in-focus and focused images with auxiliary information on aperture size \\(s\\)  and focus distance \\(f\\) ) using AR-NeRF, which was trained using \\(64 \\times 64\\)  images on the Oxford Flowers dataset.\nAR-NeRF was the same as that used to generate the samples in Figures\u00a0REF  and REF .\nWhen training AR-NeRF-R, we used \\(128 \\times 128\\)  images generated by AR-NeRF, where we increased the resolution of the generated images from \\(64 \\times 64\\)  to \\(128 \\times 128\\)  by increasing the density of input points (Section\u00a0REF ).\nWe trained the defocus renderer for 300,000 iterations using the Adam optimizer\u00a0[43] with a learning rate of 0.0003 and momentum terms \\(\\beta _1\\)  and \\(\\beta _2\\)  of 0.9 and 0.99, respectively.\nThe batch size was set to 4.\nThe learning rate was kept constant during training, except for the last \\(30\\%\\)  iterations, where the learning rate was smoothly ramped down to zero.\n"]}
{"id": "2210.01257", "categories": "cs.LG stat.ML", "paragraphs": ["For the experiments with ConvActuallys of varying depth, we train using SGD with momentum 0.9, batch size of 1028, learning rate starting at \\(10^{-4}\\)  and following a \u201creduce on plateau\u201d schedule with drops by a factor of \\(0.1\\)  after 50 epochs with no improvement, for a maximum of 500 epochs or until the learning rate hits \\(10^{-6}\\) . These parameters were chosen to avoid early stopping and allow SGD to run to convergence. We use weight decay of \\(10^{-9}\\)  (i.e., basically no decay). For the experiments training ResNets of varying depth, we train using SGD with momentum 0.9, batch size 32, initieal learning rate \\(32/512\\)  with drops by a factor of \\(0.1\\)  after 10 epochs with no improvement, for a maximum of 500 epochs or until the learning rate hits \\(10^{-6}\\)\n", "For the experiments with ConvActuallys trained with varying decay, we use the same optimizer, batch size and learning rate schedule, however we fix the depth to 4 and vary decay as indicated. For the experiments with MyrtleCNNs trained with varying decay, we use SGD with momentum 0.9, batch size of 512, initial learning rate \\(10^{-4}\\)  and a reduce on plateau schedule with drops by a factor of \\(0.1\\)  after 20 epochs with no improvement, for a maximum of 500 epochs or until the learning rate hits \\(10^{-6}\\) . Again, our motivation was to allow SGD to run to convergence. For the experiments with ResNets trained with varying decay, we use SGD with momentum 0.9, batch size of 512, initial learning rate \\(10^{-1}\\)  and a reduce on plateau schedule with drops by a factor of \\(0.1\\)  after 20 epochs with no improvement, for a maximum of 500 epochs or until the learning rate hits \\(10^{-6}\\) .\n"]}
{"id": "2212.00535", "categories": "cs.LG", "paragraphs": ["In node-subgraph and subgraph-subgraph contrasts, both GCN models have one layer and use ReLU as the activation function. The size of subgraphs in the network is set to 4. Both node and subgraph features are mapped to 64 dimensions in hidden space. Besides, we implement 400 epochs of model training and 256 rounds of anomaly score calculation.\n"]}
{"id": "2211.03970", "categories": "cs.LG math.OC", "paragraphs": ["For CLS training, we use a shallow neural network with one hidden fully-connected layer, which has 1024 neurons and an output of 3 classes. For the last layer, we use cross-entropy loss.\nFor optimizser, we use Adagrad with learning rate 0.001. For comparison, we also run Adam with learning rate \\(0.0001\\)  and \\(\\beta _1 = 0, \\beta _2 = 0.999\\) . A batch size of 3 is used in this task.\n", "Compared to CLS task, we only use 128 neurons for the hidden layer and MSE loss and batch size of 5 in REG task. In this case, we use Adam with learning rate 0.001 and \\(\\beta _1 = 0, \\beta _2 = 0.999\\) .\n", "For the Cifar10 classification task, we train the model using 1 GPU with batch size 32. Similar to the synthetic setting, For each setting, we run 20 trials and for each trial, we run 60 epochs to calculate means and standard deviations. In terms of weight initialization for convolution and fully-connected layers of VGG11, we use He initialization method (\u201cfan-out\u201d mode for convolution). For more details, please see the code attached to this supplemental material.\n"]}
{"id": "2208.11424", "categories": "cs.CV", "paragraphs": ["The database was trained using Stochastic Gradient Descent with a batch size of 128 and an initial learning rate of 0.001 and a momentum set to 0.9. These are the final hyperparameters we considered after several runs, with the goal of improving convergence speed. The model converges in about 28 hours of training on a NVIDIA Titan V GPU with 12GB memory. The margin \\(m\\)  of equation REF  is experimentally fixed to 1. The matching between \\(576\\times 720\\)  image pairs completes in less than 1 second. In our application context, the run-time is less important because panoramic image construction is typically done offline.\n"]}
{"id": "2203.05053", "categories": "cs.CV", "paragraphs": ["We ensure that all experiments on the same dataset have exactly the same setting except the label ratio \\(r\\)  for fair comparison. For each experiment, the labeled set is sampled uniformly. We experiment on all four datasets independently using label ratio \\(r\\in \\lbrace 0, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1\\rbrace \\)  with settings below.\n"]}
{"id": "2211.10872", "categories": "cs.CV", "paragraphs": ["In the first set of experiments where we showed the superiority of MetaMax over\nOpenMax, we trained DenseNet121 for 200 epochs with a batch size of 64 for all\nthree datasets. DenseNet121 has a total of 6,960,006 parameters in its 121\nlayers. Training was performed using the Adam [10]\noptimization method. The learning rate decayed by a multiplicative factor\n\\(\\gamma \\)  at the end of each epoch. In the second set of experiments where we\ndemonstrated the applicability of MetaMax on different classification networks,\nwe conducted experiments using ResNet and VGGNet following the same\nexperimental protocol.\n"]}
{"id": "2211.10812", "categories": "cs.CV cs.AI", "paragraphs": ["We used the CelebA-HQ dataset [19] for our\nexperiments. This dataset contains 30k celebrities' faces at megapixel\nresolutions (1024\\(\\times \\) 1024) and is challenging for face swapping.\nWe used a pretrained e4e encoder [43] to invert\n27176 images in the training and validation sets of CelebA-HQ to latent\ncodes, and trained our style extractor \\(h\\)  on these latent codes.\nWe trained \\(h\\)  for 10 epochs with a batch size of 4. We used an Adam\noptimizer with an initial learning rate of 1e-4, decayed to 1e-6 following\na cosine schedule over all steps. We set the loss coefficients (Eq.\u00a0REF )\nas follows: \\(\\lambda _{1}\\) =1, \\(\\lambda _{2}\\) =3.5, \\(\\lambda _{3}\\) =1,\nand \\(\\lambda _{4}\\) =0.1.\n"]}
{"id": "2211.14503", "categories": "cs.LG", "paragraphs": ["The input image used is \\(512 \\times 512\\) , mapped to an input domain \\([-1, 1]^2\\) . The sinusoidal network used is a 5-layer MLP with hidden size 256, following the proposed initialization scheme above. The parameter \\(\\omega \\)  is set to 32. The Adam optimizer is used with a learning rate of \\(3 \\cdot 10^{-3}\\) , trained for \\(10,000\\)  steps in the short duration training results and for \\(20,000\\)  steps in the long duration training results.\n", "The input image used is of size \\(256 \\times 256\\) , mapped from an input domain \\([-1, 1]^2\\) . The sinusoidal network used is a 5-layer MLP with hidden size 256, following the proposed initialization scheme above. For both experiments, the parameter \\(\\omega \\)  is set to 32 and the Adam optimizer is used. For the gradient experiments, in short and long training results, a learning rate of \\(1 \\cdot 10^{-4}\\)  is used, trained for \\(10,000\\)  and \\(20,000\\)  steps respectively. For the Laplace experiments, in short and long training results, a learning rate of \\(1 \\cdot 10^{-3}\\)  is used, trained for \\(10,000\\)  and \\(20,000\\)  steps respectively.\n", "The cat video contains 300 frames of size \\(512 \\times 512\\) . The bikes video contains 250 frames of size \\(272 \\times 640\\) . These signals are fitted from the input domain \\([-1, 1]^3\\) . The sinusoidal network used is a 5-layer MLP with hidden size 1024, following the proposed initialization scheme above. The parameter \\(\\omega \\)  is set to 8. The Adam optimizer is used, with a learning rate of \\(3 \\cdot 10^{-4}\\)  trained for \\(100,000\\)  steps in the short duration training results and for \\(200,000\\)  steps in the long duration training results.\n", "Both audios use a sampling rate of 44100Hz. The Bach audio is 7s long and the counting audio is approximately 12s long. These signals are fitted from the input domain \\([-1, 1]\\) . The sinusoidal network used is a 5-layer MLP with hidden size 256, following the proposed initialization scheme above. For short and long training results, training is performed for \\(5,000\\)  and \\(50,000\\)  steps respectively.\nFor the Bach experiment, the parameter \\(\\omega \\)  is set to \\(15,000\\) . The Adam optimizer is used, with a general learning rate of \\(3 \\cdot 10^{-3}\\) . A separate learning rate of \\(1 \\cdot 10^{-6}\\)  is used for the first layer to stabilize training due to the large \\(\\omega \\)  value.\nFor the counting experiment, the parameter \\(\\omega \\)  is set to \\(32,000\\) . The Adam optimizer is used, with a general learning rate of \\(1 \\cdot 10^{-3}\\)  and a first layer learning rate of \\(1 \\cdot 10^{-6}\\) .\n", "The sinusoidal network used is a 5-layer MLP with hidden size 256, following the proposed initialization scheme above. The parameter \\(\\omega \\)  is set to 16. The Adam optimizer is used, with a learning rate of \\(3 \\cdot 10^{-4}\\)  trained for \\(50,000\\)  steps.\n", "The statue point cloud contains \\(4,999,996\\)  points.\nThe room point cloud contains \\(10,250,688\\)  points.\nThese signals are fitted from the input domain \\([-1, 1]^3\\) .\nThe sinusoidal network used is a 5-layer MLP with hidden size 256 for the statue and 1024 for the room.\nThe parameter \\(\\omega \\)  is set to 4.\nThe Adam optimizer is used, with a learning rate of \\(8 \\cdot 10^{-4}\\)  and a batch size of 1400. All models are trained for \\(190,000\\)  steps for the statue experiment and for \\(410,000\\)  steps for the room experiment.\n{FIGURE}{FIGURE}"]}
{"id": "2211.14508", "categories": "cs.CL cs.AI", "paragraphs": ["The encoder is a 2-layer transformer with hidden size 512 and 8 attentions heads as in [5]. In the base model, We use 512-dimensional embeddings for \\(w_{i}\\) , \\(p_{i}\\) , and thus the dimension of \\(x_{i}\\)  is 1024. In the lexicon-injected parser, we concatenate another \\(q_{i}\\)  with the 128-dimensional embeddings as descirbed in section REF . We experimented with variants of input embeddings and were unaware of any significant difference in parsing performances. Overall, we performed five trials on each task with with multiple settings of random seeds, and averaged resulting performances.\n"]}
{"id": "2211.14540", "categories": "cs.CL", "paragraphs": ["We adopt the typical Transformer [46] as the model trained from scratch. We utilize a learning rate of 3e-4 and set the warming-up schedule with 4000 steps for training. We train our model for around 100 epochs. The optimization algorithm is Adam [17]. We set the maximum number of input tokens as 8192, which is the same as transformer-based baselines.\n"]}
{"id": "2205.05282", "categories": "cs.CV", "paragraphs": ["We follow [12] for the pre-training setup; an Adam optimizer with a learning rate of 0.001 is used. For miniImageNet, models are trained for 400 epochs with a batch size of 16, and for tieredImageNet, 90 epochs with a batch size of 256.\nWe also follow [12] for the fine-tuning setting; an SGD optimizer with a learning rate of 0.01, momentum of 0.9, and weight decay of \\(10^{-4}\\)  is used. The pre-trained model is trained for 100 epochs with a batch size of 4.\n"]}
{"id": "2204.08745", "categories": "cs.CV", "paragraphs": ["The experiments in this paper are deployed in paralel using 4 NVIDIA GeForce RTX 2080Ti GPUs. For all models, the batch size is set for 2 images for each GPUs. Open source MMDetection[5] toolbox is utilized for training of VFNet and TOOD architectures. While training of these architectures, default hyper-parameters and original augmentation strategies are utilized. YOLOR-P6 architecture is trained using the official GitHub repository [34], which is provided by the authors. In a similar manner, default settings are utilized for training of YOLOR-P6. All experiments are run for 100 epochs.\n"]}
{"id": "2208.09463", "categories": "cs.CV", "paragraphs": ["\nWe set the number of MPI planes \\(Z = 4\\)  due to memory constraints in our implementation.\nA higher value of \\(Z\\)  would likely improve the performance.\nAs argued in Sect. 5.8, with sparse convolutions, increasing \\(Z\\)  would have little impact on memory and speed.\n\nWe empirically find that \\(s_z = 1\\)  gives reasonable performance.\n\n\\(\\beta \\)  in Eq. 7 balances the MAE and SSIM loss and we set it to 0.15, as common in the literature\u00a0[19].\n\n\\(a\\)  in Eq. 10 controls the level of smoothness constraint on the estimated flow.\nWe set \\(a=10\\)  as suggested by Liu et al.\u00a0[32].\n\n\\(\\lambda \\)  in Eq. 11 balances the two loss terms.\nThrough a coarse grid search, we set \\(\\lambda = 10\\) .\n\nDuring inference, we run disocclusion infilling iteratively for \\(g=3\\)  following Kanchana et al.\u00a0[22].\n\n"]}
{"id": "2202.02543", "categories": "cs.CV cs.AI math.OC", "paragraphs": ["For all experiments, we apply our pretext task on the ModelNet40 [52] dataset to train the backbones. ModelNet40 contains 12,331 meshed models from 40 object categories, split into 9,843 training meshes and 2,468 testing meshes, where the points are sampled from CAD models. Following [37], each point cloud consists of 2,048 points by random sampling on the model surface from every model in ModelNet40. Our pre-training dataset is then constructed by using the training set of ModelNet40. The parameters of our projector \\(g(\\cdot )\\)  and predictor \\(q(\\cdot )\\)  are set as:\n"]}
{"id": "2209.02821", "categories": "cs.CL", "paragraphs": ["Our research was pursued in a resource-limited setting.\nFor training, we use 4 NVIDIA RTX A6000 GPUs (48GB vRAM each).\nFor inference, we use up to 16 NVIDIA GeForce RTX 2080 Ti GPUs (11GB vRAM each).\n"]}
{"id": "2205.10056", "categories": "cs.LG", "paragraphs": ["The encoder and decoder modules of AbsAE are implemented as a multilayer CNN architecture Additional details about the chosen architecture are contained in the Appendix. The hyperparameter have been tuned using trial and error, selecting the combination yielding the best performance on the validation set. All the networks used in the experiments are deterministic, i.e. \\(q_\\phi (z|x)\\)  and \\(p_\\theta (x|z)\\)  are Dirac's delta functions. The discriminator and the relational networks are implemented as a 3-layer MLP with 1024 units each. The hidden neurons use hyperbolic tangent non-linearities, while the output neurons use the sigmoid. In the experiments, we set the number of latent factors \\(N_z=8\\)  for HWF and dSprites, \\(N_z=16\\)  for Shapes3D. All tasks use a batch size of 1024 for the AbsAE's training and 128 for the ReL's training. We use the Adam optimizer with learning rate of \\(10^{-4}\\)  for HWF and dSprites, \\(10^{-5}\\)  for Shapes3D.\n", "Initially, the training starts in a warmup phase, were only the AbsAE is active. In this phase we set \\(p(z) \\sim \\mathrm {Uniform}(-1, 1)\\) , to encourage the latent codes to spread evenly across the latent space. During this phase only the AbsAE is trained. After 1000 epochs (5000 for Shapes3D), the full training phase begins: the prior distribution is changed to the GM prior \\(p(z) \\sim \\frac{1}{N}\\sum _{i=0}^N{\\mathcal {N}(\\mu _i, \\Sigma _i)}\\)  described in Section REF .\nIn this phase we also start the training of the ReL: the first step is to construct a training sample with the following structure:\n\\((z_{in_1}, ..., z_{in_R}, z_{rel}, z_{out})\\) \n"]}
{"id": "2205.10070", "categories": "cs.LG", "paragraphs": ["To analyze the effect of training data availability on prediction stability, we vary the number of node labels available for training between 1 and 60% of all nodes (steps: 0.01, 0.05, 0.1, 0.2, 0.4, 0.6) and use a fixed-size validation set of 15% of the data.\nWe sample the nodes of each class proportionally to their total class size, so that nodes of all classes are present in the test set.\nFurther, to avoid dependency on specific data splits, we repeat the experiment with 10 different data splits per graph.\nIn total, we train 42000 models for this experiment.\n", "We train the models with different optimizers: Adam, Stochastic Gradient Descent (SGD), and SGD with momentum (SGD-M), which we set to 0.9.\nWe show the results for GCN in fig:optim.\nFor prediction disagreement, SGD performs much worse than SGD-M and Adam.\nSGD-M performs on par with or better than Adam, with the exception of Pubmed.\nOverall, disagreement and error rate are correlated, but SGD-M optimization leads to lower disagreement.\nFurthermore, SGD-M decreases the average MAE between the output distributions of the models more than Adam.\nFor GAT, Adam and SGD-M perform similarly with a slight edge to Adam.\nSGD performs much worse with respect to both error rate and disagreement.\nBased on the discrepancy between GCN and GAT results, there does not appear to be a simple rule to select one of the tested optimizers to generally minimize prediction disagreement.\n", "We change the number of layers from 2 to 6.\nBetween every layer, there are dropout and activation functions, while otherwise following the previously used training procedure.\nfig:depth shows the results for GCN.\nPrediction disagreement increases with depth of the model.\nSimilarly, the error rate increases, which can be explained by a lack of training techniques for deep GNNs, e.g., residual connections or normalization.\nNevertheless, even when the model performance does not decrease much, prediction stability decreases, e.g., on the Physics dataset, the absolute disagreement increases almost four-fold.\nWe make the same observation for GAT, which suggests that the depth of a model negatively affects its prediction stability.\n"]}
{"id": "2205.09616", "categories": "cs.CV", "paragraphs": ["ConMIM pre-training is conducted on the training set of ImageNet-1K [6] dataset in a self-supervised manner. We utilize ViT-S/16 and ViT-B/16 [9] as the backbone networks. Following MoCo v3 [5], we use a 3-layer projection head on top of the backbone network for pre-training and discard it when transferring to downstream tasks.\nThe input images are all resized to \\(224\\times 224\\)  and the patch size is set to \\(16\\times 16\\) .\nWe follow the masking strategy of MAE [15], i.e., 75% patches are randomly masked.\nThe learning rate is set to 5e-4, with a warmup of 10 epochs, and cosine learning rate decay. The temperature \\(\\tau \\)  is set to 0.1 and the momentum coefficient \\(\\alpha \\)  is initially set to 0.996 with a cosine scheduler.\nViT-B/16 is pre-trained for 800 epochs in total and ViT-S/16 is pre-trained for 300 epochs if not specified.\nThe other hyper-parameters are mostly the same as BEiT [0].\nMore implementation details can be found in Appendix REF .\n"]}
{"id": "2202.11749", "categories": "cs.LG", "paragraphs": ["To ensure a fair computational budget across different network architectures, convergence criteria are set on CIFAR-10 and CIFAR-100 respectively so that all networks are trained until the training cross entropy loss falls below \\(0.19\\) , and \\(0.25\\) . For all networks, an initial learning rate of \\(0.1\\)  is decayed by a factor of \\(0.2\\)  every 150 epochs. When data augmentation is enabled, training images are randomly shifted by 4 pixels and randomly flipped horizontally. To avoid overfitting, all networks are trained with weight decay \\(5\\rm {e}-4\\) .\n"]}
{"id": "2212.02108", "categories": "cs.CL cs.LG", "paragraphs": ["All transformer models were fine-tuned using the original model hyperparameters in order to allow for better comparisons between their performances. Leveraging data parallelisation, the models were trained for up to 5 epochs with a per-device batch size of 8 on a node consisting of 16 NVIDIA Tesla K80 GPUs with 16GB of system memory per GPU, running on average for approximately eight hours per training. The small batch size was conditioned on the fact that model parallelisation was not possible within our infrastructure, leading to out-of-memory (OOM) if a larger batch size was chosen. The latter constraint also meant that most attempts to perform hyperparameter tuning led to overfitting. Saving the best model ensured that the best performance was then used for final evaluation and prediction. All post-deployment performance evaluations were carried out on a single node consisting of 8 NVIDIA Tesla V100 GPUs with 32GB of system memory per GPU and a per-device batch size of 32, taking on average approximately four hours per prediction task. All GPUs ran with cuda 11.2 and cudnn 8.1.0. Models were trained and evaluated using a stratified, random train-test split with an 80-20 ratio.\n"]}
{"id": "2205.10350", "categories": "cs.CL cs.LG", "paragraphs": ["For MT, we follow the setting of  to train the model on the WMT14 datasets. For En-De, the training set contains 4.5M parallel sentence pairs. We use newstest2013 as our dev set. For En-Fr, there are 36M parallel sentence pairs for training, and we use newstest2012+2013 as the dev set.\n", "For GEC, we use the public Lang-8\u00a0, NUCLE\u00a0, FCE\u00a0 and W&I+LOCNESS datasets\u00a0. After de-duplicating, we have around 900K sentence pairs for training. Both the dev (CoNLL-13) and test (CoNLL-14) have 1.3K sampales.\n"]}
{"id": "2211.05525", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["The implementation of our models is based on Pytorch\u00a0[23].\nUnless otherwise stated, we train the models with batch-size 128 for 400 epochs with an SGD-optimizer, a momentum set to \\(0.9\\)  and a weight decay of \\(10^{-4}\\) . In accordance to ResNet\u00a0[10] we use batch normalization followed by a ReLU activation function after every convolutional layer. The initial learning rate is set to \\(0.05\\)  and we use a cosine-annealing learning rate scheduler\u00a0[21] to adapt the learning rate during training.\n"]}
{"id": "2204.03905", "categories": "cs.CL", "paragraphs": ["We continuously pretrain both large and base versions of BART for 120k steps with a batch size of 2560. We use the same vocabulary as BART to tokenize the texts. Although the input length limitation of BART is 1024, the tokenized PubMed abstracts rarely exceed 512. Therefore, for the sake of training efficiency, we truncate all the input texts to 512 maximum length. We mask 30% of the input tokens and the masked span length is determined by sampling from a Poisson distribution (\\(\\lambda =3\\) ) as used in BART. We use a learning rate scheduler of 0.02 warm-up ratio and linear decay. The learning rate is set to 1e-4. We train the base version of BioBART on 2 DGX with 16 40GB A100 GPUs for about 100 hours and the large version of BioBART on the same devices for 168 hours with the help of the open-resource framework DeepSpeed [57].\n"]}
{"id": "2210.13591", "categories": "cs.CV", "paragraphs": ["We develop our projects upon VOLTA\n[3], which is built with PyTorch [37] and aims for speeding up multi-modal machine learning research by establishing baselines within a controlled setup, e.g. models trained on same amount of text-image pairs across different VLP models. The object detector uses ResNet-101 [15] as the backbone. We follow the U-VB architecture, where each Transformer layer has \\(M=12\\)  attention heads and the dimensionality of the hidden state is 768. The size of the pre-learned visual dictionary \\(C\\)  is chosen from \\(\\lbrace 1024, 1536, 3072\\rbrace \\) ; the number of WFH layers \\(J\\)  from \\(\\lbrace 1, 2, 3\\rbrace \\) ; \\(\\gamma \\)  in Eq.\u00a0(REF ) from \\(\\lbrace 8, 16, 32\\rbrace \\) . Throughout the experiments, the methods annotated with WFH are always trained with the attribute tokens added unless otherwise specified.\n"]}
{"id": "2210.13542", "categories": "cs.LG cs.AI cs.RO", "paragraphs": ["For SymPlan parameters, we use 150 hidden channels (or 150 trivial representations for SymPlan methods) to process the input map.\nWe use 100 hidden channels for Q-value for VIN (or 100 regular representations for SymVIN), and use 40 hidden channels for Q-value for GPPN and ConvGPPN (or 40 regular representations for SymGPPN on \\(15\\times 15\\) , and 20 for larger maps because of memory constraint).\n"]}
{"id": "2209.00945", "categories": "cs.LG", "paragraphs": ["In contrastive learning, four augmentations were used to generate positive views: TranslateX, PermutateX, Hue, and Jitter. Our pre-training data, ImageNet data was resized to match the spectrogram images. Pre-training was conducted using 40 epochs with a learning rate of \\(1e^{-6}\\)  and a batch size of 256. For MoCo hyperparameters, we used a reduced feature dimension of 64 and queue size 4,096 to decrease the computational load.\n", "In the fine-tuning step, we load the model weights pre-trained through contrastive learning and replace the last fully connected layer of ResNet18 with a randomly initialized layer. To preserve the pre-trained weights, we used a two-phase training strategy. In the first phase, we froze the pre-trained layers and only trained the last layer for 50 epochs, with a learning rate of 0.6, which decayed to 0.06 at the 40-th epoch. Next, the whole network is trained for 100 epochs with a learning rate of \\(1e^{-4}\\) . A batch size of 16 is used for all fine-tuning experiments.\n", "The implementation for RandInit-2D and ImageNet-Supervised follows exactly the same configurations as our system. The models are all based on ResNet18, trained using the SGD optimizer with a learning rate of \\(1e^{-4}\\) . Instead, RandInit-2D starts training from the random-initialized weights, and ImageNet-Supervised starts training from the weights pre-trained from ImageNet with labels. To load the weight, we used TorchVision\u2019s Multi-Weight Support API\u00a0[55] that helps directly load the pre-trained weights on ResNet18. RandInit-1D is implemented based on ConvLSTM\u00a0[52]. We used six convolutional blocks composed of convolutional, batch normalization, and ReLU activation layers, followed by a unidirectional LSTM layer composed of three hidden layers. The input and output sizes of the layers were slightly modified depending on the shape of the input data.\n", "To make a fair comparison between the baselines, batch size of 16 and training epochs of 150 was used in all experiments. The evaluations were conducted using the PyTorch framework\u00a0[53] for implementation and trained on the environment with eight NVIDIA TITAN Xp GPUs.\n"]}
{"id": "2204.01694", "categories": "cs.CV cs.LG", "paragraphs": ["Hyper parameters were tuned on the validation set and kept fixed for all methods. We use a resizing factor of \\(0.5\\)  and generate 3 `clicks' from the relevancy maps for the single image segmentation method. All other parameters were unchanged from the baseline implementation of Zabari\u00a0et al. [70].\n"]}
{"id": "2204.05525", "categories": "cs.CV", "paragraphs": ["Our implementation is based on MMSegmentation and Pytorch. We perform 80K iterations. The initial learning rate is 0.0003 and weight decay is 0.01. A poly learning rare scheduled with factor 1.0 is used. For full-resolution version, the training images are randomly scaling and then cropping to fixed size of \\(1024\\times 1024\\) . As for the half-resolution version, the training images are resized to \\(1024\\times 512\\)  and randomly scaling, the crop size is \\(1024\\times 512\\) . We follow the data augmentation strategy of Segformer for fair comparison.\n"]}
{"id": "2211.10193", "categories": "cs.LG", "paragraphs": ["We train the baseline model with \\(\\ell _{2}\\)  regularization of \\(10^{-4}\\)  for 200 epochs on CIFAR10/100, with a batch size of 128 using SGD with learning rate 0.1 and momentum 0.9. We decay the learning rate using cosine scheduling [34]. In ImageNet-ILSVRC2012, we train for 90 epochs in addition to a 5-epoch warm-up phase using a batch size of 256 with initial learning rate of 0.1 with \\(10\\times \\)  decay at 30%, 60%, and 90% of the training steps. On the input pipeline, we standardize the input to have values in the unit interval \\([0,\\,1]\\)  and use only right-left random flipping as data augmentation. We train on NVIDIA Tesla V100 GPUs for CIFAR10/100 and on TPUs for ImageNet-ILSVRC2012.\n", "In LATES, we flatten the intermediate activations and reduce dimensionality using average pooling. We place a probe after every convolutional block in ResNet and Wide-ResNet architectures, and after every convolutional layer in VGG16/19. This results in 8 probes in ResNet18, 16 probes in ResNet50, 12 probes in Wide-ResNet-28-10, 14 probes in VGG16, and 17 probes in VGG19. We train each probe with a learning rate of 0.01 and momentum 0.9 with \\(2\\times \\)  decay after every 10 epochs. All probes are trained together (in parallel) for 50 epochs, although we observe that they converge much faster. Finally, we train the aggregator model for 50 epochs with a fixed learning rate of 0.005, using the hold-out part of the dataset. We repeat all experiments five times and report averages.\n"]}
{"id": "2204.07341", "categories": "cs.CL", "paragraphs": ["We trained the models using Adam\u00a0[15] optimizer, with no warmup. We used a learning rate of \\(2.5\\times 10^{-4}\\)  which decayed to 0 at the end of training with a cosine schedule. On Wikitext-103, we trained the model with 250K steps using a batch size of 64. On enwik8 and text8, we trained the model with 100KWe used a smaller number of training steps compared to [7], since it would take too long to train one model. steps using a batch size of 40. We conducted our experiments on 2 Tesla V100.\n"]}
{"id": "2203.04221", "categories": "cs.CV", "paragraphs": ["In our experiment, we uniformly sampled 2 crops of \\(256\\times 256\\)  from each texture in a mini-batch of 8 distinct textures, to explicitly enforce the learning of intra-texture distribution on the discriminator network \\(D_{\\phi }\\) . The same trick can be applied to the generator network \\(G_{\\theta }\\)  by feeding multiple noise samples \\(\\mathbf {n}\\)  conditioned on the same latent variable \\(\\mathbf {z}\\) , but this resulted in a slower convergence of the generator and no significant performance gain. We adopted the Wasserstein distance in (REF ) and () as the losses with gradient penalty \\(=0.01\\)  to impose Lipschitz continuity, and the discriminator parameters \\(\\phi \\)  were updated twice, followed by one generator update (\\(3\\times 10^5\\)  generator iterations in total). We disabled the mixing regularization and the path-length regularization as they are time-consuming. For other hyperparameters, we followed the default protocols of StyleGAN-2, including latent space dimensionality \\(D=512\\) , learning rate \\(=0.002\\) , Adam optimizer, and exponential moving average of \\(G_{\\theta }\\) .\n", "Regarding the settings of TB, each module has \\(P=16\\)  learnable texton vectors. We applied the TB module to all layers of spatial resolution up to \\(64\\times 64\\)  to prevent high-frequency artifacts. The spatial size of the bottom TB module was set to \\(4\\times 4\\)  during training, and the number of channels of each texton vector was fixed at C\\(=512\\) . The spatial size as well as channel size of all remaining TB modules were designed to match the feature maps of their preceding blocks of Styled Conv as shown in Fig.\u00a0REF . At test time, the final output image resolution can be varied by simply modifying the spatial size of the bottom module.\n"]}
{"id": "2206.05790", "categories": "cs.CL", "paragraphs": ["All methods start with the same seed utterances, from which data augmentation proceeds in three steps. First, the augmentation method is used to generate 3 candidate utterances at a time, to allow for the different methods to cover their own hyper-parameters. For example, EDA allows for insertions, deletions, or swaps. So one of each augmentation type is generated in the candidate set. Second, we pass the candidates to a diversity ranker which calculates the BLEU score of the set of utterances if we were to add the candidate. The candidate which results in the lowest BLEU score (and thus highest diversity) is kept for consideration. In the final step, the candidate is compared as an exact match against the seed data and previously added augmentations. If the candidate is unique, then it is added to the final pool of augmentations. If the candidate is a repeat of a previous augmentation, then we retry the augmentation process. If 10 retries are accumulated for a given data augmentation method, the generation process terminates. This explains why certain methods (e.g. kNN) contains much fewer augmentation examples than others.\n", "Each method trained as a fine-tuned RoBERTa-base classifier. The models are trained for up to 14 epochs with early stopping if there was no improvement for 5 consecutive epochs. The hyperparameters we tune include learning rate, dropout rate and occasionally temperature. The batch size was kept constant at 16. We found learning rates between 1e-5 and 3e-4 to work well across methods. Dropout rate was tested among [0.0, 0.05, 0.1]. Each method received the same amount of tuning (6 rounds) to ensure fairness across methods. Each round of training took roughly 15-20 minutes on a Nvidia Tesla-V100 GPU, which was used for all experiments. This was accessed through Amazon as AWS EC2 instances.\n"]}
{"id": "2209.05406", "categories": "cs.LG", "paragraphs": ["For both METR-LA and PEMS-BAY, we used the same training strategy for the simplicity. \\(k=4\\)  spatial-temporal layers were used for the encoder, and \\(d_c=32\\) , \\(n_c=16\\) , and \\(d_e=16\\)  were set for the quantization branch in the decoder.\nSince the baselines predict the next \\(T=12\\)  steps in units of 5 minutes and each step has errors of 12 horizons, the input residuals and predictions have a size of \\(12 \\times 12\\)  and \\(12 \\times 1\\) , respectively.\nOur ResCAL is trained with the mean absolute error (MAE) and a batch size of 256. An Adam optimizer with a learning rate of 0.001, \\(\\beta _{1} =0.9\\)  and \\(\\beta _{2} =0.999\\)  is also used.\nEach dataset is split into a training set, validation set, and test set with a ratio of 7:1:2 and the model with the best validation score is selected in all experimental evaluations.\n"]}
{"id": "2205.10920", "categories": "cs.LG", "paragraphs": ["For the sake of simplicity, we consider a total of 20 clients with a participation ratio of 1.0 for the (personalized) FL training process, and train for 100 and 300 communication rounds for CIFAR10 and ImageNet-32 (i.e. image resolution of 32), respectively.\nWe decouple the local training and personalization phases for every communication round to better understand their impact on personalized FL performance.\nMore precisely, a local 5-epoch training is performed on the received global model; the local personalization phase again considers the same received global model, and we use 1 epoch in our cases.\nOnly the parameters from local training phase will be sent to server and aggregated for global model.\nWe also investigate the impact of different local personalized training epochs in Appendix\u00a0REF  and find that 1 personalization epoch is a good trade-off point for time complexity and performance.\n", "[leftmargin=12pt]\nFor head ensemble phase (personalization phase) of FedTHE and FedTHE+, we optimize the head ensemble weight \\(e\\)  by using a Adam optimizer with initial learning rate 0.1 (when training CNN or ResNet20-GN) or 0.01 (when training CCT4), and 20 optimization steps.\nThe head ensemble weight \\(e\\)  is always initialized as 0.5 for each test sample and we use \\(\\alpha =0.1\\)  and \\(\\beta =0.3\\)  for feature alignment phase.\nThese configurations are kept constant throughout all the experiments (different neural architectures, datasets).\n\nWe further list configurations and hyperparameters used for local training and local personalization. Note that our methods FedTHE and FedTHE+ also rely on them to train feature extractor, global head, and personalized head.\nFor training CNN on CIFAR10, we use SGD optimizer with initial learning rate 0.01.\nWe set weight decay to 5e-4, except for Ditto we use zero weight decay as it has already included regularization constraints.\nFor training ResNet20-GN\u00a0[30] on both CIFAR10 and ImageNet-32, we use similar settings as training CNN on CIFAR10, except we use SGDm optimizer with momentum factor of \\(0.9\\) .\nWe set the number of group to 2 for GroupNorm[97] in ResNet20-GN, as suggested in\u00a0[37].\nFor training CCT4\u00a0[28] on CIFAR10, we use Adam\u00a0[46] optimizer with initial learning rate 0.001 and default coefficients (i.e. 0.9, 0.999 for first and second moments respectively).\n\n"]}
{"id": "2207.05333", "categories": "cs.CV cs.LG", "paragraphs": ["Tag List Construction. We build the tag list based on OpenImages\u00a0(V6)\u00a0[23] to extract image tags from the co-occurrent texts. OpenImages is a large-scale visual database that includes 9,600 trainable image-level classes\u00a0(consisting of objects, scenes, attributes, actions, etc.) and 600 boxable classes\u00a0(only consisting of objects). We use the complete list of 9,600 classes to extract tags from captions and select the category tags with more than 5 samples. We also remove top most frequent tags since they can degrade the model performance even if they are correctly identified\u00a0(see Section\u00a0REF ). The final tag list consists of 1,000/2,500 classes for COCO&VG/CC-3M. An overview of the most frequent classes in COCO&VG is shown in Figure\u00a0REF .\n", "Implementation Details. We adopt ViT-B/16\u00a0[10] pre-trained on ImageNet\u00a0[8] with 85.5M parameters as the image encoder and BERT\\(_{base}\\) \u00a0[9] with 123.7M parameters as the text encoder. We pre-train the model for 30 epochs by default with the batch size of 1,024 on 8 NVIDIA V100 GPUs. The input images are uniformly resized to \\(256\\times 256\\) , and the maximum length of the text is padded to 40. The optimizer is the AdamW with a weight decay of 0.02, and cosine schedule\u00a0[32] is used with the max learning rate of 1e-4. Following [26], we also apply RandAugment\u00a0[7] but remove color changes, since text and tags usually contain color information. The threshold \\(\\tau \\)  is set to 0.6, and the changing epoch is set to 1st in SPLC.\n{FIGURE}{FIGURE}"]}
{"id": "2207.05322", "categories": "cs.LG stat.AP", "paragraphs": ["For EBMs, we use hyperparameters outer_bags=25, inner_bags=25, min_samples_leaf=25, interactions=20. For XGBoost we use eta=0.04, subsample=0.7, and max_depth=5. Random forests are trained with n_estimators=1000, min_samples_split=60, and min_samples_leaf=40. Lastly, the DNN is an MLP with 7 hidden layers of 200 neurons each. The hyperparameters of all models were each determined using 5-fold Cross-Validation to maximize AUROCs while ensuring good calibration and using heuristic methods to minimize overfitting; all parameters not mentioned are common defaults. We use 5-fold CV for all models to make computations feasible.\n"]}
{"id": "2205.13769", "categories": "cs.CV", "paragraphs": ["Pre-training Dataset. We leverage image-label pairs from the existing Inria building labeling dataset [33], which provides 180 labeled RS images, each size of \\(5000\\times 5000\\)  and spatial resolution of 0.3 m. The data is labeled into the building and non-building classes. We cut the original samples into small patches of size \\(256\\times 256\\)  with no overlap and remove patches not containing building regions. The resulting dataset contains more than 45k patch samples. We randomly split it into training (80%) and validation sets (20%). We additionally obtain the co-registered image patch of the corresponding geospatial region via Google Earth as the temporal augmentation (\\(t_{2}\\) ) for each image patch (\\(t_{1}\\) ) in the dataset.\nFor a fair comparison, the natural augmentations (bitemporal images) are used to train some other related pre-training methods that learn season invariant features. Please note that the semantic mask may not perfectly match the image of \\(t_{2}\\)  due to land-cover changes (e.g., building construction and demolition) over time, we do not include the augmented temporal image in our pre-training framework.\n"]}
{"id": "2207.06000", "categories": "cs.CL cs.LG cs.SD eess.AS", "paragraphs": ["Phoneme sequences are used as the input, which are obtained from text-normalization and grapheme-to-phoneme converter. The recordings are down-sampled from 48 kHz to 16kHz. We extracted a spectrogram with FFT size of 1024, window size of 50ms, and hop size of 12.5ms. Then, we converted it to a mel-spectrogram with 120 frequency bins. We used Hifi-GAN [25] as the vocoder to convert mel-spectrogram into waveform.\n"]}
{"id": "2202.07880", "categories": "cs.CL", "paragraphs": ["We use the exact same hyperparameters as in\u00a0[12], following\u00a0[9], with one major exception: we use a learning rate of 1e-4 instead of 1e-3, which we found to converge too quickly.\n"]}
{"id": "2208.08570", "categories": "cs.CV", "paragraphs": ["We adopt the images/labels taken from front middle-range camera from construction site \\(\\mathcal {N}\\)  as our target data. We split \\(80\\%\\)  of them as the training set, and \\(20\\%\\)  of them as validation set. Our baseline training configuration is set to use batch size 16, 300 epochs, the Adam optimizer\u00a0[14], cosine learning rate decay with initial learning rate \\(10^{-3}\\) , weak augmentation by random horizontal flip, and 10 epochs for warm up training from learning rate \\(10^{-6}\\) . The input image to the model is resized to \\(512\\times 512\\)  pixels while the ratio of original image height and width are kept the same. Scaled YOLOv4-p5, i.e., a three-head scaled YOLOv4, and Faster R-CNN with backbone ResNet-50 are selected as baseline models for one and two stage detectors, respectively.\n"]}
{"id": "2206.04192", "categories": "cs.LG cs.AI", "paragraphs": ["Each model was trained and evaluated on one of 4 GeForce RTX 2080 GPU of our internal cluster. Specifically, the training processes used the Adam optimizer [12] to optimize the self-adversarial negative sampling loss [24]. We performed hyperparameter tuning over the learning rate \\(\\lambda \\) , embedding dimensionality \\(d\\) , number of negative samples \\(\\mathit {neg}\\) , loss margin \\(\\gamma \\) , adversarial temperature \\(\\alpha \\)  and minimal denominator \\(D_{\\mathit {min}}\\) . Specifically, two mechanisms were employed to implicitly regularize hyper-parallelogram: (1) the hyperbolic tangent function \\(\\mathit {tanh}\\)  was element-wise applied to each entity embedding \\(\\mathbf {e_p}\\) , slope vector \\(\\mathbf {r_i^p}\\) , and center vector \\(\\mathbf {c_i^p}\\) , projecting them into the bounded space \\([-1, 1]^d\\) , and (2) the diagonals' sizes of each hyper-parallelogram are limited by the novel \\(D_{\\mathit {min}}\\)  parameter. In the following, we will briefly introduce the \\(D_{\\mathit {min}}\\)  parameter and its function in ExpressivE.\n"]}
{"id": "2210.09338", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["Data.\u00a0\u00a0\nFor the text data, we use BookCorpus [54], a general-domain corpus widely used in LM pretraining (e.g., BERT, RoBERTa). It has 6GB of text from online books.\nFor the KG data, we use ConceptNet [6], a general-domain knowledge graph designed to capture background commonsense knowledge. It has 800K nodes and 2M edges in total.\nTo create a training instance, we sample a text segment of length up to 512 tokens from the text corpus, then retrieve a relevant KG subgraph of size up to 200 nodes (details in Appendix REF ), by which we obtain an aligned (text, local KG) pair.\n", "Implementation.\u00a0\u00a0\nFor our encoder (\u00a7REF ), we use the exact same architecture as GreaseLM [8] (19 LM layers followed by 5 text-KG fusion layers; 360M parameters in total). As done by [8], we initialize parameters in the LM component with the RoBERTa-Large release [17] and initialize the KG node embeddings with pre-computed ConceptNet entity embeddings (details in Appendix REF ).\nFor the link prediction objective (\u00a7REF , Equation REF ), we use DistMult [45] for KG triplet scoring, with a negative exampling of 128 triplets and a margin of \\(\\gamma =0\\) .\nTo pretrain the model, we perform MLM with a token masking rate of 15% and link prediction with an edge drop rate of 15%.\nWe pretrain for 20,000 steps with a batch size of 8,192 and a learning rate of 2e-5 for parameters in the LM component and 3e-4 for the others.\nTraining took 7 days on eight A100 GPUs using FP16.\nAdditional details on the hyperparameters can be found in Appendix REF .\n"]}
{"id": "2204.00655", "categories": "cs.CV", "paragraphs": ["The specific hyper parameters used for training are as follows: image input size \\(416 \\times 416\\) , batch size of 16, 1000 epochs in length, learning rate 0.01, momentum 0.937, weight decay 0.0005, with a warm-up time of 3 epochs and a warm-up momentum of 0.8. In addition to Mosaic data augmentation, random translations of 0.1 and random scaling at 0.5 are implemented.\n"]}
{"id": "2201.08663", "categories": "cs.CV cs.LG cs.MS cs.NA math.NA", "paragraphs": ["Suggested by\u00a0[37], we truncate the Taylor polynomial to degree 20 for SVD-Taylor. To make Pad\u00e9 approximant match the same degree with Taylor polynomial, we set the degree of both numerator and denominator to 10 for SVD-Pad\u00e9. For SVD-PI, the iteration times are also set as 20. For the NS iteration, according to the setting in\u00a0[24], [14], we set the iteration times to 5. The other experimental settings follow the implementation in\u00a0[39]. We use the workstation equipped with a Tesla K40 GPU and a 6-core Intel(R) Xeon(R) GPU @ 2.20GHz for training.\n", "We use 8 Tesla G40 GPUs for distributed training and the NVIDIA Apex mixed-precision trainer is used. Except that the spectral layer uses the single-precision (i.e., float32), other layers use the half-precision (i.e., float16) to accelerate the training. Other implementation details follow the experimental setting of the original So-ViT\u00a0[41].\n", "Following the experiment of covariance pooling for CNNs\u00a0[32], the degrees of Taylor polynomial are truncated to 100 for SVD-Taylor, and the degree of both the numerator and denominator of Pad\u00e9 approximants are set to 50 for SVD-Pad\u00e9. The iteration times of SVD-PI are set to 100. In the experiment of covariance pooling, more terms of the Taylor series are used because the covariance pooling meta-layer requires more accurate gradient estimation\u00a0[32].\n", "For the SVD-based methods, usually the double-precision is required to ensure an effective numerical representation of the eigenvalues. Using a lower precision would make the model fail to converge at the beginning of the training\u00a0[32]. To resolve this issue, we first apply the NS iteration to train the network for 50 epochs, then switch to the corresponding SVD method and continue the training till the end. This hybrid approach can avoid the non-convergence of the SVD methods at the beginning of the training phase.\n"]}
{"id": "2206.10298", "categories": "cs.CL cs.SI", "paragraphs": ["We use Class-Balanced Focal Loss [5] as the loss function to aid in handling the class imbalance (as half of the dataset has zero retweets).\nThis calculates loss with an additional class balancing factor, which is especially useful for long-tailed datasets such as ours [11]. We apply AdamW [16] as the optimization algorithm to update model parameters and to allow for better generalisation. The numerical features chosen for all models are hashtags, mentions, followers, following, verified status, and text length. `Hashtags' and `mentions' are the amount of these elements in the tweet text, so is a feature derived from tweet text. In addition, the sentiment of tweet text is added as a feature for baselines. Tweet text embeddings are not used as none of the literature we review utilises them. Note that all of these features are collected when the tweet is collected and would stay consistent. With the exception of ViralBERT, a min-max scalar is used for the numerical features as there is a large variation in the values (e.g. hashtags being much lower than follower counts). The ratio for training, validation and test sets is 80:10:10, with data being split randomly into these subsets. A data loader class with a batch size of 32 is used, and models are trained until convergence on the validation set.\n"]}
{"id": "2210.11892", "categories": "cs.CL cs.IR", "paragraphs": ["Our model is trained over this large dataset for one epoch, in batches of 64 pairs, using the InfoNCE loss [13]. In Appendix , additional details are presented for readers interested in faithfully reproducing our experiments.\n"]}
{"id": "2202.08494", "categories": "cs.LG", "paragraphs": ["We look at the non-linear pendulum described in\u00a0Eq.\u00a0REF . Here, \\(\\theta \\)  is the initial condition representing the position of the pendulum in time. The phase portrait of this system (representing the true solution trajectories), showing \\(\\frac{\\mathrm {d} \\theta }{\\mathrm {d}t}\\)  against \\(\\theta \\) , is shown in\u00a0Fig.\u00a0REF subfig:extrapolatetrue.\nAn Euler-Net and an RK4-Net are trained on trajectories, spaced apart by \\(\\Delta t = 0.1\\) , starting at certain initial conditions (shown by the black lines in\u00a0Fig.\u00a0REF subfig:extrapolatetraining). We then pick a test set of a number of different initial conditions that were not in the training data. The Euler-Net and RK4-Net start at these test initial conditions and are both evaluated at a finer \\(h = 0.001\\) , representing a \\(100\\times \\)  increase in resolution. Note that we saw in\u00a0Fig.\u00a0REF  that the Euler-Net did not pass the convergence test (i.e., it had high error when evaluated at \\(h \\ll \\Delta t\\) ) while the RK4-Net did pass the test.\n"]}
{"id": "2207.09840", "categories": "cs.CV", "paragraphs": ["The optimizer of the generator the discriminator is Adam [21] with \\(\\beta _1=0.5\\)  = 0.5 and \\(\\beta _2 = 0.999\\) . The learning rate is initially 2e-4 and decreases to 1e-5 by cosine annealing decay. The model are trained for 50 epochs with batch size 1. We extract features from \\(relu\\_4\\_1\\)  layer of pretrained VGG-16 to calculate the perceptual loss. The trade-off parameters in the loss function are set as \\(\\lambda _{adv}=1\\) , \\(\\lambda _{cyc}=10\\) , \\(\\lambda _{per}=0.005\\) , \\(\\lambda _{make}=1\\) .\n"]}
{"id": "2209.09861", "categories": "cs.LG", "paragraphs": ["Boosted tree ensembles are commonly used for win probability prediction due to their ease-of-use, strong performance, and built-in feature importance calculations. For vector representations of \\(g_t\\) , we consider XGBoost\u00a0[6], LightGBM\u00a0[13], and a multilayer perceptron (MLP) as candidate models. For set representations of \\(g_t\\) , we consider Deep Sets\u00a0[40] and Set Transformers\u00a0[15]. For each parsed demo, we randomly sample one game state from each round. Then, we randomly select 70% of these game states for the train set, 10% for the validation set, and the remaining 20% for the test set. The label for each game state is determined by the outcome of the round in which the game state took place (i.e., \\(Y_i = 1\\)  if the CT side won, 0 otherwise). All benchmarks are available in a Google Colab notebook and require a high-RAM instance with a GPU.\n"]}
{"id": "2204.00987", "categories": "cs.CV", "paragraphs": ["BinsFormer is implemented with the Pytorch\u00a0[31] framework. We train the entire model with the batch size 16, learning rate \\(1e-4\\)  for 38.4k iterations on a single node with 8 NVIDIA V100 32GB GPUs, which takes around 5 hours. We utilize the AdamW optimizer\u00a0[23] with (\\(\\beta _1\\) , \\(\\beta _2\\) , \\(wd\\) ) = (0.9, 0.999, 0.01), where \\(wd\\)  is the weight decay. The linear learning rate warm-up strategy is applied for the first 30% iterations. The cosine annealing learning rate strategy is adopted for the learning rate decay. For the NYU-Depth-v2 dataset, we utilize the official 25 classes divided by folder names for the auxiliary scene understanding task. For KITTI, since the outdoor dataset is tough to classify, we omit the scene classification loss and only use ground truth depth to provide supervision.\n"]}
{"id": "2204.01227", "categories": "cs.CL", "paragraphs": ["For the training details of PG and T5, we does not apply KL annealing and the coefficient of the KL divergence is always 1.\nWe use Adam optimizer [10] with learning rate of 0.0001, and adopt early stopping if the validation loss does not decrease after 10 epochs.\nFor the hyper-parameters \\(\\lbrace v, r\\rbrace \\)  of the kernel function in eq:gp-ours, we try a range of values where \\(v \\in [0.01, 100]\\)  and \\(r \\in [0.0001, 10]\\) , and do grid search cross validation on the validation set to select the best model.\nAll experiments are independently conducted on a GPU server (RTX 2090 Ti) with 40cores CPU and 256GB Memory.\n"]}
{"id": "2207.12958", "categories": "cs.LG", "paragraphs": ["The \\(cnn_{explain}\\)  model uses a total of 6,708,450 trainable parameters. We have conducted experiments with 20 training epochs per iteration with a batch size of 128, and have trained the model using the Adam optimizer. Adam optimizer is known for its best performance for CNN. The learning rate to train the \\(cnn_{explain}\\)  is kept at 0.001. The activation function considered for all the layers is ReLu with the Softmax at the output layer. An early stopping mechanism is used to avoid overfitting and accelerate training, if accuracy does not increases in 5 epochs.\n"]}
{"id": "2201.05891", "categories": "cs.CL", "paragraphs": ["With each parser, we trained each of four conditions (UnconvertedThe half of the training data from the augment corpus gets added to the base corpus without any conversions., Converted-Lexical, Converted-GloVe, and Converted-BERT) on each of five training data amounts (250, 500, 1000, 2000, and 4000 sentences).\nFor each training data amount, we trained three times using three different samples from the original training data.\nWe then tested each of these models on the same original GUM corpus test partition.\n"]}
{"id": "2201.05984", "categories": "cs.CL", "paragraphs": ["For PR, we use 8 Nvidia-A100 GPU with batch size set at 64 per GPU and the Adam optimizer with a learning rate of \\(5e\\text{-}6\\)  for training. The max sequence length is 512 and the number of epochs is 3.\nIn the multi-task training, we increase the number of epoch to 10.\nFor EASI, we also use the same training configuration as in PR, but use 20 epochs in the training.\nBoth components use RoBERTa-base Transformer models.\nHowever, for PR, we use the public transferred model from\u00a0[6]: TANDA-RoBERTa-base-ASNQ.\n"]}
{"id": "2210.04782", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["For our experiments with Robust Contrasting Pretraining (sec:contast) and variants we use the following hyperparameters and setup. We train on 4 Nvidea V100 GPUs, with a per-gpu batch size of 8 sentences with a maximum sequence length of 128 tokens, and 64 gradient accumulation steps, for an overall batch size of \\(64\\times 8\\times 4=2048\\)  sentences. We use a masked language modeling mask probability of \\(15\\%\\)  and a learning rate of 1e-4 with the Adam optimizer [18], and used 16-bit floating point operations. See below for the arguments of the Huggingface transformers [33] masked language modelling script which we modifiedhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n"]}
{"id": "2210.04888", "categories": "cs.CV", "paragraphs": ["Hyperparameters. We use Adam optimizer\u00a0[26] for the optimization of both generator and discriminator. The learning rate for generator is \\(2\\times 10^{-5}\\) . The learning rate for discriminator is \\(2\\times 10^{-4}\\) . The loss weights as set empirically as \\(\\lambda _{\\text{off}} = 1.5\\)  and \\(\\lambda _{\\text{eik}} = 0.5\\) . For one ray, 28 query points are sampled. For the pose-guided sample, we choose to use \\(\\sigma _\\theta = 15^{\\circ }\\) .\n", "R1 Scheduler. R1 regularization is used during training to penalize gradients of discriminator. Because it is highly challenging for the generator to learn plausible human appearance, the discriminator tends to overfit quickly if low R1 is set. But too high of R1 value would harm the final generation quality. Therefore, we set a R1 scheduler empirically, where R1 decrease from 300 to 18.5. R1 is cut in half every 50,000 iterations.\n"]}
{"id": "2209.03456", "categories": "cs.CV", "paragraphs": ["For selecting the frontal and profile pairs, we apply [32] to the datasets to create frontal and profile subsets based on the yaw angle. Then, face images with an absolute yaw value less than 15\\(^\\circ \\)  are considered frontal. For training Eq.\u00a0REF , the initial learning rate is set to 0.001 and is multiplied by 0.1 every ten epochs, and weight decay and momentum are 0.00001 and 0.9, respectively. The model is trained for 20 epochs. During the training, \\(\\lambda _1=0.1\\)  and \\(\\lambda _2=1.0\\) , and the number of negative samples is 6,000.\nWe adopt ResNet50 [6] as the encoder networks for the profile and frontal views. The final feature is of size 512\\(\\times \\) 7\\(\\times \\) 7. Feature maps are reshaped to form a vector of size 25,088 and passed to a fully-connected layer of size 512 to construct the final representation. The last feature vectors of encoders are enqueued to the frontal and profile memory buffer, see Fig.\u00a0REF . The view discriminator is an MLP with two hidden layers of size 256, each followed by batch normalization and leaky relu activation function. At the top of these hidden layers, there is a single neuron with a sigmoid activation function.\nThe model is trained using Stochastic Gradient Descent (SGD) with a mini-batch size of 32 on an NVIDIA TITAN X GPU using Pytorch [30].\n"]}
{"id": "2203.00379", "categories": "cs.CV", "paragraphs": ["We build our model using PyTorch by [29] and PyTorch Lightning by [13]. According to the multi-spectral Sentinel-2 data, the number of input channels is set to \\(n_\\textrm {in} = 10\\) . Further, we choose the number of activation map channels to be \\(n_\\textrm {m} = 3\\)  and discuss the reasons for that in Section\u00a0. Overall, our modified U-Net has about 1.8 million parameters. The classifier has about 200,000 parameters and is significantly smaller than the U-Net.\n"]}
{"id": "2210.03558", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["The CAE takes in input an image with size \\((H, W, C)\\)  \\(= (256, 256, 3)\\)  and passes it to the encoder.\nThe encoder is composed by 5 convolutional layers and 5 max-pooling layers for downsampling, that return a tensor of size \\((4, 4, 64)\\) .\n", "The downsampled tensor is then passed to the layers that define the latent space. The first one is a flatten layer that transforms the three-dimensional tensor into a vector \\({\\bf x}_{\\rm enc}\\)  with 1024 entries. The encoded vector is fed to two fully connected layers, each of them reducing the length of the vector by a factor of 8, so that the vector eventually matches the size of the latent space with 16 entries. In reverse, another pair of fully connected layers increases the size of the vector, each by a factor of 8, yielding the vector \\({\\bf x}_{\\rm lat}\\)  matching the original size (i.e., 1024 entries).\nThe vector is then passed to a view layer that reshapes it into a three-dimensional tensor of size \\((4, 4, 64)\\)  and then fed to the decoder, which consists of 5 convolutional and of 5 upsampling layers. The output \\({\\bf x}_{\\rm out}\\)  with size \\((H, W, C) = (256, 256, 3)\\)  is finally generated.\n", "The CVAE decoder has the same architecture as for the CAE model, and starting from \\(\\mathbf {x}_{\\rm lat}\\)  reconstructs the new image \\(\\mathbf {x}_{\\rm out}\\)  with size \\((H, W, C) = (256, 256, 3)\\) .\n", "The input image, with size \\((H, W, C)\\)  \\(= (256, 256, 3)\\) , is passed to the encoder which is composed by 3 convolutional layers without any max-pooling layer for downsampling. A tensor of size \\((4, 4, 64)\\)  is thus produced. Each thread of size \\((1,1,64)\\)  of the tensor is replaced by the closest vector of the codebook for quantization. The codebook size is \\((K, \\tilde{C}) = (512, 64)\\) , meaning that the discrete space of the codebook is made of 512 vectors, each with 64 entries.\n"]}
{"id": "2210.03586", "categories": "cs.CV cs.LG", "paragraphs": ["We use the ResNet-18 as the encoder (dimension of encoding is 512.), a two layer MLP with ReLU and BN appended as the projector (dimension of the hidden layer and embedding are 1024 and 64 respectively). The model is trained on CIFAR-10 for 200 epochs with a batch size of 256, using Adam optimizer\u00a0[29] with a learning rate of \\(3\\times 10^{-3}\\) , and learning rate warm-up for the first 500 iterations and a 0.2 learning rate drop at the last 50 and 25 epochs. The weight decay is set as \\(10^{-6}\\) . All transformations are performed with 2 positives extracted per image with standard data argumentation (see Section\u00a0REF  for details). We use the same evaluation protocol as in W-MSE\u00a0[12].\n"]}
{"id": "2208.03345", "categories": "cs.LG cs.GR", "paragraphs": ["Vortex is a simulation of vortex structures with spatial resolution 128\\(\\times \\) 128\\(\\times \\) 128 across 30 time steps. We used the vorticity magnitude scalar field for experiments. We randomly sampled 1000 data blocks from 5 time steps as the training data.\nNyx is a cosmological simulation produced by Lawrence Berkeley National Laboratory. We used the log density field with resolution 256\\(\\times \\) 256\\(\\times \\) 256. 5000 data blocks from 5 ensemble members were randomly sampled for training.\nHurricane Isabel is a simulation of Hurricane Isabel, produced by the Weather Research and Forecast (WRF) model, courtesy of NCAR and the U.S. National Science Foundation (NSF). The data were sliced along the z dimension to remove the special value \\(1 \\times 10^{35}\\)  representing \"no data\" (the land region). In our experiment, the resolution of data is 512\\(\\times \\) 512\\(\\times \\) 96 with 48 time steps. We chose the pressure field for evaluation. Training data contain 5500 data blocks from 5 time steps.\n"]}
{"id": "2208.03306", "categories": "cs.CL", "paragraphs": ["To disentangle variations in GPU speed, we use number of updates as our computational budget in these experiments. We choose the number of updates in our budget so that training completes in roughly 48 wall-clock hours: 80k, 32k, 24k, and 12k updates for the 125M, 350M, 750, 1.3B parameter per GPU models, respectively. We additionally report the average updates per second of each model, and present performance as a function of GPU hours, to illustrate efficiency improvements. We use 16, 32, 64, and 128 GPUs in parallel for the 125M, 350M, 750M, 1.3B parameter Transformer-LM  and DEMix  baselines, respectively. We also use these GPU budgets for the seed phase in the ELMforest . For branched training, we divide these GPU budgets equally among the ELM s; for example, the 1.3B parameter per GPU ELMforest uses 16 GPUs for each of the 8 ELM s. For all models, we fix the learning rate at 0.0005 with a polynomial (linear) decay learning rate schedule and 8% warmup, which we found to be optimal for most settings after a large grid search. For all experiments, we use a batch size of 16 for each GPU, with gradient accumulation of 32 steps, and train with fp16. We train on NVIDIA V100 32GB GPUs.\n"]}
{"id": "2205.15716", "categories": "cs.LG cs.AI cs.MA", "paragraphs": ["where \\(\\rho \\)  is the density, \\(u\\)  is the velocity, \\(p\\)  is the pressure and \\(E\\)  is the total energy. \\(E\\)  is calculated by internal energy, \\(e\\) , and kinetic energy as \\(E = e + \\frac{1}{2}u^2\\) .\nThe equations are closed by the addition of an equation of state, a common choice of which is the gamma-law given by \\(p=\\rho e (\\gamma -1)\\)  where \\(\\gamma =1.4\\) .\n", "Following the training setup in\u00a0[21], we trained a system of agents to learn the order \\(r=2\\)  WENO schemes on the Sod initial condition\u00a0[19].\nDuring training, the space is discretized into \\(N=128\\)  points, i.e., the factored Dec-MDP has a system state \\(\\mathcal {S}\\)  of 128 factors, resulting in a state of the shape \\((3,128)\\)  for the 3 equations.\nFor agents trying to learn the \\(r=2\\)  WENO scheme, the system observation space has a shape of \\((3, 129, 2, 3)\\) : 3 equations, 129 agents for each equation, 2 plus and minus fluxes (because of Lax-Friedrichs flux splitting mentioned in Section\u00a0REF ) and 3 points in each stencil.\nThe corresponding action space shape is \\((3, 129, 2, 2)\\) : the first three dimensions are the same as the observation space, and the last is the 2 weights (actions) on the small stencils as given in Equation\u00a0REF .\nEach component of the observation and action space is continuous.\n", "During training, the system is evolved to 1,000 timesteps of 0.0001 seconds for a total of 0.1 seconds for each episode, and a total of 10,000 episodes.\nThe policy network for each agent is a 2-layer NN with 64 neurons each and ReLU activation.\nAn Adam optimizer\u00a0[4] with a learning rate of 0.0003 is used to train the policy gradient algorithm.\nThe training takes about 2 days on a 2.2GHz CPU (GPU was tried but turned out to be slower because most of the computations are in the environment, not during NN training).\nOnce trained, the system of agents can perform inference at a similar speed as standard WENO schemes.\n"]}
{"id": "2209.09746", "categories": "cs.CL", "paragraphs": ["To train Blender, we set the batch size to 32, the learning rate to \\(7.0\\times 10^{-6}\\) , the warmup steps to 100, the evaluation steps to \\(1,\\!000\\) , and the number of updates to \\(50,\\!000\\) . The other parameters were set to the default configuration of the huggingface transformer.\nWe used the model at the validation loss minimum point for conversation planning.\n"]}
{"id": "2203.15309", "categories": "cs.CV", "paragraphs": ["For TUD-L, we use the provided real scene training data for training. As there are only 1214 testing images and no explicit training data in Occluded-LINEMOD, we train our network based on the LINEMOD training data. To be specific, we use the PBR dataset provided by the BOP Benchmark\u00a0[22]. For testing, we follow the BOP 2019 challenge instructions and use the provided testing split for testing. In summary, there are 600 images for TUD-L, 3000 images for LINEMOD and 1445 images for Occluded-LINEMOD.\n", "We implement our DCP-based pose estimation network in Pytorch\u00a0[38] and train it from scratch. We use the Adam optimizer\u00a0[27] with a learning rate of \\(10^{-3}\\)  and mini-batches of size 32, and train the network for \\(30,000\\)  iterations. For the OT layer, we use \\(k = 50\\)  iterations and set \\(\\lambda = 0.5\\) . For our IDAM-based architecture, we train the model with the Adam optimizer\u00a0[27] until convergence, using a learning rate of \\(10^{-4}\\)  and mini-batches of size 32. We use the FPFH implementation from the Open3D\u00a0[70] library and our custom DGCNN for feature extraction. We set the number of refinement iterations for both the FPFH-based and DGCNN-based versions to 3.\nFor both frameworks, we set the number of points for \\(\\mathcal {X}\\)  and \\(\\mathcal {Y}\\)  to be 1024 and 768, respectively, encoding the fact that \\(\\mathcal {Y}\\)  only contains a visible portion of \\(\\mathcal {X}\\) . Training was performed on one NVIDIA RTX8000 GPU.\n"]}
{"id": "2210.15515", "categories": "cs.LG", "paragraphs": ["RAMP (using TD3)\n\n\n\\(n_{\\text{tot}}\\)  Number training steps\n\\(\\mathcal {M}_{\\text{train}}\\)  Training environments\n\\(\\hat{\\omega }^{k}, k=\\lbrace 1,|\\mathcal {M}_{\\text{train}}|\\rbrace \\) \nInitialize critic, actor, and replay buffer \\(\\mathbf {B}\\) \nAdd \\(\\hat{\\omega }^{k}\\)  to \\(\\Omega ^k\\)  for all \\(k=\\lbrace 1,|\\mathcal {M}_{\\text{train}}|\\rbrace \\) \n\\(i <n_{\\text{tot}}\\)\n", "\\(i\\)  mod \\(H = 0\\) \n\\(\\mathcal {M}^k \\in \\mathcal {M}_{\\text{train}}\\) \nSelect random parameters \\(\\hat{\\omega }^k\\)  from \\(\\Omega ^k\\) \nSample one episode from \\(\\mathcal {M}^k\\) :\n\\({D^k \\leftarrow \\lbrace (s_t,\\pi (s_t,\\hat{\\omega }^k),s_{t+1})\\rbrace }_{t=\\lbrace 1,T\\rbrace }\\) \nRetrain \\(\\hat{\\omega }^{k}_{\\text{new}}\\)  with \\(D^k\\)  and add to \\(\\Omega ^k\\)\n", "\\((s,a,s^{\\prime },r,d,k) \\in b\\) \nSample \\(\\hat{\\omega }^k\\)  from \\(\\Omega ^k\\) \nSet \\(s^{\\prime } \\leftarrow (s^{\\prime },\\hat{\\omega }^k)\\) \nSet \\(s \\leftarrow (s,\\hat{\\omega }^k)\\) \nUpdate critics using \\(b\\) \n\\(i\\)  mod \\(n_{\\text{training}} = 0\\) \nUpdate actor using \\(b\\) \n\\(i \\leftarrow i+1\\)\n"]}
{"id": "2210.15497", "categories": "cs.CL", "paragraphs": ["We first convert the BART-base model [36] to its LSG version by replacing the full attention in the encoder part and adding global tokens. The model is then fine-tuned on 4,096-length inputs and evaluated. To reduce computational costs, experiments on 16,384-length inputs are warm started from the 4,096-length experiments using the conversion script. The model is then fine-tuned during a single epoch if necessary using the same training parameters. We propose 3 setups for the 16,384-length. First we evaluate the model with pure extrapolation from 4,096-length (no additional training). In the second setup, we extrapolate and add 64 global tokens we choose to fine-tune. In the last setup, we extrapolate, we add 64 global tokens and we fine-tune the full model. Extrapolation is done by concatenating 4 copies of the positional embedding matrix (\\(4 \\times 4096\\) ).\n"]}
{"id": "2210.15540", "categories": "cs.CV cs.AI", "paragraphs": ["The training of the model was performed in a self-supervised fashion, using only the normal images of the dataset, over a maximum of 3000 epochs with an early stop introduced after epoch 500 which would trigger when the performance of the model on the validation set didn't improve in the previous 50 epochs.\nThe validation set was composed of 10% of the images present in the training data of the datasets. While often larger percentages are used, both MVTec and HeadCT are relatively small datasets, which led us to prefer keeping as much data as possible for the training of the model.\nThe total loss function we used is obtained by summing the L1 loss and the negative of the SSIM Similarity, as defined in [SSIM2004], calculated between the original image given as input and the reconstructed image returned as the output of the model. A formal description of the two functions is given hereafter:\n\\(L_1(X,\\hat{X})=\\sum _{i=0}^{h-1}\\sum _{j=0}^{w-1}{|X_{ij}-\\hat{X}_{ij}|}\\\\L_{SSIM}(X,\\hat{X})= -\\frac{(2\\mu _{X}\\mu _{\\hat{X}} +c_1)(2\\sigma _{X\\hat{X}}+c_2)}{(\\mu _X^2+\\mu _{\\hat{X}}^2+c_1)(\\sigma _X^2+\\sigma _{\\hat{X}}^2+c_2)}\\\\L(X,\\hat{X}) = L_1(X,\\hat{X}) + L_{SSIM}(X,\\hat{X})\\) \n", "\nX:  Is the original image\n\n\\({\\hat{X}}\\) : Is the reconstructed image\n\nh, w:  Are the height and width of the image in pixel\n\n\\({\\mu _{x}}\\) : Is mean value of image x\n\n\\({\\sigma _{x}^2}\\) : Is variance of image x\n\n\\({\\sigma _{xy}}\\) : Is the covariance of x and y\n\n\\({c_1=(k_1L)^2 \\& c_2=(k_2L)^2: }\\)  Are two variables used to stabilize the division with weak denominator\n\nL:  Is the dynamic range of the pixel-values (usually \\(2^{\\# bits per pixel}-1)\\)\n\n\\({k_1 \\& k_2}\\) : are two costants set to 0.01 and 0.03 respectively.\n\n"]}
{"id": "2211.15508", "categories": "cs.CV cs.LG cs.RO", "paragraphs": ["The resulting encoding (6 dimensions) is passed through \\(\\phi \\) , with a 6 dimensional hidden layer, obtaining a 2 dimensional encoding \\((S_x, S_y)\\)  in the embedding space \\(S\\)  that is used for training and subsequent experiments. The activation function for the hidden dimensions is LeakyRelu and the final output is processed through a \\(tanh\\)  function.\n", "The dataset used for training is the aforementioned INTERACTION dataset [18]. Training was conducted on 2842 scenes of one roundabout.\nThe resulting graphs have 6 node features, which are velocity, yaw of the vehicle and one hot encodings of the vehicle type (4 dimensions).\n", "The aforementioned triplet loss \\(\\mathcal {L}\\)  in eq:triplet is used in conjunction with the Adam optimizer. A learning rate of \\(0.001\\)  is used, and the triplet margin is set to \\(0.5\\) . The distance function used in the triplet loss is the euclidean distance.\n", "Training is conducted in 1400 epochs, with batches of size 533. The model is trained on \\(75\\%\\)  of the traffic scenes and validated using the rest.\nNegative sampling was conducted uniformly over the batch, excluding the positive sample.\n", "For generation of the augmented data, we use \\(\\sigma _{pos} = 1.5\\,m\\)  and \\(\\sigma _{vel}=2.0 \\frac{m}{s}\\)  with \\(p_{entity}=0.5\\) .\n"]}
{"id": "2205.05467", "categories": "cs.CV cs.LG", "paragraphs": ["Gradient-based Method. We used the official code of the null space class incremental learning method (NSCIL) [69], which is one of the state-of-the-art gradient-based methods. We followed the official code's setup to tune the hyperparameter \\(\\lambda \\)  from the default setup {10, 30} to the new settings {100, 1}, which is used to select the smallest singular values corresponding to the null space. The larger \\(\\lambda \\)  leads to larger approximate null space, increasing the plasticity to learn new tasks while decreasing the memorization of old tasks [69].\nWe trained the NSCIL model for 12 epochs in total, as we found that training more epochs for NSCIL resulted in performance degradation. The initial learning rate is \\(0.001\\)  for the first task and \\(0.0001\\)  for all other tasks and is divided by 2 after 4 and 8 epochs for the EASY evaluation. For batch normalization layers, the learning rate starts from \\(5 \\times 10^{-5}\\) . The other parameters are the same as official NSCIL implementation.\n", "Memory-based Method. We employed latent replay class incremental learning (LRCIL) [51], which is one of the most effective and efficient memory-based methods.\nLRCIL was trained in the NSCIL's framework, with 20 epochs in total, the initial learning rate is \\(0.001\\)  for the first task and \\(0.0001\\)  for all other tasks and is divided by 2 after 10 and 15 epochs for the EASY, HARD, LONG evaluations. For batch normalization layers, the learning rate starts from \\(5 \\times 10^{-5}\\) .\nFor the GANfake evaluation, we used 60 epochs instead and the initial learning rate is divided by 2 after 20 and 40 epoch.\nFollowing original LRCIL's implementation, there is no knowledge distillation loss. We also tried to add distillation loss with a factor \\(\\gamma _d = 0.3\\) . And we chose the second layer as latent layer.\nWe used Adam with the batch size 32 to train the network.\n", "For iCaRL, we trained its model for 30 epochs in total. The learning rate starts from \\(0.0001\\)  and is divided by \\(\\frac{10}{3}\\)  after 10 and 20 epochs for the EASY, HARD and LONG evaluations.\nFor GANfake sequences, we used 60 epochs instead and the initial learning rate \\(0.001\\)  is divided by \\(\\frac{10}{3}\\)  after 20 and 40 epoch.\nFollowing the original iCaRL's implementation, \\(\\gamma _d\\)  was set to 1 for knowledge distillation loss \\(\\ell _{distill}\\)  with temperature as \\(T=1\\) .\nWe used Adam with the batch size 32 to train the network.\n", "For CCIL, we used its official code. The initial learning rate, total epoches and learning rate decay are the same as iCaRL. We tried the method with (\\(\\gamma _d = 0.3\\) ) or without (\\(\\gamma _d = 0\\) ) knowledge distillation \\(\\ell _{distill}\\) . Also, we tried it with or without applying mixup and label smoothing techniques to the whole training process which is different from the original implementation(only for the first training phase), and we followed the relevant parameters (e.g, mixup weight). We used Adam with the batch size 32 to train the network.\nThe other parameters are the same as the official implementation of CCIL.\n"]}
{"id": "2209.01304", "categories": "cs.CV cs.AI cs.CL", "paragraphs": ["All experiments were trained on a single Titan Xp GPU. The batch size is 16, the input image size is 224\\(\\times \\) 224, the learning rate of the encoder is 1e-4, that of the decoder is 4e-4, the Adam optimizer is used in this model with a weight decay of 1e-6. Moreover, the Cosine Annealing Warm Restarts\u00a0[13] scheduler is used for scheduling the learning rate. In addition, the model uses \\(k\\) -fold cross validation with \\(k=4\\) . We also use common augmentation techniques such as HorizontalFlip, RandomCrop with probability of 0.5, and Normalize with mean and std are (0.485, 0.456, 0.406), (0.229, 0.224, 0.225), respectively. Finally, our model uses a cross entropy as the loss function.\n"]}
{"id": "2209.15031", "categories": "cs.LG", "paragraphs": ["In order to enable comparisons and reproducibility we use the same training pipeline as in previous works\u00a0[41] . We apply the vertical flip and\nthe pad-and-crop augmentations and a 16 pixel cutout\u00a0[15] after any augmentation method. We trained Wide-ResNet\u00a0[55] models in the 40-2 and 28-10 configurations.\n"]}
{"id": "2209.14922", "categories": "cs.CV cs.RO", "paragraphs": ["Training for both foggy and low-lighting setting is done by resizing images to \\(448 \\times 448 \\times 3\\)  pixels and with a batch size of 6 for 80 epochs. We use a cosine learning rate scheduler with learning rates ranging from \\(1\\times 10^{-6}\\)  to \\(1\\times 10^{-4}\\)  and an SGD optimizer with a weight decay of \\(5\\times 10^{-4}\\) .\n"]}
{"id": "2209.15001", "categories": "cs.CV cs.AI cs.LG", "paragraphs": ["One of the most important architecture-related hyperparameters in DiNA-based models is dilation values.\nBoth DiNAT and DiNAT\\(_s\\)  use a combination of NA and DiNA layers.\nWe typically set dilation values in DiNA layers to be the maximum possible value with respect to input resolutions, if known.\nFor example, ImageNet classification at 224\u00d7224 is downsampled to a quarter of the original size initially, therefore Level 1 layers take feature maps of resolution 56\u00d756 as input.\nWith a kernel size of 7\u00d77, the maximum possible dilation value is \\(\\lfloor 56 / 7 \\rfloor = 8\\) . Level 2 will take feature maps of resolution 28\u00d728 as input, leading to a maximum possible dilation value of 4.\nBecause of this, we change dilation values depending on the task and resolution.\nWe present the final dilation values we used in classification, detection, and segmentation in apptab:dinatsettings.\nNote that we only change dilation values for DiNA layers, since we found that fine-tuning NA layers to DiNA layers may result in a slight decrease in initial performance (see subsec:miscexps, tab:testtimedilationchange).\n"]}
{"id": "2209.06049", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["Creating individual train and test examples: To create individual training and testing examples, we first divided each document into multiple chunks.\nFor documents in both train and test dataset, we divided each document into a set of equal-sized disjoint chunks of contiguous text, disrespecting sentence boundaries.\nEach chunk is a unique training example.\nThis process helps us train on longer sequences, while reducing training time, as done by BERT.\nThe method described above gives us a total of 21.6M training examples, and 2.4M testing examples.\n", "We used a single RTX A6000 (48 GB) GPU for the pre-training. We used a batch size of 32 sequence pairs, and 8 gradient accumulation steps, making the effective batch size 256.\nWe trained each model for a total of 300K optimization steps, which is roughly around 4 epochs of the training data.\nWe used the default AdamW optimizer\u00a0[15] with initial learning rate of 5e-5.\nWe also used other techniques to speed up training, such as fp16 training, pinning the CPU memory for GPU transfer, and using 8 CPU workers for preparing the batches.\nIn our setup, each training experiment took approximately 20 days, and each testing experiment took more than 6 hours.\n"]}
{"id": "2209.06067", "categories": "cs.CV cs.AI", "paragraphs": ["We use ShapeNet [1] dataset for pre-training our autoencoders. ShapeNet dataset is a large point cloud dataset containing approximately 50,000 models spanning across 55 categories. The dataset contains dense points clouds and for training we sample 1024 points using Farthest Point Sampling (FPS) as the input point cloud on which we apply perturbation before feeding it to the auto-encoder. Training was done using AdamW optimizer with initial learning rate of \\(0.001\\)  with cosine annealing with a batch size of 128. SeRP-PointNet was trained for 100 epochs, whereas SeRP-Transformer was trained for 300 epochs.\n"]}
{"id": "2203.14313", "categories": "cs.CV", "paragraphs": ["Following the convention, we use the vanilla vision transformer (known as ViT\u00a0[17]) as the backbone network, \\(f^\\mathrm {B}\\!\\left(\\cdot \\right)\\) . We investigate the ViT-base model with 12 blocks and a channel dimension of 768. The input, \\(\\mathbf {x}\\) , is set to be a \\(224\\times 224\\)  image, partitioned into \\(14\\times 14\\)  tokens, each of which sees a \\(16\\times 16\\)  patch on the original image. We pre-train \\(f^\\mathrm {B}\\!\\left(\\cdot \\right)\\)  with the help of \\(f^\\mathrm {PT}\\!\\left(\\cdot \\right)\\) , which has 4 transformer blocks and the final output has the same dimensionality as the original image. The dataset is ImageNet-1K\u00a0[43]. Unless otherwise specified, the pre-training procedure elapses 300 epochs with a base learning rate starting with \\(1.5\\times 10^{-4}\\)  and decays following the cosine annealing schedule. We only use normal data augmentations, including random cropping and horizontal flipping, to generate the \\(224\\times 224\\)  image (i.e., the target output). The image degradation includes:\n"]}
{"id": "2212.01523", "categories": "cs.LG cs.DC", "paragraphs": ["Clients perform 10 local updates per round. We use PyTorch's SGD optimizer with a momentum factor of 0.9 for all tasks. For FEMNIST, OpenImage, and Google Speech, the initial learning rate is set to 0.01, 0.05, and 0.01, respectively, with a decay factor of 0.98 every 10 rounds. To obtain the best performance, we set the total mask ratio \\(q=20\\%\\)  for ShuffleNet, and \\(q=30\\%\\)  for MobileNet and ResNet-34 in STC. For APF, we set the threshold for effective perturbation, which reflects the compression ratio, to \\(0.1\\)  for all tasks. The remaining STC and APF parameters are set to their optimal values\u00a0[31], [6]. For GlueFL, the default sticky group parameters are \\(S=4K\\)  and \\(C=4K/5\\) . For ShuffleNet, the default mask shifting parameters are \\(q=20\\%\\)  and \\(q_{shr}=16\\%\\) . For MobileNet and ResNet-34, we set \\(q=30\\%\\)  and \\(q_{shr}=24\\%\\) . We use \\(I=10\\)  to regenerate the shared mask every 10 rounds. We choose these values as they produce the best performance across most tasks.\n"]}
{"id": "2211.00928", "categories": "cs.LG cs.AI", "paragraphs": ["All experiments conducted in this paper used Adam optimizer with \\(\\beta _1\\) =0.9, \\(\\beta _2\\) =0.999, and a learning rate of \\(1 \\times 10^{-4}\\) . In each round of active learning, the main model is trained on a batch of 64 examples and for 500 epochs over the training set. The exponentially moving average of the main model is computed using a decay parameter \\(\\alpha \\) =0.999. After each round of training, a batch of 1000 unlabeled examples is acquired for labeling.\n", "While fine-tuning, we set a probability threshold of 0.8 for selecting high confidence examples from the unlabeled pool. We fine-tune the model for 500 epochs, using the Adam optimizer and learning rate of \\(1 \\times 10^{-2}\\) . During fine-tuning, we randomly select four augmentations from RandAugment [50] followed by Cutout [51].\n"]}
{"id": "2207.10617", "categories": "cs.CL", "paragraphs": ["To make our comparison to other models as meaningful as possible, we follow the setup used in pretraining BERT \u2014 we use the BookCorpus\u00a0[596] and English Wikipedia for pretraining baseline models.\nThese two corpora consist of around 16GB of uncompressed text.\nWe format our inputs as [CLS] \\(x_1\\)  [SEP] \\(x_2\\)  [SEP] where \\(x_1\\)  and \\(x_2\\)  are two text segments.\nWe always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%.\nLike BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece\u00a0[275] as in XLNet \u00a0[559].\n", "We set the maximum length of \\(n\\) -gram (i.e., \\(n\\) ) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as \u201cWhite House correspondents\u201d).\n", "All the model updates use a batch size of 4096 and a Lamb optimizer with learning rate 0.00176\u00a0[569].\nWe train all models for 125,000 steps unless otherwise specified.\nTraining was done on Cloud TPU V3.\nThe number of TPUs used for training ranged from 64 to 1024, depending on model sizes. Code and pretrained models are available at https://github.com/google-research/albert.\n", "For the pretrained language model checkpoints, we use the 125 million parameters (125M) and the 1.3 billion parameters (1.3B) dense model from [12]. These pretrained models have shown results comparable to GPT3 across various tasks.\n", "For self-supervised training, we use a subset of documents from the RoBERTa training corpus [321] that contains four domains: BookCorpus plus Wikipedia, CC-News, OpenWebText, and Stories. Specifically, we randomly sample 100k documents from each domain except Stories where we only sample 10k documents as the documents there are much longer than the others. The final training data contains approximately 1 million instances with 250k training instances per task.The average numbers of example per instance for each data source are: 6.9 for BookCorpus plus Wikipedia, 5.3 for CC-News, 3.5 for OpenWebText, and 7.2 for Stories. For the 125M model, we train for 10 epochs, which takes roughly 1 day on a V100 GPU. For the 1.3B model, we train for 5 epochs, which takes roughly 3 days on 2 V100 GPUs.\n", "We subsampled half a million paraphrase pairs from ParaNMT-50M\u00a0[542] as our training set. We use SemEval semantic textual similarity (STS) task 2017\u00a0[55] as a development set. For semantic similarity evaluation, we use the STS tasks from 2012 to 2016\u00a0[8], [9], [6], [5], [7] and the STS benchmark test set\u00a0[55]. For evaluating syntactic similarity, we propose several evaluations. One uses the gold parse trees from the Penn Treebank\u00a0[340], and the others are based on automatically tagging and parsing five million paraphrases from ParaNMT-50M; we describe these tasks in detail below.\n", "For training with the PRL, we require a training set of sentential paraphrase pairs. We use ParaNMT, a dataset of approximately 50 million paraphrase pairs.\nTo ensure there is enough variation between paraphrases, we filter out paraphrases with high BLEU score between the two sentences in each pair, which leaves us with around half a million paraphrases as our training set. All hyperparameter tuning is based on the BLEU score on the development set (see appendix for more details). Code and data are available at https://github.com/mingdachen/syntactic-template-generation.\n"]}
{"id": "2211.11886", "categories": "cs.LG cs.GT cs.MA", "paragraphs": ["Individual networks are 64 cells GRU enclosed with fully connected layers (see Fig REF ).\nThe mixer network is the same as in [28] with an embedded size of 32.\nThe individual and mixer networks are the same for the three methods.\nWe used the default parameters of MAVEN policy networks provided by [21].\nFor QVMix, the \\(V\\)  network is a copy of the QMIX network with only one output for each \\(V\\)  network.\n", "For each learning scenario, networks are updated regardless of how episodes have been generated.\nNetworks are updated from a replay buffer that collects the 5000 latest played episodes and 32 of them are sampled from it to update the network.\nThe network update is performed every eight episodes in the \\(3m\\)  map and every episode in the \\(3s5z\\)  map.\nThe difference is justified by the desire to increase the number of network updates for \\(3s5\\)  to improve performances, especially against the heuristic.\nThe epsilon greedy exploration starts with an epsilon equal to 1 decreasing linearly to \\(0.05\\)  during 2 million timesteps.\nThis is perhaps the main difference with respect to the provided parameters that decreases the epsilon only during \\(0.5\\)  million timesteps.\nThe discount factor is \\(\\gamma = 0.99\\)  and the learning rate is \\(0.0005\\) .\nTarget networks are updated every 200 episodes.\nWe refer the reader to [21] for further parameter definitions for MAVEN optimisation.\n"]}
{"id": "2210.04024", "categories": "cs.LG", "paragraphs": ["To begin an inference (i.e., forward propagation) on a DNN model, the entire parameters should be in GPU memory such that GPU kernels can directly access them. For that, a three-step approach is usually used, which is depicted in Fig.\u00a0REF . The model file is first read from disk to a CPU memory buffer ([baseline=(char.base)]\nshape=circle,fill,inner sep=0.5pt] (char) white1;).\nWhen allocating CPU buffers, there are several choices provided by the Nvidia CUDA runtime, which will be detailed in Section\u00a0REF .\nThen the parameters are copied to a GPU memory buffer ([baseline=(char.base)]\nshape=circle,fill,inner sep=0.5pt] (char) white2;).\nWhen the source CPU buffer happens to be a pageable memory by the usual malloc() function that is not under the control of the CUDA runtime, the copy is done via a hidden staging area, incurring possible blockings and delays in case of a staging area shortage. After the copy operation, GPU kernels can access and execute the DNN layers in the GPU memory buffer ([baseline=(char.base)]\nshape=circle,fill,inner sep=0.5pt] (char) white3;). As explained in Section\u00a0REF , CPU and GPU memory buffers are from the same shared DRAM space. Thus, both buffers should be accounted for when estimating the memory usage of a DNN inference system. The read operation is processed by CPU, while the copy and kernel operations are executed by GPU. Since GPUs have two separate processing units for them (i.e., copy engine and execution engine), read and copy operations can run simultaneously\u00a0[24]. As a result, read, copy, and kernel operations can run fully in parallel, providing a great chance for optimizing the DNN execution architecture.\n"]}
{"id": "2204.11531", "categories": "cs.CV", "paragraphs": ["In the following experiments, we choose the same network architectures as AugMix [23], including All Convolutional Network [42], DenseNet-BC (\\(k=2,d=100\\) ) [24], 40-2 WideResNet [47] and ResNeXt-29 (\\(32 \\times 4\\) ) [45]. We use stochastic gradient descent with an initial learning rate of 0.1 and ReduceOnPlateau scheduler. We train all architectures over 150 epochs.\n", "Recent works [38], [15] substantiate the claim that increased robustness against regular or universal adversarial perturbations [3], [35] does not imply increased robustness against common corruptions.\nIn this part, we discuss whether our generated samples from VITA can be integrated into the existing adversarial training process to improve the adversarial robustness of the model.\nAll images of CIFAR-10 are normalized into [0, 1].\nThe adversarial test data are bounded by \\(l_{\\infty }\\)  perturbations with \\(\\epsilon _{test} = 0.031\\) .\nWe use the same settings as the corruption robustness evaluation experiment to train our image-to-image translation framework.\nThe only difference is the regularization terms in multi-source robust training.\nHere, we use strong adversarial examples via the regularization term proposed by TRADES [49].\nUsing a regularization term from TRADES [49] and an early stopping scheme from FAT [50], we deploy multi-source adversarial training to verify the effectiveness of samples generated by VITA towards adversarial robustness.\nThe backbone of our network is Wide ResNet. Models are trained using SGD with 0.9 momenta for 100 epochs, with the initial learning rate of 0.01 divided by ten at epoch 60.\nOur supplementary materials contain further information about training and evaluating with additional models.\n"]}
{"id": "2206.03687", "categories": "cs.CV", "paragraphs": ["CIFAR-10\u00a0[20]. The image size and feature size are set as \\(128 \\times 128\\)  and \\(8 \\times 8\\) , respectively. Considering that the anomalies in CIFAR-10 are semantically different objects (not structural damages or texture perturbations in MVTec-AD\u00a0[2]), the features in deep layers containing more semantic information must be helpful. Therefore, the feature maps from stage-1 to stage-5 are selected. These features are resized and concatenated together to form a 720-channel feature map. The reduced channel dimension is set as 256. Our model is trained for 1000 epochs on 8 GPUs (NVIDIA Tesla V100 16GB) with batch size 128 by AdamW optimizer [18] (with weight decay \\(1 \\times 10^{-4}\\) ). The learning rate is \\(1 \\times 10^{-4}\\)  initially, and dropped by 0.1 after 800 epochs. The layer numbers of both encoder and decoder are 4. The neighbor size, jittering scale, and jittering probability are chosen as 5\\(\\times \\) 5, 20, and 1, respectively.\n"]}
{"id": "2212.01927", "categories": "cs.LG cs.CV", "paragraphs": ["We evaluate our approach on two models: ResNet-50 and RAFA-Net.\nWith ResNet-50, two runs with different random seeds for each combination of learning rate \\(\\lbrace 0.001,0.0001,0.00001\\rbrace \\)  and batch size \\(\\lbrace 8,16\\rbrace \\)  are used for hyperparameter tuning. For data augmentation, images are loosely cropped around the center in the training and testing datasets with random flipping.\nWith RAFA-Net, we use the training parameters and data augmentation used in\u00a0[2].\n", "We evaluate BEL by applying it on HRNetV2-W18. HRNetV2-W18 feature extractor's output is 240 channels of size \\(64 \\times 64\\) . For heatmap regression, a \\(1\\times 1\\)  convolution is used to get \\(P\\)  heatmaps of size \\(64 \\times 64\\) , where \\(P\\)  is the number of landmarks.\nSince BEL-x predicts \\((x,y)\\)  coordinates directly we modify the architecture of HRNetV2-W18 to support direct prediction of landmarks. Figure\u00a0REF  shows the modified architecture of HRNetV2-W18 for BEL-x.\n", "We use two runs with different random seeds to decide the learning rate. We consider learning rates \\(\\lbrace 0.0003,0.0005,0.0007\\rbrace \\)  and \\(\\theta \\in \\lbrace 10,30\\rbrace \\) .\n"]}
{"id": "2203.06906", "categories": "cs.CL", "paragraphs": ["\nData: We use the training data as in MacBERT. It consists of the Chinese Wikipedia dumphttps://dumps.wikimedia.org/zhwiki/latest/, encyclopedia, community question answering, news articles, etc. The total training data has 5.4B words and takes about 20G disk space.\n\nTokenization: We use WordPiece tokenizer [28] as in BERT. To detect the Chinese word boundaries, we use LTP [0] for word segmentation. Note that the Chinese word segmentation is only used for selecting the whole word to perform whole word masking, i.e., only affect which tokens are chosen for masking. The input for PERT is still handled by the WordPiece tokenizer.\n\nVocabulary: We directly use the vocabulary of Chinese BERT-basehttps://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip and other PLMs with a vocabulary size of 21128.\n\nHyper-parameters: We use a maximum sequence length of 512 throughout the whole pre-training process.\n\nOptimization: We use a batch size of 416 (base-level) or 128 (large-level) with an initial learning rate of 1e-4. We perform a linear warmup schedule with the first 10K steps. The total training step is 2M. We use Adam [12] with weight decay (rate = 0.1) optimizer with beta values (0.9, 0.999) and an epsilon value 1e-6.\n\nTraining Device: The training was done on a single TPU v3-8 (128G HBM).https://cloud.google.com/tpu/\n\n", "\nData: We use English Wikipedia and BooksCorpus [31] as the pre-training data, which is widely used in the previous literature.\n\nTokenization: We use WordPiece tokenizer [28] as in BERT.\n\nVocabulary: We directly use the vocabulary of English BERT-base-uncasedhttps://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip with a vocabulary size of 30522.\n\nHyper-parameters: We use a maximum sequence length of 512 throughout the whole pre-training process.\n\nOptimization: We use a batch size of 416 (base-level) or 128 (large-level) with an initial learning rate of 1e-4. We perform a linear warmup schedule with the first 10k steps. The total training step is 2M. We use Adam [12] with weight decay (rate = 0.1) optimizer with beta values (0.9, 0.999) and an epsilon value 1e-6.\n\nTraining Device: The training was done on a single TPU v3-8 (128G HBM).\n\n"]}
{"id": "2210.05047", "categories": "cs.CL", "paragraphs": ["We use the Adam optimizer\u00a0[13] with \\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.98\\)  and \\(\\epsilon = 10^{-9}\\) ; and (ii) increase the learning rate linearly for the first \\(4K\\)  training steps and decrease it thereafter; (iii) use batch size of \\(32K\\)  source tokens and \\(32K\\)  target tokens. Checkpoints are saved after every \\(10K\\)  iterations during training. We train models with maximum of \\(300K\\)  iterations.\n", "We use dropout of \\(0.1\\)  and label-smoothing of \\(0.1\\) .\nFor evaluation we use Sacrebleu with hash value of nrefs:1|case:mixed|eff:no|tok:13a|\n"]}
{"id": "2210.05059", "categories": "cs.CL", "paragraphs": ["We employed Transformer big architecture with 6 encoder and 6 decoder layers. Hidden size was set to 1024 and maximum input length to 1024 tokens. We employed a joint source-target language subword vocabulary of size \\(32K\\)  using Sentencepiece algorithm\u00a0[18].\n", "We use the Adam optimizer\u00a0[15] with \\(\\beta _1 = 0.9\\) , \\(\\beta _2 = 0.98\\)  and \\(\\epsilon = 10^{-9}\\) ; and (ii) increase the learning rate linearly for the first \\(4K\\)  training steps and decrease it thereafter; (iii) use batch size of \\(32K\\)  source tokens and \\(32K\\)  target tokens. Checkpoints are saved after every \\(10K\\)  iterations during training. We use dropout of \\(0.1\\)  and label-smoothing of \\(0.1\\) . We train models with maximum of \\(300K\\)  iterations.\n"]}
{"id": "2205.10192", "categories": "cs.CL", "paragraphs": ["We investigate the performance of both our systems, TreeKvD and GraphKvD,\nacross a number of values of working memory size, WM\\(=\\lbrace 5,20,50,100\\rbrace \\) , and set the maximum recall path length \\(R=5\\)  and maximum tree persistence \\(\\Psi =5\\) .\nFor TreeKvD, search for the optimal recalled path is finished early if the total overlap score of a candidate path is higher than \\(0.5\\) .\nFor GraphKvD, we use a decay factor \\(\\gamma =0.01\\) .\n"]}
{"id": "2209.15329", "categories": "cs.CL cs.AI eess.AS", "paragraphs": ["The network architecture of SpeechLM follows that of HuBERT [15] for a fair comparison.\nSpecifically, the Base model consists of \\(L\\) =12 Transformer layers where both the Speech Transformer and the Shared Transformer have 6 layers.\nThe Large model doubles the number of Transformer layers.\nThe convolutional layers downsample the input waveform to a frame rate of 20ms.\nThe CTC layer consists of a single 1-D convolutional layer followed by a linear layer, which outputs the probabilities of text characters\nAll models are pre-trained on 32 GPUs for 400K steps.\nTo align with HuBERT, the update frequency is set to 4 for Large models to simulate 128 GPUs.\nThe batch size for the Base model is 4375 tokens after down(up)-sampling for both speech and text input, and for the Large model it is set to 2800.\nThe text loss (\\(\\mathcal {L}_{UCTC}\\) ) is weighted by 0.1The effect of different weights (\\(\\lambda \\) ) is reported in Appendix  REF.\nMore details about the model configuration and training details can be found in Appendix REF .\n", "The Base model has 12 Transformer layers with the attention dimension of 768 and attention heads of 12, the Large model has 24 Transformer layers with the attention dimension of 1024 and attention heads of 16.\nThe convolutional layers have 512 channels and kernel sizes of [10,3,3,3,3,2,2], resulting in a downsampling rate of 320.\nThe CTC layer is a single 1-D convolutional layer with a kernel size of 2, whose channel matches the Transformer dimension.\nIt is then followed by a linear projection to the text characters.\nAll models are pre-trained on 32 GPUs for 400K steps including 32K warming-up steps.\nWe use Adam [19] with \\(\\beta _1\\) =0.9, \\(\\beta _2\\) =0.98 for optimization.\nThe maximum learning rate is set to \\(5e-4\\)  and decays linearly to zero after the warming-up steps.\n"]}
{"id": "2211.13527", "categories": "cs.CL", "paragraphs": ["In this section, the detail the main hyper-parameters that were used for finetuning the pretrained encoders. It is worth noting that we use the same set of hyperparameters for all the different encoders [25], [23]. The dropout rate [88] is set to \\(0.2\\)  , we train with a batch size of 32, we use ADAMW [54], [68], [3]. Additionally, we set the weight decay to \\(0.01\\) , the warmup ratio to \\(0.06\\) , and the learning rate to \\(10^{-5}\\) . All the models were trained during 20k iterations with different seeds.\n"]}
{"id": "1711.10712", "categories": "cs.CL", "paragraphs": ["We set state size of the dialogue-level and utterance-level LSTM as 200 and 150 respectively. Hidden layer size of the policy network is set as 100. We used randomly initialized word embedding of size 300. Adam optimization method [22] with initial learning rate of 1e-3 is used for mini-batch training. Dropout rate of 0.5 is applied during training to prevent the model from over-fitting.\n", "In dialogue simulation, we take a task-oriented dialogue as successful if the goal slot values estimated by the state tracker fully match to the user's true goal values, and the system is able to offer an entity which is finally accepted by the user. Maximum allowed number of dialogue turn is set as 15. A positive reward of +15.0 is given to the agent at the end of a success dialogue, and a zero reward is given in a failure case. We apply a step penalty of -1.0 for each turn to encourage shorter dialogue in completing the task.\n"]}
{"id": "1708.01682", "categories": "cs.CV", "paragraphs": ["The Caffe package\u00a0 is used throughout the experiments.\nAll images are normalized to 256-by-256 before further processing.\nThe embedding size is set to \\(D = 512\\)  for all embedding vectors, and no normalization is conducted before computing loss.\nWe omit the comparison on different embedding sizes as the performance change is minor.\nThis fact is also evidenced in\u00a0.\nGoogLeNet\u00a0 pretrained on ImageNet ILSVRC dataset\u00a0 is used for initialization and a randomly initialized fully connected layer is added.\nThe new layer is optimized with 10 times larger learning rate than the other layers.\nWe fix the base learning rate to \\(10^{-4}\\)  for all datasets except for the CUB-200-2011 dataset, for which we use a smaller rate \\(10^{-5}\\)  as it has fewer images and is more likely to meet the overfitting problem.\nWe use SGD with 20k training iterations and 128 mini-batch size.\nStandard random crop and random horizontal mirroring are used for data augmentation.\nNotice that our method incurs negligible computational cost compared to traditional triplet loss. Therefore, the training time is almost same as other baselines.\n"]}
{"id": "1708.01759", "categories": "cs.CL", "paragraphs": ["We set the network parameters based on several experiments performed on the development set of one of the cross-validation folds (see Section\u00a0REF ).We use embedding size 300, learning rate 0.0001, dropout probability 0.5, and two fully connected \\(\\tanh \\)  layers (\\(k=2\\) ).\n", "We train the network for 500 passes over the training data, checking Pearson and Spearman correlations on the validation set after each pass (with equal importance). We keep the configuration that yielded the best correlations overall.\nFor setups using synthetic training data (see Section\u00a0REF ), we first perform 20 passes over all data including synthetic, keeping the best parameters, and then proceed with 500 passes over the original data.\nTo compensate for the effects of random network initialisation, all our results are averaged over five runs with different initial random seeds following [37].\n"]}
{"id": "1708.00277", "categories": "cs.CV", "paragraphs": ["We use NNS1 network from [19] for training which is a reduced version of Google's inception architecture [23]. We increase the dimension of the embedding layer from 128 to 512 and adjust Softmax layer for 2,558 identities. The network is fed with \\(96 \\times 96\\)  pixel images which are augmented randomly with cropping (between \\(\\%70-\\%100\\) ), location, aspect ratio (between \\(7/8-8/7\\) ), flipping (\\(0.5\\)  chance), blurring (\\(0.5\\)  chance), brightness, contrast and saturation in every iteration. Input images are linearly scaled to have zero mean and unit norm. SGD is optimized by Adam solver [11] with batch size of 1024 on Nvidia Titan X GPU and we use MatConvNet library [26] with a number of modification. We set weight decay to 0.0005 and use batch normalization to avoid overfitting. Training is started with a learning rate of 0.001 and divided by 10 at the 15th and 25th epochs and stopped at 30th epoch.\n"]}
{"id": "1707.00110", "categories": "cs.CL", "paragraphs": ["We use a similar setup to the Toy Copy task, but use 512 RNN and embedding units, train using 8 distributed workers with 1 GPU each, and train for at most 1M steps. We save checkpoints every 30 minutes during training, and choose the best based on the validation BLEU score.\n"]}
{"id": "1710.01408", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["After splitting both the training and validation sets into blocks, we train using the augmented data only as input to the network shown in Figure\u00a0REF .\nWe use the Adam optimizer\u00a0[25] with an initial learning rate 0.001, a momentum of 0.9 and a batch size 32.\nThe learning rate (\\(lr\\) ) is iteratively reduced based on the current number of epochs, according to:\n\\(lr_{new} = lr_{initial}\\times (1.0-\\frac{epoch_{current}}{epoch_{total}})\\) \n{FIGURE}", "This proceeds for a total of 30 epochs, i.e.\u00a0 \\(epoch_{total} = 30\\) .\nWe monitor the progress of the validation loss and save the weights if the loss improves.\nIf the loss does not improve after 3 epochs, training is terminated and the weights with the best validation loss are used for testing.\nTraining our network takes around 12 to 18 hours to converge using a Tesla p40 GPU and Keras\u00a0[9] with the Tensorflow backend.\nThe feed forward time during testing is 3.7s for the full scene (\\(\\sim \\)  412k points).\nFigure REF  shows the loss and overall accuracy progress during training and validation.\n"]}
{"id": "1704.07287", "categories": "cs.CL cs.LG cs.SD", "paragraphs": ["Both the encoder and decoder are 3-layer deep LSTM-RNNs with 256 hidden units in each layer. For the location-aware attention, the convolution operation uses 5 convolution filters of width 40 each. We use 512-dimensional embedding vectors to represent words and linearized parsing symbols, such as \u201c(S\".\n", "A relatively small number of configurations are explored for the acoustic-prosodic features, tuning based on dev set parsing performance. Pause embeddings are tuned over \\(\\lbrace 4, 16, 32\\rbrace \\)  dimensions. For the CNN, we try different configurations of filter size combinations \\(\\in \\lbrace [10,25,50], [5,10,25,50]\\rbrace \\) , and number of filters per filter size \\(N \\in \\lbrace 16,32,64,128\\rbrace \\)So when we use filter sizes [10, 25, 50] with 16 of each type, we have a total of 48 filters. Also, note that the filter sizes are actually \\(6\\times 10\\) , \\(6 \\times 25\\) , etc., but since the feature dimension is fixed (6 in our case), we specify only the filter width.. These filter size combinations are motivated by the fact that the average word length in our dataset is 25 frames, so intuitively the different filter sizes are capturing f0 and energy phenomena on various levels: \\(w=5, 10\\)  for sub-word, \\(w=25\\)  for word, and \\(w=50\\)  for word and context.\n"]}
{"id": "1704.07073", "categories": "cs.CL", "paragraphs": ["The input and output vocabularies are collected from the training data, which have 119,504 and 68,883 word types respectively.\nWe set the word embedding size to 300 and all GRU hidden state sizes to 512.\nWe use dropout [23] with probability \\( p = 0.5 \\) .\n"]}
{"id": "1705.03127", "categories": "cs.CL cs.AI", "paragraphs": ["\nContext size\\( C \\) : increasing \\( C \\)  has been reported to boost accuracy through a larger number of training samples, but also increases training time. Mikholov et al suggest randomizing the size of the context range for each training sample with probability \\( \\frac{1}{C} \\) , where \\( C \\)  is the maximum context size. In practice, for \\( C=5 \\) , this means selecting \\( C=1,2,...,5 \\)  with probability \\( 0.2 \\)  each.\n\nMinimum count \\( w_{\\text{min}} \\) for words to be included in the vocabulary: words that are observed less often than \\( M \\)  times are replaced by a token \u2018UNK\u2019 for unknown and all treated alike.\n\nSubsampling frequency \\( F_{\\text{sub}} \\) : as mentioned above, recommended at \\( 10^{-5} \\) .\n\nSize of negative samples: 2-5 for large samples, 5-20 for smaller training sets.\n\nEmbedding size \\( D \\) : the dimensionality of the word vector increases computational complexity just as increasing the training set size. [3] suggest that conventional vector size choices of 50-100 are too small and report significantly better results for ranges 300-1,000.\n\nEpochs to train: ranges from 3-50 have been reported, but this choice often depends on the constraints imposed by the size of the training set, the computational complexity resulting from other parameter choices, and available resources.\n\n"]}
{"id": "1703.04887", "categories": "cs.CL", "paragraphs": ["For the Transformer, following the base model in [26], we set the dimension of word embedding as 512, dropout rate as 0.1 and the head number as 8. The encoder and decoder both have a stack of 6 layers. We use beam search with a beam size of 4 and length penalty \\(\\alpha =0.6\\) . For the RNNSearch, following [3], We set the hidden units for both encoders and decoders as 512. The dimension of the word embedding is also set as 512. We do not apply dropout for training the RNNSearch. During testing, we use beam search with a beam size of 10 and length penalty is not applied.\n"]}
{"id": "1710.06371", "categories": "cs.CL", "paragraphs": ["We adopt the original Attract-Repel model setup without any fine-tuning. Hyperparameter values are set to: \\(\\delta _{att}=0.6\\) , \\(\\delta _{rep}=0.0\\) , \\(\\lambda _{reg}=10^{-9}\\)  . The models are trained for 5 epochs with the AdaGrad algorithm [14], with batch sizes set to \\(k_1=k_2=k_3=128\\)  for faster convergence.\n"]}
{"id": "1712.07316", "categories": "cs.CL cs.LG stat.ML", "paragraphs": ["For the models trained on the IWSLT'16 English to German dataset, the hyper parameters were kept largely equivalent to that of the default OpenMT LSTM baseline.\nAll models were unidirectional and the dimensionality of both the word vectors and hidden states were 600, required as many of the generated architectures were residual in nature.\nThe models were 2 layers deep, utilized a batch size of 64, and standard dropout of \\(0.2\\)  between layers.\nThe learning rate began at 1 and decayed by \\(0.5\\)  whenever validation perplexity failed to improve.\nWhen the learning rate fell below \\(0.03\\)  training was finished.\nModels were evaluated using a batch size of 1 to ensure RNN padding did not impact the results.\n"]}
{"id": "1704.03285", "categories": "cs.CV", "paragraphs": ["For training, we randomly select 13 consecutive blurry frames from artifically blurred videos (i.e., \\(\\textbf {B}_1,\\ldots ,\\textbf {B}_{13}\\) ) ,\nand crop a patch per frame. Each patch is 128x128\u00a0pixels in size, and a randomly chosen pixel location is used for cropping all 13 patches.\nMoreover, we use a batch size of 8, and employ Adam\u00a0[19] for optimization with an initial learning rate of 0.0001, which is decreased exponentially (decay rate = 0.96) with an increasing number of iterations.\n"]}
{"id": "1711.02512", "categories": "cs.CV", "paragraphs": ["Structure-from-Motion (SfM). Our training samples are derived from the dataset used in the work of Schonberger\u00a0et al. \u00a0[16], which consists of 7.4 million images downloaded from Flickr using keywords of popular landmarks, cities and countries across the world.\nThe clustering procedure\u00a0[17] gives around 20k images to serve as query seeds.\nThe extensive retrieval-SfM reconstruction\u00a0[55] of the whole dataset results in \\(1,474\\)  reconstructed 3D models.\nRemoving overlapping models leaves us with 713 3D models containing more than 163k unique images from the initial dataset.\nThe initial dataset contains, on purpose, all images of Oxford5k and Paris6k datasets.\nIn this way, we are able to exclude 98 clusters that contain any image (or their near duplicates) from these test datasets.\n", "Each training and validation tuple contains 1 query, 1 positive and 5 negative images.\nThe pool of candidate positives consists of \\(k=100\\)  images with the closest camera centers to the query.\nIn particular, for method \\(m_3\\) , the inlier-overlap threshold is \\(t_i=0.2\\) , and the scale-change threshold \\(t_s=1.5\\) .\nHard negatives are re-mined 3 times per epoch, i.e. roughly every \\(2,000\\)  training queries.\nGiven the chosen queries and the chosen positives, we further add 20 images per model to serve as candidate negatives during re-mining.\nThis constitutes a training set of around 22k images per epoch when all the training 3D models are used.\nThe query-tuple selection process is repeated every epoch. This slightly improves the results.\n"]}
{"id": "1701.01546", "categories": "cs.CV", "paragraphs": ["We train the model by minimizing the reconstruction error of the input volume. We use Adam optimizer to allow it taking the role of setting the learning rate automatically based on the model\u2019s weight update history. We use mini-batches of size 64 and each training volume is trained for a maximum of 50 epochs or until the reconstruction loss of validation data stop decreasing after 10 consecutive epochs. Hyperbolic tangent is chosen as the activation function of spatial encoder and decoder. To ensure the symmetry of the encoding and decoding function, we did not use rectified linear unit (ReLU) despite its regularization ability because activated values from ReLU have no upper bound.\n"]}
{"id": "1711.09576", "categories": "cs.LG cs.FL", "paragraphs": ["As our focus was extraction, we trained all networks to \\(100\\%\\)  accuracy on their train sets, and of these we considered only those that reached \\(99.9{+}\\%\\)  accuracy on a dev set consisting of up to 1000 uniformly sampled words of each of the lengths \\(n\\in {1,4,7,...,28}\\) . The positive to negative sample ratios in the dev sets were not controlled.\n", "For languages where the positive class was unlikely to be found by random sampling\u2014e.g. balanced parentheses or emails\u2014we generated positive samples using tailored functions.For instance, a function that creates emails by uniformly sampling 2 sequences of length \\(2{-}8\\) , choosing uniformly from the options .com, .net, and all .co.XY for X,Y lowercase characters, and then concatenating the three with an additional @. In these cases we also generated negative samples by mutating the positive examples.By adding, removing, changing, or moving up to 9 time characters.\nWherever a test set is mentioned, it was taken as a 1:1 sample set from the same distribution generating the positive and negative samples.\n"]}
{"id": "1703.07971", "categories": "cs.CV", "paragraphs": ["As a preprocessing step, all images of the evaluation dataset are rescaled so that the smaller side of the image is always 256 pixels. We calculate mean and standard deviation of pixel intensities separately for each scene and use them to normalize intensity value of every pixel in the input image.\n", "We trained our models using random crops (\\(224\\times 224\\) ) and performed the evaluation using central crops at the test time. All experiments were conducted on two NVIDIA Titan X GPUs with data parallelism using Torch7\u00a0[4]. We minimize the loss function (REF ) over a training part of each scene of the evaluation dataset using Adam\u00a0[15] (\\(\\beta _1=0.9\\) , \\(\\beta _2=0.99\\) ). The scale factor \\(\\beta \\)  (REF ) varies between 1 to 10. Training mini-batches are randomly shuffled in the beginning of each training epoch. We further used set the weight decay as \\(10^{-5}\\) , used a mini-batch size of 40 and the dropout probability as \\(0.5\\) . These parameter values were kept fixed during our experiments.\n"]}
{"id": "1702.05803", "categories": "cs.CV", "paragraphs": ["The CNN training procedure for the two networks involves optimizing the multinomial logistic regression objective (softmax) using stochastic gradient descent with Nesterov momentum [11]. The input to both networks is a \\(224 \\times 224\\)  RGB patch image sampled at the highest magnification. The batch size was set to 128 and 22 for the CNN I and II, momentum to 0.9. We used L2-regularization (\\(\\lambda _{CNN - I} = 0.003\\)  and \\(\\lambda _{CNN - II} = 0.0001\\) ) and dropout regularization with ratio 0.5 [12] (only applied to the last two layers of the network with \\(1 \\times 1\\)  convolutions). We used an adaptive learning rate scheme. The learning rate was initially set to 0.01 and then decreased by a factor of 5 if no increase in performance was observed on the evaluation set, over a predefined number of epochs which we refer to as epoch patience (\\(E_p\\) ). The initial value of \\(E_p\\)  was set to 10. We increased this value by 20% after each drop incidence in the learning rate. This prevented the network from dropping the learning rate too fast at lower learning rates. The weights of our networks were initialized using the strategy by He et al. [13].\n", "The data for training our CNN II exhibits high class imbalance in its distribution (there exists considerably more normal stroma than cancer associated stroma). Although we tried to increase the capacity of the network in learning discriminative features to distinguish the minority class by uniformly sampling the data for each mini-batch, we may fall into the risk of training a very sensitive mode. Because the class distribution in each mini-batch does not represent the actual skewed distribution of the data, a small number of false positives in each mini-batch may translate to vast regions of false positives in the actual WSI. To ameliorate this effect, besides uniform sampling in each mini-batch, we gradually increased the missclassification loss for the normal class. The loss weight factor for the negative samples was initially set to 1, and multiplied by 1.0034 after each epoch (the weight factor becomes 2 by epoch 200). This ensured that the network learns discriminative features from the beginning of the training process and gradually learns the class distribution of the data as well.\n"]}
{"id": "1703.02504", "categories": "cs.CL cs.IR cs.LG", "paragraphs": ["Preprocessing and Word Embeddings.\nThe word embeddings are learned on an unsupervised corpus containing \\(300M\\) \u00a0tweets. We apply a skip-gram model of window-size\u00a05 and filter words that occur less than 15 times\u00a0[31]. The dimensionality of the vector representation is set to \\(d=52\\) . Our experiments showed that using a larger dimension did not yield any significant improvement.\n"]}
{"id": "1711.11310", "categories": "cs.CL", "paragraphs": ["We set the number of units in LSTM cell as 128. The default LSTM forget gate bias is set to 1. Word embedding size is set as 128. We randomly initialize the word embedding matrix and fine-tune it with other model parameters during model training. Dropout rate 0.5 is applied to the non-recurrent connections during model training for regularization. Maximum norm for gradient clipping is set to 5. We perform mini-batch training (batch size 16) using Adam optimization with initial learning rate of 1e-3.\n"]}
{"id": "1702.07191", "categories": "cs.CV", "paragraphs": ["From the experimental result, we can see that pretraining using our cleansed datasets, VGR-Dense and VGR-Frequent, performs better than the pretraining on ImageNet. When the class label is used as the target, pretraining using the VGR-Frequent dataset has 22.28% Rec@100 on visual relationship detection, is the best choice, with 2.27% gain when compared with the baseline. Pretraining using the VGR-frequent dataset slightly outperforms pretraining using the VGR-Dense dataset, with 0.32% Rec@100 improvement. Category labels perform better than word vector labels for pretraining, with 0.96% Rec@100 improvement on visual relationship detection when using the VGR-frequent dataset. Besides, with word vector supervision, the two datasets have similar performance.\n"]}
{"id": "1710.05956", "categories": "cs.CV", "paragraphs": ["To initialize the weights of the network, we adopted the strategy proposed in [16] that allows very deep architectures to converge rapidly. In this strategy, a zero-mean Gaussian distribution of standard deviation \\(\\sqrt{2/n_l}\\)  is used to initialize the weights in layer \\(l\\) , where \\(n_l\\)  denotes the number of connections to units in that layer. Momentum was set to 0.6 and the initial learning rate to 0.001, being reduced by a factor of 2 after every 5 epochs (starting from epoch 10). Network parameters are optimized via the RMSprop optimizer, with cross-entropy as cost function. The network was trained for 30 epochs, each one composed of 20 subepochs. At each subepoch, a total of 1000 samples were randomly selected from the training images and processed in batches of size 5.\n"]}
{"id": "1703.04746", "categories": "cs.DL", "paragraphs": ["We apply this model to a database of 151,082 papers from the Physical Review journals. For each paper, we extract the three parameters of the model: \\(b\\)  (proportional to the initial citation velocity \\(br\\) ), \\(a\\)  (a `rich-get-richer' parameter), and \\(r\\) , a characteristic inverse time scale. We then cluster papers together in terms of those three parameters using the clustering method called DBSCAN.\n"]}
{"id": "1709.09783", "categories": "cs.CL", "paragraphs": ["Our neural network models are implemented using TensorFlow\u00a0[0]. We use a siamese BiRNN with a single layer in each direction with 512-dimensional word embeddings and 512-dimensional recurrent states. We use GRU as recurrent activation functions since they consistently outperformed LSTM by a small margin in our experiments. The hidden layer of the fully connected layers has 256 hidden units. We initialize all parameters uniformly using TensorFlow's default uniform unit scaling initialization, except for all biases being initialized to zero. To train our models, we use Adam optimizer\u00a0[12] with a learning rate of 0.0002 and a minibatch of 128 examples. Models are trained for a total of 15 epochs. To avoid exploding gradients, we apply gradient clipping such that the norm of all gradients is no larger than 5\u00a0[20]. We apply dropout to prevent overfitting with a probability of 0.2 and 0.3 for the non-recurrent input and output connections respectively. Training is performed on a single GPU.\n"]}
{"id": "1702.04125", "categories": "cs.CV", "paragraphs": ["During training, we use the Adam optimizer\u00a0[4], with \\(L_2\\)  loss and dropout rate set to 80% for training.\nTraining is performed up to 500,000 epochs with randomized minibatches consisting of 16 samples, where each sample contains one input image at current relative time \\(t_0=0\\) , a temporal displacement \\(\\Delta t\\)  and the real target frame at the desired temporal displacement \\(\\Delta t\\) .\nOn a Titan X GPU, training took approximately 16 hours with, on average, about 100,000 training samples (varying in each action category).\nWe argue that the type of action can be automatically detected, and is better incorporated by training a network per action category.\nThus, we opt to perform separate preliminary experiments for each action instead of training one heavy network to anticipate video frames corresponding to all the different possible actions.\n"]}
{"id": "1704.01792", "categories": "cs.CL", "paragraphs": ["We use the same vocabulary for both encoder and decoder.\nThe vocabulary is collected from the training data and we keep the top 20,000 frequent words.\nWe set the word embedding size to 300 and all GRU hidden state sizes to 512.\nThe lexical and answer position features are embedded to 32-dimensional vectors.\nWe use dropout [19] with probability \\( p = 0.5 \\) .\nDuring testing, we use beam search with beam size 12.\n"]}
{"id": "1702.01478", "categories": "cs.CV", "paragraphs": ["For each mini-batch, we randomly pick two training images and from each image, we randomly select 16 foreground samples and 48 background samples, resulting in 128 samples in one mini-batch. The glimpse generation layer is initialized from zero-mean Gaussian distributions with standard deviations 0.0001. The glimpse generation layer does not have a bias term. All the recurrent layers are initialized from zero-mean Gaussian with standard deviations 0.01 and the biases are set to 0. The fully connected layer applied to the glimpse vectors have 32 output neurons. We multiply the return by 0.1 to control the balance against the classification loss and regression loss.\n"]}
{"id": "1703.02921", "categories": "cs.CV", "paragraphs": ["We use rendered images of 3D models from ShapeNet\u00a0[3] both for training and testing. We use the entire car category (7497 models) and a subset of the chair category (698 models) with sufficient texture. For each model, we render images from a total of 54 viewpoints corresponding to 3 different elevations (0, 10, and 20) and 18 azimuth angles (sampled in the range \\([0,340]\\)  with 20-degree increments). The desired transformation is encoded as a 17-D one-hot vector corresponding to one of the rotation angles between input and output views in the range \\([20,340]\\) . Note that we did not encode 0 degree as it is the identical mapping. For each category, 80% of 3D models are used for training, which leaves over 5 million training pairs (input view-desired transformation) for the car category and \\(0.5\\)  million for the chair category. We randomly sample input viewpoints, desired transformations from the rest 20% of 3D models to generate a total of \\(20,000\\)  testing instances for each category. Both input and output images are of size \\(256\\!\\times \\!256\\!\\times \\!3\\) .\n"]}
{"id": "1706.02493", "categories": "cs.CV", "paragraphs": ["In all experiments, we fixed the weights \\(\\alpha =1\\)  and \\(\\beta = 0.00025\\)  in Equation (REF ). We explore the influence of the rare class ratio \\(\\rho \\)  in Section REF  and the region size \\(R\\)  in Sections REF  and REF . In other experiments we fixed \\(\\rho =93\\%\\)  and \\(R=129\\) .\n"]}
{"id": "1707.08976", "categories": "cs.CL", "paragraphs": ["We reimplemented the generative model described in [2] and trained it on the Penn Treebank [8] using their published hyperparameters and preprocessing. However, rather than selecting the final model based on reranking performance, we instead perform early stopping based on development set perplexity. We use sections 2-21 of the Penn Treebank for training, section 22 for development, and section 23 for testing. The model's action space consists of 26 matching pairs of Open and Close actions, one for each nonterminal, and 6,870 Shift actions, one for each preprocessed word type. While we use this particular model for our experiments, we note that our subsequent discussion of inference techniques is equally applicable to any generative parser that adheres to the framework described above in Section\u00a0.\n"]}
{"id": "1705.02735", "categories": "cs.CL cs.CY", "paragraphs": ["All the models in our experiments are trained on the Trafficking-10k designated training set and tested on the designated test set.\nHyperparameter evaluation is performed using a subset of training set as validation set.\nThe HTDN model is trained using the Adam optimizer [6].\nThe neural weights were initialized randomly using Xavier initialization technique [3].\nThe random forest model uses 10\u00a0estimators, with no maximum depth, and minimum-samples-per-split value of 2.\nThe linear SVM model uses an \\(\\ell _2\\) -penalty and a square hinge loss with \\(C = 1\\) .\n"]}
{"id": "1706.01433", "categories": "cs.CV", "paragraphs": ["\nTraining steps: \\(5\\cdot 10^5\\)\n\nBatch Size: 4\n\nGradient Descent Optimizer: Adam, learning rate \\(5\\cdot 10^{-4}e^{-t / \\alpha }\\)  where \\(\\alpha = 1.5\\cdot 10^5\\)  and \\(t\\)  is the training step.\n\nRollout frame temporal discount (factor by which future frames are weighted less in the loss): \\(1 - \\gamma \\)  with \\(\\gamma = e^{-t / \\beta }\\)  where \\(\\beta = 2.5\\cdot 10^4\\)  and \\(t\\)  is the training step.\n\n"]}
{"id": "1703.03126", "categories": "cs.CV", "paragraphs": ["All SRCNNs are trained with the same set of parameters, selected using those found to work well by Dong et al.\u00a0[9]. Layer 1 consists of 64 filters of 9x9 kernels, layer 2 consists of 32 fully connected neurons (1x1 filters), and the output layer uses a 5x5 kernel (see Figure\u00a0REF ). Higher resolution models which have a greater number of sub-images may gain from larger kernel sizes and an increased number of filters. Each network is trained using Adam optimization\u00a0[19] with a learning rate of \\(10^{-4}\\)  for the first two layers and \\(10^{-5}\\)  for the last layers. Each model was trained for \\(10^{7}\\)  iterations with a batch size of 200. Tensorflow\u00a0[0] was utilized to build and train DeepSD. Training harnessed three Titan X GPUs on an NVIDIA DIGITS DevBox by independently training each SRCNN. Inference was then executed sequentially on a single Titan X GPU on the same machine.\n"]}
{"id": "1709.07902", "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "paragraphs": ["For the Seq2Seq-FHVAE model, each \\(LSTM\\)  network consists of one layer with 256 hidden units, while each \\(MLP\\)  network is one layer with the output dimension equal to the variable whose mean or log variance the \\(MLP\\)  parameterizes, and variances \\(\\sigma ^2_{\\mathbf {z}_1} = \\sigma ^2_{\\mathbf {\\mu }_2} = 1\\) , \\(\\sigma ^2_{\\mathbf {z}_2} = 0.25\\) . We experiment with various dimensions for the latent variable \\(\\mathbf {z}_1\\)  and \\(\\mathbf {z}_2\\) . All models were trained with stochastic gradient descent using a mini-batch size of 256 to minimize the negative discriminative segment variational lower bound plus an \\(L2\\) -regularization with weight \\(10^{-4}\\) . The Adam\u00a0[20] optimizer is used with \\(\\beta _1 = 0.95\\) , \\(\\beta _2 = 0.999\\) , \\(\\epsilon = 10^{-8}\\) , and initial learning rate of \\(10^{-3}\\) . Training continues for 500 epochs unless the segment variational lower bound on the development set does not improve for 50 epochs. The \\(\\mathbf {\\mu }_2\\)  for the sequences in the development set and the test set is estimated using the closed form solution in Section 2.2.\n"]}
{"id": "1703.03906", "categories": "cs.CL", "paragraphs": ["All of the following experiments are run using our own software framework based on TensorFlow [0]. We purposely built this framework to enable reproducible state-of-the-art implementations of Neural Machine Translation architectures. As part of our contribution, we are releasing the framework and all configuration files needed to reproduce our results. Training is performed on Nvidia Tesla K40m and Tesla K80 GPUs, distributed over 8 parallel workers and 6 parameter servers per experiment. We use a batch size of 128 and decode using beam search with a beam width of 10 and the length normalization penalty of 0.6 described in [22]. BLEU scores are calculated on tokenized data using the multi-bleu.perl script in Moseshttps://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl. Each experiment is run for a maximum of 2.5M steps and replicated 4 times with different initializations. We save model checkpoints every 30 minutes and choose the best checkpoint based on the validation set BLEU score. We report mean and standard deviation as well as highest scores (as per cross validation) for each experiment.\n"]}
{"id": "1710.11354", "categories": "cs.CV", "paragraphs": ["For an illustration, see Fig.REF . There are a total of \\(N=20\\)  agents present in the scene. Estimation of the row of matrix \\(\\mathbf {A}\\)  corresponding to agent p requires 50 previous frames (assuming \\(L=2.5N\\) ) whereas the neighborhood based estimation reduces this to 23. Also consider a case where agents p and r interact with each other but are not within the spatial proximity owing to neighborhood constraint. The interaction is captured when intersection of neighborhoods of p and r has at least one interacting agent, in this case its q who is in the spatial proximity of both.\n"]}
{"id": "1705.06599", "categories": "cs.CV", "paragraphs": ["For the order of Grassmann manifold \\(p\\) , which is bounded by the number of images in an image set \\(M\\) , we test its effects on the algorithm performance for all datasets, as shown in Fig. REF . These experimental results demonstrate that the order of Grassmann manifold has little impact on the clustering accuracy in most cases, especially for small p. For convenience and fairness, we set \\(p = 10\\)  in our experiments except for Ballet dataset where \\(p = 6\\) .\n{FIGURE}", "The error tolerance \\(\\varepsilon \\)  is also an important parameter in controlling the terminal condition, which bounds the allowed reconstructed error. We experimentally seek a proper value of \\(\\varepsilon \\)  to make the iteration process stop at an appropriate level of reconstructed error. Here we set \\(\\varepsilon = 1.0\\times 10^{-5}\\)  for all experiments.\n"]}
{"id": "1707.07469", "categories": "cs.CL cs.LG", "paragraphs": ["The BiLSTM encoder layer use 300D hidden states, thus 600D as it\u2019s a bidirectional encoder. Dropout\u00a0[16] is implemented with a dropout rate of 0.2 to prevent the model from overfitting. Parameter weights for premise encoder and hypothesis encoder are shared using siamese architecture. The Adam optimizer\u00a0[11] is used for training with backpropagation.\n", "The model has been implemented using Keras and we have released the code https://github.com/yanghanxy/CIAN. The training took approximately one hour for one epoch on GeForce GTX TITAN, and we stopped training after 40 epochs as an early stopping regularization.\n"]}
{"id": "1708.02932", "categories": "cs.CV", "paragraphs": ["Using the same setup and \\(\\gamma /\\mu \\)  values as in Fig. REF , in Fig. REF  we plot mAP as a function of \\(\\gamma /\\mu \\)  on three datasets. Note that the optimal performance (at \\(\\gamma /\\mu =1\\) ) for this architecture is obtained for an operating point that makes better use (closer to uniform) of the support of the blocks, as exemplified by the corresponding curves (pink) on the right in Fig. REF . This supports one of our original motivations that fostering uniformity of the support would encourage the system to use the support to encode semantic information.\n"]}
{"id": "1712.05884", "categories": "cs.CL", "paragraphs": ["To train the feature prediction network, we apply the standard\nmaximum-likelihood training procedure (feeding in the\ncorrect output instead of the predicted output on the decoder side, also\nreferred to as teacher-forcing)\nwith a batch size of 64 on a single GPU. We use the Adam optimizer\n[28] with\n\\(\\beta _1=0.9, \\beta _2=0.999, \\epsilon =10^{-6}\\)  and a learning rate of\n\\(10^{-3}\\)  exponentially decaying to \\(10^{-5}\\)  starting after 50,000 iterations.\nWe also apply \\(L_2\\)  regularization with weight \\(10^{-6}\\) .\n", "We train with a batch size of 128 distributed across 32 GPUs\nwith synchronous updates, using the Adam optimizer with\n\\(\\beta _1=0.9, \\beta _2=0.999, \\epsilon =10^{-8}\\)  and a fixed learning rate of\n\\(10^{-4}\\) . It helps quality to average model weights over recent updates. Therefore\nwe maintain an exponentially-weighted moving average of the network parameters\nover update steps with a decay of 0.9999 \u2013 this version is used for inference\n(see also [28]).\nTo speed up convergence, we scale the waveform targets by a factor of \\(127.5\\) \nwhich brings the initial outputs of the mixture of logistics layer closer to\nthe eventual distributions.\n"]}
{"id": "1707.09725", "categories": "cs.CV", "paragraphs": ["If the dataset does not define a training/test set, a stratified\n67 / 33 split is applied. If the dataset does not\ndefine a validation set, the training set is split in a stratified manner into\n90 training set / 10 test set.\n"]}
{"id": "1711.03167", "categories": "cs.LG", "paragraphs": ["Tbl. REF  lists the hyper parameters choice. Note that for smaller datasets (i.e., Horse, MSR-SenseCam), we can directly train OrderNet on entire dataset. In other words, \\(b_o = b\\)  and \\(t = 1\\) . Note that the number of frames is reduced to 20 and 30 for ballet fouette and tennis serve actions, respectively.\n"]}
{"id": "1710.10398", "categories": "cs.CL", "paragraphs": ["All models are trained on a single Titan X GPU with two supporting CPU threads, using TensorFlow r1.1 [20] and optimized using Adam [21] with a mini-batch size of 64 for LSTM (BasicLSTMCell) models and 32 for CNN models (unless otherwise mentioned). For the LSTM models, we use a learning rate of 0.001. For the CNN models, a smaller learning rate of 0.0002 was preferred. The learning rate is decayed by 5% whenever validation loss doesn't decrease over two epochs. We report average training time per epoch for each model as both wall-clock hours (\\(t_{wc}\\) ) and CPU-hours (\\(t_{cpu}\\) ).\n"]}
{"id": "1705.10500", "categories": "cs.LG", "paragraphs": ["This section addresses the parameter estimation of \\(p(\\theta |\\mathbf {y})\\) , which comprises the parameter estimation of \\(p(\\theta )\\) , the estimation of the variances \\(\\sigma _{{s}_i}^2\\) , \\(\\forall i\\) , and the estimation of the covariance \\(\\mathbf {\\Sigma }_\\mathbf {\\eta }\\) . Let \\(\\mathbf {U}_{\\cdot j}\\)  denote the sparsity pattern of the sparse code \\(\\mathbf {A}_{\\cdot j}\\) , \\(j=1, \\ldots , B\\) . The \\(i\\) th element of \\(\\mathbf {U}_{\\cdot j}\\)  is defined as \\({U}_{ij}=\\textbf {1}_{\\text{supp}(\\mathbf {A}_{\\cdot j})}(i)\\) , where \\(\\text{supp}(\\mathbf {A}_{\\cdot j})\\)  denotes the support of \\(\\mathbf {A}_{\\cdot j}\\) . The set of vectors \\(\\mathbf {U}=[\\mathbf {U}_{\\cdot 1} \\ldots \\mathbf {U}_{\\cdot B}]\\)  can be employed to model a prior distribution for the sparsity pattern of signals belonging to the same class as the training data. Since the signal to be reconstructed, \\(\\mathbf {x}\\) , belongs to this class, the set of column vectors of \\(\\mathbf {U}\\) , which are referred to as training dataset II, are used to learn the parameters of \\(p(\\theta )\\) . As mentioned in Section REF , RBMs and DBNs are used for modeling such a prior distribution.\n", "For the estimation of the covariance matrix \\(\\mathbf {\\Sigma }_{\\mathbf {r}}\\) , independence is assumed between the representation error coefficients \\({r}_i\\)  and \\({r}_j\\)  for \\(i\\ne j\\) . Therefore, the covariance matrix \\(\\mathbf {\\Sigma }_\\mathbf {r}\\)  is a diagonal matrix, whose diagonal is formed by the variances of the representation error coefficients \\(\\sigma _{{r}_i}^2\\) , for \\(i=1, \\ldots , N\\) . For the estimate of \\(\\mathbf {\\Sigma }_\\mathbf {r}\\) , denoted as \\(\\hat{\\mathbf {\\Sigma }}_\\mathbf {r}\\) , the representation error of the learned dictionary \\(\\mathbf {E}=[\\mathbf {E}_{\\cdot 1} \\ldots \\mathbf {E}_{\\cdot B}]\\)  is employed. More precisely, each \\(i\\) th diagonal element of \\(\\mathbf {\\Sigma }_\\mathbf {r}\\)  is estimated as\n\\(\\hat{\\sigma }_{{r}_i}^2=\\frac{1}{B} \\sum _{j=1}^B {E}_{ij}^2.\\) \n"]}
{"id": "1705.04267", "categories": "cs.CV stat.ML", "paragraphs": ["Images from 7 patients were used as the training datasets and the CNN denoisers were trained on 2D slices. There were 3,933 slices in total in the training dataset, and 150 patches with size of \\(40\\times 40\\)  were randomly extracted from each slice at each cascade. The order of the patches were shuffled within each patient but there was no inter-patient shuffle. The patches were divided by a factor of 512 Housefield units (HU) for normalization.\n", "The convolution kernels in a single denoising CNN was \\(3\\times 3\\)  with an output feature map with 64 channels except for the last layer, where there was only 1 channel. Zero padding was used to keep the input and output of the same size. The weights of the convolution kernels were initialized with Xavier weightings and 0 biases [8]. 3 different depths of CNN were used in the study, where the number of \"Conv+BN+ReLU\" modules in Figure\u00a0REF  were 5, 10, and 15. The CNNs were denoted as CNN5, CNN10, and CNN15 respectively.\n", "The CNN in each cascade were trained with Adaptive Moment Estimation (ADAM) method [18] with a minibatch of 100 patches. It was trained with a weight penalty of \\(10^{-4}\\) . The parameters for ADAM was chosen as suggestions in the original work, except that the learning rate was set to \\(10^{-4}\\) . A total iteration number of 90,000 was used for training, which was equivalent to approximately 15 epoches on the training dataset. The neural network were implemented with Caffe [15].\n"]}
{"id": "2010.01165", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["\nMisspelled words were fixed only when 1 change away from the correct word for words under 6 characters, and 2 changes away for words above 6 characters.\n\nFor each concept we calculate long and short embeddings and take the average of both. The long embedding takes into account s = 9 words from left and right (as shown in Equation 2). The short embedding takes into account s = 2 words from left and right. The exact numbers for s were calculated by testing the performance of all possible combinations for s in the range [0, 10].\n\nThe context similarity threshold used for recognition is 0.3 unless otherwise specified. This means for a given concept candidate, or sequence of words, to be recognised and linked to the given concept the concept similarity provided by Equation 2 would be greater than 0.3.\n\n", "We train MedCAT self-supervised over MIMIC-III using the entirety of UMLS, 3.82 Million concepts from 207 separate vocabularies. We use \u00a02.4M clinical notes (nursing notes, notes by clinicians, discharge reports etc.) on a small one-core server taking approximately 30 hours to complete.\n"]}
{"id": "2010.07489", "categories": "cs.LG", "paragraphs": ["In the baseline experiments of the main paper, the training of the defenseless DNNs is performed on the poisoned CIFAR-10 training set. CIFAR-10 contains 60k color images with size \\(32\\times 32\\)  evenly distributed in 10 classes. The training set contains 50k images (5k per class) and the test set contains the remaining 10k images. Note that in our experiments, the backdoor training images are crafted using clean training images of the CIFAR-10 training set due to the limited number of training images. In practice, an attacker may not have access to the training set. In other words, the originally clean images of the backdoor training images are collected by the attacker and are not typically present in the training set. Hence in our experiments, once a training image from the CIFAR-10 training set is used to create a backdoor training image, its clean, backdoor-free version will no longer appear in the training set. Theoretically, our training set cleansing problem can be much easier without this setting. Suppose \\(f(\\cdot ;\\Theta ^{\\ast })\\)  is the DNN trained on the poisoned training set where the backdoor pattern is \\({\\bf v}^{\\ast }\\) . If for any backdoor training image \\({\\bf x}\\) , its clean version \\(\\tilde{\\bf x}\\)  also exists in the training set, we will have \\(p_s(\\tilde{\\bf x};\\Theta )\\)  very close to 1 for some \\(s\\in {\\mathcal {S}}^{\\ast }\\)  where \\({\\mathcal {S}}^{\\ast }\\)  is the set of source classes. Then \\({\\bf v}^{\\ast }\\)  will be associated with a high peak of objective of (9) in the main paper.\n", "For both DNN architectures, our training is performed for 100 epochs, with batch size 32 and learning rate 0.001. Adam optimizer is used with decay rate 0.9 and 0.999 for the first and second moment, respectively. Training data augmentation is used, including random crop, random horizontal flip, and random rotation.\n"]}
{"id": "2007.08979", "categories": "cs.CV", "paragraphs": ["Each training image is first resized to 256\\(\\times \\) 256, cropped to 224\\(\\times \\) 224, and flipped horizontally at random for data augmentation.\nIt is then degraded on-the-fly by the synthetic distortion generator of the ImageNet-C\u00a0[10], where the type and intensity of distortion are chosen at random.\nNote that only the 15 seen corruptions are used during training as described in Sec.\u00a0REF .\nFor optimization, we employ Adam\u00a0[12] with learning rate 0.001, and decay the learning rate every 8 epochs by 10.\nOur model is trained for 30 epochs with mini-batches of size 112.\n"]}
{"id": "2012.01955", "categories": "cs.CV cs.CY cs.MM", "paragraphs": ["During training we applied random cropping and horizontal flipping in order to make the model less prone to overfitting. Each model has been fine-tuned using a weighted cross entropy loss and an Adam optimizer with a learning rate of 1e-4 and a weight decay of 5e-4.\nWe set the batch size to 32 for the training of the IMAGO classifier and to 64 for the training of the IMAGO-FACES and IMAGO-PEOPLE models.\n"]}
{"id": "2009.00681", "categories": "cs.CV cs.AI", "paragraphs": ["We down-sampled all the videos to 1 fps. We kept parameters identical for the SSM-LSTM and LSTM for a fair comparison. The LSTM hidden state was 64-dimensions. During training, batch size was set to 32. An Adam [44] optimizer was applied with a learning rate of 0.0025. The number of training epochs was 20. We first trained the CNN model for 20 epochs then fine-tuned the CNN model during temporal model training. For Gabor feature calculation, 10 different scales, \\(\\sigma \\) , ranging from 10 to 30 frames, were applied. After that, features of different sizes were concatenated together.\n"]}
{"id": "2007.06755", "categories": "cs.CV", "paragraphs": ["Our joint-based model was created by adding joints and painting skinning weights to a generic base mesh in Blender. We exported the model to .npz file and fit the parameters on Tensorflow. We use AdamOptimizer throughout the training. We fit joint transformations using a learning rate of \\(10^{-3}\\)  for 3k iterations. We fit the global skinning weights across the training set using a learning rate of \\(10^{-4}\\)  and batch size of 85 for 3k iterations. We fit joint transformations and skinning weights iteratively for 5 cycles. On a single Nvidia V100, fitting joint transformations took 1 minute per scan (\\(\\sim \\) 1.5 hours on training set), and fitting the skinning weights for 5 cycles took \\(\\sim \\) 20 hours. Neural skinning weights were trained using a learning rate of \\(10^{-3}\\)  during the first phase of training the autoencoder, and learning rate of \\(10^{-4}\\)  during the second phase of fine-tuning the decoder with a batch size of 85 for 30k iterations. It took \\(\\sim \\) 40 hours to train neural skinning weights on a V100 GPU. At test time, we built the correspondence on point clouds using NICP. We fit identity by fitting joint transformation for 500 iterations and \\(Z\\)  for 500 iterations for 2 cycles, then fit expressions for 500 iterations, then repeated the iterative fitting between identity and expressions for 2 cycles. The fitting took \\(\\sim \\) 2 minutes with a Nvidia 1080Ti.\n"]}
{"id": "2004.00567", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["The approach of using annealing training parameters can be questioned in general.\nIts intention is to optimize the model stronger in the beginning and fine-tune it towards the end using a lower bound threshold.\nThis bound was not implemented in this approach and therefore the learning rate, the clip range, and the entropy bonus coefficient equal 0 at the end of the training.\nAs new subtasks are continually introduced in the environment dependent on reaching higher floors, the agent is required to explore new ways to act, based on how its performance is changing due to environmental shifts.\nMaking training parameters subject to adaptation from data would further increase the agent's ability to deal with changing tasks.\n"]}
{"id": "2011.09011", "categories": "cs.CV cs.LG", "paragraphs": ["We use the sandwich sampling rule and always train the smallest and biggest sub-networks in the search space as regularization (see Eqn.\u00a0(REF )).\nWe set \\(n=2\\)  in Eqn.\u00a0(REF ). This way, at each iteration, a total of 4 sub-networks are evaluated.\nWe use in-place knowledge distillation, i.e.,\nall smaller sub-networks are supervised by the largest sub-network.\nTo handle different input resolutions,\nwe always fetch training patches of a fixed size (e.g., 224x224 on ImageNet) and then rescale them to our target resolution with bicubic interpolation.\n", "We use SGD with a cosine learning rate decay. All the training runs are conducted with 64 GPUs and the mini-batch size is 32 per GPU.\nThe base learning rate is set as 0.1 and is linearly scaled up for every 256 training samples.\nWe use AutoAugment [8] for data augmentation and set label smoothing coefficient to \\(0.1\\) .\nUnless specified, we train the models for 360 epochs.\nWe use momentum of 0.9, weight decay of \\(10^{-5}\\) ,\ndropout of \\(0.2\\)  after the global average pooling layer, and stochastic layer dropout of \\(0.2\\) . We don't use synchronized batch-normalization.\nFollowing [42], we only enable weight decay and dropout\nfor training the largest DNN model. All other smaller sub-networks are trained without regularization.\n"]}
{"id": "2002.01523", "categories": "cs.LG stat.ML", "paragraphs": ["For the experiment in Figure\u00a0REF  the depth 32 networks were trained with learning rates of \\(0.1\\)  for batch normalization and \\(0.009\\)  for NormReLU. The depth 110 networks were trained with learning rates of \\(0.05\\)  for batch normalization and \\(0.005\\)  for NormReLU. To compare with fixup initialization as in Figure\u00a0REF  we use the fixup-110 architecture as proposed in the work of\u00a0[85]. We switch on data augmentation as used in\u00a0[85] and train with fixup initialization using a learning rate of \\(0.1\\)  and the learning rate schedule as proposed by the authors in\u00a0[85]. We train with NormReLU using a learning rate of \\(0.005\\) . For the experiment in Figure\u00a0REF  we use a learning rate of \\(0.001\\)  for depth 32 and of \\(0.0005\\)  for depth 64. The same learning rate is used for both NormReLU and SeLU. Finally, for the experiment in Figure\u00a0REF  we use a learning rate of \\(0.01\\)  at depth 32 and of \\(0.009\\)  at depth 110 and keep it the same for both the activations.\n"]}
{"id": "2012.12206", "categories": "cs.LG cs.CV", "paragraphs": ["We evaluate FracNN on both CIFAR-10\u00a0[23] and ILSVRC12 ImageNet\u00a0[21] classification datasets. We augment the input images using random horizontal flip and random crop. Color jitter is used only for ImageNet. We follow the two-step training strategy as described in Real-to-Binary Net\u00a0[31]. In the first step, the activations are binarized but the weights are floating-point. The weight decay is 1e-5. In the second step, weights are binarized and initialized from the first step. Activations are still binary. The weight decay is zero. To train the FracBNN, the first two steps are the same except that the activations are quantized to 2 bits. We add a third step that initializes the weights from the second step, and applies the fractional convolutional layer. The first and the last layer are excluded. We use cross-entropy loss during the training on CIFAR-10. For ImageNet, we calculate the KL divergence between the softmax output of a teacher model and that of the trained model as the loss function, same as ReActNet\u00a0[28]. In our experiments the teacher model is a pretrained ResNet-50.\n", "For CIFAR-10, we train the model for 300 epochs in each training step with a batch size of 128. The initial learning rate is 1e-3, and decays linearly to 0 in each epoch. For ImageNet, we train the model for 120 epochs in each step. The batch size is 256. The initial learning rate is 5e-4 and also decays linearly to 0 in each epoch. We use PyTorch\u00a0[34] to specify models and training scripts. All training experiments are completed on NVIDIA RTX 2080Ti GPUs.\n"]}
{"id": "2012.01542", "categories": "cs.CV", "paragraphs": ["For all the datasets, DLib\u00a0[25] is considered to detect and align faces, as well as extracting 68 landmarks. We train the model on the CASIA-WebFace\u00a0[55] dataset. In the training set, for each image, the image from a different ID that provides closest landmarks to its landmarks in terms of \\(L_2\\)  norm is selected. Neighbor face is transformed spatially using Equation\u00a0REF . This image is aligned again to compensate for the displacements caused by the spatial transformation. All images\nare resized to \\(112 \\times 112\\)  and pixel values are scaled to \\([-1, 1]\\) .\n", "The initial value for the learning rate is set to\n\\(0.1\\)  and multiplied by \\(0.9\\)  in intervals of five epochs until its value is less than or equal to \\(10^{-6}\\) . The model is trained for 600K iterations. We select \\(\\alpha _g= 9.4\\) , \\(\\lambda ^1_a = 1.3\\) , and \\(\\lambda ^1_g = 0.75\\) . For training the network using Equation\u00a0REF , each fully-connected layer of size 256 is fed to a fully-connected of size 64, and then to a single unit. Here, considering \\(\\lambda ^2_a=\\lambda ^2_g=1\\) , the network is trained using the learning rate of \\(10^{-2}\\)  and is dropped similar to the rate mentioned above.\n"]}
{"id": "2011.13417", "categories": "cs.CV cs.GR", "paragraphs": ["We implemented our models in Pytorch[43]. Our models and sequences are small enough so we train on a single NVIDIA-V100 GPU with 32 GB memory. We use the Adam [27] optimizer, with a constant learning-rate of \\(10^{-4}\\) , and linear warmup for 500 iterations. The element generation model is trained for 40 epochs, while the other models are trained for 80 epochs. It takes approximately 6 hours to train for our largest model for constrained generation.\n"]}
{"id": "2012.10495", "categories": "cs.CV cs.LG", "paragraphs": ["In all experiments, we train the U-Net model to 10 epochs with an accumulated batch size of 64. We use the Adam optimizer [16] with an initial learning rate of 0.0001 that decays linearly after 5 epochs. Additionally, we utilize 16-bit precision training to increase training speed and reduce GPU memory consumption.\n"]}
{"id": "2005.12515", "categories": "cs.CL", "paragraphs": ["Our model is based on BERT model architecture [3], which includes a multi-layer bidirectional Transformer. In particular, we use the original BERT BASE configuration: 12 hidden layers, 12 attention heads, 768 hidden sizes. The total number of parameters in this configuration is 110M.\nAs per the original BERT pre-training objective, our pre-training objective consists of two tasks:\n", "For model optimization [42], Adam optimizer with \\(\\beta _1=0.9\\)  and \\(\\beta _2=0.98\\)  is used for 1.9M steps. The batch size is set to 32, and each sequence contains 512 tokens at most. Finally, the learning rate is set to 1e-4.\n"]}
{"id": "2006.12195", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["We train the fully connected DAG architectures 10 times for each dataset using different initialisation but the same learning regime. All models are trained for 100 epochs using the SGD algorithm with starting learning rate 0.1, momentum 0.9, batch size 128 and weight decay 1e-4. In the 80th and 90th epoch the learning rate is decreased by factor 10. For the retrain experiments we use the same settings but with different initialisation seed. All networks were trained using the GeForce RTX 2080\u00a0Ti graphic card.\n"]}
{"id": "2012.09608", "categories": "cs.LG cs.AI", "paragraphs": ["CSHC has six hyper parameters. We generate 50 trees using the number generator from\u00a0[16]. For each tree, we sample with repetition from the training set until we obtain a multi-set of samples which amounts to 80% of the total training set of unique samples. To create each tree, we use two times the square root of all features, chosen uniformly at random. The last three hyper parameters determine when we stop the hierarchical refinement of clusters. First, we enforce that at least 2 samples remain within each cluster. Second, we limit the depth of the trees to be at most 15. And finally, we stop refining the clusters when the improvement by an additional split drops below 2%.\n", "For the LP-based weighting scheme, we set \\(\\gamma =80\\) . The recourse threshold \\(\\rho \\)  is set to \\(0.5\\) , which means that we only trust a classifier selection method outright when the support for the highest ranked class is at least twice that of the second most supported class. Note that all these parameters are set to the same values for all benchmarks we consider in the experiments. Naturally, these hyper-parameters could be tuned for each benchmark individually, for example by means of a cross validation. To demonstrate the effectiveness of the method proposed, we leave all CSHC parameters and the parameters for the modifications we introduced at the same default values for all benchmarks.\n"]}
{"id": "2006.14374", "categories": "cs.CV cs.RO", "paragraphs": ["We used the following parameters for our method on both datasets.\n\\(c=0.01\\)  for IGNNS, \\(t=2.0\\)  [m] (meters) for the boundary derivation, and \\(\\lambda _s=0.2\\) , \\(\\lambda _a=1.6\\) , \\(\\lambda _d=0.2\\) , \\(w=\\bar{d}\\) , \\(\\tau _p=\\tau _q=1.0/\\sqrt{8}\\) , \\(\\tau _u=\\tau _v=1.0/\\sqrt{12}\\)  for the energy minimization.\n\\(N_{\\rm ransac}=1000\\)  and \\(t_{\\rm ransac}=0.2\\)  [m] for the ground detection.\nThe number of iterations is 400 for Komaba and 200 for KITTI.\nThis is because it took more iterations to converge for Komaba since the initial depth maps are sparser.\n", "We used the following parameters on baseline methods.\nFor JNN and Kopf et al's method [8], we used \\(\\alpha =100\\) , \\(\\beta =0.2\\) .\nFor ADT aided methods, based on our tuning, we used \\(a=5.0\\) , \\(b=0.5\\)  for [1], and \\(a=10.0\\) , \\(b=0.5\\)  for \u201cIGNNS+ADT\u201d, and the same parameters as our method for the energy minimization.\nFor Hirata et al.'s method [6], we used the manual semantic labels and motion stereo provided in the dataset and conducted parameter tuning to adjust to our sampling.\nFinally, for Ku et al's method [9], we used their publicly available implementation as is, which was tuned for the KITTI depth completion benchmark by the authors.\n", "Additionally, we used the following parameters for the pre-processing on KITTI.\n\\(r _{\\rm occ}= 256.0 * d^{-1}\\)  and \\(t_{\\rm occ}=2.0\\)  [m] for the occluded background filtering (see Appendix ).\n"]}
{"id": "2012.09364", "categories": "cs.LG cs.CR", "paragraphs": ["It is worth noticing that our proposal can be generalized to multi-parties and the situations that the data holders collaboratively calculate \\(i\\)  (\\(1 \\le i \\le L\\) ) hidden layers instead of the first hidden layer only. Therefore, the existing method [4] is one of our special cases, i.e., \\(\\mathcal {A}\\)  and \\(\\mathcal {B}\\)  collaboratively calculate all the neural networks using cryptographical techniques, without the aid of server.\n"]}
{"id": "2005.10043", "categories": "cs.CL", "paragraphs": ["We train all models with maximum likelihood estimation, and use label smoothing [50] with smoothing factor 0.1.\nThe optimizer is Adam [23] with learning rate 2, \\(\\beta _1\\) =0.9 and \\(\\beta _2\\) =0.998.\nWe also apply learning rate warmup over the first 8,000 steps and decay as in [51].\nGradient clipping with maximum gradient norm 2.0 is also utilized during training.\nAll models are trained on 4 GPUs (Tesla V100) for 500,000 steps with gradient accumulation every four steps.\nWe apply dropout with probability 0.1 before all linear layers in our models.\nThe number of hidden units in our models is set as 256, the feed-forward hidden size is 1,024, and the number of heads is 8.\nThe number of transformer encoding layers, graph encoding layers and graph decoding layers are set as 6, 2 and 8, respectively.\nThe parameter \\(\\sigma \\)  is set as 2.0 after tuning on the validation dataset.\nDuring decoding, we use beam search with beam size 5 and length penalty with factor 0.6.\nTrigram blocking is used to reduce repetitions.\n", "For the models with pretrained LMs, we apply different optimizers for the pretrained part and other parts as in [34].\nTwo Adam optimizers with \\(\\beta _1\\) =0.9 and \\(\\beta _2\\) =0.999 are used for the pretrained part and other parts, respectively.\nThe learning rate and warmup steps for the pretrained part are set as 0.002 and 20000, while 0.2 and 10000 for other parts.\nOther model configurations are in line with the corresponding pretrained LMs. We choose the base version of BERT, RoBERTa and XLNet in our experiments.\n"]}
{"id": "2006.10866", "categories": "cs.CV", "paragraphs": ["We utilize the Caffe [5] framework to train the ResNet101-FasterRCNN baseline models, and the Detectron [1] Caffe2-based framework to train the ResNeXt101-FasterRCNN-FPN models. We train our object detection models on a compute machine with eight Tesla V-100 GPUs.\nFor models trained in Caffe, we use a single GPU with a batch size of 1, which is the default setting.\nFor models trained in Detectron, we utilize a batch size of two images per graphics card, for a total effective batch size of 16, and use synchronized SGD.\n", "The embedding model is trained using PyTorch on one p3dn.24xlarge Amazon EC2 instance with eight Tesla V100 graphics cards. We use all 8 GPUs on the machine for DistributedDataParallel training, and we also use Nvidia's Apex libraryhttps://github.com/NVIDIA/apex for mixed precision training.\n", "The hyperparameters we use are mostly the same as\u00a0[23]. We replace our base model with ResNeXt101_32x8d\u00a0[18]. We use SGD with momentum of 0.9, weight decay of 1e-4, and gamma of 0.1. We start the training with base model frozen, and train the newly initialized parameters for 2 epochs with learning rate 0.6. We then train end-to-end with a batch size of 128 per GPU and apply gamma to reduce learning rate every 5 epochs for a total of 15 epochs of end-to-end training. During training, we apply horizontal mirroring, random crops, and color jitter to resized 256x256 images, while during testing we center crop to a 224x224 image from the resized image.\n"]}
{"id": "2007.15531", "categories": "cs.LG stat.ML", "paragraphs": ["Scalar \\(\\epsilon \\)  in\u00a0(REF ) is set to 10. The embedding dimensionality, \\(d\\) , is set to 64 and the hidden layer width \\(d_h\\)  for all fully connected layers is set to 128. The number of layers \\(L\\)  in the fully-connected TS model is equal to 3 and the number of blocks \\(R\\)  is equal to 2. We use weight decay of 1e-5 to regularize fully-connected layers. The model is trained using the Adam optimizer with default tensorflow settings and initial learning rate of 0.001 for 60 epochs. The learning rate is annealed by a factor of 2 every 6 epochs starting at epoch 43. One epoch consists of 800 batches of size 4 and the model takes the history of 12 points and predicts 12 points (60 min) ahead in one shot. Each training batch is assembled using 4 time points chosen uniformly at random from the training set and the histories of all nodes collected at each of the time points. METR-LA has 207 sensor nodes and in PEMS-BAY has 325, resulting in the batches consisting of \\(207\\cdot 4 = 828\\)  and \\(325\\cdot 4 = 1300\\)  time-series, respectively. The objective function used to train the network is MAE, averaged over all nodes and all forecasts within horizon \\(H=12\\) :\n\\(\\mathcal {L} = \\frac{1}{HN} \\sum _{i=1}^H \\sum _{v=1}^N |y_{v, T+i} - \\widehat{y}_{v, T+i}|.\\) \n"]}
{"id": "2005.13119", "categories": "cs.CL", "paragraphs": ["[leftmargin=*]\nGated Recurrent Units(GRU) [3]: we test hidden size from 200 to 600, dropout rate from 0.2 to 0.8, batch size in [32, 64, 128, 256].\n\nTextCNN [15]: we search the best performance in batch size in [32, 64, 128, 256], dropout rate from 0.3 to 0.7, kernel numbers, which is numbers of convolution kernels of each size type, from 100 to 600, kernel size in [(1,2,3),(3,4,5),(5,6,7),(7,8,9)].\n\nBERT [4]: we test learning rate in [2e-5, 3e-5, 5e-5], training epochs in [2.0, 3.0, 4.0] and batch size in [16, 32].\n\n", "[leftmargin=*]\nGRU-ITA: for GRU-ITA on MultiWoz, batch size is 32, hidden size is 300, dropout rate is 0.3. On DailyDialogue, batch size is 64, hidden size is 500, dropout rate is 0.5. On CCPE, batch size is 32, hidden size is 200, dropout rate is 0.8.\n\nTextCNN-ITA: for TextCNN-ITA on MultiWoz, batch size is 64, kernel numbers is 400, kernel size is (7,8,9), dropout rate is 0.3. On DailyDialogue, batch size is 32, kernel numbers is 400, kernel size is (5,6,7), dropout rate is 0.5. On CCPE, batch size is 64, kernel numbers is 600, kernel size is (5,6,7), dropout rate is 0.4.\n\nBERT-ITA: the maximum sequence length to 128, batch size is 32 and the number of training epochs is 3.0 to 4.0.\n\n"]}
{"id": "2007.08752", "categories": "cs.CV cs.LG eess.SP", "paragraphs": ["Stochastic gradient descent (SGD) is used as the optimizer in our experiments with 6000 burn-in mini-batches, 0.9 momentum, \\(1e^{-3}\\)  learning rate, \\(5e^{-4}\\)  weight decay, and mini-batch size 32. During the burn-in time, the learning rate increases gradually until it reaches the target learning rate\n\\(\\alpha _{current} = \\min \\lbrace \\alpha _{target} (\\frac{N_{batches}}{N_{burn in}})^4, \\alpha _{target}\\rbrace \\) \n"]}
{"id": "2008.13504", "categories": "cs.CV cs.RO", "paragraphs": ["where \\(\\mathcal {V}\\)  is the set of backprojected 3D points \\({_{B}}{\\mbf v}{_{}}\\)  in the frame \\(B\\) , \\(\\mathcal {I} = \\lbrace 0,1,2,3,4\\rbrace \\)  denotes the pyramid levels, \\({\\mathbf {\\xi }}_0\\)  is the predicted pose from the pose network and the other \\({\\mathbf {\\xi }}_i\\)  are the estimated poses at the final iteration of Gauss-Newton optimisations on the respective pyramid level. This formulation enables the network to learn both feature and uncertainty representations in an end-to-end fashion, without the need for a ground truth feature map or ground truth correspondences, and without requiring an explicit definition of the uncertainty model. We set the feature map channels to be 8. Note that the uncertainty is defined as a scalar value. We unroll the Gauss-Newton optimisation and train all the models together from scratch using ADAM\u00a0[30] for 30 epochs, with a learning rate initialized at 0.0005 and reduced at epochs [5, 10, 20]. When combining the ICP residual, we do a further fine-tuning for 10 epochs.\n"]}
{"id": "2008.10866", "categories": "cs.LG cs.IR cs.SI stat.ML", "paragraphs": ["\nWhen using Twitter-LDA to project user interactions to a cross-network topical space, we set the number of topics, \\(K^t\\)  to 60 using a grid search algorithm. The final results were not highly sensitive to minor changes in \\(K^t\\)  \\((\\approx \\pm 5)\\) .\n\nWe used a grid search algorithm and a two-fold cross validation setup to obtain \\(K\\)  and \\(\\lambda \\)  values, which were set to 60 and 0.5, respectively.\n\nBy configuring the model using the above parameters and using a grid search algorithm, we found optimum values for \\(\\beta \\)  and \\(\\gamma \\)  to be 0.8 and 0.3, respectively.\n\nThe learning rate \\(\\mu \\)  was set to a fairly small value, 0.001, to obtain the local minimum.\n\n"]}
{"id": "2008.10845", "categories": "cs.LG cs.AI cs.IR stat.ML", "paragraphs": ["We used Adaptive Moment Estimation (Adam)  for optimizations since Adam adaptively updates the learning rate during training. We set the initial learning rate to 0.1, a fairly large value for faster initial learning before the rate is updated by Adam. We used only one hidden layer for all neural architectures, and given the size of the output layer \\(H_L\\) , the size of the hidden layer was set to \\(H_L\\times 2\\)  to reduce the number of hyper-parameters. We used the dropout regularization technique and the dropout was set to 0.4 during training to prevent neural networks from overfitting.\n"]}
{"id": "2002.06810", "categories": "cs.CV", "paragraphs": ["To have a fair comparison, we follow the setting in\u00a0[48] to conduct the image compression experiment. Each image in the training dataset was decomposed into \\(32\\times 32\\)  non-overlapping patches and we train 200 epochs in total. The architecture of RNN used in the following experiments is the same as that described in\u00a0[48], and networks were trained using the PyTorch toolbox. As for the subsequent CNN \\(\\mathcal {F}(\\cdot )\\)  (see Fcn.\u00a0REF ) for extracting visual features of original and compressed images, we used the ResNet-18 network\u00a0[17] which shows excellent performance on visual recognition tasks (e.g., an \\(89.1\\%\\)  top-5 accuracy and an \\(69.8\\%\\)  top-1 accuracy on the ILSVRC 2012 dataset with 1000 different labels). All parameters in this network were pre-trained on the ImageNet dataset and will be fixed in the following experiments. Note that, \\(\\mathcal {F}(\\cdot )\\)  used here consists of the first 14 convolutional layers in ResNet-18, since the size of the input image is much smaller than that in the original ResNet-18. In specific, for a given input image size of \\(32\\times 32\\) , \\(\\mathcal {F}(\\cdot )\\)  outputs a 512-dimensional feature.\n"]}
{"id": "2002.09843", "categories": "cs.LG cs.CR cs.CV stat.ML", "paragraphs": ["Theorem 4 \nFor any given perturbed global model parameters \\(\\widehat{W}=\\lbrace \\widehat{W}^{(l)}\\rbrace ^{L}_{l=1}\\) , there always exist infinitely many different model parameters \\(W=\\lbrace  W^{(l)}\\rbrace ^{L}_{l=1}\\)  and noises \\((\\lbrace  R^{(l)}\\rbrace _{l=1}^{L}, R^{(a)})\\)  satisfying Eq (REF ), i.e.,\n\\(\\widehat{W}^{(l)}=\\left\\lbrace \\begin{aligned}& R^{(l)} \\circ W^{(l)} , \\textnormal { for } 1\\le l\\le L-1,\\\\& R^{(l)} \\circ W^{(l)} + R^{(a)}, \\textnormal { for } l =L.\\end{aligned}\\right.\\) \n\n", "As shown in Section REF , the noises \\(R^{(l)}\\in \\mathbb {R}^{n_l\\times n_{l-1}}\\)  and \\(R^{(a)}\\in \\mathbb {R}^{n_{L}\\times n_{L-1}}\\)  used for perturbing are given as:\n\\(R^{(l)}_{ij}&=&\\left\\lbrace \\begin{aligned}& r^{(1)}_i , ~~~~~~~~\\textnormal { when } l=1 \\\\& r^{(l)}_i / r^{(l - 1)}_j, \\textnormal { when } 2\\le l \\le L-1 \\\\& 1 / r^{(L - 1)}_j, ~~\\textnormal { when } l=L\\end{aligned}\\right. \\\\R^{(a)}_{ij}& =& \\gamma _i \\cdot r^{(a)}_{i}.\\) \n", "where the vectors \\(\\mathbf {r}^{(l)}=(r^{(l)}_{1},r^{(l)}_{2},\\ldots , r^{(l)}_{n_{l}}) \\in \\mathbb {R}^{n_l}_{> 0 }\\)  for \\(l=1, 2, \\dots , L-1\\) , and \\(\\mathbf {r}^{(a)}=(r^{(a)}_{1},r^{(a)}_{2},\\ldots ,r^{(a)}_{n_{L}}) \\in \\mathbb {R}^{n_L}\\)  with pairwise different components are randomly selected and kept secret by the server.\n", "Next, we show that we can find infinite pairs \\(W=\\lbrace W^{(l)}\\rbrace ^{L}_{l=1}\\)  and \\((R=\\lbrace R^{(l)}\\rbrace ^{L}_{l=1}, R^{(a)})\\)  to construct a given \\(\\widehat{W}=\\lbrace \\widehat{W}^{(l)}\\rbrace ^{L}_{l=1}\\) . Specifically, for any given perturbed global model parameters \\(\\widehat{W}=\\lbrace \\widehat{W}^{(l)}\\rbrace ^{L}_{l=1}\\) , based on the above equations, we can construct the model parameters \\(W=\\lbrace  W^{(l)}\\rbrace ^{L}_{l=1}\\)  as follows:\n\\(\\left\\lbrace \\begin{aligned}& W^{(l)}=\\frac{1}{R^{(l)}}\\circ \\widehat{W}^{(l)}, \\textnormal { for } l=1,2,\\dots , L-1,\\\\& W^{(L)}=\\frac{1}{R^{(L)}}\\circ (\\widehat{W}^{(L)}-R^{(a)}),\\end{aligned}\\right.\\) \n", "where for \\(l=1,2, \\dots , L\\) , \\(\\frac{1}{R^{(l)}}\\in \\mathbb {R}^{n_l\\times n_{l-1}}\\)  is defined as: the \\((i,j)\\) -th entry of \\(\\frac{1}{R^{(l)}}\\)  is\n\\(\\frac{1}{R^{(l)}}_{ij}=(R^{(l)}_{ij})^{-1}\\) .\n"]}
{"id": "2007.13693", "categories": "cs.CV cs.LG", "paragraphs": ["As a final step, images are normalized to have values in the range [0, 1] and standardized with \\(\\mu = 0.5\\)  and \\(\\sigma = 0.5\\)  (same value for all three channels). Notice that during validation, the data augmentation is adapted. In this phase, steps 1 and 3 are avoided and, step 2 does a center crop instead of random one.\n"]}
{"id": "2005.04078", "categories": "cs.CV cs.LG", "paragraphs": ["To keep training and inference time relatively short, network input images and target labels are center-cropped to an aspect ratio of 2:1 and resized to a resolution of \\( {512}{px} \\times {256}{px} \\) . The input images are converted to a one-hot representation. In order to counter class imbalance in the dataset, the loss function is modified to weigh semantic classes according to the logarithm of their relative occurrence. During training, the Adam optimizer with a learning rate of\n1e-4 and parameters \\( \\beta _1 = 0.9 \\)  and \\( \\beta _2 = 0.999 \\)  is applied to batches of size 5.\n"]}
{"id": "2008.12338", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["Architectures: For MNIST- \\(\\ell _\\infty \\)  experiments, we consider a CNN architecture with the following configuration (same as [48]). Feature extraction consists of the following sequence of operations: two layers of 2-D convolutions with 32 channels, kernal size 3, RelU activation each, followed by maxpooling by factor 2, followed by two layers of 2-D convolutions with 64 channels, kernel size 3, ReLU activation, and finally another maxpool (by 2) operation. This is followed by the classification module, consisting of a fully connected layer of size 1024 \\(\\times \\)  200, ReLU activation, dropout, another fully connected layer of size \\(200 \\times 200\\) , ReLU activation and a final fully connected layer of size \\(200 \\times 10\\) . Effectively this network has 4 convolutional and 3 fully connected layers. We use batch size of 128 with this configuration.\n", "For MNIST-\\(\\ell _2\\)  experiments, we consider the LeNet5 model from the Advertorch library (same as [10]). This consists of a feature extractor of the form - two layers of 2-D convolutions, first one with 32 and second one with 64 channels, ReLU activation and maxpool by factor 2. The classifier consists of one fully connected layer of dimension \\(3136 \\times 1024\\)  followed by ReLU activation, and finally another fully connected layer of size \\(1024\\times 10\\) . We use batch size of 50 with this configuration.\n", "For CIFAR-\\(\\ell _\\infty \\)  experiments we consider a WideResNet with 34 layers and widening factor 10 (same as [48] and [25]). It consists of a 2-D convolutional operation, followed by 3 building blocks of WideResNet, ReLU, 2D average pooling and fully connected layer. Each building block of the WideResNet consists of 5 successive operations of batch normalization, ReLU, 2D convolution, another batch normalization, ReLU, dropout, a 2-D convolution and shortcut connection. We use batch size of 128 with this configuration.\n", "For CIFAR-\\(\\ell _2\\)  experiments, we consider a ResNet with 20 layers. This ResNet consists of a 2-D convolution, followed by three blocks, each consisting of 3 basic blocks with 2 convolutional layers, batch normalization and ReLU. This is finally followed by average pooling and a fully connected layer. We use batch size of 256 with this configuration.\n", "Training SGD and Entropy SGD models for MNIST experiments: For SGD, we trained the 7-layer convolutional network setup in [48], [4] with the MNIST dataset, setting batch size of 128, for \\(\\ell _\\infty \\)  SGD optimizer using a learning rate of 0.1, for 50 epochs. For Entropy SGD, with 5 langevin steps, and \\(\\gamma =10^{-3}\\) , batch size of 128 and learning rate of 0.1 and 50 total epochs.\n"]}
{"id": "2003.10200", "categories": "cs.LG stat.ML", "paragraphs": ["The setup for the Neural Spline Flow was the following (refer to [2] for a comprehensive description of the hyperparameters)These hyperparameters were chosen due to the proximity of the nature of the density to the ones explored in the original NSF paper for similar problems and worked well.: 128 hidden features, tail bound of 3.5 (i.e., the space transformed within the \\([-3.5,3.5]^3\\)  cube), 10 composed transformations, 1024 batch size, validation size of 500k, \\(5\\times 10^{-5}\\)  learning rate, 100k training steps with cosine annealing scheduler. The complete set of configuration file can be found in the code attached.\n", "We redefine the target probability as described in Eq.\u00a0(REF ), with a multivariate standard normal distribution in dimension 3 as the support function \\(p_{\\text{support}}\\left( \\mathbf {x} \\right)\\)  and \\(\\alpha =0.05\\) . With this new objective, the exploding divergence is no longer an issue, as \\(p_{\\text{new}}\\left( \\mathbf {x} \\right)>0\\)  in the \\([-3.5,3.5]^3\\)  cube.\n"]}
{"id": "2003.07042", "categories": "cs.CV", "paragraphs": ["We use the 800 standard 2K resolution\ntraining images of the DIVerse 2K (DIV2K) dataset [0]. We extracted \\(192 \\times 192\\)  patches from the training images as a training set. The size of the stride is 192. The training set consisted of 55,500 patches. Unlike previous works [21], [48], [6], we did not perform any data augmentation on the training set (e.g., random flipping).\n"]}
{"id": "2009.04554", "categories": "cs.CV", "paragraphs": ["We used the Adam [13] algorithm as our training optimizer. The batch size was set to 4 on a NVIDIA 1080Ti GPU. The learning rate was initially set to 0.002, and then was divided by 10 at 40 epochs. Our model has been trained for a total of 50 epochs.\n"]}
{"id": "2010.01892", "categories": "cs.CV", "paragraphs": ["For CIFAR-10, we followed the implementation of [8]. In the ILSVRC-2012 experiments, we used the default parameter settings of [8]. The pre-trained models are from PyTorch\u00a0[26]'s TorchVision library. Data augmentation strategies are the same as in PyTorch\u00a0[26]'s official examples. For pruning the pre-trained model, we pruned for 100 epochs, used a learning rate of \\(0.001\\)  and reduce the learning rate by half after 50 epochs. For pruning the model from scratch on CIFAR-10, we use the normal training schedule without an additional fine-tune process.\n"]}
{"id": "2010.01810", "categories": "cs.CV eess.IV", "paragraphs": ["Our model is implemented using the PyTorch framework. The network is trained with the SUN dataset [38] comprising 256\\(\\times \\) 128 pixels. Considering our GPU memory, the batch sizes of \\(G_e\\)  and \\(G_c\\)  networks are 8 and 16, respectively. We use the AdamP optimizer [10] with \\(\\beta _1\\)  = 0 and \\(\\beta _2\\)  = 0.99. Generators \\(G_e\\)  and \\(G_c\\) , are trained with the learning rate of \\(10^{-4}\\)  until the losses plateau separately. We lower the learning rate to \\(10^{-5}\\)  and continue to train \\(G_e\\)  and \\(G_c\\)  until convergence.\n", "We use a Canny edge detector [4] to generate an edge map. The sensitivity of the Canny edge detector is controlled by the standard deviation of the Gaussian smoothing filter (\\(\\gamma \\) ). In the experimental results, when \\(\\gamma \\)  is 2, the best result is shown.\nFig.\u00a0REF  illustrates the process of image completion based on the edge map generated by the edge map generator in each step. The first row indicates when the size of the mask corresponds to task 1; the second row shows the result of the intermediate task (16th mask), and the third row shows the result of the last task (32nd mask). The result of the image completion generator depends on the edge map.\n"]}
{"id": "2001.06902", "categories": "cs.CV", "paragraphs": ["We include additional details of the training setup used for each experiment. We considered two different multi-scale backbone networks, i.e. HRNet and FPN. For HRNet, we use bilinear upsampling and concatenation followed by two convolutional layers to decode the multi-scale features in the feature aggregation unit. For FPN, the feature aggregation module decodes the multi-scale features as in panoptic feature pyramid networks\u00a0[16]. In both cases, the non-linear function that produces the task attention mask in the FPM is implemented as two basic residual blocks \u2013 that aggressively reduce the number of channels \u2013 followed by a \\(1 \\times 1\\)  convolutional layer.\n"]}
{"id": "2002.02887", "categories": "cs.LG stat.ML", "paragraphs": ["We use the same overall training framework, as defined by\u00a0[39], including the stratified uniform sampling of TS in the source dataset to train the model. One model is trained per frequency split of a dataset (e.g. Yearly, Quarterly, Monthly, Weekly, Daily and Hourly frequencies in M4 dataset). All reported accuracy results are based on an ensemble of 30 models (5 different initializations with 6 different lookback periods). One aspect that we found important in the zero-shot regime, which is different from the original training setup, is the scaling/descaling of the input/output. We scale/descale the architecture input/output by the dividing/multiplying all input/output values over the max value of the input window. We found that this does not affect the accuracy of the model trained and tested on the same dataset in a statistically significant way. In the zero-shot regime, this operation prevents catastrophic failure when the target dataset scale (marginal distribution) is significantly different from that of the source dataset.\n"]}
{"id": "2004.01486", "categories": "cs.CV", "paragraphs": ["For each method, eleven models are trained. This allows to evaluate the robustness of the training. Models are trained with a batch size of 8 using the Adam optimizer\u00a0[30] and the learning rate is initialized with 8e-4. After 12 subsequent epochs without validation loss improvement, the learning rate is multiplied by 0.25 till a minimum learning rate of 6e-5 is reached. The training is stopped when 28 subsequent epochs without improvement occurred or 200 epochs are reached. To learn the cell distances and the neighbor distances, PyTorch's SmoothL1Loss is used and both losses are added. The loss functions used to train the compared methods are provided in the supplementary file S2. During training the augmentations flipping\u00a0(probability: 75), scaling\u00a0(30), rotation\u00a0(30), contrast changing\u00a0(30), blurring\u00a0(30), and noise\u00a0(30) are applied randomly in this order, and the training images are min-max normalized into the range [-1, 1].\n", "We performed the experiments using a system with two NVIDIA TITAN RTX GPUs, Ubuntu 18.04, and a Intel Core i9-9900K CPU with 64 GB RAM. The methods are implemented in Python and PyTorch is used as deep learning framework. Implementations of the proposed method and of the compared methods are available at https://bitbucket.org/t_scherr/cell-segmentation-and-tracking/.\n"]}
{"id": "2004.01377", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["We can set different \\(\\alpha \\)  and \\(\\beta \\)  (=1 by default) for different inner loops in Alg.\u00a0 and REF  and refer \\(\\alpha _i\\)  and \\(\\beta _i\\)  as the coefficients in the \\(i^{\\text{th}}\\)  inner loop.\nWe use M-SGD with momentum=0.9, weight decay=0.00005.\n", "\nS-MLDG: \\(\\alpha _1\\) =\\(\\alpha _2\\) =\\(\\alpha _3\\) =0.9, \\(\\gamma \\) =0.001 and \\(\\beta _{4}\\) =2.0.\n\nFFO-S-MLDG: \\(\\alpha _1\\) =\\(\\alpha _2\\) =\\(\\alpha _3\\) =1.0, \\(\\gamma \\) =0.9 and \\(\\beta _{4}\\) =1.1.\n\nS-Undo-Bias: \\(\\gamma \\) =0.005 and \\(\\lambda \\) =1000.0.\n\n", "\nS-MLDG: \\(\\alpha _1\\) =0.05, \\(\\alpha _2\\) =0.6, \\(\\gamma \\) =0.001 and \\(\\beta _{3}\\) =1.2.\n\nFFO-S-MLDG: \\(\\alpha _1\\) =\\(\\alpha _2\\) =0.3, \\(\\gamma \\) =0.01 and \\(\\beta _{3}\\) =1.5.\n\nS-Undo-Bias: \\(\\gamma \\) =0.01 and \\(\\lambda \\) =50.0.\n\n", "\nS-MLDG: \\(\\alpha _1\\) =\\(\\alpha _2\\) =0.002, \\(\\gamma \\) =0.001 and \\(\\beta _{3}\\) =1.85.\n\nFFO-S-MLDG: \\(\\alpha _1\\) =\\(\\alpha _2\\) =0.01, \\(\\gamma \\) =0.9 and \\(\\beta _{3}\\) =1.75.\n\nS-Undo-Bias: \\(\\gamma \\) =0.001 and \\(\\lambda \\) =100.0.\n\n"]}
{"id": "2012.01793", "categories": "cs.LG cs.AI", "paragraphs": ["Here, we provide details about the settings we use for training our\nmodels. Unless otherwise specified, in all cases, when we say \u201cramp\nup to \\(a\\) \u201d, it means increasing the value from 0 to \\(a\\)  during\nthe first \\(t_{\\text{ru}}\\)  training steps (\\(t_{\\text{ru}}\\) \nis a predefined ramp-up length) using a sigmoid-shaped function \\(e^{-5(1-x)^{2}}\\) \nwhere \\(x\\in [0,1]\\) . On the other hand, when we say \u201cramp down from\n\\(a\\) \u201d, it means decreasing the value from \\(a\\)  to 0 during the last\n\\(t_{\\text{rd}}\\)  training steps using another sigmoid-shaped function\n\\(1\\text{\u2014}e^{-12.5x^{2}}\\)  where \\(x\\in [0,1]\\) . The ramp-up\nand ramp-down functions are taken from [51].\n"]}
{"id": "2007.14082", "categories": "cs.LG stat.ML", "paragraphs": ["During training, we use a single sample per event interval to calculate the loss function as we find using multiple samples does not improve performance, as shown in the appendix\u00a0[32].\nAll UNIPoint models tested employ an RNN with 48 hidden units, a batch size of 64, and are\ntrained using Adam [13] with \\( L2 \\)  weight decay set to \\( 10^{-5} \\) .\nThe validation set is used for\nearly stopping: training halts if the validation loss does not improve by more than \\( 10^{-4} \\)  for 100 successive mini-batches. The training for one of the real world datasets (e.g., StackOverflow) takes approximately 1 day.\n"]}
{"id": "2006.12467", "categories": "cs.LG cs.CL stat.ML", "paragraphs": ["We trained common self-attention architectures of depths \\(L=6,12,18,24,30,36,48\\)  and varying widths, such that the network sizes range between \\(10^6\\)  and \\(6\\cdot 10^{8}\\)  (full details on the widths of the trained architectures are given in the appendix). We trained decoder-only (unidirectional) models, by optimizing the autoregressive log-likelihood of the training examples.\nWe used a smaller than usual vocabulary size of 2000 so that the vocabulary embedding parameters, given by \\(d_x\\cdot V\\)  for a vocabulary of size \\(V\\) , would constitute a small fraction of the learned parameters for all data points.\nAutoregressive models were shown to work well even on character level vocabularies (e.g.,\u00a0[45]); due to modeling a joint distribution over the text, they are less sensitive to vocabulary size than bidirectional models\u00a0[34].\n"]}
{"id": "2012.15516", "categories": "cs.CL", "paragraphs": ["While AraBERT was trained using the MLM objective, AraELECTRA is pre-trained using the RTD objective.\nThe RTD approach trains two neural network models, a generator G and a discriminator D or AraELECTRA, as shown in Figure\u00a0REF .\nG takes a corrupted input sequence, where random tokens are replaced with the [MASK] token, and learns to predict the original tokens that have been masked.\nThe generator network G is in our case a small BERT model with 12 encoder layers, 4 attention heads, and 256 hidden sizeIn the generator, the input embeddings of size 768 are first projected into the generator hidden size with the addition of a linear layer..\nThe discriminator network D then takes as input the recovered sequence from the output of G and tries to predict which tokens were replaced and which tokens are from the original text.\n"]}
{"id": "2002.05954", "categories": "cs.LG cs.AI cs.RO stat.ML", "paragraphs": ["\nDiscount \\(\\gamma = 0.98\\)  for all agents.\n\nAdam optimizer. Learning rate \\(0.001\\)  for all actors and critics.\n\nSoft updates using moving average; \\(\\tau =0.05\\)  for all controllers.\n\nReplay buffer size was designed to store 500 episodes, similarly as in\u00a0[12]\n\nWe performed 40 updates after each epoch on each layer, after the replay buffer contained at least 256 transitions.\n\nBatch size 1024.\n\nNo gradient clipping\n\nRewards 0 and -1 without any normalization.\n\nObservations also were not normalized.\n\n2 HER transitions per transition using the future strategy\u00a0[18].\n\nExploration noise: 0.05 and 0.1 for the planning and control layer respectively.\n\n"]}
{"id": "2012.12395", "categories": "cs.CV", "paragraphs": ["At training time, we use a spatial X-Y region of size \\(144\\times 80\\)  meters, where each grid cell is \\(0.2\\times 0.2\\)  meters. On the height dimension, we take the range from -2 to 3.5 meters with a 0.2 meter interval, leading to 29 bins.\nFor temporal information, we take all the 3D points from the past 5 timestamps. Thus our input is a 4 dimensional tensor consisting of time, height, X and Y.\n", "For both our early-fusion and late-fusion models, we train from scratch using Adam optimizer [12] with a learning rate of 1e-4. The model is trained on a 4 Titan XP GPU server with batch size of 12. We train the model for 100K iteration with learning rate halved at 60K and 80K iterations respectively.\n"]}
{"id": "2003.00652", "categories": "cs.LG stat.ML", "paragraphs": ["[leftmargin=*]\nExperiment 1: synthetic ball throwing trajectories: We train GANs with local discriminators (with localization width equals to \\(1, 2, 3, 5, 8, 11, 15\\) ) for 500 epochs, with learning rate \\(0.0001\\)  and batch size 128. We repeat each experiment 10 times and report the averages with uncertainties.\n\nExperiment 2: real Bayes-nets: We train the standard GANs and the Bayes-net GANs for 100 epochs, with learning rate \\(0.001\\)  and batch size 128. We repeat each experiment 5 times and report the averages with uncertainties.\n\n"]}
{"id": "2003.00804", "categories": "cs.CV", "paragraphs": ["Stochastic gradient descent (SGD) was used. Following [28], we set weight decay and Nesterov momentum to 0.0005 and 0.9, respectively. Each mini-batch contained 8 task instances. The meta-learning model was trained for 60 epochs, and 1000 mini-batchs for each epoch. We set the initial learning rate to 0.1, then multiplied it by 0.06, 0.012, and 0.0024 at epochs 20, 40 and 50, respectively, as in [6]. The results, which are marked by \u201c+ens\u201d were used the 60 models saved after each epoch to become an ensemble model. For the final training, the training classes set was augmented by the validation classes set. When we only chose one model, we will chose the model at the epoch where we got the best model during training on the training classes set. The results of the final run are marked by \u201c+val\u201d in this subsection. Since the base idea of \u201c+ens\u201d was proposed by other works and \u201c+val\u201d is popular for meta-learning, we do not explain more details about them.\n", "For data augmentation, we adopted random crop, horizontal flip, and color (brightness, saturation, and contrast) jitter data augmentation following the work of [6], [18]. In the experiments of comparing Task Aug and Image Aug by rotating, R2-D2 was applied, and we set \\({\\rm T}\\)  to 80000. In the evaluation of Task Aug for ProtoNets and M-SVM, we set \\(p_{max}\\)  to the value getting the best results for R2-D2.\n"]}
{"id": "2008.12378", "categories": "cs.CV", "paragraphs": ["Data.\nWe use SYNTHIA\u00a0[45], which consists of over \\(20,000\\)  rendered images and corresponding pixel-level semantic annotations, where 13 classes of objects are labeled for aiding segmentation and scene understanding problems.\nWe also use Cityscapes\u00a0[13], which contains a set of diverse street scene stereo video sequences and over \\(5,000\\)  frames of high-quality semantic annotations, where 30 classes of instances are labeled in the segmentation masks.\n", "Data.\nWe use data from the Automatic Cardiac Diagnosis Challenge (ACDC)\u00a0[4], which contains cardiac cine-MR images acquired from different MR scanners and resolution on 100 patients. Images were resampled to 1.37 \\(mm^2\\) /pixel resolution and cropped to \\(224\\times 224\\)  pixels. Manual segmentations are provided for the left ventricular cavity, the myocardium and right ventricle in the end-systolic and end-diastolic cardiac phases. In total there are 1920 images with manual segmentations and 23,530 images with no segmentations.\n", "Data.\nWe use DeepFashion\u00a0[36], a large-scale dataset with over 800,000 diverse images of people in different poses and clothing, that also has annotations of body joints. We only used full-body images, specifically 32,032 images for training and 7,997 images for testing.\n", "where \\(\\mathcal {L}_{rec}\\)  is the mean absolute error between the reconstructed and the input image. \\(\\mathcal {L}_{equiv}\\)  is an equivariance cost, that ensures that the mean and covariance of the parts coordinates don't change after some style transformation. Based on the implementation details presented in\u00a0[39], we set \\(\\lambda _1=\\lambda _2=1\\) .\n"]}
{"id": "2012.14456", "categories": "cs.CV cs.AI", "paragraphs": ["The transfer learning is utilized for VGG19, ResNet18 and DenseNet121 over the Caltech256 dataset with Adam optimizer [33] using categorical cross-entropy loss function. The data augmentations used for this training setting are: random rotation from 0 to 40 degree, width and height shift range up to \\(0.2\\) , random shear, random zoom and horizontal flips. For Caltech256 dataset, the Adam optimizer is used with learning rate of \\(0.001\\)  and \\(0.0001\\)  for the first 20 epochs and next 20 epochs, respectively.\n{FIGURE}", "The VGG16, ResNet101 and DenseNet121 are trained over TinyImageNet with data augmentations like Gaussian blur over \\(50\\%\\)  of the images with random sigma between 0 to \\(0.5\\) , horizontal and vertical flips, cropping images by \\(-10\\%\\)  to \\(20\\%\\) , and affine transformations such as scaling up to \\(80\\%\\)  to \\(120\\%\\)  of image/height, translation by \\(-20\\)  to \\(+20\\)  relative to height/width (per axis), rotation by \\(-45\\)  to 45 degrees, shear by \\(-16\\)  to 16 degrees, Coarse Dropout by \\(2-5\\%\\) , and brightness and contrast normalization. For TinyImageNet dataset, the Adam optimizer is used with learning rate of \\(0.001\\)  along with cyclic learning rates of \\(0.0001 - 0.0006\\) , \\(0.00001 - 0.00006\\)  and \\(0.000001 - 0.000006\\)  for 48 epochs, 12 epochs and next 12 epochs, respectively using the categorical cross-entropy loss function.\n", "The training and test sets used for the experiments are summarized in Fig. REF . First, the experiments are performed over the original training images to show the impact of CCP attacks on test set images. Later, the CCP attacked training images are also used in the training set to increase the robustness of the model against the CCP attack on test set images. The settings for training are same for both types of images in the training set with or without the CCP attack. Note that the values of scale \\(s\\)  and bias \\(b\\)  in the CCP attack are set to 2 and 0, respectively, for the CIFAR10 dataset and 1 and 30, respectively, for the Caltech256 and TinyImagenet datasets for visually appealing generated images.\n"]}
{"id": "2012.14511", "categories": "cs.LG", "paragraphs": ["1,657 legal cases were represented in the originating dataset, with 973 Litigation type cases. The number of principal components for use in SVD was specified as 5, capturing approximately 97.91% of the total amount of variation in the data. When applying T-SNE, the number of components was specified as 2, with the perplexity, learning rate, number of iterations, and the embedding initialization specified as 25, 50, 10000, and Principal Components Analysis (PCA), respectively. When applying DBSCAN, the maximum distance between two samples was set to 4.5 and the minimum number of samples in a neighborhood for a point to be considered a core point was set to 10.\n", "\nPrecision: The number of true positives tp over the number of false positives fp and true positives tp, \\( \\frac{\\textit {tp}}{\\textit {tp} + \\textit {fp}} \\) .\n\nRecall: The number of true positives tp over the number of false negatives fn and true positives tp, \\( \\frac{\\textit {tp}}{\\textit {fn} + \\textit {fp}} \\) .\n\nF1 Score: The harmonic mean of the precision and recall, defined as\n2 * \\( \\frac{precision * recall}{precision + recall} \\)  .\n\nAccuracy: The number of true positives tp over the total number of samples in the validation set n.\n\nCoverage: The number of samples which fall above a specified prediction confidence threshold (set to 90%) over the total number of samples in the validation set n.\n\n"]}
{"id": "2007.00843", "categories": "cs.CV cs.LG", "paragraphs": ["The spatial CNN parameters are optimized using stochastic gradient descent with momentum set to 0.9 Code and LENs-4 dataset are publicly available at https://github.com/mcgridles/LENS.The initial learning rate is set to 5\\(x10^{-4}\\)  and decayed over multiple epochs using a Plateau learning rate scheduler with patience set to 1. For every video in a mini-batch of 64 videos, 3 frames are randomly sampled within equally spaced temporal windows and a consensus of the frame predictions provides a video-level prediction for calculating the loss.\n", "The temporal CNN parameters are also optimized using stochastic gradient descent with 0.9 momentum. The initial learning rate is set to 1\\(x10^{-2}\\)  and decayed over epochs using a Plateau learning rate scheduler with patience set to 3. For every mini-batch, 64 videos are randomly selected and 1 stacked optical flow is randomly sampled in each video.\n", "All models are trained on Google Cloud, as large memory and computation capabilities are required to train deep neural networks. A virtual machine is deployed using the Deep Learning VM image in the Google Cloud Marketplace on the Google Cloud Compute Engine, where the PyTorch, FastAI, and NVIDIA configuration settings are applied. Due to the training datasets taking over 100 Gigabytes (GB) in memory, a 500 GB zonal persistent disk is added and mounted to the VMs for dataset storage. The GPU used is an NVIDIA Tesla P100.\n"]}
{"id": "2007.04589", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["For all models, we use Residual Network [20] backbones following [41]. For training the models on all datasets, we adopt the Adam optimizer [26] with a learning rate of \\(2\\times 10^{-4}\\)  and batch size of 64, following [19], [41]. Specifically, for CIFAR-10, CIFAR-100 and STL-10, we follow settings in [42] by linearly decaying learning rate over 100K generator steps, each taken every 5 discriminator update steps. For ImageNet, we follow [41] by increasing the number of generator updates to 450K steps instead, but with no learning rate decay. For CelebA, we follow [9] by taking 100K generator steps, each taken after 2 discriminator updates and with no learning rate decay.\n"]}
{"id": "2010.04879", "categories": "cs.CV", "paragraphs": ["For base models training on CIFAR-10, we set the batch size to 64 for DenseNet and 128 for other architectures. Weight decay is set to \\(1e-4\\) . The models are trained for 160 epochs with the learning rate starting from \\(0.1\\)  and divided by 10 at epoch 80 and 120. These are all the most common training settings\u00a0[4], [21], [7] for models for CIFAR-10. For all base models training on TinyImageNet and ImageNet, the batch size is set to 256, and weight decay is \\(1e-4\\) . All models are trained for 100 epochs.\n"]}
{"id": "2001.04118", "categories": "cs.CV eess.IV eess.SP", "paragraphs": ["\\(T\\)  is the sampling period, \\(\\upsilon ^{(p)}\\)  and \\(\\upsilon ^{(s)}\\) \nare, respectively, 3D vectors of noise variances for the components\nof the centroid and shape parameter (in logarithm) of the ellipsoid.\nThis transition density describes a nearly constant velocity model\nfor the centroid and a Gaussian random-walk for the shape parameter.\nGaussianity of the logarithms of the half-lengths is equivalent to\nmodeling the half-lengths as log-normals, which ensure that they are\nnon-negative. Note that these log-normals have mean 1, and variances\n\\(e^{\\upsilon _{i}^{(s)}}-1\\) , \\(i=1,2,3\\) , where \\(\\upsilon _{i}^{(s)}\\) \nis the \\(i\\mathrm {^{th}}\\)  components of \\(\\upsilon ^{(s)}\\) . Hence,\nthe observed half-lengths are randomly scaled versions of their nominal\nvalues, with an expected scaling factor of 1.\n{FIGURE}", "Unlike WILDTRACKS where objects enter the scene from anywhere at the\nboundary, in CMC we know the location of objects entering the scene.\nHence, we specify the birth parameters as \\(P_{B,+}(\\ell )=0.001\\)  and\n\\(f_{B,+}(x,\\ell )=\\mathcal {N}(x;\\mu _{B,+},0.1^{2}\\mathrm {I}_{9})\\) \nwhere\n\\(\\mu _{B,+}= & [2.03\\;0\\;0.71\\;0\\;0.825\\;0\\;-\\hspace{0.0pt}1.2\\;-\\hspace{0.0pt}1.2\\;-\\hspace{0.0pt}0.18]^{T}.\\) \n", "Each state is augmented \\(x\\)  with a discrete mode or\nclass \\(m\\hspace{0.0pt}\\in \\hspace{0.0pt}\\lbrace 0,1\\rbrace \\) , where \\(m\\hspace{0.0pt}=\\hspace{0.0pt}0\\) \ncorresponds to a standing state and \\(m\\hspace{0.0pt}=\\hspace{0.0pt}1\\) \ncorresponds to a fallen state. We consider the single-object state\nas \\((x,m)\\) , with single-object density \\(p^{(\\xi )}(x,m)=p^{(\\xi )}(x|m)\\mu {}^{(\\xi )}(m)\\) .\nThe following single-object transition density and observation likelihood\nare used\n\\(f\\!_{S,+}(x_{+}m_{+}\\!|x,m)\\!\\!=\\!\\!f_{S,+}^{(m_{+})}\\!(x_{+}\\!|x,\\ell ,m)\\delta _{\\ell }[\\ell _{+}]\\mu _{+}\\!(m_{+}\\!|m),\\!\\!\\!\\) \n\\(\\!\\!\\!g^{(c)}(z^{(c)}|x,m)\\propto g_{e}^{(c)}\\!(z_{e}^{(c)}\\!|m)\\times \\\\\\hspace{0.0pt}\\hspace{0.0pt}\\mathcal {N}\\!\\left(\\!z^{(c)};\\Phi ^{(c)}(x)\\!+\\!\\!\\left[\\!\\!\\!\\!\\begin{array}{c}0_{2\\times 1}\\\\-\\upsilon _{e}^{(c,m)}\\!\\!/2\\end{array}\\!\\!\\!\\right]\\!\\!,\\textrm {diag\\!\\!}\\left(\\!\\left[\\!\\!\\!\\begin{array}{c}\\upsilon _{p}^{(c)}\\\\\\upsilon _{e}^{(c,m)}\\end{array}\\!\\!\\!\\!\\right]\\!\\right)\\!\\!\\right)\\!\\!.\\hspace{0.0pt}\\hspace{0.0pt}\\) \n", "The mode transition probabilities are \\(\\mu _{+}(0|0)=0.6\\) , \\(\\mu _{+}(1|0)=0.4\\) ,\n\\(\\mu _{+}(0|1)=0.6\\)  and \\(\\mu _{+}(1|1)=0.4\\) .\n", "For a standing object, i.e. \\(m\\hspace{0.0pt}=\\hspace{0.0pt}0\\) , we have\n\\(\\upsilon _{e}^{(c,0)}\\hspace{0.0pt}=\\hspace{0.0pt}\\upsilon _{e}^{(c)}\\hspace{0.0pt}=\\hspace{0.0pt}[0.01,0.0025]^{T}\\) \nin the above observation likelihood. Further, standing objects typically\nhave a bounding box size ratio (y-axis/x-axis) greater than one, thus\nthe mode dependent likelihood component is chosen as \\(g_{e}^{(c)}(z_{e}^{(c)}|0)=e^{\\rho \\left(({[0,1]z_{e}^{(c)}}{[1,0]z_{e}^{(c)})-1}\\right)}\\) \nfor all cameras, where \\(\\rho =2\\)  is a control parameter. The transition\ndensity to another standing state \\(f_{S,+}^{(0)}(x_{+}|x,\\ell ,0)\\) ,\nis the same as per the previous subsection.\n"]}
{"id": "2012.15682", "categories": "cs.CL", "paragraphs": ["For source-training,\nwe finetune the pretrained encoder for 10 epochs with batch size 32.\nFor target-adapting to every target language,\nthe few-shot data is a sampled bucket in this language,\nand we finetune on the bucket for 50 epochs with early-stopping of 10 epochs.\nThe batch size is set to the number of shots in the bucket.\nEach target-adapting experiment is repeated 40 times using the 40 buckets.\nWe use the Adam optimizer [37]\nwith default\nparameters in both stages with learning rates searched over \\(\\lbrace 1e-5, 3e-5, 5e-5, 7e-5\\rbrace \\) .\nFor CLS tasks, we use mBERT's [CLS] token as the final representation.\nFor NER and POS, following [12], we use\na linear classifier layer on top of the representation of each tokenized\nword, which is its last\nwordpiece [28].\n", "We set the maximum sequence length to 128\nafter wordpiece tokenization [72], in all\nexperiments.\nFurther implementation details\nare shown in our Reproducibility Checklist in Appendix \u00a7.\n"]}
{"id": "2007.11465", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["\nMNIST ([20]): A set of centered 28\\(\\times \\) 28 handwritten digits from 0-9 in black and white. It consists of 60000 training samples and 10000 test samples. The dataset was normalized to the interval \\([0,1]\\) . We use a training/validation split of 50000/10000 images.\n\nCIFAR-10 (): The CIFAR datasets are RGB image datasets displaying real world objects at a resolution of 32\\(\\times \\) 32. The CIFAR-10 dataset includes ten different types of objects. It consists of 50000 training and 10000 test samples. We adopt a standard data augmentation scheme including, standardization, mirroring and shifting of the images. We use a training/validation split of 45000/5000 images.\n\nCIFAR-100 (): This dataset has the same specifications as CIFAR-10, but consists of 100 classes of objects. For training we use the same data augmentation as for CIFAR-10. We use a training/validation split of 45000/5000.\n\nSVHN ([21]): This RGB image dataset consists of house numbers taken from Google Street View, with a single digit to classify. It consists of 73257 training samples and 26032 test samples at a resolution of 32\\(\\times \\) 32, the dataset was normalized to the interval \\([0,1]\\) . We use a training/validation split of 63257/10000 images.\n\n", "Since the architecture uses Dense Blocks, we use the training setup of DenseNet as presented in [2]. We use a stochastic gradient descent optimizer with a Nesterov momentum of 0.9, using a batch size of 64. For the CIFAR datasets, we use a base learning rate of 0.1 and decay the learning rate after 150, 200 and 250 epochs by a factor of 0.1. The dropout rate in the Dense Blocks is set to zero. For MNIST and SVHN we train the network for a maximum of 40 epochs and decay the learning rate after 20 and 30 epochs by a factor of 0.1.\nWe used a weight decay scaling factor of \\(\\lambda _{\\mathrm {WD}}=10^{-4}\\)  and a scaling factor of \\(\\lambda _{\\mathrm {WS}}=0.2\\)  for the Wasserstein loss and \\(\\lambda _{\\mathrm {R}}=0.1\\)  for the reconstruction loss.\n"]}
{"id": "2010.00067", "categories": "cs.CV", "paragraphs": ["We trained and tested our model on an Intel 2.6 GHz CPU cluster with NVIDIA TITAN RTX GPUs. The learning rate is set at \\(2\\times 10^{-3}\\)  and regularization parameter \\(1\\times 10^{-3}\\)  for Adam optimizer. Batch size is set to 12. Also, at each frame during training, we sample a random frame as previous frame going back up to 45 frames in order to provide more challenging matches. This introduces more cases of occlusion and significant appearance changes making our algorithm more robust during testing.\n"]}
{"id": "2010.15877", "categories": "cs.CL cs.AI", "paragraphs": ["In the CQA dataset, since the annotated action sequence are not provided, we randomly sampled 1% of the training set (approx. 10K out of 944K training samples) and followed\u00a0[4] to annotate them with pseudo-gold action sequences by using a BFS algorithm.\nWe denoted the 1% questions and relevant pseudo-gold action sequences as \\(Q_{pre}\\) .\nThe \\(Q_{pre}\\)  was used to train the LSTM-based programmer, which was further optimized through the Policy Gradient (PG) algorithm\u00a0[19], [16] with another 1% unannotated questions from the training set.\nWe denoted this model by PG, which is also a model variant proposed in\u00a0[6].\nWe trained the meta learner on another 2K training samples (\\(Q_{meta}\\)  in Alg.REF ), representing only approx. 0.2% of the training set.\nThis meta learner is our full model: MRL-CQA.\n", "In our work, we chose the attention-based LSTM model instead of the Transformer\u00a0[17] to design the programmer.\nWe set the sizes of embedding and hidden units in our LSTM model as 50 and 128 respectively, thus the maximum number of the parameters in our model is about 1.2M.\nHowever, the base model of the Transformer (12 layers, 12 heads, and 768 hidden units) has 110M parameters\u00a0[20], which are much more than those of our model.\nGiven the small size of the training samples and the weak supervision signal (reward in our work), it is harder to train the model with more parameters.\nTherefore we chose LSTM rather than the Transformer.\n", "We implemented our model in PyTorch and employed the Reptile meta-learning algorithm to optimize the meta-learned policy\u00a0[11].\nThe weights of the model and the word embeddings were randomly initialized and further updated within the process of training.\nIn meta-learning, we set \\(\\eta _1 = 1e-4\\)  (Equation\u00a0REF ) and \\(\\eta _2 = 0.1\\)  (Equation\u00a0REF ).\nWe set \\(N = 5\\)  and threshold value at \\(0.85\\)  when forming the support set.\nFor each question, we generate five action sequences to output the answers.\nThe Adam optimizer is used in RL to maximizes the expected reward.\n"]}
{"id": "2008.06266", "categories": "cs.CV", "paragraphs": ["Our proposed architecture is trained on patches of size \\(288 \\times 288\\)  pixels similar to the implementation in [12]. Compared to other segmentation domains, crack segmentation can be seen as a binary classification problem, as pixels can only be assigned a probability of containing a crack or not. Let \\(X\\)  be a training dataset consisting of \\(N\\)  samples \\(x\\) , \\(X = \\lbrace x_1, x_2 \\ldots x_N\\rbrace \\)  as well as \\(Y\\)  being the respective ground truth dataset to \\(X\\)  consisting of \\(N\\)  samples \\(y\\) , \\(Y = \\lbrace y_1, y_2 \\ldots y_N\\rbrace \\)  then each ground truth pixel \\(i\\)  of any given sample \\(y\\)  is \\(y_{i} \\in [0,1]\\) .\n"]}
{"id": "2005.13798", "categories": "cs.CL", "paragraphs": ["To train the ConCET model, the parameters for CNN and BiLSTM described in Figure REF  were chosen based on our experience and previous literature. Finally, we trained the overall model with an Adams optimizer and a learning rate of 0.001.\nAll experiments for ADAN were conducted using the topic classifier API made available to the teams by the Amazon Alexa Prize [9]. To train the FastText modelhttps://fasttext.cc, character 5-grams with word embedding of size 300 were used. Finally, VDCNN results are reported based on a publicly available implementation. https://github.com/zonetrooper32/VDCNN. The results are reported for a 29-layer VDCNN, based on the original paper.\n"]}
{"id": "2005.02354", "categories": "cs.CL", "paragraphs": ["We used the hyper-parameters specified in latest version (3) of Google's Tensor2Tensor\u00a0[36] implementation, with the exception of the dropout rate, as we found \\(0.3\\)  to be more robust across all the models trained on Europarl.\n", "Models are optimized using Adam\u00a0[13] and following the learning schedule specified by\u00a0vaswani2017attention with\u00a08,000 warm-up steps.\nWe employed label smoothing \\(\\epsilon _{ls} = 0.1\\) \u00a0 during training and we used beam search with a beam size of 4 and length penalty \\(\\alpha = 0.6\\) \u00a0[38].\n"]}
{"id": "2010.10755", "categories": "cs.CL cs.IR", "paragraphs": ["\u00a0\u00a0\u00a0We use dropout layers at the end of both stages with a rate as 0.3 for avoid over-fitting.\nFor the optimizer, we use Adam for both stages, with the learning_rate=0.001,\nbeta_1=0.9,\nbeta_2=0.999, and\nepsilon=1e-07.\n"]}
{"id": "2011.10227", "categories": "cs.LG cond-mat.mtrl-sci", "paragraphs": ["BlackThe code is implemented using the Python libraries Keras [35] Blackand Tensorflow [36]. During the training phase, Blackthe adaptive moment estimator known as the Adam optimizer [37] is used, and the learning rate Blackis set at \\(10^{-3}\\) . BlackNote that Adam is an optimizer based on gradient and momentum, which is used to minimize the loss function and update the weight matrices in StressNet. The dataset contains  61 Blackgroups of high-fidelity HOSS simulations, Blackand 55 of them Blackare selected as the training data Blackto build the model. BlackThe remaining simulations are used to test model performance after training. In the training process, one epoch represents that the model was trained once throughout the entire training dataset. During each epoch, the dataset is ordered randomly and split into batches. Generally, the training process contains multiple epochs. At each epoch, six simulations are randomly selected and set aside for model validation. Note that validation is conducted at the end of each epoch to assess the model's performance. The Blackgoal of validation is to indicate the model performance in data unseen during the epoch to avoid overfitting. This is different from testing \u2014 which is conducted just once after all training \u2014 on data never fed to the network during the training process.\n", "BlackIn summary, at each epoch, BlackStressNet is trained on 49 simulations, Blackand validated on six simulations. After training, the model is tested on the Blackremaining six simulations. BlackTo prevent overfitting, the order of Blackfeeding simulations Blackis shuffled every 30 epochs, Blackand the shuffling process is repeated 60 times, which means Blackthere are 1,800 epochs total. When Blackthe dynamic loss function Blackis applied, the value of \\(\\lambda \\)  Blackis set to \\(0.9\\)  Blackfor the first 600 epochs, and Blackthen, it is changed to \\(0.1\\)  Blackfor the remaining simulations. The training process takes between 8 to 20 hours on a single BlackNVIDIA GeForce GTX 1080Ti GPU, depending on the number of epochs.\n"]}
{"id": "2011.10232", "categories": "cs.CV cs.GR eess.IV", "paragraphs": ["For Funt's dataset, we used randomly selected 13 images for testing and the rest 211 images for training. For Kalantari's dataset, we used the provided 15 test and 74 training images. In the training phase, we randomly sampled 32\\(\\times \\) 32-sized patches from each training image set and randomly applied each of a horizontal flip, a vertical flip, and a swapping of horizontal and vertical axes (transpose) for data augmentation. The used optimizer is Adam\u00a0[52], where the learning rate was set to 0.001 and the parameters \\((\\beta _1, \\beta _2)\\)  were set to \\((0.9, 0.999)\\) . We performed 3,000 times mini-batch updates, where the mini-batch size was set to 32.\n"]}
{"id": "2006.06863", "categories": "cs.LG cs.NE", "paragraphs": ["Each architecture was trained for 200 epochs with 256 batch size. The initial channel is 16. We used the SGD optimizer with an initial learning rate of 0.1, followed by a cosine learning rate schedule through the training. The momentum rate was set to 0.9. We used a weight decay of \\(5 \\times 10^{-4}\\)  and a norm gradient clipped at 5. Cutout technique was not used in the training. The supernet training setup is consistent with architecture candidates. For supernet training, we changed the initial learning rate to 0.025 and total epochs to 300. The batch size is 128 and the weight decay was set to \\(1 \\times 10^{-4}\\) . Each sub-supernet approximately took 40-50 epochs to converge after transfer learning. For each NAS algorithm, we used the same setup as described in the NasBench-201 \u00a0[15]. We used 6 P100 GPUs to train the supernet and 5 sub-supernets.\n{FIGURE}"]}
{"id": "2006.07084", "categories": "cs.CV cs.LG eess.IV", "paragraphs": ["We use the DFDC dataset for model training. The dataset contains approximately 20,000 real videos and 100,000 fakes. Of those, we use 1000 videos for validation, 1202 for testing, and the rest for training. The validation and test sets have equal number of real and fake videos. We also generate two face datasets with these videos using both the baseline and the proposed pre-processing approach. For the proposed training dataset generation, we apply our pre-processing approach to all frames of the video in order to extract faces.\nTo deal with the class imbalance, we sample 16 of these faces from the real videos and only 4 from the fake ones during batch generation. We randomly sample the same amount of frames every epoch from each video to increase training diversity. For the baseline pre-processing dataset, we sample the same number of detected faces.\n"]}
{"id": "2009.01016", "categories": "cs.LG eess.SP stat.ML", "paragraphs": ["The forgetting factor, on the other hand, decreases the weight of old data exponentially during the recursive training process. For instance, when \\(\\lambda =0.995\\) , a set of data a year ago is penalized by the factor of \\((0.995)^{365}=0.16\\) . This forgetting factor also allows the regularization term to vanish, adapting to the size of the training set since the term converges to zero when the number of elements in \\(\\mathbb {D}\\)  is getting bigger.\nThe modified problem in Eq.\u00a0(REF ) will be equivalent to the original problem of Eq.\u00a0(REF ) when we set \\(\\rho =0\\)  and \\(\\lambda =1\\) , which means no regularization and no forgetting process.\n"]}
{"id": "2009.01027", "categories": "cs.LG cs.AI cs.CV stat.ML", "paragraphs": ["For searching and evaluation in the standard DARTS space (we name it as S0 for simplicity), we keep the same settings as in DARTS [25] . We follow R-DARTS [41] for their proposed reduced spaces S1- S4 (harder than S0). However the inferred models are trained with two different settings from R-DARTS [41] and SDARTS [3]. The difference lies in the number of layers and initial channels for evaluation on CIFAR-100. R-DARTS sets 8 layers and 16 initial channels. Instead, SDARTS uses 20 and 36 respectively. For the proxyless searching on ImageNet in search space (we named it S5) by FBNet [37], we use the SGD optimizer for weight and Adam (\\(\\beta _1=0.5\\)  and \\(\\beta _2=0.999\\) , 0.001 learning rate) for architecture parameters with the batch-size of 768. The initial learning rate is 0.045 and decayed to 0 within 30 epochs following the cosine schedule. We also use L2 regularization with 1e-4. It takes about 4.5 GPU days on Tesla V100. More details are provided in the appendix. We also use NAS-Bench-201 (S6) since DARTS performs severely bad. In a word we use 7 different search space to conduct the experiments, which involves three datasets.\n"]}
{"id": "2005.00116", "categories": "cs.CV cs.LG", "paragraphs": ["All models were trained for ten epochs (as we have a large training set), using the Adam optimizer with a learning rate of \\(10^{-4}\\) . Checkpoints were taken at each quarter of an epoch, and inference was run on the validation set to determine the validation ROC AUC at that checkpoint. The models were early-stopped by monitoring validation ROC AUC at each checkpoint with a patience of three checkpoints. The batch size was based on a GPU capacity of 12 GB (Tesla K80), and was set to 32 or 16 based on the model. The best model was saved at each checkpoint when the model resulted in an improved validation ROC AUC. Under this setting, the Baseline Model took about 13 hours to converge, and the Sequence Models took about six hours to converge.\n"]}
{"id": "2011.12982", "categories": "cs.CV", "paragraphs": ["As described in the main part,\nour training procedure is inspired by Tong et al.\u00a0[27]:\nwe use SGD with Nesterov momentum and cosine learning rates decay.\nWe follow Goyal et al.'s\u00a0[23] recommendation for the learning rate magnitude: \\(\\mathrm {lr}=\\frac{0.1}{256} \\times \\mathrm {batch size}\\) .\nThe augmentations include random resized crop, RandAugment\u00a0[13] and Erasing\u00a0[69].\nWe train for 600 epochs with batches of 1024 images of resolution \\(224\\times 224\\)  pixels (except for CIFAR-100 where the resolution is \\(32\\times 32\\) ).\nFor Grafit with \\(\\mathcal {L}_\\mathrm {inst}\\)  we use \\(T=4\\)  different data-augmentations on ImageNet and \\(T=8\\)  on CIFAR-100.\nFor the supervised loss we use one data-augmentation in order to have the same training procedure as our supervised baseline.\n"]}
{"id": "2003.03879", "categories": "cs.CV", "paragraphs": ["We consider the state-of-the-art regularization methods of Cutout [1], Mixup [41], CutMix [40], label smoothing [35], ShakeDrop [38], and their combinations for experiments. We optimize the models with the SGD with momentum. We set the batch size to 64 and training epochs to 300. The learning rate is initially set to \\(0.25\\)  and is decayed by the factor of \\(1/10\\)  at \\(150^{\\mathrm {th}}\\)  and \\(225^{\\mathrm {th}}\\)  epochs. We also employ random crop and random flip augmentations for all methods, unless specified otherwise.\n"]}
{"id": "2002.12804", "categories": "cs.CL", "paragraphs": ["We followed the same model size as BERT\\(_{\\textsc {base}}\\) \u00a0[5] for comparison purposes.\nSpecifically, we used a 12-layer Transformer with 12 attention heads.\nThe hidden size was 768, and inner hidden size of feed-forward network was 3072.\nThe weight matrix of the softmax classifier was tied with the token embedding matrix.\nWe also add relative position bias\u00a0[21] to attention scores.\nThe whole model contains about 110M parameters.\n"]}
{"id": "2012.04581", "categories": "cs.CV", "paragraphs": ["The weights of both convolutional and fully-connected layers are initialized with the Xavier initialization proposed in [4].\nWe set the parameter to random values uniformly drawn from [\\(-rv\\) , \\(rv\\) ], where \\(rv\\)  is defined as, \\(rv = \\sqrt{6 \\over (d_{in} + d_{out})}\\)  where \\(d_{in}\\)  and \\(d_{out}\\)  are the size of input and output channels, respectively. All biases are initialized to 0. For batch normalization layers, weights are initialized to 1.\n", "The optimizer proposed in [21] is used for training. We use Categorical Cross Entropy as the loss function. Each model is trained for 100 epochs with a batch size of 8. The learning rate is initialized to \\(0.1\\) . Learning rate adjustment is one of the crucial steps while training. We use the cosine annealing strategy proposed in [19]. The cosine annealing decreases the learning rate from the initial value to 0 by following the cosine function. The intuition behind using annealing is that it helps to traverse quickly from the initial parameters to a range of good parameter values with a smaller learning rate. Thus, we can explore the deeper and narrower parts of the loss function, which potentially improves the training progress by avoiding the divergence.\n{FIGURE}"]}
{"id": "2005.06852", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["The model under attack.\nWe start from a neural network of 3 hidden layers with 32 hidden units for COMPAS and German Credit and 128 for Adult, due to its larger encoded input.\nEach of the hidden units has a ReLU activation.\nThis activation function is computationally efficient and mitigates the issue of vanishing gradients since the function never saturates, which makes it one of the most popular activation functions.\nFor the output units, a softmax activation was used to get the classification and a linear activation for COMPAS.\nThe network, including the adversarial reader, is trained with the Adam optimizer with \\(\\beta _1=0.9, \\beta _2=0.9999\\)  and an initial learning rate \\(l_r = 0.01\\) , which is adjusted by a factor of\u00a0\\(0.1\\)  when reaching a plateau.\n", "We also approximate\u2014relying on the earlier discussed transferability of attacks\u00a0[11]\u2014the attacked model by an SVM with a radial basis function kernel. We set the hyperparameters \\(C\\)  and \\(\\gamma \\)  with a grid search with a reduced number of values: respectively \\(\\lbrace 0.0001; 0.001; 0.01; 0.1; 1.0\\rbrace \\)  and \\(\\lbrace 0.01; 0.1; 1; 10; 100;1000\\rbrace \\) .\nWe performed 10-fold cross-validation.\n"]}
{"id": "2005.10745", "categories": "cs.CV cs.LG eess.IV", "paragraphs": ["To generate the \\(N\\times K \\times F\\)  array containing neighborhood relationships, we define a search radius \\(R\\) =25m and sample \\(K\\) =128 points from within each spherical region, thus creating an array of dimensions \\(2,048\\times 128\\times 10\\)  where each of the original 2,048 points within a block is represented by 128 points from its 25m neighborhood, each with a 10-dimensional feature vector. While sampling 128 points from the 25m radius area seems very small, applying this for every point within the block sufficiently covers the neighborhood when viewed together.\n", "During the training process, we use the Adam optimizer\u00a0[17] with an initial learning rate 0.001 and a momentum of 0.9. Since the inputs to the network are a \\(2048\\times 10\\)  array and a \\(2048\\times 128\\times 10\\)  array, we choose a batch size of 12 due to memory limitations in our current computing environment.\nThe learning rate is linearly reduced after each iteration as a function of the current number of epochs. We train the network for a total of 16 hours for 20 epochs and monitor the progress of the validation loss to save the weights as the loss improves.\nWe train using an NVIDIA Tesla P40 GPU and Keras\u00a0[5] with the Tensorflow backend.\n"]}
{"id": "2003.05198", "categories": "cs.LG cs.CR stat.ML", "paragraphs": ["The loss function in Eq. (REF ) is difficult to be solved due to the complex architectures.\nWe learn the loss function using iterative optimization method via gradient descent using back propagation [12], as summarized in Algorithm 2.\nBoth forward computation and backward computation need communication between \\(\\mathcal {A}\\) , \\(\\mathcal {B}\\) , and the server, in a decentralized manner.\nDuring training, all the private data (\\(\\textbf {X}_A\\) , \\(\\textbf {X}_B\\) , and \\(\\textbf {y}\\) ) and private data related model parameters (\\(\\mathbf {\\theta }_A\\) , \\(\\mathbf {\\theta }_B\\) , and \\(\\mathbf {\\theta }_y\\) ) are kept by data holders. Therefore, data privacy is kept to a large extent.\n", "It is worth noticed that our proposal can be generalized to the situations that the data holders collaboratively calculate \\(i\\)  (\\(1 \\le i \\le L\\) ) hidden layers instead of the first hidden layer only. Therefore, the existing method [16] is one of our special cases, i.e., \\(\\mathcal {A}\\)  and \\(\\mathcal {B}\\)  collaboratively calculate all the neural networks using secure multi-party computation techniques, without the neutral server.\n"]}
{"id": "2002.01808", "categories": "cs.CL cs.LG", "paragraphs": ["We use \\(\\text{RoBERTa}_{LARGE}\\)  (L=24, H=1024, A=16, 355M params) implementation by Huggingfacehttps://github.com/huggingface/transformers as the pre-trained model in all our experiments.\nAs for each adapter layer, we denote the number of transformer layer as \\(N\\) , the hidden dimension of transformer layer as \\(H_A\\) , the number of self-attention heads as \\(A_A\\) , the hidden dimension of down-projection and up-projection layers as \\(H_d\\)  and \\(H_u\\) .\nIn detail, we have the following adapter size: \\(N=2\\) , \\(H_A=768\\) , \\(A_A=12\\) , \\(H_u=1024\\)  and \\(H_d=768\\) . The RoBERTa layers where adapter layers plug in are {0,11,23}, and different adapter layers do not share parameters. Thus the total parameters for each adapter model are about 42M, which are much smaller than \\(\\text{RoBERTa}_{LARGE}\\)  and make the training process memory efficient. It should be noticed that RoBERTa is fixed during training and the parameters of adapters are trainable and initialized randomly. Then we describe how to inject different knowledge into knowledge-specific adapters as below.\n"]}
{"id": "2009.11839", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["We first use SGD to analyze change in norm of model parameters, denoted as \\(\\Delta \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}\\) . Assume mini-batch \\(X_{i}\\)  is used to update model parameters at the current iteration.\n\\(\\begin{split}\\Delta \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2} &= \\left\\Vert {\\bf \\Theta }(t) - \\eta {\\bf d}({\\bf \\Theta }(t); X_{i})\\right\\Vert _{2}^{2} - \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}\\\\&= {\\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}} - 2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}({\\bf \\Theta }(t); X_{i}) + \\eta ^{2} \\left\\Vert {\\bf d}({\\bf \\Theta }(t); X_{i})\\right\\Vert _{2}^{2} - {\\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}}\\\\&= - 2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}({\\bf \\Theta }(t); X_{i}) + \\mathcal {O}(\\eta ^{2})\\\\&\\approx - 2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}({\\bf \\Theta }(t); X_{i}),\\\\\\end{split}\\) \n", "As mentioned before, to compute importance estimates for network pruning, most pruning frameworks stop the training process and compute estimates for gradient/Hessian using several mini-batches of data. To account for this, we can take expectation over the mini-batch sampling process. For example, if input data is denoted as \\(X\\) , under expectation, eq:changeinnorm changes as follows:\n\\(\\begin{split}E_{X_{i} \\sim X}\\left[\\Delta \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}\\right] &= E_{X_{i} \\sim X}\\left[- 2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}({\\bf \\Theta }(t); X_{i})\\right]\\\\&= - 2 \\eta \\, {\\bf \\Theta }(t)^{T} E_{X_{i} \\sim X}\\left[{\\bf d}({\\bf \\Theta }(t); X_{i})\\right]\\\\&= - 2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf g}({\\bf \\Theta }(t)).\\\\\\Rightarrow E_{X_{i} \\sim X}\\left[\\frac{\\Delta \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}}{\\eta }\\right] &= -2 {\\bf \\Theta }(t)^{T} {\\bf g}({\\bf \\Theta }(t)).\\\\\\end{split}\\) \n", "Therefore, using eq:expectednorm and eq:flownorm, it can be evidently seen that up to a first-order approximation, the expected rate at which norm of model parameters evolves under SGD is exactly the same as that under gradient flow. In the main paper, we use this relationship to conclude Observations 2 and 3. Thus, our analysis in this section indicates Observations 2, 3 are in fact valid approximations under SGD training as well.\n", "eq:changeinnorm shows that when mini-batch \\(X_{i}\\)  is used to take an optimization step, up to a first-order approximation, the change in norm of model parameters (\\(\\Delta \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}\\) ) is as follows:\n\\(\\begin{split}\\Delta \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2} &= - 2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}({\\bf \\Theta }(t); X_{i}).\\\\\\end{split}\\) \n", "Note that in the above equation, the Hessian estimate is based on mini-batch \\(X_{j}\\)  only. Using this relationship, the change in norm of model parameters can be approximated as follows:\n\\(\\begin{split}\\Delta \\left\\Vert {\\bf \\Theta }(t) - \\eta {\\bf d}({\\bf \\Theta }(t); X_{i})\\right\\Vert _{2}^{2} &= - 2 \\eta \\, \\left({\\bf \\Theta }(t) - \\eta {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)\\right)^{T} {\\bf d}\\left({\\bf \\Theta }(t) - \\eta {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right); X_{j} \\right)\\\\&= - 2 \\eta \\, \\left({\\bf \\Theta }(t) - \\eta {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)\\right)^{T} \\left({\\bf d}\\left({\\bf \\Theta }(t); X_{j} \\right) - \\eta {\\bf H}({\\bf \\Theta }(t); X_{j}) {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)\\right)\\\\=& -2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{j} \\right) + 2 \\eta ^{2}\\, {\\bf \\Theta }(t)^{T} {\\bf H}({\\bf \\Theta }(t); X_{j}) {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right) \\\\&+ 2 \\eta ^{2}\\, {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{j}\\right) + \\mathcal {O}(\\eta ^{3})\\\\\\approx & -2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{j} \\right) + 2 \\eta ^{2}\\, {\\bf \\Theta }(t)^{T} {\\bf H}({\\bf \\Theta }(t); X_{j}) {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right) \\\\&+ 2 \\eta ^{2}\\, {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{j}\\right),\\\\\\end{split}\\) \n", "where we ignore terms of the order of \\(\\mathcal {O}(\\eta ^{3})\\) . This above result can now be used to calculate \\(\\Delta ^{2} \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}\\) , our desired quantity.\n\\(\\begin{split}\\Delta ^{2} \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2} &= \\Delta \\left\\Vert {\\bf \\Theta }(t) - \\eta {\\bf d}({\\bf \\Theta }(t); X_{i})\\right\\Vert _{2}^{2} - \\Delta \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}\\\\=& [-2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{j} \\right) + 2 \\eta ^{2}\\, {\\bf \\Theta }(t)^{T} {\\bf H}({\\bf \\Theta }(t); X_{j}) {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right) \\\\&+ 2 \\eta ^{2}\\, {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{j}\\right)] - [-2 \\eta \\, {\\bf \\Theta }(t)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{i} \\right)] \\\\=& -2 \\eta \\, {\\bf \\Theta }(t)^{T} \\left({\\bf d}\\left({\\bf \\Theta }(t); X_{j} \\right) - {\\bf d}\\left({\\bf \\Theta }(t); X_{i} \\right)\\right) + 2 \\eta ^{2}\\, {\\bf \\Theta }(t)^{T} {\\bf H}({\\bf \\Theta }(t); X_{j}) {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right) \\\\&+ 2 \\eta ^{2}\\, {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{j}\\right).\\\\\\end{split}\\) \n", "We again use the fact that importance estimates for network pruning are estimated by stopping model training and computing gradient/Hessian estimates over several mini-batches of data. We denote this through an expectation over input data \\(X\\) , where our random variables are the mini-batches \\(X_{i}\\)  and \\(X_{j}\\) . As mini-batches are sampled independently, we use the result that expectation of product of their functions can be independently evaluated: i.e., \\(E_{X_{i}, X_{j} \\sim X}[f(X_{i})g(X_{j})] = E_{X_{i}\\sim X}[f(X_{i})] E_{X_{j} \\sim X}[g(X_{j})]\\) .\n\\(\\begin{split}\\Rightarrow E_{X_{i}, X_{j} \\sim X}\\left[\\Delta ^{2} \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}\\right] &= -2 \\eta \\, E_{X_{i}, X_{j} \\sim X}\\left[{\\bf \\Theta }(t)^{T} \\left({\\bf d}\\left({\\bf \\Theta }(t); X_{j} \\right) - {\\bf d}\\left({\\bf \\Theta }(t); X_{i} \\right)\\right)\\right] \\\\&+ 2 \\eta ^{2}\\, E_{X_{i}, X_{j} \\sim X}\\left[{\\bf \\Theta }(t)^{T} {\\bf H}({\\bf \\Theta }(t); X_{j}) {\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)\\right] \\\\&+ 2 \\eta ^{2}\\, E_{X_{i}, X_{j} \\sim X}\\left[{\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)^{T} {\\bf d}\\left({\\bf \\Theta }(t); X_{j}\\right)\\right]\\\\&= -2 \\eta \\, {\\bf \\Theta }(t)^{T} \\left(E_{X_{j} \\sim X}\\left[\\left({\\bf d}\\left({\\bf \\Theta }(t); X_{j} \\right)\\right] - E_{X_{i} \\sim X}\\left[{\\bf d}\\left({\\bf \\Theta }(t); X_{i} \\right)\\right)\\right] \\right)\\\\&+ 2 \\eta ^{2}\\, {\\bf \\Theta }(t)^{T} E_{X_{j} \\sim X}\\left[{\\bf H}({\\bf \\Theta }(t); X_{j})\\right] E_{X_{i}\\sim X}\\left[{\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)\\right]\\\\&+ 2 \\eta ^{2}\\, E_{X_{i} \\sim X}\\left[{\\bf d}\\left({\\bf \\Theta }(t); X_{i}\\right)\\right]^{T} E_{X_{j} \\sim X}\\left[{\\bf d}\\left({\\bf \\Theta }(t); X_{j}\\right)\\right]\\\\&= -2 \\eta \\, {\\bf \\Theta }(t)^{T} \\left({{\\bf g}({\\bf \\Theta }(t))} - {{\\bf g}({\\bf \\Theta }(t))}\\right) + 2 \\eta ^{2}\\, {\\bf \\Theta }(t)^{T} {\\bf H}({\\bf \\Theta }(t)) {\\bf g}({\\bf \\Theta }(t))\\\\&+ 2 \\eta ^{2}\\, {\\bf g}({\\bf \\Theta }(t))^{T} {\\bf g}({\\bf \\Theta }(t))\\\\&= 2 \\eta ^{2} \\left(\\left\\Vert {\\bf g}({\\bf \\Theta }(t))\\right\\Vert ^{2} + {\\bf \\Theta }(t)^{T} {\\bf H}({\\bf \\Theta }(t)) {\\bf g}({\\bf \\Theta }(t))\\right).\\\\\\end{split}\\) \n\\(\\begin{split}\\Rightarrow E_{X_{i}, X_{j} \\sim X}\\left[\\frac{\\Delta ^{2} \\left\\Vert {\\bf \\Theta }(t)\\right\\Vert _{2}^{2}}{\\eta ^{2}}\\right] = 2 \\left(\\left\\Vert {\\bf g}({\\bf \\Theta }(t))\\right\\Vert ^{2} + {\\bf \\Theta }(t)^{T} {\\bf H}({\\bf \\Theta }(t)) {\\bf g}({\\bf \\Theta }(t))\\right).\\\\\\end{split}\\) \n", "As can be evidently seen in eq:z2 and eq:flowz2, up to a first-order approximation, the expected rate at which change of norm of model parameters changes under SGD is exactly the same as that under gradient flow. In the main paper, we use this relationship to conclude Observations 4 and 5. Thus, our analysis in this section indicates Observations 4, 5 are in fact valid approximations under SGD training as well.\n"]}
{"id": "2007.04206", "categories": "cs.LG stat.ML", "paragraphs": ["We use the ResNeXt-29 (\\(32{\\times }4\\) ) architecture [19], which was the best-performing of those tested by [6]. We use SGD with Nesterov momentum, an initial learning rate of 0.1 decayed following a cosine schedule [12], and weight decay of 0.0005. Input images are pre-processed with standard random left-right flipping and cropping prior to any augmentations. We train ensembles for 250 epochs, and otherwise train for 200 epochs, following [17].\nWe initialise the per-ensemble parameters (the vectors that produce the rank-1 modification of the weight matrices) for BatchEnsemble with a \\(N(1,0.5^2)\\)  distribution, and always use 4 ensemble members. We use a batch size of 128, which is 4\\(\\times \\)  larger when using BatchEnsemble (each ensemble member sees a copy of the same 128 images).\n"]}
{"id": "2004.09036", "categories": "cs.CL cs.IR cs.LG", "paragraphs": ["The model is implemented by Kerashttps://keras.io/.\nWe use pre-trained Glove as word embedding, the dimension of which is 300.\nThe train and dev batch size are 1024 and 512.\nThe kernel size, filter number, and block size of CNN are 7, 128, and 7 by tuning on the dev set.\nThe fix-length of prompts and responses are 40 and 280 according to the length distribution of prompts and responses in the training data.\nNadam\u00a0[1] is used as our optimizer with a learning rate of 0.002.\nThe loss function is binary cross-entropy.\nThe epoch size is 20, and we apply early-stop when dev loss has not been improving for three epochs.\nOur GCBiA model and inference code is released here.https://github.com/zhayefei/off-topic-GCBiA\n"]}
{"id": "2012.04337", "categories": "cs.LG cs.AI cs.CV", "paragraphs": ["To verify the practical usability of MORPH on real-world noisy labels, we performed a classification task on Webvision 1.0 and FOOD-101N. We followed exactly the same configurations in the previous work\u00a0[4], [16], [19]. For Webvision 1.0, we trained an InceptionResNet-V2 from scratch for 120 epochs using SGD with a momentum of \\(0.9\\)  and an initial learning rate of \\(0.1\\) , which was divided by 10 after 40 and 80 epochs\u00a0(refer to [4]). For FOOD-101N, we trained a ResNet-50 with the ImageNet pretrained weights for 60 epochs using SGD with a momentum of \\(0.9\\)  and an initial learning rate of \\(0.01\\) , which was divided by 10 after 30 epochs\u00a0(refer to [16]). Regardless of the dataset, we used a batch size of 64, a dropout of \\(0.4\\) , and a weight decay of \\(0.001\\) . Random crops and horizontal flips were applied for data augmentation.\n"]}
{"id": "2012.08336", "categories": "cs.LG cs.DC cs.NI math.OC", "paragraphs": ["For all experiments, we initialize our model with \\(\\mathbf {w}_0=\\mathbf {0}\\)  and SGD batch size \\(b=64\\) . In each round, we uniformly sample \\(K\\)  devices at random, which run \\(E\\)  steps of SGD in parallel. For the prototype system, we use an initial learning rate \\(\\eta _0=0.01\\)  with a fixed decay rate of \\(0.996\\) . For the simulation system, we use decay rate \\(\\frac{{\\eta }_0}{1+r}\\) , where \\(\\eta _0=0.1\\)  and \\(r\\)  is communication round index. We evaluate the aggregated model in each round on the global loss function. Each result is averaged over 50 experiments.\n"]}
{"id": "2010.02974", "categories": "cs.LG cs.AI cs.MA", "paragraphs": ["All algorithms are implemented in the PyMARL framework [40]. All our experiments use \\(\\epsilon \\) -greedy scheme where \\(\\epsilon \\)  is decayed from \\(\\epsilon =1\\)  to \\(\\epsilon =0.05\\)  over {\\(250k\\) , \\(500k\\) } time steps. All our tasks use a discount factor of \\(\\gamma =0.99\\) . We freeze the trained policy every \\(30k\\)  timesteps and run 20 evaluation episodes with \\(\\epsilon =0\\) . We use learning rate of \\(0.0005\\)  with soft target updates for all experiments. We use a target network similar to [30] with \u201csoft\" target updates, rather than directly copying the weights: \\(\\theta ^- \\leftarrow \\beta *\\theta + (1-\\beta )*\\theta ^-\\) , where \\(\\theta \\)  are the current network parameters. We use \\(\\beta =0.005\\)  for PP and \\(m\\) -step experiments and \\(\\beta =0.05\\)  for SC2 experiments. This means that the target values are constrained to change slowly, greatly improving the stability of learning. All algorithms were trained with RMSprop optimizer by one gradient step on loss computed on a batch of 32 episodes sampled from a replay buffer containing last 1000 episodes (for SC2, we use last 3000 episodes). We also used gradient clipping to restrict the norm of the gradient to be \\(\\le 10\\) .\n", "The probability \\(\\alpha \\)  of action selection based on target task in UneVEn with uniform and greedy action selection schemes increases from \\(\\alpha =0.3\\)  to \\(\\alpha =1.0\\)  over \\(\\lbrace 250k, 500k\\rbrace \\)  time steps. For sampling related tasks using normal distribution, we use \\(\\mathcal {N}(w, \\sigma \\textbf {I}_d)\\)  centered around target task \\(w\\)  with \\(\\sigma \\in \\lbrace 0.1, 0.2\\rbrace \\) . At the beginning of each episode, we sample six related tasks, therefore \\(|\\nu |=6\\)  (for SC2, we use \\(|\\nu |=3\\) ).\n"]}
{"id": "2010.02990", "categories": "cs.LG cs.SY eess.SY", "paragraphs": ["\nGD fixed step size: \\(\\eta =10^{-3}\\)\n\nRGF Euler disc. w/fixed step size: \\(q=2.2,\\;\\eta =10^{-3}\\)\n\nRGF Euler disc. w/fixed step size: \\(q=3,\\;\\eta =10^{-2}\\)\n\nRGF Euler disc. w/fixed step size: \\(q=6,\\;\\eta =10^{-2}\\)\n\nRGF Euler disc. w/fixed step size: \\(q=10,\\;\\eta =10^{-2}\\)\n\nGD Nesterov acceleration fixed step size: \\(\\eta =10^{-4};\\mu =0.9\\)\n\nSGF Nesterov-like disc. w/fixed step size: \\(q=2.2,\\;\\eta =10^{-4},\\;\\mu =0.9\\)\n\nSGF Nesterov-like disc. w/fixed step size: \\(q=3,\\;\\eta =10^{-3},\\;\\mu =0.9\\)\n\nSGF Nesterov-like disc. w/fixed step size: \\(q=6,\\;\\eta =10^{-3},\\;\\mu =0.9\\)\n\nSGF Nesterov-like disc. w/fixed step size: \\(q=10,\\;\\eta =10^{-2},\\;\\mu =0.09\\)\n\nRGF Runge Kutta disc. w/fixed step size: \\(q=2.2,\\;K=2,\\;\\eta =10^{-2},\\;\\beta _1=0.09,\\;\\alpha _1=\\alpha _2=0.5\\)\n\nRGF Runge Kutta disc. w/fixed step size: \\(q=3,\\;K=2,\\;\\eta =10^{-2},\\;\\beta _1=0.09,\\;\\alpha _1=\\alpha _2=0.5\\)\n\nRGF Runge Kutta disc. w/fixed step size: \\(q=6,\\;K=2,\\;\\eta =10^{-2},\\;\\beta _1=0.09,\\;\\alpha _1=\\alpha _2=0.5\\)\n\nRGF Runge Kutta disc. w/fixed step size: \\(q=10,\\;K=2,\\;\\eta =10^{-2},\\;\\beta _1=0.09,\\;\\alpha _1=\\alpha _2=0.5\\)\n\n", "\nGD: \\(\\eta =4.10^{-2},\\;\\mu =0.9,\\;\\textnormal {Nesterov=True}\\)\n\nRGF: \\(\\eta =4.10^{-2},\\;\\mu =0.9\\)\n\nSGF: \\(\\eta =4.10^{-3},\\;\\mu =0.9\\)\n\nADAM: \\(\\eta =8.10^{-4}\\)  (remaining coefficients=nominal values)\n\nRMS: \\(\\eta =10^{-3}\\)  (remaining coefficients=nominal values)\n\nADAGRAD: \\(\\eta =10^{-3}\\)  (remaining coefficients=nominal values)\n\nADADELTA: \\(\\eta =4.10^{-2},\\;\\rho =0.9,\\;\\epsilon =10^{-6},\\;\\textnormal {weight decay}=0\\)\n\n"]}
{"id": "2010.05322", "categories": "cs.CV", "paragraphs": ["Data Out of the 149 images in the training subset, 99 of them are used for training the model and the remaining 50 are used as the validation set. The network is trained with single image input (i.e., batch size = 1).\n", "Architecture We use an U-Net [10] based network that has \\([16, 32, 64, 128, 256, 128, 64, 32, 16]\\)  filters for the key-value detector. It uses \\(3 \\times 3\\)  convolutional filters for every convolution layers. Following a convolutional layer are the Rectified Linear Unit (ReLU) activation function and a batch normalization layer. Furthermore, instead of transposed convolution [11], resize-convolution (nearest-neighbour interpolation up-sampling followed by normal convolution) was used as the method of choice for up-sampling the feature map. We also use the same network but with normal convolution replaced by CI-Deformable convolution [12], which helps the network more flexible at spatial modeling.\n", "Loss Function A combination of dice loss and categorical cross-entropy loss. The dice loss is only calculated on the first three layers, ignoring the channel corresponding to the background class. For the categorical cross-entropy loss, a weighted version is used. A set of weights proportional to [1.0, 1.0, 1.0, 0.3], which is normalized so that the weights sum up to 1.0, is applied during cross-entropy calculation. This effectively makes the weight of the background be one-third of other classes. Finally, the final loss value is a weighted sum of the dice loss value and the cross-entropy loss value:\n\\(loss_{final} = 4 \\times loss_{dice} + 0.5 + 0.5 \\times loss_{cross-entropy}\\) \n", "Other configurations Kernel weights are initialized with He Normal Initialization [12] and are normalized using L2 normalization with a factor of 0.01. All convolutional layers use \u201csame\u201d padding and kernels of size \\(3 \\times 3\\) . During training, Adam optimizer with learning rate = 0.0001 is used. The model is trained for 200 epochs but with early stopping procedure in place, also the weights with the best result will be restored upon termination of training.\n"]}
{"id": "2010.05351", "categories": "cs.CV", "paragraphs": ["For training schedule, we used cosine annealing with one warm up epoch [3]. The total number of epochs is 15 for most models. The initial learning rate of the cosine cycle is tuned for each model, which ranges from \\(1e-4\\)  to \\(3e-4\\) . The learning rate in the warm up epoch is always one tenth of the initial learning rate of the cosine cycle. Batch size is 64 for all models. All training were done on NVIDIA Tesla V100 GPUs in mixed precision. Up to 8 GPUs were used in parallel.\n"]}
{"id": "2002.01145", "categories": "cs.CL", "paragraphs": ["The dimensions of the LSTM layer and the attention layer were set to 200.\nThe depth of the LSTM layer was set to 2.\nThese sizes were based on the setting of the LSTM NER tagger with ELMo in the study of [20] peters-etal-2018-deep.\nAll parameters were initialized with [6] glorot2010understanding's method.\nFor all methods, we applied Dropout [21] to the input of the LSTM layers.\nAll dropout rates were set to 0.3.\nWe used Adam [11] with an initial learning rate of 0.001 as our optimizer.\nAll gradients were averaged by the number of sentences in each mini-batch.\nThe clipping threshold value for the gradients was set to 5.0.\nThe maximum training epoch was set to 20.\nWe used \\(\\lbrace 1,2,3,4\\rbrace \\)  as \\(\\mathbf {d}\\)  in Eq.(REF ) and Eq.(REF ).\nThe maximum mini-batch size was set to 16, and the order of mini-batches was shuffled at the end of each training epoch.\nWe adopted early stopping to the models based on maximizing per-sentence accuracy (i.e., how many summaries are fully reproduced) of the development data set.\n"]}
{"id": "2010.12854", "categories": "cs.CL cs.AI", "paragraphs": ["We train the entire model on the factoid QA task to ensure that the document representations do capture factual knowledge. We primarily use the SQuAD reading-comprehension dataset\u00a0[22] containing more than 100,000 crowd-sourced factoid questions. We further augment this dataset with about 500,000 rule-based questions from the UnsupervisedQA(UQA) dataset\u00a0[15]. This allows increasing the size of the training dataset while also introducing question diversity. To avoid these automatically generated questions overwhelming training, we ensure that the same number of questions are selected from both the datasets in each batch (by duplicating SQuAD questions). In the same vein, we evaluate each model based on their performance on the SQuAD task.The scores on UQA correlate well with the scores on SQuAD, with close to 90 F1 for most models.\n", "Unless otherwise mentioned, we use the BART-Large model in all our experiments, and optimize the model with cross-entropy loss. We set the learning rate to 1e-5 for the weights initialized from the BART model, and to 1e-4 for randomly initialized newly added weights, which is shown beneficial in [19]. For other hyper-parameters, we follow [14].\n"]}
{"id": "2002.10546", "categories": "cs.CL", "paragraphs": ["Both the LSTM layers from the decoder and the feedforward layer that processes the spans used 250 hidden units. Versions of the model using learned word embeddings and character based CNN representations used the same parameters as for the POS tagger \u2013 50 and 128, respectively.\n", "Training was performed using all sentences from the PPCEME training section of length \\(\\le \\)  300, which resulted in the exclusion of 65 sentences (out of 85,398). The parser was trained using a batch size of 500 tokens and the Adadelta optimizer for up 100 epochs using early stopping.\n"]}
{"id": "2011.02853", "categories": "cs.CV cs.RO", "paragraphs": ["For training the networks, we use Adam optimizer, whose parameters are empirically set as learning rate is \\(0.001\\) , beta1 is \\(0.9\\) , beta2 is \\(0.999\\) . Training data are split into mini-batches with a batch size of 64. The training process is finished when validation accuracy begins to decrease.\n"]}
{"id": "2011.02686", "categories": "cs.CL", "paragraphs": ["Unless stated otherwise, we use default parameters from the original DRG work by li2018delete.\nWith 1 NVIDIA Tesla V100 GPU, it takes a couple of days to train with our implementation of the style transfer model. We use a batch size of 256, a maximum sequence length of 30, a vocab size of 20K, and word-level tokenization. We train for 100,000 steps and evaluate for 100 steps.\n", "For model-specific parameters, we use a word embedding dimension of 128, an attention mechanism, a bidirectional LSTM encoder with 1 layer of 512 hidden dimensions and a dropout of 0.2, an LSTM decoder also with 1 layer of 512 hidden dimensions, a beam search decoder with a beam width of 5 for the data augmentation Scenario #1 in REF  and a beam width of 3 for Scenario #2. We also use a max norm of 3 for regularization and Adam for optimization with a learning rate of 0.0001.\n", "For the parameters specific to the \u201cDelete, Retrieve, Generate\u201d architecture [16], we use \\(n\\) -gram attributes and an attribute salience threshold of 10.\n", "Training consisted of 15,000,000 steps with a batch size of 100 and a learning rate of 0.01 for the first 10 million steps and 0.001 afterwards. In the Transformers layers, we used an attention dropout of 0.1 and ReLu dropout of 0.1.\n"]}
{"id": "2008.03824", "categories": "cs.CV cs.GR", "paragraphs": ["We implement our neural reflectance field and ray marching in PyTorch.\nDuring training, we randomly sample \\(50\\times 50\\)  pixel rays as a batch to train our network under collocated light as described in Sec.\u00a0REF .\nWe use Adam optimizer with an initial learning rate of 0.0001.\nWe use \\(N_1=64\\)  coarse samples and \\(N_2=128\\)  fine samples to adaptively sample light rays when building the adaptive transmittance volume and camera rays when computing the final radiance.\n", "We supervise the regressed radiance values from both the coarse and the fine network\nwith the ground truth radiance \\(\\tilde{L}\\)  from the captured images using the \\(L_2\\)  loss.\nSince we consider opaque objects,\nwe also regularize the ray transmittance (from the fine network), forcing it to be close to 0 or 1, which is helpful to get a clean background.\nOur total loss function is given by:\n\\(\\sum _q \\Vert  L^q_{\\text{coarse}} - \\tilde{L}^q\\Vert ^2 + \\Vert  L^q_{\\text{fine}} - \\tilde{L}^q\\Vert ^2 + \\beta [\\log (\\tau ^q_{{c}})+\\log (1-\\tau ^q_{{c}})],\\) \n", "where \\(q\\)  denotes a pixel ray and \\(\\beta = 0.0001\\)  is a hyper-parameter that controls the strength of the regularization term.\n"]}
{"id": "2004.11744", "categories": "cs.CV", "paragraphs": ["We divided available data into train/validation set with 15:1 ratio with more detailed description in Section\u00a0REF . The optimization method is SGDR and learning rate is set to decay from \\(0.1\\)  to \\(0.001\\) . We train the model with 5 cycles, each of which contains 100 epochs.\n"]}
{"id": "2001.10090", "categories": "cs.CV", "paragraphs": ["We use Adam optimizer to train. Learning rate = 0.0001 with linear decay rate 0.95 per 50k iterations. The total number of iteration is 400k to 1.2 million depending on the data. Batch size is set to 128. Larger or smaller batch sizes all lead to similar result.\n"]}
{"id": "2010.07972", "categories": "cs.CL cs.AI", "paragraphs": ["Pre-training: We train amber on the Wikipedia data for 1M steps first using the default hyper-parameters as mBERThttps://github.com/google-research/bert except that we use a larger batch of 8,192 sentence pairs, as this has proven effective in [21]. We then continue training the model by our objectives for another 1M steps with a batch of 2,048 sentence pairs from Wikipedia corpus and parallel corpus which is used to train XLM-15 [7]. We use the same monolingual data as mBERT and follow [7] to prepare the parallel data with one change to maintain truecasing. We set the maximum number of subwords in the concatenation of each sentence pair to 256 and use 10k warmup steps with the peak learning rate of 1e-4 and a linear decay of the learning rate. We train amber on TPU v3 for about 1 week.\n"]}
{"id": "2007.05008", "categories": "cs.CV eess.IV", "paragraphs": ["We elected to use the original VGG19 network during the style-transfer sample generation phase due to its light-weight yet powerful contextual representation ability [20]. Following Gatys et al., we take the output of the fourth convolutional layer for content and the outputs of convolutional layers one through five for style [8]. We chose to initialize \\(\\textbf {x}_{out}\\)  to be an exact copy of \\(\\textbf {x}_{cont}\\) , allowing us to retain high content image fidelity while reducing the number of iterations needed to produce a meaningful output (100 iterations to generate each sample, as shown in Fig. REF ). This allowed us to generate each image in \\(1.84 \\pm 0.03\\)  seconds using a single GTX TITAN V gpu and pytorch, making it faster than other popular data augmentation techniques. This also helps soften the transfer of spatial relationships, allowing only slight structural modifications to occur that do not alter the desired target concept. We elected to use an \\(\\alpha \\)  value of \\(2\\times 10^{-4}\\)  following visual inspection of the generated samples by an experienced nephropathologist; this ensured that the generated samples retained the morphological characteristics of their respective content image while capturing the texture and color characteristics of their respective style image. When generating each \\(\\textbf {x}_{out}\\) , we selected one content and style image at random from the training set, irrespective of their associated label or section of origin. The generated style-transfer images were assigned the label of their corresponding content image and then appended to the training set before each augmented training experiment.\n"]}
{"id": "2008.10292", "categories": "cs.CV", "paragraphs": ["Task losses. For semantic segmentation and human parts segmentation we use a cross-entropy loss (loss weights \\(\\omega _t = 1\\)  and \\(\\omega _t = 2\\)  in Eq.\u00a0REF , respectively), for saliency estimation a balanced cross-entropy loss (\\(\\omega _t = 5\\) ), for depth estimation a \\(\\mathcal {L}_1\\)  loss (\\(\\omega _t = 1\\) ), for surface normal estimation a \\(\\mathcal {L}_1\\)  loss with unit vector normalization (\\(\\omega _t = 10\\) ) and for edge detection a weighted cross-entropy loss (\\(\\omega _t = 50\\) ). For edge detection, the positive pixels are weighted with 0.95 and the negative pixels with 0.05 on PASCAL-Context, while on NYUD-v2 the weights are 0.8 and 0.2. \\(\\omega _t\\)  for each task was found by conducting a logarithmic grid search over candidate values with single-task networks.\n", "Optimization hyperparameters. Model weights \\(\\theta \\)  are updated using Stochastic Gradient Descent (SGD) with momentum of 0.9 and weight decay 0.0001. The initial learning rate is set to 0.005 and decayed during training according to a `poly' learning rate policy\u00a0[3]. For the architecture distribution parameters \\(\\alpha \\) , we use an Adam optimizer\u00a0[17] with learning rate 0.01 and weight decay 0.00005. We use a batchsize of 8 and 16 for ResNet-50 and MobileNetV2, respectively.\n", "Architecture search. We update the supergraph sequentially for each task. Before the architecture search, we `warm up' the supergraph by training each operation's model weights \\(\\theta \\)  (initialized with ImageNet weights) on the corresponding task only for 2000 iterations. The architecture distribution parameters \\(\\alpha \\)  are initialized with zeros. During the search, we alternatively train \\(\\alpha \\)  on 20% of the data and \\(\\theta \\)  on the other 80%. This cycle is repeated until \\(\\theta \\)  has received 40000 updates. Over the course of training, the Gumbel-Softmax temperature \\(\\tau \\)  is annealed linearly from 5.0 to 0.1. Importantly, we use the batch-specific statistics for batch normalization during the \\(\\alpha \\)  update phase and reset the batch statistics before training \\(\\theta \\)  after every architecture change. Furthermore, to equalize the scale of candidate operations for the search, we disable learnable affine parameters in the last batch normalization of every operation. Finally, the momentum of the \\(\\theta \\) -optimizer is reset after every change to the architecture.\n"]}
{"id": "2008.10183", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["We train all models using SGD with a momentum of \\(\\gamma = 0.9\\)  and weight decay. For MNIST, we use a batch size of 100 and train with an initial learning rate of \\(0.1\\)  decaying by \\(0.1\\)  at every 25k batches for 250 epochs, and use weight decay of \\(0.0005\\) . For CIFAR-10/100 we use a batch size of 64, and train with an initial learning rate of \\(0.1\\)  decaying by \\(0.1\\)  at the 80th and 120th epochs for 160 epochs. We set the weight decay parameter to be \\(0.0001\\) . For CIFAR-10/100 experiments, we use standard data augmentations (random horizontal flip, translation by 4 pixels). For Pascal VOC, we train with a batch size of 32 and weight decay 0.0005 for 120,000 steps at a learning rate of 0.001 decreasing the learning rate at 80,000 and 100,000 steps. Regularization coefficients are initialized at one for all \\(\\lambda _j\\)  and have their own optimizer but follow the same decay rate. This also reduces our approach to Lasso for the first batch.\n"]}
{"id": "2005.03626", "categories": "cs.CV cs.LG", "paragraphs": ["Our networks were trained using a octa-core i7 3.40GHz CPU with a GTX-1070Ti GPU. The training used RMSprop\u00a0[19] optimization with a momentum of 0.9, a decay of 0.9 and epsilon of 0.1; batch normalization with a decay of 0.9997 and epsilon of 0.001; fixed learning rate of 0.004; L2 regularization with 4e-5 weight; focal loss with alpha of 0.7 and gamma of 2.0; and batch size of 32 images and 200 epochs for training. In the NMS step an IoU coefficient of 0.6 was used to suppress duplicate detection.\n"]}
{"id": "2006.05353", "categories": "cs.CV cs.CG cs.LG", "paragraphs": ["Optimization:\nTo update the network weights, we use Adam optimizer\u00a0[51].\nThe learning rate is set in a cyclic way, as suggested by\u00a0[87].\nThe initial and the maximum learning rates are set to \\(10^{-6}\\)  and \\(5 \\cdot 10^{-4}\\)  respectively.\nThe cycle size is \\(20k\\)  iterations.\n"]}
{"id": "2002.05067", "categories": "cs.CV cs.RO eess.IV", "paragraphs": ["We implement the proposed architecture by Python and TensorFlow\u00a0[54]. The networks are trained for 100 epoch by using Adam\u00a0[55] algorithm to optimize the loss functions. The batch size is set as 4 for the training of completion network and is set as 8 for the training of super-resolution network. All the learning ratios are set to \\(10^{-4}\\) . The numbers of the layers in the encoding and decoding parts of the completion network are both 7 as shown in Fig.REF , and 5 layers are employed in the super-resolution network as shown in Fig.REF .\n"]}
{"id": "2012.05825", "categories": "cs.LG", "paragraphs": ["For SVHN, CIFAR10/CIFAR100 and ImageNet, we train ensembles of ResNet20\n(). The models are initialized with weights pretrained for 100\nepochs on the labeled training set. We fine-tune each model for 10 epochs using\nSGD with momentum \\(0.9\\) , and a learning rate of \\(0.001\\) . The weights are\ntrained with an \\(\\ell _2\\)  regularization coefficient of \\(5e-4\\) . We use a batch\nsize of 128 for all scenarios, unless explicitly stated otherwise. We used the\nsame hyperparameters for all settings.\n", "For pretraining, we perform SGD for 100 epochs and use the same architecture and\nhyperparameters as described above, with the exception of the learning rate that\nstarts at \\(0.1\\) , and is multiplied by \\(0.2\\)  at epochs 50, 70 and 90.\n", "For the medical data sets, we train a Densenet-121 as the authors do in the\noriginal paper (). For \\(\\) ++, we do not use random weight\ninitializations, but instead we start with the ImageNet weights provided with\nTensorflow. The training configuration is exactly the same as for ResNet20,\nexcept that we use a batch size of 32 due to GPU memory restrictions, and for\nfine tuning we use a constant learning rate of \\(10^{-5}\\) .\n"]}
{"id": "2010.11703", "categories": "cs.CV cs.RO", "paragraphs": ["Aiming to train the base network, we use the SGD optimizer with an initial learning rate of 0.001, with 25 epochs as maximum number of training while during every 10 epochs the learning rate decays to half of the previous.\nSimilarly, for the attention module, the SGD optimizer was selected with an initial learning rate to be set at 0.01, at maximum number of 20 epochs and learning rate which decays to half of the previous every 10 epochs.\nWe implement the two networks using the batch size of 256.\n"]}
{"id": "2010.11700", "categories": "cs.CV cs.LG", "paragraphs": ["The investigated models are trained using Adam optimizer with learning rate of 1e-4 and batch size of 64.\nEach model is trained twice, once using coarsely segmented iris as input and once using normalized iris. We set the initial number of epochs to 100 and the early stopping patience parameter to 5,\ncausing DeepIrisNet, MobileNetV3, DenseNet, and ResNet to stop after 41, 19, 21 and 29 epochs, respectively on normalized iris training data, and after 23, 19, 23 and 18 epochs, respectively for the coarsely segmented iris images.\n"]}
{"id": "2006.09191", "categories": "cs.LG stat.ML", "paragraphs": ["When retraining a model with frequency \\(r\\) ,\nthe model is optionally fine-tuned initially, then repeatedly fine-tuned on queries \\(r,\\ 2r,\\ 3r,\\ldots \\) \nuntil the query budget is reached.\nAll results use the rank-based weighting function defined in eq:weightingfunction\nunless otherwise specified.\nWe consider a budget of \\(B=500\\)  function evaluations, which is double the budget used in [36], [27].\n"]}
{"id": "2001.03152", "categories": "cs.CV cs.LG", "paragraphs": ["Implementation details: For both proposed methods, we use ResNet-50\u00a0[14] pre-trained on ImageNet as a backbone. For the first stage, an initial learning rate of \\(\\mathrm {0.1}\\)  is used which is later divided by \\(\\mathrm {10}\\)  following the standard step decay process for the learning rate. Following this, during the second stage of training, we train the network with a learning rate of \\(\\mathrm {0.01}\\)  for both methods. For the CAM-based approach, we set \\(\\mathrm {\\lambda _{1}}\\)  and \\(\\mathrm {\\lambda _{2}}\\)  to be \\(0.1\\)  and \\(0.01\\)  respectively.\n"]}
{"id": "2006.16765", "categories": "cs.LG", "paragraphs": ["We use four types of model in the experiments: multi-layer perceptron (MLP) [0], LeNet5 [25], a convolutional neural networks (CNN1) with two 3x3 convolution layers (the first with 6 channels, the second with 16 channals, each followed with 2x2 max pooling and ReLu activation) and two FC layers, and a convolutional neural networks (CNN2) with three 3x3 convolution layers (each with 128 channels followed with 2x2 max pooling and ReLu activation) and one FC layer. MLP/LeNet5 are used for MNIST dataset and CNN1/CNN2 are used for CIFAR10/100 datasets. The optimizer we choose is the SGD algorithm, with \\(momentum=0.9\\) , \\(weight\\_decay=5\\times 10^{-4}\\)  and \\(batchsize=128\\) .\n"]}
{"id": "2010.02311", "categories": "cs.LG stat.ML", "paragraphs": ["The value of \\(\\lambda \\)  and \\(\\epsilon \\)  in equation REF  are set based on the statistics of \\(\\ell _1(f(\\mathbf {x}_j), \\mathbf {y}_i)\\) . Our goal is to have a decent number of suggested \\(\\mathbf {x}\\) 's that have a property vector that is within \\(\\ell _1\\)  distance of \\(\\epsilon \\)  from the desired property vector:\na simple heuristic is to choose them such that, if we plan at train time to draw \\(K\\)  samples from \\(p(\\cdot |i)\\) , we see the original paired values \\(\\mathbf {x}_j\\)  with probability roughly \\(1/K\\) ;\nappropriate values of \\(\\epsilon \\)  can then be selected by inspecting the dataset.\nFor example, when we condition on 9 properties in QM9, while sampling \\(K = 10\\)  values of \\(\\mathbf {x}_j\\)  for each \\(\\mathbf {y}_i\\)  when evaluating the loss, we set \\(\\epsilon = 0.3\\)  and \\(\\lambda =1\\) :\nunder these values for any given \\(\\mathbf {y}\\)  from the training set we have a minimum of one and a maximum of 168 suggested \\(\\mathbf {x}\\) 's, with an average\nof 13.\nSimilarly for ChEMBL dataset, we set \\(\\epsilon = 0.4\\) .\nIf we condition on a single, smooth, property, such as LogP, we set \\(\\epsilon = 5 \\times 10^{-5}\\)  and \\(\\lambda = 10^5\\) , since in that case we can find many molecules that have a practically identical properties.\n"]}
{"id": "2012.03551", "categories": "cs.CL cs.AI", "paragraphs": ["Pre-training large-scale language model from scratch is expensive, so we use the RoBERTa\\(_{\\text{BASE}}\\)  implemented by Huggingfacehttps://github.com/huggingface/transformers. as the initialization of our model.\nWe follow the model configuration in RoBERTa\\(_{\\text{BASE}}\\)  and continue the knowledge-guided pre-training on English Wikipediahttps://hotpotqa.github.io/wiki-readme.html. for 5 epochs. Details of the knowledge span extraction from Wikipedia is shown in Sec\u00a0REF .\nDifferent from randomly masking a certain number of subwords in general pre-training, knowledge-related spans are not uniformly distributed in the corpus. Therefore, we use dynamic span masking to select the knowledge spans, thus different spans are learned in different pre-training epochs.\nFurthermore, we employ dynamic span replacement to choose different negative knowledge spans for each epoch.\nThe details of pre-training experimental settings are described in Sec\u00a0REF .\n"]}
{"id": "2003.02751", "categories": "cs.LG cs.CE stat.ML", "paragraphs": ["Fig.\u00a0REF  analyzes the effect of availability of data on the training. We computed the exact solution on four different uniform grids of size \\(10\\times 10\\) , \\(40\\times 40\\) , \\(160\\times 160\\) , and \\(640\\times 640\\) ; and carried out the parameter identification process. We performed the comparison using force-complete data and a network with 10\u00a0layers and 20\u00a0neurons per layer (network\u00a0iii). The training process found good approximations to the parameters for all cases, including that with only \\(10\\times 10\\) \u00a0points. The results show that fewer data points require many more epoch cycles, but the overall computational cost is far lower.\n{FIGURE}"]}
{"id": "2012.12519", "categories": "cs.CV", "paragraphs": ["     We use resnet50 as our backbone network. For all the convolutional part, we use the pretrained model on ImageNet to initialize the network. The parameter initialization methods of the FC layer (see Fig. REF ) will be discussed later. For res4 (the last convolutional block) of resnet50, the stride is set to 1. In the training stage and testing stage, the input images are all processed as follows: (1) resize image size to \\(224\\times 224\\) , (2) the pixel value is linearly adjusted to [0,1], (3) the mean and standard deviation are set to [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225] as the same as the ImageNet. Using Adam parameter optimization strategy, the batch size is set to 64. As in [42], we set \\(\\lambda \\)  to 0.5, \\(\\alpha \\)  to 0.003, and we set \\(\\mu \\)  in formula REF  to 0.005. The learning rate begins at 0.0001. The learning rate adjustment strategy is as follows. For the VeRi-776 dataset, MSM T17 dataset and market1501 dataset, the learning rate is divided by 10 at 10 and 17 epoch, and the total epochs is 22. For the VehicleID dataset, the learning rate is divided by 10 at 50 and 60 epoch, and the total epochs is 65.\n"]}
{"id": "2008.07832", "categories": "cs.CV cs.LG", "paragraphs": ["Hyperparameters.  The radius hyperparameter \\(\\gamma =12\\)  in Eqs.\u00a0(REF ,REF ,REF ), temperature \\(T=1.5\\)  in Eq.\u00a0(REF ), \\(\\lambda _{\\mathcal {G}}=0.1\\)  in Eq.\u00a0(REF ), and \\(\\lambda _{\\mathcal {G} \\rightarrow \\mathcal {F}}=0.1\\)  in Eq.\u00a0(REF ) are selected according to the evaluations on the validation set.\n", "Training Details. All models are trained up to 50,000 batch iterations of 12 images using SGD with 0.9 momentum. KD starts only after 10,000 iterations (\\(\\lambda _{\\mathcal {G} \\rightarrow \\mathcal {F}}=0\\)  from 0\u201310,000 iterations). Learning rate starts from 0.01 and is linearly increased every batch iteration up to 0.12 until 500 iterations (i.e. the warm-up period). After the warm-up, the learning rate is decayed by 0.2 (0.1) for Motifs (VCTree) models if no improvement in R@100 is observed within two successive validation rounds (2,000 iterations / round). The training is early terminated once the learning rate is decayed for three times.\n"]}
{"id": "2009.14776", "categories": "cs.CV cs.LG", "paragraphs": ["We adopt ResNet-50 [19] as the backbone network for training JCL on the ImageNet1K dataset. For the hyper-parameters, we use positive key number \\(M^{\\prime }=5\\) , softmax temperature \\(\\tau =0.2\\)  and \\(\\lambda =4.0\\)  in Eq.(REF ) (see definitions in section REF ). We also investigate the impact of these hyper-parameters tuning in section REF . Other network and parameter settings strictly follow the implementations in MoCo v2 [8] for fair apple to apple comparisons. We attach a two-layer MLP (Multiple Layer Perceptrons) on top of the global pooling layer of ResNet-50 for generating the final embeddings. The dimension of this embedding is \\(d=128\\)  across all experiments. The batch size is set to \\(N=512\\)  that enables applicable implementations on an 8-GPU machine. We train JCL for 200 epochs with an initial learning rate of \\(lr=0.06\\)  and \\(lr\\)  is gradually annealed following a cosine decay schedule [31].\n"]}
{"id": "2009.12577", "categories": "cs.CV", "paragraphs": ["We compare our approach with other existing works for ciphers recognition, categorized in two main classes: the unsupervised methods [15] and [16], which were designed, same as ours, for handwritten ciphers with invented alphabets, and the supervised HTR method based on MDLSTM applied to numerical ciphers [14] that was also applied to Borg and Copiale ciphers in [15]. During the experiments, we tried different settings to analyze the performance of our model in depth. In our method, we vary the number of shots (i.e. the number of examples per class in the support images) and the confidence threshold. The confidence threshold means that we only transcribe the predicted symbol if its similarity score is higher than a given threshold (we used 0.4, 0.6 or 0.8 in our experiments). As explained in [15], the use of a confidence threshold is necessary for ciphered handwritten recognition because whenever the similarity score is low, it is better to ask for human intervention rather than making a wrong prediction, which would lead to error propagation during the deciphering process. Indeed, few errors in the transcription can highly affect the cryptanalysis (i.e. finding the deciphering key).\n"]}
{"id": "2005.04876", "categories": "cs.LG stat.ML", "paragraphs": ["We set the embedding size to 256, i.e the input and output tokens will be encoded into a 256 dimension vector. Number of hidden layers in encoder and decoder stacks, which are the combination of attention and feed-forward sublayers, is set to 4. We set the number of heads, number of linear projections of activations before undergoing attention mechanism to 8. The dimensionality of inner layer in feed-forward network is set to 512.\n"]}
{"id": "2005.04986", "categories": "cs.LG math.DS", "paragraphs": ["\\(N_h\\) , the dimension of hidden layers, is a parameter that needs to be carefully chosen.\nWe conduct the ablation test on the pendulum, the\nLotka\u2013Volterra, the Kepler, and the H\u00e9non\u2013Heiles problems to compare the validation loss using different \\(N_h\\) . Figure REF  shows the results of the test. From figure REF (a), it can be seen that the validation loss after convergence for the pendulum problem drops significantly after increasing \\(N_h\\)  from 8 to 16 and then stays relatively similar with higher \\(N_h\\) . Therefore, we choose to use 16 as \\(N_h\\)  for the pendulum problem. Following the same logic, we choose 8, 8, and 16 as \\(N_h\\)  for the Lotka\u2013Volterra, the Kepler, and the H\u00e9non\u2013Heiles problems. Notice that \\(N_h\\)  for the lower-dimensional problem, namely, the pendulum problem, is larger than \\(N_h\\)  for the higher dimensional problem, the Kepler problem. This is because, for the higher-dimensional problem, the degree of freedom is actually more limited. This is due to the prior knowledge that the forces between objects are the same.\n{FIGURE}", "We record the training loss for all the problems at the epochs specified above. It is worth noticing that the training loss of our model is at \\(10^{-5}\\)  order of magnitude and below, which indicates our model's ability to fit the training data. As we can see from figure REF , the prediction results using Taylor-net match perfectly with the ground truth for all three systems, even though the \\(T_{train}=0.01\\)  is \\(2000\\pi \\)  times shorter than the \\(T_{predict}=20\\pi \\)  in figure REF  (a) and (b), and 1000 times shorter in figure REF  (c). In particular, our model predicts the dynamics of the chaotic system, the H\u00e9non\u2013Heiles system (REF ) extremely well, which regular neural networks fail to do. The results indicate the compelling predictive ability of our model. This can be seen more clearly in REF  when we compare Taylor-net with other methods.\n{FIGURE}"]}
{"id": "2010.01486", "categories": "cs.CL cs.LG", "paragraphs": ["All models are implemented using the Transformers package [40], and trained for a maximum of 20 epochs. Training is performed using an Adam optimizer with linear warmup [18]. We also simulate a batch size of 16 using gradient accumulation and an actual batch size of 4. For GPT2, the learning rate is \\(2*10^{-5}\\)  and for GPT we use a learning rate of \\(6.25*10^{-5}\\) . All other hyperparameters follow [28], [27]. We retrieve the top \\(k=1\\)  inferences from memory.For GPT2 we use memory during training and decoding. For GPT, we report results using training-only memory. We use the 124M parameter version of the GPT2 model.\n"]}
{"id": "2010.01557", "categories": "cs.CV cs.AI", "paragraphs": ["We then proceed to use the cropped and aligned faces in our training routines. We maintain the same original dimensions of (120,120,3) pixels. For the FC-S, we provide a sequence with 10 images, having an input vector of dimension (10, 120,120,3). For all our experiments, we maintain a batch size of 1024 and trained all the networks using an RTX Quadro 4000 GPU.\n"]}
{"id": "2009.04642", "categories": "cs.CV", "paragraphs": ["Implementation details. The optical flow is estimated by ScopeFlow [2], a recent state-of-the-art algorithm. We fix the optical flow estimation module in all training stages. The adaptive flow filtering module is kept the same as QVI [23]. During training, a spatial patch size of \\(512 \\times 512\\)  is cropped from the original \\(1280 \\times 720\\)  frames. The mini-batch size is set to 12. We use \\(L_1+10*L_{lap}\\)  loss to train the model. The training process is detailed in REF  and Fig. REF . We implement our models with PyTorch framework and train them using 4 NVIDIA GeForce RTX 2080 Ti GPUs. The entire training process lasts about 6 days.\n{FIGURE}"]}
{"id": "2011.07553", "categories": "cs.LG", "paragraphs": ["Considering the case where we have a raw feature dimension of inputs as \\(R\\) , we choose the intermediate feature dimension to be \\(K<R\\) . A CDT with two cascading trees of depth \\(d_1\\)  and \\(d_2\\)  and a SDT with depth \\(d\\)  are compared. Supposing the output dimension is \\(O\\) , we can derive the number of parameters in CDT as:\n\\(N(CDT)=[(R+1)(2^{d_1}-1)+K\\cdot R\\cdot 2^{d_1}]+[(K+1)(2^{d_2}-1)+O\\cdot 2^{d_2}]\\) \n", "Considering an example for Eq.\u00a0REF  and Eq.\u00a0REF  with SDT being depth of 5 while CDT has \\(d_1=2, d_2=3\\) , raw feature dimension \\(R=8\\) , intermediate feature dimension \\(K=4\\) , and output dimension \\(O=4\\) , we can get \\(N(CDT)=222\\)  and \\(N(SDT)=343\\) . It indicates a reduction of around \\(35\\%\\)  parameters in this case, which will significantly increase interpretability.\n", "In another example, when \\(R=8, K=4, O=4\\) , the numbers of parameters in CDT or SDT models are compared in Fig.\u00a0REF , assuming \\(d=d_1+d_2\\)  for a total depth of range 2 to 20. The Ratio of numbers of model parameters is derived with: \\(\\frac{N(CDT)}{N(SDT)}\\) .\n{FIGURE}"]}
{"id": "2011.07466", "categories": "cs.CV cs.LG stat.ML", "paragraphs": ["The cGAN and CcGAN are trained for 30,000 iterations on the training set with the Adam [47] optimizer (with \\(\\beta _1=0.5\\)  and \\(\\beta _2=0.999\\) ), a constant learning rate \\(10^{-4}\\)  and batch size 256.\n", "The rule of thumb formulae in Rmk\u00a0REF  are used to select the hyper-parameters for HVDL and SVDL, where we let \\(m_{\\kappa }=2\\) . Thus, the three hyper-parameters in this experiments are set as follows: \\(\\sigma =0.0473\\) , \\(\\kappa =0.004\\) , \\(\\nu =50625\\) .\n", "The cGAN and CcGAN are trained for 40,000 iterations on the training set with the Adam [47] optimizer (with \\(\\beta _1=0.5\\)  and \\(\\beta _2=0.999\\) ), a constant learning rate \\(10^{-4}\\)  and batch size 512. The rule of thumb formulae in Section  are used to select the hyper-parameters for HVDL and SVDL, where we let \\(m_{\\kappa }=1\\) .\n", "The cGAN and CcGAN are trained for 5000 iterations on the training set with the Adam [47] optimizer (with \\(\\beta _1=0.5\\)  and \\(\\beta _2=0.999\\) ) and a constant learning rate \\(10^{-4}\\) . The rule of thumb formulae in Section  are used to select the hyperparameters for HVDL and SVDL, where we let \\(m_{\\kappa }=2\\) .\n", "Please note that we use different batch sizes for cGAN and CcGAN in this experiment. The batch size for cGAN is 512. Differently, for all CcGAN methods in this experiment, the batch size is 512 for the generator but 32 for the discriminator. The reason that we use different batch sizes for the generator and discriminator in CcGAN is based on some observations we got during training. In this experiment, if the generator and the discriminator in CcGAN have the same batch size, the discriminator loss often decreases to almost zero very quickly while the generator loss still maintains at a high level. Consequently, at each iteration, the discriminator can easily distinguish the real and fake images while the generator cannot fool the discriminator and won't improve in the next iteration, which implies a high imbalance between the generator update and the discriminator update. To balance the training of the generator and the discriminator, we deliberately decrease the number of images seen by the discriminator at each iteration to slow down the update of the discriminator so that the generator can catch up.\n", "The cGAN and CcGAN are trained for 20,000 iterations on the training set with the Adam [47] optimizer (with \\(\\beta _1=0.5\\)  and \\(\\beta _2=0.999\\) ) and a constant learning rate \\(10^{-4}\\) . The rule of thumb formalue in Section  are used to select the hyperparameters for HVDL and SVDL, where we let \\(m_{\\kappa }=5\\) .\n", "Please note that, similar to the Cell-200 experiment, we use different batch sizes for the generator and discriminator in four CcGAN methods. The batch size is set to 64 and 512 respectively for the discriminator and generator in CcGAN. Please refer to Supp. REF  for the reason.\n"]}
{"id": "2010.12408", "categories": "cs.LG stat.ML", "paragraphs": ["For fairness, we use the same neural network model scale as the baseline models: two-layer neural network with 64 hidden units.\nWe also use the same \\(K\\)  and \\(\\alpha \\)  as APPNP, i.e., \\(K=10, \\alpha =0.1\\)  for three citation graphs, and \\(K=10, \\alpha =0.2\\)  for co-authorship graph.\nThe overall loss function is: \\(LOSS = \\lambda _1 L_1 + \\lambda _2 L_2\\) , where \\(L_1\\)  represents loss in Equation\u00a0REF , and \\(L_2\\)  represents regularization on the weights of the first neural network layer.\nWe fix \\(\\lambda _2 = 0.005\\) , and find the best \\(\\lambda _1=0.05\\) .\nWe use the Adam optimizer with a learning rate of \\(lr = 0.1\\) \u00a0[11],\nThe dropout rate for neural model is \\(0.0\\) .\nThe addition parameter \\(\\varepsilon \\)  in Equation(\u00a0REF ) is set as 100 for all datasets.\n"]}
{"id": "2010.12251", "categories": "cs.CL", "paragraphs": ["All models were implemented in PyTorch [16] and trained and evaluated on AWS p3.8xlarge instances with Intel Xeon E5-2686 CPUs, 244 GB memory, and 4 NVIDIA Tesla V100 GPUs. We used Adam [11] as the optimization algorithm. All models (i.e., DIM, DCM, and NLU re-ranker) were trained for 10 epochs with a 4096 batch size.\n"]}
{"id": "2009.09781", "categories": "cs.CL cs.LG", "paragraphs": ["DiaSeq With respect to DiaSeq, we use a two-layer MLP to extract features from the raw state representation. First, we sort the action order according to the action frequency in the training set. All action combinations in the dataset will be transferred to an action path based on the action order. Three special actions \u2013 PAD, SOA, EOA, corresponding to padding, start of action decoding and end of action decoding \u2013 are added to the action space for action decoder training. We use beam search to predict the action combinations and beam size is set to 6. The action embedding size is set to 30; the hidden size of the GRU is 50.\n", "DiaAdv For the policy network of DiaAdv, a two-layer MLP is used to extract state features followed by 166 dense layers and Gumbel-Softmax functions consecutively. To sample a discrete action representation, we implemented the \u201cStraight-Through\" Gumbel-Softmax Estimator\u00a0[9]; the temperature \\(\\tau \\)  for each function is set to \\(0.005\\) . As to the discriminator, a three-layer MLP takes as input the concatenation of dialogue state and action, and outputs a real value as the reward for the state-action pair.\n"]}
{"id": "2006.02548", "categories": "cs.LG stat.ML", "paragraphs": ["In addition, in all our experiments (tabular and MNIST) the integrand networks used to model the monotonic transformations have their parameters shared and receive an additional input that one hot encodes the index of the transformed variable. The models are trained until no improvement of the average log-likelihood on the validation set is observed for 10 consecutive epochs.\n", "For all experiments the batch size was 100, the learning rate \\(10^{-3}\\) , the weight decay \\(10^{-5}\\) . For the graphical conditioners the number of epochs between two coefficient updates was chosen to 10, the greater this number the better were the performance however the longer is the optimization. The CNN is made of 2 layers of 16 convolutions with \\(3\\times 3\\)  kernels followed by an MLP with two hidden layers of size 2304 and 128. The neural network used for the Coupling and the autoregressive conditioner are neural networks with \\(3 \\times 1024\\)  hidden layers. For all experiments with a monotonic normalizer the size of the embedding was chosen to 30 and the integral net was made of 3 hidden layers of size 50.\nThe models are trained until no improvements of the average log-likelihood on the validation set is observed for 10 consecutive epochs.\n"]}
{"id": "2003.03212", "categories": "cs.CV", "paragraphs": ["For all experiments, we implemented models using the PyTorch 1.3.1 framework. We performed all data extraction and model training on a desktop machine with a NVIDIA TITAN X GPU card. We directly utilized the default implementations of Adam optimizer [18] with the initial learning rate of \\(1.0\\text{E-04}\\)  and an adaptive scheduler that halves the learning rate with patience parameter 3 based on the sum of the avgADE and avgFDE in validation. The batch size is 64 for all baselines and the proposed model except AttGlobal-CAM-NF, where the batch size of 4 is used. We train the model for maximum 100 epochs, however, the early stopping is applied when the over-fitting of a model is observed.\n"]}
{"id": "2008.02268", "categories": "cs.CV cs.GR cs.LG", "paragraphs": ["In the following, we document the selected hyperparameters for NeRF-W.\nA grid search over the hyperparameter space was employed to select the below values.\nNote that the values were optimized for the Brandenburg Gate scene of the Phototourism dataset and reused for the remaining scenes.\nAs in NeRF, we employ an MLP architecture for modeling density and radiance.\nWe apply 8 layers of 512 hidden units for the `Base' component, and 4 layers of 128 hidden units for both `Static` and \u201cTransient / Uncertainty' components; see Figure\u00a0REF ).\nRegarding our positional encoding functions \\(\\gamma \\)  we use 15 frequencies for when encoding position and 4 when encoding viewing direction.\nDuring training, 512 points per ray are sampled from each of the coarse and fine models for a total of 1024 points per ray.\nWe double that number during evaluation to [group-separator=,]2048.\nThe latent embedding vector for appearance has an embedding dimension of size \\(n^{(a)}=48\\) .\nFor transient objects, we use an embedding dimension of size \\(n^{(\\tau )}=16\\) .\nWe select a value of \\(0.01\\)  for the \\(L_1\\)  regularizer multiplier \\(\\lambda _{u}\\)  and \\(0.03\\)  as the minimum importance \\(\\beta _{\\min }\\) .\nModels are trained with Adam over 300,000 iterations with a batch size of 2048 and an initial learning rate of \\(0.001\\) , decaying ten-fold every 150,000 iterations.\n"]}
{"id": "2010.01657", "categories": "cs.CL", "paragraphs": ["We use the GPT2-medium model for all question generation models. The batch size is 2 and we fine-tune for 7 epochs. For SQuAD pre-training, the batch size is 1 and we fine-tune for 1 epoch.\nWe use the Adam [14] optimizer with \\((\\beta _1,\\beta _2)=(0.9,0.999)\\)  and a learning rate of 5e-5. The parameters are tuned by manually inspecting generated questions in the validation set.\n"]}
{"id": "2011.00731", "categories": "cs.CV cs.CG cs.LG math.CV", "paragraphs": ["In our experiment, we use a well-known CNN DenseNet-201\u00a0[22], which is a densely connected convolutional network with 201 layers. All layers after the third dense block are truncated. The images are partitioned into \\(m = 10 \\times 10\\) , \\(12 \\times 12\\) , \\(14 \\times 14\\) , \\(16 \\times 16\\)  or \\(18 \\times 18\\)  patches. For each registration, we consider all choices of \\(m\\)  and choose the one that gives the best registration performance in terms of the similarity of the warped image and the target image and the smoothness of the mapping. Also, in practice we keep only the top 6 to 50 values of the correlation matrix \\(C\\)  depending on the size of the features and set all other values to be 0.\n", "As for the weighting factors in the proposed energy model\u00a0(REF ), in general we set \\(\\alpha = 5\\) , \\(\\rho = 50\\) , \\(\\beta = 25\\rho \\) , and \\(\\gamma = 5\\rho \\) . The parameter in Equation\u00a0(REF ) is set to be \\(\\sigma =1\\) .\n"]}
{"id": "2006.11604", "categories": "cs.CV cs.CR cs.LG stat.ML", "paragraphs": ["The following hyperparameter settings were used during training. For each of these settings we performed our experiments with momentum set to 0.0, 0.2, 0.5 and 0.9. In all our plots we clearly mention which hyperparameter settings have been used to obtain that plot. In Section\u00a0, we discuss our observations with momentum set to zero and in Section\u00a0, we discuss our observations with non-zero momentum.\n", "\nLR (see [15]). Here learning rate is fixed to 0.01 and batch size is varied and training is done with this setting for 100 epochs. Results for this hyperparameter setting are shown in light blue colour in all our plots.\n\nLR/BS(see [9]). Here learning rate to batch size ratio is kept constant. We set the ratio to 0.00015625 and training is done with this fixed setting for 100 epochs and varying batch sizes. Results for this hyperparameter setting are shown in purple in all our plots.\n\nFor comparison with [17], we also train models using exactly the settings from their paper. Here the learning rate is set to 0.01 and momentum to 0.9, and learning rate is decayed by half after every 5 epochs, for a total of 100 epochs. Results for this hyperparameter setting are shown in red in all our plots, and we refer to this as Benchmark.\n\n", "Data sets MNIST dataset consists of \\(70,000\\)  images of \\(28 \\times 28\\)  size, divided into 10 classes. \\(55,000\\)  used for training, \\(5,000\\)  for validation and \\(10,000\\)  for testing. Fashion MNIST dataset consists of \\(70,000\\)  images of \\(28 \\times 28\\)  size, divided into 10 classes. \\(55,000\\)  used for training, \\(5,000\\)  for validation and \\(10,000\\)  for testing. CIFAR-10 dataset consists of \\(60,000\\)  images of \\(32 \\times 32\\)  size, divided into 10 classes. \\(40,000\\)  used for training, \\(10,000\\)  for validation and \\(10,000\\)  for testing.\n"]}
{"id": "2007.08019", "categories": "cs.CV cs.LG", "paragraphs": ["Image representation.\nFor all experiments we use a publicly-available, state-of-the-art model for image retrieval\u00a0[32]github.com/filipradenovic/cnnimageretrieval-pytorch to extract the underlying features.\nWe use the best-performing model from the project page (trained on Google Landmarks 2018 data\u00a0[28]), consisting of a ResNet101 trunk followed by generalized-mean pooling and a whitening layer, which produces features of 2048 dimensions.\nFollowing\u00a0[32], we extract features at 3 scales (\\(1, \\sqrt{2}, 1/\\sqrt{2}\\) ), mean-aggregate them, and finally \\(\\ell _2\\) -normalize to form the final 2048D representation.\n"]}
{"id": "2006.10190", "categories": "cs.LG cs.RO stat.ML", "paragraphs": ["The training environment is designed so that the learning agent can be exposed to various situations. The system models described in the previous section are used with the following parameters : \\(\\tau =0.5\\) , \\(V=\\text{diag}(0.2, 0.01)\\) , \\(r_\\text{margin}=1.0\\) , \\(\\nu _0=0.0\\)  [m/s]. The maximum sensing range is \\(r_\\text{sensor}=10\\)  [m] and its field of view is 120 degrees. A set of motion primitives, or the action space, is \\(\\mathcal {A}=\\lbrace (v,\\omega )|v\\in \\lbrace 0,1,2,3\\rbrace  [m/s], \\omega \\in \\lbrace 0, -\\pi /2, \\pi /2\\rbrace  [rad/s]\\rbrace \\)  and the time horizon of an episode, \\(T\\)  is set to 100.\nIn single-target scenarios, the noise constants of the target model and the belief model are set as \\(q=q_b=0.5\\) , and the maximum target velocity is set as \\(\\nu _{\\max }=3.5\\)  [m/s]. For two-target scenarios, lower values are used for training: \\(q=q_b=0.2\\)  and \\(\\nu _{\\max }=1.0\\)  [m/s]. As mentioned above, it is infeasible for an agent to keep tracking multiple targets that are diverging from each other with limited dynamic constraints. Additionally, high noise constant of the target model makes a target to be quickly diverged from its corresponding belief while the agent is tracking another target (when it is not available to cover both targets at once). While this requires an ability to explore near the belief when losing the target, it may also result in a case where committing to one target gives a higher return.\n", "Map. To learn a policy that is robust to various environmental configurations, we randomly generate a map from a set of obstacle candidates as shown in Fig. REF  (a). These obstacles include both convex concave polygons resulting in more challenging navigation tasks. At each episode, four randomly selected obstacles are placed at the center of each quadrant of an empty domain with random orientations (see Fig. REF (c)). The map resolution (cell size) is 0.4 [m] and the map dimension is \\(72.4 \\times 72.4\\)  [\\(\\text{m}^2\\) ].\n", "Initialization. For each episode, the robot is randomly initialized in a given domain. In single-target domains, the initial belief position is randomly chosen within a distance between 5.0 to 20.0 [m] from the agent, and the initial target position is randomly placed within (0.0, 20.0)[m] range from the belief. Since the agent may be required to travel between targets in two-target domains, smaller ranges are used \u2013 (5.0, 10.0)[m] range for a distance between beliefs and the agent, and (0.0, 10.0)[m] range for a distance between a belief and its corresponding target. These ranges are chosen considering the maximum sensing range of the agent. Note that having a target placed too far from its corresponding belief can lead to learning a policy that does not trust the belief.\n"]}
{"id": "2010.16417", "categories": "cs.CV", "paragraphs": ["We train all condition modules and the backbone generator jointly. The Adam optimizer [30] is used with a batch size of 8 and the total epoch number of 50. The learning rates for the generator and discriminator are set to \\(0.0001\\)  and \\(0.0004\\) , respectively. The structural loss weight \\(\\lambda _s\\)  is 10, while all other loss weights, \\(\\lambda _c\\) , \\(\\lambda _p\\) , \\(\\lambda _a\\) , and \\(\\lambda _f\\)  are simply set to 1 (Sec.\u00a0REF ). To obtain the dilated target mask \\(M^{\\prime }\\)  (Sec.\u00a0REF ), \\(M\\)  is randomly expanded by \\([0.03, 0.07]\\)  of the image width.\n"]}
{"id": "2003.02204", "categories": "cs.CV cs.LG", "paragraphs": ["The proposed model, TICPan, trained using ADAM optimizer with default Pytorch parameters and weights were initialized with He normal initialization [6]. All experiments were trained for 1000 epochs and the learning rate was initialized with \\(8e^{-4}\\)  with decay after 400 epochs. The LeakyReLU layers parameter was set to \\(\\alpha =0.2\\)  and the dropout layer was set to \\(0.5\\) .\n"]}
{"id": "2002.11910", "categories": "cs.CL", "paragraphs": ["The weights and hyper-parameters in the LSTM module are adjusted in each iteration using stochastic gradient descent for two stages in tandem. The first stage is based on the CWS input text, and the second stage on NER input text. Since the corpus for CWS is much larger than the corpus for NER, the former corpus is randomly sub-sampled during each training epoch, with each sample containing 13,500 sentences for training step one, and 1,350 sentences for training step two. Models are trained until the F1 score of NER model converges on the validation dataset, or up to 30 epochs.\n"]}
{"id": "2009.11201", "categories": "cs.CL", "paragraphs": ["We use three different settings, corresponding to each stage of training. For the first stage, we use the Adam optimizer [19] with a learning rate of 0.0002, weight decay of 0.2 and batch size of 2048 examples. We use a learning rate schedule consisting of a linear warmup of 4000 steps to a value 0.0002 followed by a linear decay for 1.2 million steps. At every step, we choose a single dataset from which to draw a whole batch using the following process: with equal probability, choose either monolingual or parallel. If the choice is monolingual, then we select one of the monolingual datasets uniformly at random. If the choice is parallel, we use a temperature-based sampling scheme based on the numbers of samples with a temperature of 5 [4]. In the second stage, we retain the same settings for both rounds of leveraging synthetic data except for the learning rate and number of steps. In the first round, we use the same number of steps, while in the second round we only use 240 thousand steps, a 1/5th of the original.\n", "For the final phase, we bucket sequences by their sequence length and group them up into batches of at most 2000 tokens. We train the model with 8 NVIDIA V100 GPUs, assigning a batch to each one of them and training synchronously. We also use the Adamax optimizer instead, and cut the learning rate by four once more.\n"]}
{"id": "2009.11232", "categories": "cs.CV", "paragraphs": ["Empirically, We set mini-batch size as 100, optimizer as Adam and initial learning rate as \\(1\\times 10^{-3}\\) . The parameters \\(\\alpha ,\\beta \\)  in the task-specific regression loss are set \\(10,0.1\\)  for Charades-STA; \\(2,0.4\\)  for ActivityNet Captions, respectively. We use two-layer encoder-decoder with 4 heads for two datasets. The trade-off parameters \\(\\lambda _{ta}\\)  and \\(\\lambda _{sd}\\)  are set to 1.\n"]}
{"id": "2012.15028", "categories": "cs.CV", "paragraphs": ["The proposed architecture requires no pre-training and it can be trained end-to-end. The number of subspace \\(K\\)  is set by experience to 16 for all modules.\n", "In the training stage, the weights of the whole network are initialized according to\u00a0[19]. We use Adam\u00a0[22] optimizer with momentum terms (\\(0.9\\) , \\(0.999\\) ). The initial learning rate is set to \\(2\\times 10^{-4}\\)  and the strategy of decreasing the learning rate is cosine annealing. The training process takes \\(700,000\\)  minibatch iterations.\n"]}
{"id": "2009.02831", "categories": "cs.CV eess.IV", "paragraphs": ["We used two datasets for validation: 1).\u00a0LiTS - Liver Tumor Segmentation Challenge dataset\u00a0[0]. It consists of 131 contrast-enhanced 3D abdominal CT scans. 2). Multi-phasic MRI scans of 36 local patients with HCC (note that the CT and MRI scans are\u00a0unpaired and\u00a0unmatched). Considering the clinical practise, we chose CT scans as source domain and MRI scans as target domain. We use 5-fold cross validation on the CT and MRI datasets, and normalized as zero mean and unit variance. In both WDGDA and SegModule part, input size of 3D modules is \\(256\\times 256\\times 5\\) , and for 2D modules is \\(256\\times 256\\) . To avoid over-fitting, we used standard data augmentation methods, including randomly flipping and rotating along the axial plane.\nWe evaluated two variations of the proposed method: our proposed 3D Wasserstein Distance Guided Domain Adaptation model without content discriminator (3D-WDGDA), and 3D-WDGDA with content discriminator (3D-WDGDA\\(^{c}\\) ).\n"]}
{"id": "2011.04512", "categories": "cs.CL cs.LG", "paragraphs": ["We use the Flair\u00a0[21], [22], [23] NER model\u00a0https://github.com/flairNLP/flair trained on the CoNLL NER dataset to assign named entities in a training data. Flair NER model was trained to predict 4 entities (e.g., `Locations (LOC)', `miscellaneous (MISC)', `organizations (ORG)', and `persons (PER)'). Also, the POS tag is labeled on the Switchboard dataset, and we used it to train the POS prediction task.\n"]}
{"id": "2012.06320", "categories": "cs.CV cs.LG", "paragraphs": ["The prediction errors are reported in meters over interpolated real-world coordinates in ETH-UCY dataset. We run experiments over two settings for observation/prediction lengths. In the first part, we observe 8 frames (3.2 seconds) and predict 12 steps (4.8) seconds. In the second part, we observed 4 frames (1.6 seconds) and predicted the next 8 frames (3.2 seconds).\nWe implement all model variants using TensorFlow (1.13) [66]. The experiments are deployed in a leave-one-out on desktop Intel\u00ae Xeon(R) CPU of frequency 3.50-GHz using Ubuntu 16.04 at a learning rate of 5e-3, decay rate of 0.95 and a dropout of 0.80. The maximum size variable (maxSize) of pedestrians is also set to 20 per frame. We repeated the testing experiments 10 times over each set to verify the errors stability. \\(GLSTM_o\\)  and \\(GLSTM_{nu}\\)  (outputSize) is 8 and embedding size is 128. Size of \\(h_S\\)  and \\(h_O\\)  is [20 x 128]. CNN encodes the static scene at compressed size [8x8] for compactness.\n"]}
{"id": "2006.10004", "categories": "cs.CV cs.LG eess.IV", "paragraphs": ["For training and testing our neural network we use the MS COCO dataset [31] that contains a large number of natural images. We take 2000 images for training and 1000 for testing. Each image is turned to greyscale and randomly cropped to a \\(64 \\times 64\\)  pixel window. For the purpose of data augmentation, we also take \\(128 \\times 128\\)  crops for some images and downsample them by a factor of 2, obtaining again images of size \\(64 \\times 64\\) . As spectral TV decomposition is not invariant to cropping and resizing, this augmentation needs to be done during the data generation process and cannot be automated during training. After standardising the dataset to have zero mean and a standard deviation of 1, we generate \\(K = 50\\)  ground truth bands (REF ) using the model driven approach in Section\u00a0REF . The bands are then combined dyadically to form 6 spectral bands. In this way, we make sure that smaller structures are decomposed in great detail while larger structures are grouped together in higher bands.\n", "We train our network only for the first 5 bands, since the 6th band contains the residual \\(f_r\\)  as described in (REF ) and can be recovered by subtracting the sum over bands 1-5 from the initial image. We use the Adam optimiser [26] with an initial learning rate of \\(10^{-3}\\)  and multi step learning rate decay. Our neural network is trained with a batch size of 8 and for 5000 epochs on an NVIDIA Quadro P6000 GPU with 24 GB RAM.\n"]}
{"id": "2005.12246", "categories": "cs.CL", "paragraphs": ["We additionally tuned the \\(\\alpha \\)  parameter used to weight the loss terms in eq:cstep over validation sets. We found that the value of \\(\\alpha \\)  is important for obtaining text representations containing less dialectal information. A large \\(\\alpha \\)  easily leads to over-fitting and a drastic drop in validation accuracy for hate speech classification. However, a near zero \\(\\alpha \\)  severely reduces both training and validation accuracy. We ultimately set \\(\\alpha =0.05\\) .\n"]}
{"id": "2004.07159", "categories": "cs.CL", "paragraphs": ["PALM is trained with a dropout rate of 0.1 on all layers and attention weights, and a GELU activation function\u00a0[12] used as GPT. The learning rate is set to 1e-5, with linear warmup over the first 10k steps and linear decay. The pre-training procedure runs on 16 NVIDIA V100 GPU cards for 800K steps, with each minibatch containing 64 sequences of maximum length 500 tokens.\n", "Pre-training Dataset. We use documents of English Wikipedia and BookCorpus\u00a0[41] as our pre-training corpus, and perform WordPiece tokenization as BERT\u00a0[5]. The documents are split into sentences. Different from BERT, we use multiple consecutive sentences up to 400 tokens as the source text input to the encoder, and use the subsequent consecutive sentences up to 100 tokens as the target text to the decoder. The pre-training dataset \\((\\mathcal {X},\\mathcal {Y})\\)  is constructed from the documents by a sliding window with the stride of one sentence, resulting in 50M \\((x,y)\\)  pre-training pairs.\n"]}
{"id": "2011.03372", "categories": "cs.LG cs.AI eess.SP", "paragraphs": ["To test the generality on larger image classification tasks, we moved the FDNAS general net to ImageNet for training. Following the general mobile setting [20], we set the input image size to \\(224\\times 224\\)  and the model's FLOPs were constrained to below 600M.\nWe use an SGD optimizer with a momentum of 0.9. The initial learning rate is 0.4 and decays to 0 by the cosine decay rule. Then the weight decay is 4e-5. The dropout rate is 0.2.\nIn order to fit the FDNAS net to ImageNet's image size, Layer 1, 3, 6, 8, and 16 are set to the downsampling layers.\n"]}
{"id": "2004.10746", "categories": "cs.LG cs.AI", "paragraphs": ["In Equation REF , the objective is to train a policy network \\(\\pi _{\\theta }\\)  that maximizes the expected value (\\(E\\) ) of the reward (\\(R_{p,g}\\) ) over the policy network's placement distribution. To optimize the parameters of the policy network, we use Proximal Policy Optimization (PPO) [33] with a clipped objective as shown below:\n\\(L^{CLIP}(\\theta ) = {\\hat{E_t}}[min(r_t(\\theta )\\hat{A_t}, clip(r_t(\\theta ), 1 - \\epsilon , 1 + \\epsilon )\\hat{A_t})]\\nonumber \\) \n"]}
{"id": "2007.09365", "categories": "cs.CV", "paragraphs": ["We use SGD optimizer with momentum to train our model. The momentum is fixed as 0.9 and the weight decay is set to 0.0001. We employ a \"poly\" learning rate policy where the initial learning rate is multiplied by \\((1-\\frac{iter}{max\\_iter})^{power}\\) . The initial learning rate is set to 0.01 and the power is set to 0.9.\nWe use batch size of 16 and train our model for 40k iterations for NYUDv2 and 60k iterations for Cityscapes.\nFor data augmentation, we use random cropping, random horizontal flipping and random scaling with scale \\(\\in \\lbrace 0.75,1,1.25,1.5,1.75,2\\rbrace \\) .\nBesides, we adopt the bootstrapped cross-entropy loss as in [31] for Cityscapes experiments.\n"]}
{"id": "2005.00458", "categories": "cs.CL", "paragraphs": ["Hyperparameter setup: We used 3 layers of transformer encoders and decoders with a maximum sequence length of 45 words. The word embedding dimension is 256 with 300 iterations of pre-training the generator before training our GAN.\n", "In Stage 1, there is minimal overlap of vocabulary between the languages. This is in contrary to data in typical style transfer datasets which have overlapping vocabulary spaces. Hence the discriminator learns much faster than generator in our case. To combat this, the learning rate in Stage 1 for the generator is 1e-3 and the discriminator is 1e-4. For Stage 2, the generator and discriminator are initialized with models learnt in Stage 1 thereby transferring the knowledge of each of the languages. However, to quickly adapt to the parameter space of Stage 2, we use slanted triangular learning rate [27] with a short linear increase period followed by longer decay period. Adam optimizers are used throughout the model. We plan to release our code, models and generated samples upon acceptance.\n"]}
{"id": "2010.10907", "categories": "cs.CL", "paragraphs": ["We follow the setup of Transformer base model\u00a0[39]. More precisely, the number of layers in the encoder and in the decoder is \\(N=6\\) . We employ \\(h = 8\\)  parallel attention layers, or heads. The dimensionality of input and output is \\(d_{model} = 512\\) , and the inner-layer of a feed-forward networks has dimensionality \\(d_{ff}=2048\\) .\n"]}
{"id": "2002.04165", "categories": "cs.CL", "paragraphs": ["For the sentence-level embedding extractor, the parameters of BERT framework are pretrainedThe model is available at https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip. The CLS output from BERT framework with regard to an input sentence is a 768-dim vectorWe did not use the 1024-dim embedding due to limited RAM on GPU, and the output dimension size of the sentence-level embedding is set to 256 with a FC layer that project the 768-dim vectors to 256-dim vectors. We pretrain the parameters on this FC layer so that it works as a fixed sentence-level embedding extractor. We use the 360 example sentences in the ACE annotation guideline and use a Softmax layer with 33 (the number for event types) labels and train a sub-framework to predict the existence of an event type in the guideline sentences. After we remove the softmax layer, we consider the output from FC layer as a sentence-level embedding that captures the features indicating the existence of event types.\n", "For the core network, we use 200-dim pre-trained Word2Vec\u00a0[6] embeddings which are trained from Wikipedia article dump on January 1st, 2017 and tokenized with spaCy. 50-dim PoS tag embeddings, and 20-dim character embeddings (from a character-based Bi-LSTM network with the original input of 32-dim character embedding and each direction has 10-dim hidden state size). The Bi-LSTM that extracts the token's context embeddings is set to 128-dim hidden state on both directions, with 256-dim as the whole embedding size and a total of 384 after being concatenated with memory embeddings. The FC layer before the CRF layer has an output dimension of 128.\n", "We have two optimizers in this framework (will be discussed in the following subsection). Both of them are Adam[7], with learning rate set to \\(0.001\\) .\n"]}
{"id": "2004.14781", "categories": "cs.CL", "paragraphs": ["For the hyperparameters in self-adaptive ensemble scheme, based on the best Hits@10 on WN18RR/FB15k-237 dev set, we set batch size \\(=32/64\\) , learning rate \\(=10^{-3}/10^{-5}\\) , number of training epochs \\(=1\\) , number of negative samples \\(=5/10\\) , and margin \\(=0.60/0.44\\)  in hinge loss function.\n"]}
{"id": "2011.06176", "categories": "cs.LG cs.AI", "paragraphs": ["There are also some parameters specific to LIAF, wherein NTR and All-Sharing configurations are adopted. We set \\(V_{th}=0.5\\)  in equation (4), \\(\\alpha =0.3\\)  and \\(\\beta =0\\)  in equation (6). The scaled exponential linear unit (SELU)[53] is applied as the activation function in DirectLIAF, which induces a self-normalizing property. For LIF-SNN, all LIAF layers in the network are replaced by LIF layers. For ConvLSTM network, all LIAF layers are replaced by ConvLSTM layers. All convolutional kernels in these networks are in the size of \\(3 \\times 3\\) .\n"]}
{"id": "2012.11310", "categories": "cs.CV cs.GR", "paragraphs": ["While the presented methodology can be applied to any skinned model, it requires of a database of valid poses. SMPL [16] is the current standard in the literature for human analysis and garment animation. This model is an LBS with PSD obtained through thousands of accurate 3D scans of different subjects. Its underlying skeleton is defined as a set of \\(K = 24\\)  joints. Public pose databases are available for this model (AMASS [19]). We then choose SMPL for the experimental part. Once a body model is selected, a garment or outfit is designed for the body in rest pose, with an approximate resolution of 1cm in our experiments. Then, initial blend weights for the cloth are obtained by proximity. Finally, the proposed neural network is trained with the pose database. We keep a subset of pose sequences as test. We train the model for 10 epochs with an initial batch size of 4, doubled every two epochs with Adam optimizer and learning rate \\(lr = 10^{-3}\\) .\n{FIGURE}"]}
{"id": "2010.11463", "categories": "cs.LG cs.CR", "paragraphs": ["The neural network architecture used in the experiments is LeNet5\u00a0[47]We change input channel to 3 for SVHN dataset.. \\({\\cal L}_{\\mathrm {mixcon}}\\)  is applied to the outputs of the 2nd convolutional layer blocks of LeNet5. The experiments use three datasets:\nMNIST [46], Fashion-MNIST [74], and SVHN [58].\n", "Neural network is optimized using cross-entropy loss and SGD optimizer with learning rate 0.01 for 20 epochs. We do not use any data augmentation or manual learning rate decay. \\(\\mathsf {MixCon}\\)  loss is applied to the output of 2nd convolutional layer blocks in LeNet5. We train the model with different pairs of \\((\\lambda ,\\beta )\\)  in Eq.\u00a0(REF ) for the following testing. Specifically, we vary \\(\\lambda \\)  from: \\(\\lbrace 0.01, 0.1, 0.5, 1, 2, 5, 10, 100\\rbrace \\)  and \\(\\beta \\)  from: \\(\\lbrace 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}, 10^{-7}, 10^{-8} \\rbrace \\) .\n"]}
{"id": "2010.04976", "categories": "cs.CL cs.LG", "paragraphs": ["For Binary classifiers, we use a single-layered recurrent unit, batch size of 64, hidden, and input size of 50 units. For LSTM and ONLSTM, the initial learning rate is 0.005, while for the GRU and DRNN, it is 0.01. No gradient clipping is performed to train the classifier.\n"]}
{"id": "2010.04755", "categories": "cs.CL", "paragraphs": ["The learned-W models (EL, ML) have \\(2*|\\mathcal {C}|^2+d*|\\mathcal {C}|=4,950\\)  learnable parameters. The fixed-W models (EF, MF) have \\(d*|\\mathcal {C}|=4,500\\)  learnable parameters. The time taken to train each model varies based on the number of training samples\u2014as a rule of thumb, training the learned models takes about 1.5-2 hours per 10,000 samples, while training the fixed models takes about 1 hour per 10,000 samples. Training all of the model variants necessary to reproduce this paper in full takes about 24 hours. Testing either model type takes only several minutes per 1,000 samples.\n"]}
{"id": "2011.00652", "categories": "cs.CV", "paragraphs": ["Our entire MVAF-Net network is end to-end trainable, and it is trained by ADAM optimizer with a fixed weight decay of[34] 0.01. The learning schedule is a one-cycle policy with the max learning rate set to 3e-3, the division factor 10, and the momentum range from 0.95 to 0.85. The mini-batch size is set to 2, and the model is trained for 40 epochs. In the detection head, two anchors with different angles \\((0^{\\circ },90^{\\circ })\\)  were used.\n"]}
{"id": "2006.09717", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["In general, the generated synthetic data correspond to \\(32\\times 32\\)  grayscale images, with the standard settings being \\(10,000\\)  training samples, \\(10,000\\)  test samples, and \\(\\epsilon =1\\) . The value of \\(\\sigma \\)  varies depending on the experiment under study.\n", "Regarding the setup and parameters for training the networks used for the experiments of Sec.\u00a0 and Sec.\u00a0REF : they were all trained for 20 epochs, on batches of size 128, minimizing a Cross-Entropy loss using SGD with a linearly decaying learning rate (max lr. \\(0.5\\) ) and without any explicit regularization.\n"]}
{"id": "2002.09405", "categories": "cs.LG physics.comp-ph stat.ML", "paragraphs": ["Noise type.\n(Figure\u00a0REF a,e) We experimented with 4 different modes for adding noise to the inputs. only_last adds noise only to the velocity of the most recent state in the input sequence of states. correlated draws a single per-particle and per-dimension set of noise samples, and applies the same noise to the velocities of all input states in the sequence. uncorrelated draws independent noise for the velocity of each input state. random_walk draws noise for each input state, adding it to the noise of the previous state in sequence as in a random random walk, as an attempt to simulate accumulation of error in a rollout. In all cases the input states positions are adjusted to maintain \\(\\dot{\\mathbf {p}}^{t_k} \\equiv \\mathbf {p}^{t_{k}} - \\mathbf {p}^{t_{k-1}}\\) . To facilitate the comparison, the variance of the generated noise is adjusted so the variance of the velocity noise at the last step is constant. We found the best rollout performance for random_walk noise type.\n"]}
{"id": "2011.03083", "categories": "cs.CV cs.CR cs.LG", "paragraphs": ["For PGD, we set \\(\\epsilon \\)  to \\(8/255\\) , the attack step size \\(\\alpha = 0.01\\) , and the number of attack iterations to 7, the same values as in [7]. For FGSM, we choose the same \\(\\epsilon \\)  value as above.\n", "We performed DNR based training for 200/170/60 epochs for CIFAR-10/CIFAR-100/Tiny-ImageNet, with a starting learning rate of \\(0.1\\) , momentum value of \\(0.9\\) , and weight decay value of \\(5e^{-4}\\) . For CIFAR-10 and CIFAR-100 the learning rate (LR) was reduced by a factor of \\(0.2\\)  after 80, 120, and 160 epochs. For Tiny-ImageNet we reduced the LR value after 30 and 50 epochs. In addition, we hand-tuned \\(\\rho \\)  to \\(10^{-4}\\)  and set the pruning rate \\(p=0.5\\) . We linearly decreased the pruning rate every epoch by \\(\\frac{p}{total\\; epochs}\\) . Finally, to balance between the clean and adversarial loss, we set \\(\\beta \\)  to \\(0.5\\) . Lastly, note that we performed warm-up sparse learning [12] for the first 5 epochs with only the clean image loss function before using the hybrid loss function with dynamic regularization (see Eq. REF ) for robust compression for the remaining epochs.\n"]}
{"id": "2011.03203", "categories": "cs.CL", "paragraphs": ["The hyperparameters in our model are heavily influenced by previous findings. For the RoBERTa model [22], we use the pre-trained distilled version proposed in sanh2019distilbert with 6 layers containing 12 attention heads and a hidden size of 768, as implemented by Wolf2019HuggingFacesTS.\nThe structural features used as inputs for the classification module are encoded as 10-dimensional embeddings for each of the 28 organizational features. During training we use the AdamW optimizer [23] with a learning rate of 0.001 and a weight decay value of 0.01 for both pretraining and fine-tuning. We further apply gradient norm clipping at 0.2 [30]. The learning rate was scheduled as in NIPS20177181, using 4000 warm-up steps. Due to the variable size trees in the training data, we aggregate documents with identical number of EDUs into batches of size 20 during pretraining and 5 for fine-tuning. All model configurations are trained by early stopping if the performance of neither structure nor nuclearity improves over 3 consecutive epochs on the development dataset. Our models are trained using PyTorch [31] on a GTX 1080 Ti GPU with 11GB of memory.\nOur code and model-checkpoints will be made publicly available with the publication of this paperhttp://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/Software.html.\n"]}
{"id": "2007.02861", "categories": "cs.LG cs.SI stat.ME stat.ML", "paragraphs": ["\nWe denote the prior with \\({\\alpha _0}\\)  and use \\({\\alpha _0}= 1\\)  for a so-called \"flat\" prior, where the probability density is constant over the parameter space. This corresponds to choosing all hyperparameters as one, i.e.\n\\(p({ \\mathcal {M}^{{\\mathcal {G}}}_{K}}|{\\alpha _0}=1, K, {\\mathcal {G}}) = \\prod _{k=0}^{K} \\prod _{h\\in { \\mathcal {P}_{{\\mathcal {G}}}(k) }} \\text{Dir}({\\vec{\\pi }_{h}};\\vec{1}_{|{\\mathcal {S}(h)}|}) = \\text{\\it const.}\\) \n", "This yields a Bayesian method to infer parameters of a multi-order model based on a large collection of paths \\({\\mathcal {D}}\\)  and a fixed maximum order \\(K\\) .\nTo learn the optimal order \\(K\\)  we again apply Bayes rule.\nWe first choose the orders that we want to compare, e.g., \\(K \\in \\lbrace 0, 1,\\ldots , {K_{\\text{max}}}\\rbrace \\)  up to a maximum value of \\({K_{\\text{max}}}\\) .\nWe denote the choice of the prior over \\(K\\)  as \\(p(K| {\\kappa _0}) = {\\kappa _0}(K)\\) .\nFor the inference of the optimal \\(k\\)  order for \\(k\\) -th order Markov chain models of unconstrained sequences (rather than multi-order models of paths), [34] mention two common priors: a uniform prior and a prior that additionally penalizes more complex models.\nDue to its superior performance for our problem, we limit our discussion to the uniform prior (see results for the latter prior in sec:Exponential penalization of higher orders):\n\\({\\kappa _0}(K) \\mathrel {\\stackrel{\\makebox{[}0pt]{\\mbox{\\normalfont \\tiny def.}}}{=}}({K_{\\text{max}}}+1)^{-1} = \\text{\\it const.}\\) \n"]}
{"id": "2012.01914", "categories": "cs.LG cs.AI", "paragraphs": ["In preliminary experiments we noticed that agents learned very slowly,\nand so to aid the training and overcome the problem of the sparse\nreward function, we use curriculum learning\u00a0[2] with\nphases shown in figure\u00a0REF . This technique lets\nthe agent gradually learn the best moves to obtain victory: for\ninstance, in the first phase it is very easy to win the game, as the\nenemy has only 1 HP and only one attack is needed to defeat it; in\nthis way the model can learn to reach its objective without worrying\ntoo much about other variables. As training proceeds, the environment\nbecomes more and more difficult to solve, and the \u201cgreedy\u201d strategy\nwill no longer suffice: the agent HP will vary within a range of\nvalues, and the enemy will be more difficult to defeat, so it must\nlearn how to use the collectible items correctly and which attack is\nthe best for every situation. In the final phases loot can be\ndifficult to find and the HP, of both agent enemy, can be within a large\nrange of values: the system must develop a high level of strategy to\nreach the end of the game with the highest score possible.\n"]}
{"id": "2001.04775", "categories": "cs.CV", "paragraphs": ["Training was run on a GTX 1080 Ti GPU (12GB RAM).\nThe SIP subnet is first pre-trained on DIV2K data (batch size 64), then the complete MVA+SIP network is fine-tuned end-to-end on the 3D training set.\nWe use the Adam optimizer [25] with learning rate \\(10^{-4}\\) .\nDue to memory limitations, the batch size was set to 4 for the multi-view dataset.\n", "Ground Truth Generation.  An issue in texture\nSR is the lack of ground truth, , texture with a\nhigher resolution than that of the input images.\nFor each scene, we generated a pseudo-ground truth texture atlas by\nrunning our MVA \\(L^1\\)  primal-dual model with an upsampling factor\n\\(\\times 2\\)  from all available views (, using significantly more\nviewpoints than for testing the complete network).\nSince upsampling \\(\\times 4\\)  in the same way leads to visually\nimperfect results even with many views, we instead downsample the\ninput views to \\(\\times 0.5\\)  their original size for experiments with\n\\(\\times 4\\)  super-resolution.\nWe note that the ground truth data may be biased towards the MVA subnet, since\nit is generated with it, but for training and testing we use fewer and LR input views.\n", "[topsep=1pt,itemsep=-1.5mm,leftmargin=*]\n20 views emulate dedicated texture recording with healthy redundancy; thus favoring the MVA step.\n\n3 views represent a more offhanded scenario with only few images; thus relying more on the SIP step.\n\n"]}
{"id": "2001.04708", "categories": "cs.CV", "paragraphs": ["The proposed model was trained end-to-end to perform relative lane ID estimation on a set consisting of\n244 sequence of 2500 image each (\\(\\sim \\) 600k images). The data that cannot be disclosed due to legal and security restrictions was exclusively recorded in the Chinese city of Shanghai at different dates, seasons, weather conditions, day times and routes over a span of time from June 2018 until February 2019. For the labeling, a semi-automatic strategy was used to generate proper lane count and lane ID labels for each image following our previously detailed double convention scheme based on a high-accuracy GPS device and a pre-stored high-definition map of the city.\n", "For training, we employ a custom version of Pytorch\u00a0[19] and make use of the Adam optimizer\u00a0[10]. We train the network up to 300K iterations with batches of size 2 containing sequences of consecutive frames of length 4\nand a learning rate \\(\\lambda = 10^{-4}\\)  that is divided by 2 every 20k iterations starting from iteration 150k. We set momentum values to \\(\\beta _1 = 0.9\\)  and \\(\\beta _2 = 0.999\\)  with weight-decay \\(\\delta = 10^{-4}\\) .\nInput images were introduced in sequential form under the resolution \\(H \\times W = 256 \\times 512\\)  and underwent heavy random augmentations performed on the fly...\n", "Our proposed network performs a classification task to estimate the lane count, left and right IDs out of a total number of \\(N = 8\\)  classes . The proposed cost function for training the network is composed of three terms summarized as follows:\n"]}
{"id": "2004.08596", "categories": "cs.CV", "paragraphs": ["First, the DAPnet has 4 layers of feature abstraction. In each layer, the point clouds are divided by group sampling. We use 3 convolutional layers for feature extraction on these groups, and the number of convolution kernels is gradually increased.\nAfter 3 convolutional layers, we apply max-pooling to obtain the global features of each group \\(L_1\\) . Then, for the final output \\(L_4\\)  of feature abstraction, we can get the enhanced feature results of \\(L_9\\)  and \\(L_{11}\\)  by the point and group attention modules. The following 4 layers are feature propagation. Except for the last layer, the input is concatenated to the output of the corresponding feature abstraction. Finally, by comparing the obtained score of all original points with the corresponding label \\(c\\) , the classifier performs classification.\n"]}
{"id": "2003.13845", "categories": "cs.CV cs.GR", "paragraphs": ["To train rcan [41], we use the default hyper-parameters.\nFor the rest of the translation of models,\nwe use a custom translation network as described earlier,\nwhich is based on pix2pixHD [38].\nMore specifically, we use 9 and 3 residual blocks\nin the global and local generators, respectively.\nThe learning rate we employed is \\(0.0001\\) ,\nwhereas the Adam betas are \\(0.5\\)  for \\(\\beta _1\\)  and \\(0.999\\)  for \\(\\beta _2\\) .\nMoreover, we do not use the vgg features matching loss\nas this slightly deteriorated the performance.\nFinally, we use as inputs 3 and 4 channel tensors\nwhich include the shape normals \\(\\mathbf {N_O}\\)  or depth \\(\\mathbf {D_O}\\) \ntogether with the rgb \\(\\mathbf {A_D}\\)  or grayscale \\(\\mathbf {A_D^{gray}}\\)  values of the inputs.\nAs mentioned earlier, this substantially improves the results by accentuating the details in the translated outputs.\n{FIGURE}{FIGURE}"]}
{"id": "2003.13853", "categories": "cs.CV", "paragraphs": ["We randomly initialize the weights following a Gaussian distribution, and optimize the model using RMSProp with batch size 128, with a learning rate of 0.0001. Inspired by existing methods\u00a0[27], [29], we employ the hinge loss variant of the GAN loss in our experiment. For the three hyper-parameters in Eq.\u00a05 (\\(\\tau _{cls}\\) , \\(\\tau _{cmp}\\)  and \\(\\tau _{ent}\\) ), we use the same parameter values as in\u00a0[22]. Two of the hyper-parameters in Eq.\u00a08 (\\(\\lambda _{a}\\)  and \\(\\lambda _{r}\\) ) are set to the values used in FUNIT\u00a0[27]. This demonstrates how our method is not particularly sensitive to hyper-parameter tuning and it works with the default values.\nThe remaining parameters (\\(\\lambda _{c}\\)  and \\(\\lambda _{e}\\) ) are optimized with a line search \\(10^p\\)  with \\(p\\in \\lbrace -5,-4,...,4,5\\rbrace \\) . In all experiments, we use the following hyper-parameters: \\( \\lambda _{a} =1 \\) , \\(\\lambda _{c} = 0.1\\) , \\(\\lambda _{r} = 0.1\\) , \\(\\lambda _{e} = 0.1\\) . For the experiment on noise-tolerant pseudo-labeling, we use the following hyper-parameters: \\(\\tau _{cls} = 1/C\\) , \\(\\tau _{cmp}=0.1/C\\)  and \\(\\tau _{ent}=0.8/C\\) , where \\(C\\)  is the number of categories. We use 8 V100 GPUs in an NVIDIA DGX1 machine to perform all our experiments.\n"]}
{"id": "2003.09711", "categories": "cs.LG stat.ML", "paragraphs": ["We use the state-of-the-art neural network architecture DenseNet [17]. We follow the same setup as in [17], with depth \\(L=100\\) , growth rate \\(k=12\\)  (Dense-BC) and dropout rate 0. All neural networks are trained with stochastic gradient descent with Nesterov momentum [9], [19]. Specifically, we train Dense-BC with momentum \\(0.9\\)  and \\(\\ell _2\\)  weight decay with a coefficient of \\(10^{-4}\\) . For GTSRB, we train it for 10 epochs; for CIFAR-10 and CIFAR-100, we train it for 100 epochs. For in-distribution dataset, we use batch size 64; For outlier exposure with \\(\\mathcal {D}_{\\text{out}}^{\\text{OE}}\\) , we use batch size 128. The initial learning rate of \\(0.1\\)  decays following a cosine learning rate schedule [27].\n"]}
{"id": "2002.07898", "categories": "cs.LG stat.ML", "paragraphs": ["CIFAR10 [39]\ncontains 60,000 \\(32 \\times 32\\)  color images divided into 10 classes. 50,000 images are used for training and 10,000 images for testing.\nCIFAR100 [39] is also constituted of \\(32\\times 32\\)  color images. However, it includes 100 classes with 50,000 images for training and 10,000 images for testing.\nSVHN [40]\ncontains 630,420 color images with size \\(32 \\times 32\\) . 604,388 images are used for training and 26,032 images are used for testing.\n", "For CIFAR datasets, the normalized input image is \\(32 \\times 32\\)  randomly cropped after \\(4\\times 4\\)  padding on each sides\nof the image and random flipping, similarly to [30], [41]. No other data augmentation is used. For SVHN, we normalize the range of the images between 0 and 1. All the models are trained on an Nvidia V100 32Gb GPU with 128 mini-batch size.\nThe models of both PlainNet and ResNet architectures are trained by SGD optimizer with momentum equal to 0.9 and a weight decay of \\(5\\times 10^{-4}\\) . On CIFAR datasets, the algorithm starts with a learning rate of 0.1. 200 epochs are used to train the models, and the learning rate is reduced by 0.2 at the 60-th, 120-th, 160-th and 200-th epochs. On SVHN dataset, a learning rate of 0.01 is used at the beginning and is then divided by 10 at the 80-th and 120-th epochs within a total of 160 epochs. The same settings are used as in [41].\n"]}
{"id": "2011.09553", "categories": "cs.CL cs.AI cs.IR", "paragraphs": ["We use the pre-trained BERT model ([BERT-Base, Uncased]), which has 12 hidden layers of 768 units and 12 self-attention heads to encode utterances and schema descriptions. The hidden size of LSTM decoder is also 768. The dropout probability is 0.1. We also use beam search for decoding, with a beam size of 5. The batch size is set to 8. Adam\u00a0[12] is used for optimization with an initial learning rate of 1e-4. Hyper parameters are chosen using the validation dataset in all cases.\n"]}
{"id": "2012.02792", "categories": "cs.LG", "paragraphs": ["The total number of trainable parameters, say (\\({N}\\) ) can be calculated by adding up the parameters updated in normal training phase and in WUS phase. This is shown in equation\u00a0REF  where \\({E_{wus}}\\) , and \\({E_{normal}}\\)  are the number of epochs that WUS phase (i.e., only biases of the last x layers are updated; where x is 1 for 1L; x is 2 for 2L, and so on), and normal training phase (i.e., both weights and biases of the all layers are updated) are employed, respectively. Likewise, \\({w}\\) , and \\(b\\)  is the number of weights and biases exist in the network.\n\\(N = (E_{wus} * b) + (E_{normal} * (b + w))\\) \n"]}
{"id": "2011.10867", "categories": "cs.LG cs.CR", "paragraphs": ["For parallel ensemble processing, after trying values in \\(1\\le E \\le 10\\) , we set \\(E=10\\) , which yields the best performance, increasing clean\naccuracies by up to \\(5\\%\\)  (see Appendix).\n", "Default attack parameters: Unless otherwise stated, we use the following parameters for \\(\\ell ^{\\infty }\\)  bounded PGD with EOT for CIFAR-10 trained models: an attack budget of \\(\\epsilon = 8/255\\)  (as is typical in the benchmarks we consider), a step size of \\(\\delta = 1/225\\) , a number of \\(N_S=20\\)  steps, a number of \\(N_R=1\\)  restarts, and a number of \\(N_E=40\\)  realizations for EOT. The same default attack parameters are used for attacking models trained on Imagenette, but given the lack of benchmarks, we test\nseveral attack budgets \\(\\epsilon \\in \\lbrace 2/255,4/255,8/255\\rbrace \\) .\n", "Benchmarks: Our benchmarks are the PGD adversarially trained (AT) [22], R+FGSM adversarially trained [43], and TRADES [46] defenses for the same classifier architecture. We reimplement these, to enable stress-testing these defenses with attacks of varying computational complexity. We train these models for 100 epochs with the same cyclic learning rate that we use for our models, and verify, for ResNet-32 classifier for CIFAR-10 and EfficientNet-B0 for Imagenette, that we can reproduce results obtained using\nthe original code . For PGD AT, training hyperparameters are \\(\\epsilon =8/255\\) , \\(\\delta =1/255\\) , \\(N_S=10\\) , \\(N_R=1\\) . For RFGSM AT, they are \\(\\epsilon =8/255\\) , \\(\\alpha =10/255\\) . For TRADES, they are \\(\\epsilon =8/255\\) , \\(\\delta =1/255\\) , \\(N_S=10\\) , \\(N_R=1\\) , and \\(\\lambda _{\\text{TRADES}}=1/6\\) .\n"]}
{"id": "2011.11765", "categories": "cs.CV cs.LG", "paragraphs": ["We use similar configurations as SimCLR v2. Specifically, we use ResNet-50 as the base encoder, with a 3-layer MLP projection head. We use a 65k memory buffer for the momentum encoder, with a momentum of 0.999. Following [42], we use random crops, color distortion, Gaussian blur, and RandAugment\u00a0[13] for data augmentation. We also adopt multi-crop\u00a0[6] with a support size of eight, which is shared with false negative cancellation. We use the attraction strategy with max aggregation for false negative cancellation, while the top-\\(k\\)  is set to 10 and a threshold of 0.7 is used for filtering the scores. We pretrain for 1000 epochs on 128 Cloud TPUs with a batch size of 4096. We use the LARS optimizer with a learning rate of 6.4, a cosine schedule for decaying the learning rate, and a weight decay of \\(1 \\times 10^{-4}\\) .\n"]}
{"id": "2011.09378", "categories": "cs.CL", "paragraphs": ["We use the MultiWOZ 2.0 corpus\u00a0[2] to test the performance of the models. MultiWOZ is a collection of conversations between humans in a Wizard-of-Oz fashion, where one person plays the role of a dialogue system and the other one a user. The user is tasked to find entities, e.g. a restaurant or a hotel, that fit certain criteria by interacting with the dialogue system. The corpus simulates a multi-domain task-oriented dialogue system interaction, i.e. multiple domains may occur in the same dialogue or even the same turn. The corpus is fully annotated with a total of 10438 dialogues in English, it is one of the most challenging and largest corpora of its kind. We use the training, validation, and test set partitions provided in the corpus, amounting to 8438 dialogues for training, and 1000 each for validation and testing. All numbers reported are based on evaluation on the test set.\n"]}
{"id": "2011.04062", "categories": "cs.LG", "paragraphs": ["We evaluate MLAS under different pre-training parameters in this set of experiments.\nATT and SEQ are not included in this set of experiments since they only utilize one data type. Output dimension is set to 5. Minimum cluster size is set at 50.\nFigure\u00a0REF  presents the results under different pre-training parameters. This confirms that our proposed MLAS method is not sensitive to different pre-training parameters.\n"]}
{"id": "2011.04122", "categories": "cs.CV cs.LG", "paragraphs": ["For training the localisation network, we follow the settings in\u00a0[1], training with batch size of 16, where each sample in the batch in a sequence of 5 consecutive RGB frames. RGB and depth data were normalised to lie between \\([0,1]\\) . We maintain the buffer size \\(b\\)  of\u00a0[1] to 4 frames, and extract 1024 feature representations for each observation frame. We use the ADAM optimiser\u00a0[29], with first and second momentum terms of \\(0.5\\)  and \\(0.999\\)  values respectively. Across all experiments, we choose \\(\\lambda _{CE} = 0.1\\) , which was found to be the optimal value via performing a random walk parameter sweep of \\(\\lambda _{CE}\\)  values in a range over \\([0.001,10]\\) . We set \\(\\alpha =0.02\\)  and \\(\\beta =0.04\\) ; similarly these values were found through a random walk parameter sweep of \\(\\alpha \\)  and \\(\\beta \\)  values in a range over \\([0.001,0.5]\\) . These values of \\(\\alpha \\)  and \\(\\beta \\)  are consistent with what we would expect; since there is a regularising term \\(L_{CE}\\)  (Eq.\u00a0REF ) which affects \\(p(\\bar{\\ell }|\\mathcal {I}_s)\\)  from the source domain, in order to balance the adaptation, a larger \\(\\beta \\)  (attracting force towards the target domain) was found to be more effective.\n", "We first pretrain our localisation network with a learning rate of \\(10^{-3}\\)  for 10 epochs on source domain data before adaptation. For adaptation, we train with a batch size of 32. We employ the Two-Time Update Rule (TTUR)\u00a0[21], using a 3:1 ratio in learning rates between the discriminator and generator networks of \\(3e\\text{-}4\\)  and \\(1e\\text{-}4\\)  respectively. In this GAN setup, our pretrained localisation network acts as the generator network which is concurrently trained with both discriminator networks for 10 epochs on the target domain data.\n"]}
{"id": "2003.07335", "categories": "cs.CV eess.IV", "paragraphs": ["The encoder and decoder parts of the VAE are both implemented using CNN architectures specified in Fig.\u00a0REF . The encoder takes the video frames as input and outputs the mean and variance of the distribution over underlying low-dimensional latent process. The decoder takes samples drawn from latent distributions as input and outputs the recovered version of the background in original input.\nWe trained G-LBM using the VAE architecture in Fig.\u00a0REF  by minimizing the loss function defined in Eq.\u00a0(REF ). We trained the G-LBM model using gradient descent based on Adam optimization with respect to the parameters of the encoder and decoder, i.e., \\(\\theta \\)  and \\(\\phi \\) , respectively. We employed learning rate scheduling and gradient clipping in the optimization setup. Training was performed on batches of size 3 video clips with 40 consecutive frames i.e 120 video frames in every input batch for 500 epochs.\n"]}
{"id": "2003.00547", "categories": "cs.CV", "paragraphs": ["We use exactly the same settings to train these models. All networks are trained using stochastic gradient descent (SGD) with a weight decay of \\(5\\times 10^{-4}\\)  and momentum of 0.9. The weights initialized according to \u00a0[38]. The biases are initialized with zero. On CIFAR-10 and CIFAR-100, we trained for 300 epochs, with a mini-batch size of 128. The initial learning rate is set to 0.1 and decayed by a factor of 0.1 after 120 and 240 epochs. Unless otherwise specified, we adopt batch normalization (BN)\u00a0[9] right after each convolution, nonlinear activation is performed right after BN. Dropout regularization\u00a0[39] is employed in the fully-connected layers, with a dropout ratio of 0.5.\n", "We trained i-ResNet\u00a0[44], a flow-based generative model that consists of 100 residual blocks. Each block is a composition of a multi-layer perception with state sizes of 128-128-128-128 and nonlinearities (e.g. ReLU, ELU). We adopt activation normalization\u00a0[45] after each residual block and do not use batch normalization (BN) or dropout in this experiments. Adam optimizer was used with a weight decay of \\(10^{-5}\\) . The learning rate is set to \\(10^{-3}\\) . Models are trained for 50,000 steps with a mini-batch size of 500. We used the brute-force computing log-determinant.\n"]}
{"id": "2008.00141", "categories": "cs.CV", "paragraphs": ["where \\(N=43\\)  is the number of classes, \\(y_i\\)  and \\(p_i\\)  are the ground truth and predicted label for the \\(i\\) -th class, respectively.\n", "The optimization method here is stochastic gradient descent, with a learning rate \\(lr={1e-5}\\) , momentum \\(=0.9\\) , and weight decay \\(={4e-5}\\)\n"]}
{"id": "2007.13087", "categories": "cs.LG cs.IR stat.ML", "paragraphs": ["In our experiments, we implement XDBoost with TensorFlow.Tensorflow: https://www.tensorflow.org The dimension of the embedding layer is set to 64. For the optimization we use the Adam optimizer [17] with a mini-batch size of 1,024, and the learning rate is set to 0.0001. For all deep models, the layer depth is set to three, and all activation functions are ReLU, except for the last activation which is sigmoid. The last activation in each DLR component is tanh. The number of neurons per layer is 128 for the Avazu dataset and 256 for the Taboola dataset. For classification components (i.e., DLC and all baselines), we use binary cross-entropy as the loss. For DLR components, we use the mean absolute error (MAE) as the loss, since MAE is not sensitive to outliers. We conduct our experiments using several RTX 2080 TI GPUs.\n"]}
{"id": "2010.00889", "categories": "cs.LG cs.AI", "paragraphs": ["For hyperparameter tuning, we performed a grid search on the training set and chose the model with the lowest validation loss. The validation loss is the sum of activity-related validation loss and time-related validation loss. The number of LSTM or T-LSTM units were set to 64 or 100. For the dropout rate (for both dense layers), we tried the values \\(0.0\\)  and \\(0.2\\) . We choose Nadam as an optimization algorithm, as used in [14]. Nesterov accelerated gradient (NAG) calculates the step using the \u2018lookahead\u2019 algorithm, which approximates the next parameters. Adam optimizer estimates learning rates based on initial moments of the gradients. Nadam is a combination of both and is robust in noisy datasets.\nFurthermore, we tested a range of different learning rates \\(\\lbrace 0.0001, 0.0002, 0.001, 0.002, 0.01\\rbrace \\)  since this is known to have a large impact on LSTMs [7].\nWe trained each model for 150 epochs, with a batch size of 64 and apply early stopping with patience 25 for regularization.\n"]}
{"id": "2011.01706", "categories": "cs.LG cs.AI stat.ML", "paragraphs": ["Assuming the deep neural network has \\(\\ell \\)  hidden layers, and the \\(i\\) th layer has \\(H_i\\)  neurons in DQN, VDQN, NoisyNet, and AVDQN. The dimension of the input layer is \\(I\\)  and the dimension of the output layer of AVDQN and VDQN is \\(n_A\\cdot n_V\\)  and \\(n_V\\) , respectively, where \\(n_A\\)  is the number of parameters of \\(q(Q)\\) . In total, the number of parameters of AVDQN is \\((I+1)H_1+\\sum _{i=1}^{\\ell -1}(H_{i}+1)H_{i+1}+n_A(H_{\\ell }+1)n_V\\) . In AVDQN, \\(n_A=2\\)  because \\(q(Q)\\)  follows Cauchy or Gaussian distribution. In VDQN, each weight has an auxiliary posterior distribution, thus the total number of parameters is \\(((I+1)H_1+\\sum _{i=1}^{\\ell -1}(H_{i}+1)H_{i+1}+(H_l+1)n_V)\\times n^{\\prime }_A\\) , where \\(n^{\\prime }_A=2\\)  since each weight follows a Gaussian distribution.\n"]}
{"id": "2003.13401", "categories": "cs.CV cs.LG", "paragraphs": ["We define the loss function as a weighted combination of two separate losses. A prediction \\(\\hat{y}\\)  is composed by the prediction of each of the 26 discrete categories and the 3 continuous dimensions, \\(\\hat{y} = (\\hat{y}^{disc},\\hat{y}^{cont})\\) . In particular, \\(\\hat{y}^{disc} = (\\hat{y}^{disc}_1,...,\\hat{y}^{disc}_{26})\\)  and \\(\\hat{y}^{cont} = (\\hat{y}^{cont}_1,\\hat{y}^{cont}_2,\\hat{y}^{cont}_{3})\\) . Given a prediction \\(\\hat{y}\\) , the loss in this prediction is defined by \\({L=\\lambda _{disc}L_{disc}+\\lambda _{cont}L_{cont}}\\) , where \\(L_{disc}\\)  and \\(L_{cont}\\)  represent the loss corresponding to learning the discrete categories and the continuous dimensions respectively. The parameters \\(\\lambda _{(disc,cont)}\\)  weight the contribution of each loss and are set empirically using the validation set.\n", "where \\(\\hat{y}^{disc}_i\\)  is the prediction for the i-th category and \\(y^{disc}_i\\)  is the ground-truth label. The parameter \\(w_i\\)  is the weight assigned to each category. Weight values are defined as \\(w_i=\\frac{1}{ln(c+p_i)}\\) , where \\(p_i\\)  is the probability of the i-th category and \\(c\\)  is a parameter to control the range of valid values for \\(w_i\\) . Using this weighting scheme the values of \\(w_i\\)  are bounded as the number of instances of a category approach to 0. This is particularly relevant in our case as we set the weights based on the occurrence of each category for each batch. Experimentally, we obtained better results using this approach compared to setting the global weights based on the entire dataset.\n", "where \\(\\hat{y}^{cont}_{k}\\)  and \\({y}^{cont}_{k}\\)  are the prediction and the ground-truth for the k-th dimension, respectively, and \\(v_k\\in \\lbrace 0,1\\rbrace \\)  is a binary weight to represent the error margin. \\(v_k=0\\)  if \\(|\\hat{y}^{cont}_{k} - y^{cont}_{k}| < \\theta \\) . Otherwise, \\(v_k=1\\) . If the predictions are within the error margin, i.e. error is smaller than \\(\\theta \\) , then these predictions do not contribute to update the weights of the network.\n", "The Smooth \\(L_{1}\\)  loss refers to the absolute error using the squared error if the error is less than a threshold (set to 1 in our experiments). This loss has been widely used for object detection\u00a0[42] and, in our experiments, has been shown to be less sensitive to outliers. Precisely, the Smooth \\(L_{1}\\)  loss is defined as follows\n\\(SL_{1cont}(\\hat{y}^{cont})=\\sum _{k=1}^{3}{v_k}\\left\\lbrace \\begin{array}{ll}0.5x^2, if |x_k| < 1\\\\|x_k| - 0.5, otherwise\\\\\\end{array}\\right.\\vspace{-0.56917pt}\\) \n", "where \\(x_k=(\\hat{y}_k^{cont} - y_k^{cont})\\) , and \\(v_k\\)  is a weight assigned to each of the continuous dimensions and it is set to 1 in our experiments.\n", "We train our recognition system end-to-end, learning the parameters jointly using stochastic gradient descent with momentum. The first two modules are initialized using pre-trained models from Places\u00a0[36] and Imagenet\u00a0[43] while the fusion network is trained from scratch. The batch size is set to 52 - twice the size of the discrete emotion categories. We found empirically after testing multiple batch sizes (including multiples of 26 like 26, 52, 78, 108) that batch-size of 52 gives the best performance (on the validation set).\n"]}
{"id": "2005.08648", "categories": "cs.CV", "paragraphs": ["All frames were resized to 128x96 pixels in order to smooth noise and reduce both training time and memory requirements. Mean intensity was removed from each frame.\nTo build the ground-truth masks, we selected \\(r_d\\)  equal to 6 pixels, as to completely overlay the joints. The \\(W_s\\)  was set to 2 for training and 0 for testing, while \\(W_d\\)  was set to 3. This way, a temporal clip was 0.5s long.\n", "For training the detection and regression network, we set an initial learning rate of 0.01 with a learning decay of 10% every 10 epochs, and a momentum of 0.98. We used a batch size of 8 and set a number of epochs equal to 100.\nWe selected the best model as the one that maximized the detection accuracy and minimized the mean absolute error on the validation set, for the detection and regression network, respectively.\n"]}
{"id": "2011.00184", "categories": "cs.CV", "paragraphs": ["For the temporal gated convolution model, we use Amsgrad [35] as the optimizer and train for 80 epochs. We set the batch size as 1024 in the training. For Human3.6M, we adopt an exponentially decaying learning rate schedule, starting from \\(\\eta = 0.001\\)  with a shrink factor \\(\\alpha = 0.95\\)  applied on each epoch.\n", "For the global pose trajectory estimation, we set \\(\\lambda _1=1\\) , \\(\\lambda _2=1\\)  and use \\(F=100\\)  frames in the optimization.\n"]}
{"id": "2008.10444", "categories": "cs.CV cs.LG", "paragraphs": ["For datasets CIFAR-10 and CIFAR-100, We applied the standard horizontal flip and random crop data augmentation.\nThe training protocols of CNN-5 and residual networks are not the same.\nFor CNN-5, we train it by Adam with batch size 256 (140 epochs; an initial learning rate of 0.001, decayed by a factor of 0.2 at epochs 40, 80, and 120).\nFor residual networks, We use the standard Stochastic Gradient Descent (SGD) with a weight decay of 0.0001 and a Nesterov momentum of 0.9 (200 epochs; an initial learning rate of 0.1, decayed by a factor of 0.2 at epochs 60, 120, and 160).\nFor KD, we set \\(M=4\\)  on the CIFAR-10, following the experiments [7], [9], [11], and \\(M=10\\)  on the CIFAR-100.\nThe weights of the knowledge transfer loss are obtained by held-out validation as in [9] on a subset of the training set (CIFAR-10/100: \\(\\lambda _{ICC} = 1500 / 1800\\) , \\(\\lambda _{KD} \\& \\lambda _{Tf-KD} = 300 / 800\\) , \\(\\lambda _{LT} = 80/150\\) , \\(\\lambda _{AT} = 1000/400\\)  and \\(\\lambda _{SP} = 1600 / 550\\) ).\n", "Due to the complex of the ILSVRC2012 dataset, a series of data-augmentation techniques are applied, including cropping, rescaling, and randomly mirroring the image.\nFor all networks, The SGD with a weight decay of 0.0001 and a Nesterov momentum of 0.9 is used with batch size 256 (90 epochs; an initial learning rate of 0.1, decayed by the cosine annealing).\nFor KD, we fix \\(M=5\\) , following the experiments in [7], [12], [11].\nThe weights of the knowledge transfer loss are obtained as in [9] on a subset of the training set (\\(\\lambda _{ICC} = 2000\\) , \\(\\lambda _{KD} \\& \\lambda _{Tf-KD} = 350\\) , \\(\\lambda _{LT} = 100\\) , \\(\\lambda _{AT} = 120\\)  and \\(\\lambda _{SP} = 850\\) ).\n"]}
{"id": "2008.10427", "categories": "cs.CL cs.AI", "paragraphs": ["\nFor SEQ2SEQ models, we used a 256 unit hidden size LSTM with 2 layers and a 128 unit input embedding dimension. The learning rate we used for all the models is 4E-3.\n\nFor Transformer, we used a 512 unit hidden size, 512 unit input embedding dimension, 2 attention header and 4 layers.\n\nWe used Adam as the optimizer to optimize on the cross-entropy loss.\n\nWe averaged the results over 3 different seeds.\n\nWe used a truncated history of last 100 tokens as context to keep the training uniform across the models.\n\n"]}
{"id": "2008.10480", "categories": "cs.CV", "paragraphs": ["Our implementation is based on the Pytorch 1.1 framework [8], using 8 NVIDIA Tesla P40 for training. The images are resized to 448\\(\\times \\) 448 before data augmentation. We use SGD [1] as the optimizer, the initial learning rate is set to 0.01, and lambda poly is used to adjust the learning rate. For most models, we first train 24 epochs on the GLDv2 clean dataset, and then train 12 epochs on the GLDv2 cluster dataset. For ResNest200, it is 12 epochs and 6 epochs.\n"]}
{"id": "2010.04427", "categories": "cs.CV cs.LG", "paragraphs": ["We train our models using TensorFlow[17]. We used GPU server. GPU server is built by Intel Xeon Silver 4216 CPU(32 cores, 2.1GHz), 512GB memory and Nvidia Titan RTX GPU 1EA. Initial learning rate is 1e-2 and learning rate decay rate is initial learning rate divided by the number of each epoch. Image size is 224 X 224. Input images are pre-processed to 224 X 224, whatever magnitudes they have. Binary cross entropy was used as loss function. The number of epochs is 32 and batch size is 32.\n"]}
{"id": "2004.13671", "categories": "cs.CL", "paragraphs": ["In our active learning setup, we begin by training our model on a 700-document subset of the full training set. We discard the labels of the remaining 2102 documents. In each round of active learning, we choose 280 unlabelled documents, and query up to \\(Q\\)  annotations per document.\nWe then add these documents to the labelled set and continue training our model on this set (now with new documents).\nAfter all documents have been labelled, we retrain our model on the full document set from scratch, resetting all model and trainer parameters.\n", "Init: \\(D_F\\)  = {first 700 docs}, \\(D_U\\)  = {remaining docs}, \\(D_A = \\emptyset \\) , \\(ML = CL = \\emptyset \\) ;\n", "Init: \\(D_F\\)  = {first 700 docs}, \\(D_U\\)  = {remaining docs}, \\(D_A = \\emptyset \\) , \\(ML = CL = \\emptyset \\) ;\n"]}
{"id": "2008.11647", "categories": "cs.CV cs.LG", "paragraphs": ["PyTorch [24] has been the framework chosen to carry out the experiments. All experiments have been trained and tested on a single NVIDIA GTX TITAN X GPU. We have used Adam [25] as an optimizer with a learning rate of \\(10^{-4}\\) . The loss function used for training is the bce loss. To make computations deterministic, a fixed random seed has been established in all pseudorandom number generators. Finally, to avoid unnecessary processing, validation patience with a value of five has been set, i.e., if validation losses stop improving during five epochs, the training will end.\n"]}
{"id": "2010.15487", "categories": "cs.CV eess.IV", "paragraphs": ["The performance of classifiers trained using the GCCS loss was tested on MNIST [16], FMNIST [17], SVHN [18], CIFAR-10 and CIFAR-100 [19]. For less complex datasets such as MNIST, FMNIST, and SVHN, the experiments were conducted using ResNet-18 [45] as the feature extraction network. For the more challenging CIFAR-10 and CIFAR-100 datasets, the Shake-Shake-96 and Shake-Shake-112 [46] regularization networks have been employed respectively, using a widen factor equal to 6 for the former and 7 for the latter. The encoder's last layer is followed by a fully-connected layer that outputs a vector with dimension \\(D\\) .\nWe trained each network for a total of 1800 epochs. For better network convergence, we employed cosine learning rate decay [47] with an initial value of \\(0.01\\)  as well as weight decay with a rate set to \\(0.001\\) . Finally, dropout regularization [48] with a \\(0.8\\)  keep probability value was applied to all the fully connected layers in the network.\n"]}
{"id": "2010.10392", "categories": "cs.CL", "paragraphs": ["We train each model using 16 Tesla\u00a0V100-SXM2-16GB GPUs and following the implementation and parameters in the NVIDIA codebaseMore specifically, we adapt these scripts to our needs.. Each complete pre-training phase consists of two steps:\n", "Step 1\n3,519 updates with a batch sizeWe use gradient accumulation for larger batch sizes. of 8,192 and a learning rate of \\(6.10^{-3}\\)  on sequences of size 128.\nStep 2\n782 updates with a batch size of 4,096 and a learning rate of \\(4.10^{-3}\\)  on sequences of size 512.\n\n"]}
{"id": "2002.10905", "categories": "cs.CV cs.HC cs.LG stat.ML", "paragraphs": ["For training on both datasets we used an initial learning rate of \\(10^{-2}\\)  together with the stochastic gradient decent (SGD)\u00a0[60] optimizer. The parameters for the optimizer are \\(weight decay = 10^{-4}\\)  and \\(momentum = 0.9\\) . For the loss function we used the weighted log multi class loss together with the softmax function. After each five hundred epochs, the learning rate was reduced by \\(10^{-1}\\)  until it reached \\(10^{-6}\\)  when the training was stopped. For data augmentation, we used random jitter that changes the value of a position to up to 2% around its original value. In addition, we shifted the entire input scanpath by a randomly selected value (the same value for all entries). We also used different input sizes where it has to be noted that in one batch, all length where equal because, otherwise, computational problems arise due to the not aligned data.\n", "For training on both datasets, we used an initial learning rate of \\(10^{-4}\\)  and changed it after ten epochs to \\(10^{-3}\\) . This was done to avoid numerical problems for the random initialized models, which end up in not a number results (NaN). As optimizer, we used adam\u00a0[43] with the parameters \\(weight decay = 5*10^{-4}\\) , \\(momentum1 = 0.9\\) , and \\(momentum2 = 0.999\\) . As loss function, we used the L2 loss for the first hundred epochs. Afterwards, we used the L1 loss function to improve the accuracy of the network. The learning rate was decreased by \\(10^{-1}\\)  after each five hundred epochs and the training was stopped at a learning rate of \\(10^{-6}\\) . For data augmentation, we used random jitter that changes the value of a position to up to 2% from its original value. In addition, we shifted the entire input scanpath by a randomly selected value (the same value for all entries). We also used different input sizes, where it has to be noted that in one batch, all length where equal because otherwise computational problems arise due to the not aligned data. In addition, it is important to note that for the training, only parts without error are selected since otherwise, our network would learn to reconstruct errors or what is most likely, is that it would learn nothing.\n", "For training, we used an initial learning rate of \\(10^{-4}\\)  and changed it after one hundred epochs to \\(10^{-3}\\) . As optimizer, we used stochastic gradient decent (SGD)\u00a0[60]. The parameters for the optimizer are \\(weight decay = 10^{-6}\\)  and \\(momentum = 0.9\\) . As loss function we used the L2 loss in combination with the KL divergence as described in Section\u00a0REF . The learning rate was decreased by \\(10^{-1}\\)  after each thousand epochs and the traing was stopped at a learning rate of \\(10^{-6}\\) . We did not use any data augmentation technique since the reparametriztion trick already induces some deformation in the output data.\n"]}
{"id": "2002.09891", "categories": "cs.LG stat.ML", "paragraphs": ["+3|B1|+|B2| + |B3|Lcons\nwhere \\(\\lambda _{1}, \\lambda _2\\)  and \\(\\lambda _{3}\\)  are regularization coefficients. \\(B_1, B_2\\)  and \\(B_3\\)  are child batch size. To compute the losses, we divide each training batch into three small child batches of size \\(B_1, B_2\\)  and \\(B_3\\) , respectively, and they are constructed as follows. We first randomly select \\(B_1\\)  samples \\( \\mathcal {X}_{1} \\)  from the whole dataset, together with their augmented version \\( \\mathcal {X}^{\\prime }_1 \\) , to make the first small batch of size \\(B_1 \\) . Then, we randomly select two subsets \\( \\mathcal {X}_{l1} \\)  and \\( \\mathcal {X}_{l2} \\)  from \\( \\mathcal {X}_L \\)  (together with their labels \\( \\mathcal {Y}_{l1} \\)  and \\( \\mathcal {Y}_{l2} \\)  from \\( \\mathcal {Y}_{L} \\) ), each of size \\(B_2\\) , to make the second small batch. For the third one, we randomly divide \\(\\mathcal {X}_1\\)  into two equal subsets \\(\\mathcal {X}_2\\)  and \\(\\mathcal {X}_3\\)  to make the third small batch of size \\( B_1 / 2 \\) . We define the structure of one training batch as\n\n\nbatch:=\n", "batch3: (X2,X3)}\nObviously, \\( W \\equiv 1 \\)  for all pairs in \\( batch_1 \\) , so it can be used to evaluate \\( L_{sup\\_W} \\) , \\( L_{cons} \\)  and \\( L_{unsup} \\)  (with \\(W=1\\) ). For \\( batch_2 \\) , we could evaluate all the loss terms, including \\( L_{sup\\_f},L_{sup\\_W}, L_{unsup}\\)  and \\( L_{cons} \\) . For \\( batch_3 \\) , we only evaluate the unsupervised term \\( L_{unsup}\\)  and \\(L_{cons} \\) . Note that \\( W \\)  in \\( L_{unsup} \\)  in \\( batch_3 \\)  is network's output.\n", "[tb]\nEnd-to-End Semi-Supervised Similarity Learning\n\n Input:\n\\( (x_i, x_j) \\)  := training batches in equation (REF ); \\( (\\mathcal {Y}_{l1}, \\mathcal {Y}_{l2}) \\)  := one-hot labels of \\( (\\mathcal {X}_{l1},\\mathcal {X}_{l2}) \\)  in \\( batch_2 \\) ; \\( W_1 \\equiv 1 \\)  for \\( batch_1 \\) ; \\( W_2 \\)  := corresponding one-hot similarity labels of \\( batch_2 \\) ; \\( f_{\\theta }(x), \\Phi _{\\alpha }(g(x)) \\)  := neural network with parameters \\( \\theta , \\alpha \\) .\n"]}
{"id": "2002.06349", "categories": "cs.LG cs.CV stat.ML", "paragraphs": ["As mentioned in the paper, all the experiments with synthetic data were trained in the same way, namely using SGD with a linearly decaying learning rate (max lr. 0.1), no explicit regularization, and trained for 500 epochs.\n"]}
{"id": "2001.10399", "categories": "cs.LG stat.ML", "paragraphs": ["We study the effect of the number of initial clean data instances by conducting the experiments with initial clean sample set sizes of 50 and 150 instances for all datasets except letter. The clean instances are chosen randomly form the trainign set.\nTo speed up training, we limit the data sets to \\(N=1050\\)  samples (including the initial set). For letter, since the number of classes is higher and the dataset is more complex, we let the initial set be 150 or 450 clean data instances and the total training size \\(N=8000\\) . After the initial clean data batch, noisy data arrives in batches of 50 instances. We evaluate two different constant noise rates of \\(60\\%\\)  and \\(80\\%\\) .\n", "As classifier, we try two types of standard ML techniques: random forest and SVM.\nAs quality model, we use SVM since it is a well studied model to be used with active learning.\nThe code is written in Python using the multi-class SVM in scikit-learn\u00a0[13]. Therefore the results of two methods would be different.\nUnder the static policy, QActor  queries the true label of either the 3 or 5 most informative noisy samples per batch via the oracle.\nUnder the dynamic policy,\nQActor  starts with 5 queries per batch and adapts the number at each batch arrival as per Equation\u00a0REF . In the results, we identify the static policy with the number of queries per batch arrival in brackets, e.g. (3), and the dynamic policy by \\(^D\\) .\nWe repeat each experiment 100 times and report the average accuracy computed on the test set.\n", "As QActor  classifier for CIFAR-10 and CIFAR-100 we use the two Convolutional Neural Network (CNN) architectures defined in\u00a0[23] with ReLU activation functions, softmax activation as image classifier and cross-entropy as loss function. We train the models by using stochastic gradient descent with momentum \\(0.9\\) , learning rate \\(0.01\\) , and weight decay \\(10^{-4}\\) . QActor  and all baselines are implemented using Keras v2.2.4 and Tensorflow v1.12, except co-teaching which uses PyTorch v1.1.0.\n", "With CIFAR-10 QActor  is trained initially with clean 10000 instances and 60 epochs. The remaining 40000 instances come in batches of size 1000 with \\(30\\%\\)  (\\(60\\%\\) ) noise. Under static policy, in each batch we query 50 (100) samples, i.e. \\(5\\%\\)  (\\(10\\%\\) ) of the batch size, actively from the oracle and retrain the model for 20 epochs. Similarly, the dynamic policy uses \\(B\\)  equal to only \\(5\\%\\)  of the total noisy data. At the end of each batch, we test the model with the test set of 10000 instances. Rollback uses \\(a\\)  = 20%. For CIFAR-100 we increase the arriving data batch size to 10000 and 60 epochs per batch to cope with the higher complexity.\nFor fair comparison, baselines are also trained under the same data arrival pattern and the same CNN structure. All baselines use the same parameters as from their papers except for D2L. Here we reduce the dimensionality estimation interval to 40 and 10 for the initial and subsequent batches, respectively. This keeps roughly the original ratio against the overall training period.\nFor each experiment we report the average accuracy covering the last received 10000 samples.\n"]}
{"id": "2006.13561", "categories": "cs.LG cs.CL stat.ML", "paragraphs": ["We followed model specifications in [16] and optimization settings in [9], with some minor modifications.\nSpecifically, we used word embeddings of dimension 512, feedforward layers with inner dimension 2048, and multi-headed attentions with 8 heads.\n", "We trained the models for 200,000 updates for En-De and 150,000 updates for En-Fr translation tasks. Finally, we averaged the last 5 checkpoints to obtain the final models for evaluation. The segment size \\(b\\)  in the segment-based masking method was set to 5.We did not tune \\(b\\) ; tuning \\(b\\)  might improve the results further.\n"]}
{"id": "2008.00720", "categories": "cs.LG stat.ML", "paragraphs": ["For each run, we set a fixed seed for random number generation, build the model, run 500 training epochs, and finally evaluate the classification accuracy on the non-training samples. We generally perform 100 of these runs for each experimental setting and report averages and standard deviations. Only in a few slow baseline experiments did we reduce the number of runs to 10. All experiments are run on a laptop with an Intel\nCore i7 processor and an NVIDIA GeForce GTX 950M.\n"]}
{"id": "2011.08951", "categories": "cs.CL", "paragraphs": ["For the CNN model, we use a kernel width of 150 for all convolutions.\nTo insert pretrained entity embeddings, we replace the candidate document convolution layer, followed by a single layer MLP to reduce the embedding to 150 dimensions.\nWe apply dropout to the word embedding layer with a probability of 0.2.\n", "For the RNN model, we use single-layer GRUs with hidden size 300 to embed the left and right context around a mention.\nWe do not tie the weights of the two GRUs.\nThe MLP attention module takes the candidate entity's embedding as the attention context, applying a linear transform to the embedding to map it to 300 dimensions.\nWe concatenate the entity embedding to the outputs of the attention module for the left and right contexts and pass the concatenated output to a classifier module consisting of a 300x300 MLP with ReLU activation, followed by a 300x1 linear layer to compute the score.\nWe apply dropout of 0.2 to the word embeddings and dropout of 0.5 to the MLP in the classification module.\nTo use pretrained entity embeddings, we replace the randomly initialized entity embeddings with our pretrained embeddings.\nIn cases where a pretrained entity embedding is unavailable, we randomly initialize the entity's embedding from a uniform distribution between -0.1 and 0.1.\nFor both the RNN and CNN, we initialize the word embeddings using GoogleNews Word2Vec vectorshttps://code.google.com/archive/p/word2vec.\n", "For the Transformer model, we use the distilbert-base-uncased model available through the HuggingFace libraryhttps://huggingface.co/distilbert-base-uncased.\nWhen using pretrained entity embeddings, we replace the randomly initialized entity embeddings with our pretrained embeddings and randomly initialize any missing entity embeddings from a normal distribution with mean 0 and standard deviation 0.02.\nWhen the pretrained embeddings do not match the 768 dimension output of the DistilBERT context encoder, we map the context encoder's output to match the size of the embeddings with a single linear layer.\n", "All of our models are trained to convergence using hinge loss, with early stopping based on the model's loss on the validation set.\nWe set our patience for early stopping to 3 epochs for AIDA-CoNLL and 5 for TAC-KBP.\nWe use batch size 16 for the CNN and RNN models and 32 for the transformer model.\nDuring training, we apply gradient clipping of 1.0 for the transformer model and 5.0 for the CNN and RNN.\nWe use Adam [22] with an initial learning rate of 1e-3 for the CNN and RNN models, while for the transformer model we use the weight decay-fixed implementation of Adam from HuggingFace [28] with initial learning rate of 2e-5 and epsilon 1e-8.\nWe additionally use a learning rate schedule for the transformer model, with linear decay over the course of training based on an expected maximum number of steps equal to 10 training epochs times the number of batches for the dataset.\nWhen training the CNN and RNN models on the Wikipedia EL corpus, we use all the same model and training settings as described above, but use batch size 512.\n"]}
{"id": "2006.05713", "categories": "cs.CV", "paragraphs": ["The Google/Inception, C3D, and 2-D LSTM networks were constructed with Python using the Keras Library with contrastive and triplet loss functions. To train the Siamese Network and Triplet Loss networks, we used batch sizes of 32 with 16 identities per batch and two images/sequences per identity. There are 16 positive pairs and 16 random negative pairs for Siamese Network and Triplet Loss Network employed semi-hard negative mining on each batch with 32 images/sequences. The Adam optimizer was used from Keras with a learning rate of \\(0.0001\\) , beta 1 value of \\(0.99\\) , and beta 2 value of \\(0.99\\)  across the various network architectures.\n"]}
{"id": "2007.14682", "categories": "cs.CV cs.CL", "paragraphs": ["In all our experiments, the video features and text (word) inputs are embedded into a 500-dimensional and 300-dimensional space respectively. The LSTMs in the encoder-decoder network have a hidden state size of 512, and the LSTM block used to encode the contextual text in the pointer generator network has a hidden state size of 256. During training, dropout\u00a0[51] rate of \\(0.5\\)  is applied on the video feature input, embedded word input, embedded context input, and all LSTM outputs.\nThe training is performed with the Adam\u00a0[52] optimizer using a learning rate of \\({e-4}\\) .\n"]}
{"id": "2006.04270", "categories": "cs.LG cs.CV cs.NE", "paragraphs": ["The experiments are conducted on ResNets (18, 34, 50, and 101 layers)\u00a0[34], AlexNet\u00a0[32], SqueezeNet v1.1\u00a0[33], and Deep Compression\u00a0[0]. The results are averaged over five independent runs. A grid hyper-parameter search is conducted based on the Top-1 accuracy for all models, including initial learning rates in \\(\\lbrace 0.01,{}0.1,{}1\\rbrace \\) , Stochastic Gradient Descent (SGD)\u00a0[35] and Adadelta\u00a0[36] optimizer, exponential and step learning rate decays with gamma values in \\(\\lbrace 25,{}50\\rbrace \\) , and batch sizes of 64 and 128. The Adadelta optimizer with Step adaptive learning rate (step: every 50 epoch at gamma rate of 0.1) and weight decay of \\(10e^{-6}\\)  is used. The number of epochs is 200 and the batch size is 128. Random dropout is not used in the EDropout experiments. For the other models, where applicable, the random dropout rate is set to 0.5. The early state convergence in\u00a0(REF ) is used with a threshold of 100 epochs. The models are implemented in PyTorch\u00a0[37] and trained on three NVIDIA Titan RTX GPUs.\n"]}
{"id": "2007.05667", "categories": "cs.CV", "paragraphs": ["We follow standard hyperparameters used for fine-tuning [23], [48], [36]: 30 epoch learning rate 1e\\(^{-3}\\)  on SGD optimizer. That is except comparison with Chen et al. [37] as the authors train the pruned model with the standard hyperparameters used for training from scratch: 160 epoch with initial learning rate 0.1 and decays on epoch [81, 122] by 0.1.\n", "Filter pruning: For filter pruning, we prune total 500 and 100 filters in VGG19 and ResNet56 respectively for global-based filter importance criteria such as weight norm, Taylor, Feature maps. We follow the same iterative pruning hyperparameter setup as Taylor [23]. We prune 100 filters each 10 minibatches. Other pruning methods, we report results using their published code with default setup setting such as slimming [30] and ECC[20].\n", "Filter pruning. We follow the same setup as Taylor [23] for global-based filter pruning, we prune 100 filters each 30 minibatches for 10 iterations. For methods like ECC [20], slimming [30], SSS [36] and HRank [39], we report using their default hyperparameter either reported in their papers or using their code. We fine-tune using the same setup as previously mentioned in CIFAR.\n"]}
{"id": "2005.01026", "categories": "cs.LG cs.DC stat.ML", "paragraphs": ["For each device's data, we used 80% for training and 20% for testing. The federated learning algorithm was to train an optimal global model using all devices' training data. The learned global model was sent to each device for further fine-tuning with local training data, and then the fine-tuned local model was tested on each device. For the initialization of cluster centers in FeSEM, we conducted 20 times of pure clustering with different randomized initialization, and then the \u201cbest\u201d initialization, which has the minimal intra-cluster distance, was selected as the initial centers for FeSEM. For the local update procedure of FeSEM, we set \\(N\\)  to 1, meaning we only updated the \\(W_i\\)  for one time in each local update.\n"]}
{"id": "2007.16013", "categories": "cs.CL cs.LG stat.ML", "paragraphs": ["\nThe default model is an LSTM model comprising of two 300-unit layers, and a skip connection\u00a0[8] over both LSTM layers.\nThe input is 300-dimension subword embedding (\\(e(w_{<t})\\) ) learned with the model, and the output is a softmax layer.\n\nThe context encoder is a single layer LSTM with 256 units layer and 0.2 dropout. The input embeddings are shared with the default model and are frozen during the compositional LM training.\n\nComponent embeddings (\\(ce^{i}\\) ) have dimensionality of 256.\n\nThe activation model is a 2-layer LSTM with 128 units, the LSTM's output is projected into a scalar.\nThe input to activation model has dimensionality of \\(514 = 256+256+1+1\\)  (see Eq.\u00a0REF ). We use 0.2 dropout between LSTM layers.\n\nFinally, the attention model is a single 128-unit LSTM layer with its output projected into a scalar.\nThe attention model's input has 514 dimensions (see Eq.\u00a0REF ). We add 0.2 dropout before and after the LSTM layer.\n\n"]}
{"id": "2010.03412", "categories": "cs.CL cs.LG", "paragraphs": ["We adopt the base Transformer model\u00a0[29].\nWe pre-train models with the supervised objective until convergence, and fine-tune on the mixed parallel and monolingual data as in prior work\u00a0[26], [5]. We use the Adam optimizer\u00a0[17] with a batch size of\u00a032 sentences and checkpoint the model every\u00a02500 updates.\nAt decoding time, we use beam search with a beam size of\u00a05.\nThe LMs in Dual Learning are RNNs\u00a0[21] with\u00a0512 hidden units. All model and training details are in Appendix\u00a0.\n", "For preprocessing, we normalize punctuations and apply tokenization, true-casing, and joint source-target Byte Pair Encoding\u00a0[27] with\u00a0\\(32,000\\)  operations. We set the maximum sentence length to\u00a050.\n", "We adopt the base Transformer model\u00a0[29] with\u00a0\\(d_{\\text{model}}=512\\) ,\u00a0\\(d_{\\text{hidden}}=2048\\) ,\u00a0\\(n_{\\text{heads}}=8\\) ,\u00a0\\(n_{\\text{layers}}=6\\) , and \\(p_\\text{drop}=0.1\\) . We tie the source and target embeddings with the output layer weights\u00a0[25], [23].\n", "We use the Adam optimizer\u00a0[17] with a batch size of\u00a032 sentences and checkpoint the model every\u00a02500 updates. Training hyperparameters and stopping criteria are constant across all comparable experimental conditions. Initial learning rates for pre-training and fine-tuning are respectively set to\u00a0\\(10^{-4}\\)  and\u00a0\\(2 \\times 10^{-5}\\) . We decay the learning rate by\u00a0\\(30\\%\\)  and reload the best model after\u00a03 checkpoints without improvement. We apply early stopping after repeating this process for\u00a05 times. We adopt the same learning rate decay and stopping criteria during fine-tuning. For batch-level IBT and Dual Learning, we check whether both models improve validation perplexity. For epoch-level IBT, we run for\u00a03 iterations.\n"]}
{"id": "2006.02049", "categories": "cs.CV cs.LG cs.NE", "paragraphs": ["We use distributed training with 8 nodes for the final models, and scale up the learning rate by the number of distributed nodes (e.g., 8\\(\\times \\)  for 8-node training). The batch size is set to be 256 per node. We use label smoothing and AutoAugment in the training. Additionally, we set the weight decay and momentum for batch normalization parameters to be zero and 0.9, respectively\n", "We implement the EMA model as a copy of the original network (they share the same weights at \\(t=0\\) ). After each backward pass and model weights update, we update the EMA weights as\n\\(w_{t+1}^{ema} = \\alpha w_{t}^{ema} + (1-\\alpha )w_{t+1}\\) \n", "where \\(w_{t+1}^{ema}\\) , \\(w_{t}^{ema}\\) , and \\(w_{t+1}\\)  refer to the EMA weight at step \\(t+1\\) , EMA weight at step \\(t\\) , and model weight at \\(t+1\\) . We use an EMA decay \\(\\alpha \\)  of 0.99985, 0.999, and 0.9998 in our experiments on ImageNet, CIFAR-10, and COCO, respectively. We further provide the training curves of FBNetV3-G in Fig.\u00a0REF .\n"]}
{"id": "2004.08483", "categories": "cs.LG stat.ML", "paragraphs": ["We use two basic configurations: base and large. Base uses 12 layers, 768 hidden size, 12 attention heads, local attention radius \\(r=84\\) , and relative position maximum distance \\(k=12\\) . Large uses 24 layers, 1024 hidden size, 16 heads, \\(r=169\\) , and \\(k=24\\) . We used 128, 230 and 460 global tokens for models with 512, 4096 and 8192 long input size respectively in NQWith gradient checkpointing, ETC can scale beyond this, but we limit our experiments to 8192 tokens for this paper., 256 global tokens in HotpotQA, 430 in WikiHop, and 512 in OpenKP.\n", "Pre-training: We place all word piece tokens in the long input and add one auxiliary token per sentence to the global input.\nWe defaulted to BERT's 30k English uncased word piece vocabulary. Models were pre-trained using the original BERT datasets, except that documents with fewer than 7 sentences were filtered out. Unless stated otherwise, base models were pre-trained with the same total number of tokens as the original BERT, and for large models, twice as many. We used the LAMB optimizer\u00a0[41] with learning rate set to \\(\\sqrt{8}\\times 10^{-3}\\) .\n"]}
{"id": "2009.06200", "categories": "cs.CV cs.LG", "paragraphs": ["The training process is optimized using the adaptive moment estimation (Adam) algorithm\u00a0[22]. We use all the default parameters with an initial learning rate of \\(1\\times 10^{-4}\\) . The Adam algorithm adaptively computes the learning rate. However, to improve the convergence speed of hyperparameter tuning, we also use a traditional learning rate decay\u00a0[53]. Lastly, to evaluate the performance of the network, we use the categorical cross-entropy loss (Eq.\u00a0REF ) computed from the target labeled image \\(y_i\\)  (ground truth) and the predicted class probabilities \\(\\bar{y}_i\\)  for each class. During the network training process, this loss function is gradually optimized by the Adam algorithm.\n\\(\\mathcal {L_C}(y,\\bar{y}) = -\\sum y_i \\log {\\bar{y_i}}\\) \n"]}
{"id": "2006.09507", "categories": "cs.LG cs.AI", "paragraphs": ["The model parameters for this experiment consist of parameters to train the PPO algorithm and the values of these parameters are similar to the original work of [28]. However, some adjustments have been made to fit the OBSP environment. The algorithm is set to train for \\(750.000\\)  steps, depending on the size of the problem instance, the episode requires 50 up to 100 actions.\n", "We initialize a Neural Network similar to [28] with two hidden layers of 64 units, and tanh nonlinearities, outputting the mean of a Gaussian distribution for our action space of 31 units. The clipping parameter that showed the best performance in [28] was set to 0.2. This ensures that the updated policy cannot differ from the old policy by 0.2. The discount factor \\(\\gamma \\)  is set to \\(0.9999\\)  instead of \\(0.99\\) . This means that we care more about reward that is received in the future than immediate reward. Both the DRL agent and the simulation model are utilized on the same processing machine with an Intel(R) Core(TM) i7 Processor CPU @ 2.80GHz and 32GB of RAM.\n"]}
{"id": "2012.07177", "categories": "cs.CV", "paragraphs": ["paragraph4\n.5em plus1ex minus.2ex-.5emRobustness to backbone initialization.\nCommon practice for training Mask R-CNN is to initialize the backbone with an ImageNet pre-trained checkpoint. However He\u00a0\u00a0[24] and Zoph\u00a0\u00a0[71] show that a model trained from random initialization has similar or better performance with longer training. Training models from ImageNet pre-training with strong data-augmentation (RandAugment\u00a0[6]) was shown to hurt the performance by up to 1 AP on COCO. Figure\u00a0REF  (left) demonstrates that Copy-Paste is additive in both setups and we get the best result using Copy-Paste augmentation and random initialization.\n", "paragraph4\n.5em plus1ex minus.2ex-.5emCopy-Paste is additive to large scale jittering augmentation.\nRandom scale jittering is a powerful data augmentation that has been used widely in training computer vision models. The standard range of scale jittering in the literature is 0.8 to 1.25\u00a0[38], [24], [5], [14]. However, augmenting data with larger scale jittering with a range of 0.1 to 2.0\u00a0[55], [10] and longer training significantly improves performance (see Figure\u00a0REF , right plot). Figure\u00a0REF  demonstrates that Copy-Paste is additive to both standard and large scale jittering augmentation and we get a higher boost on top of standard scale jittering. On the other hand, as it is shown in Figure\u00a0REF , mixup\u00a0[64], [67] data augmentation does not help when it is used with large scale jittering.\n"]}
{"id": "2009.10054", "categories": "cs.CV cs.LG", "paragraphs": ["\\(K=36\\)  objects are detected by pretrained faster R-CNN [29], and a 2048 dimensional vector for each object is extracted by pretrained ResNet-152 [11].\nQuestion tokens are trimmed to a maximum of 14 words, and pretrained GloVe [26] is used for word embedding.\nThe batch size is 256.\nWe include all hyperparameters in Appendix for reproducibility.\n", "For regularization of the attention network, we use training samples of TinyImage, VNQ, and QRPE for \\(P_{\\text{anomaly}}\\)  in Eq\u00a0(REF ).\nNote that there is no overlap of anomaly data between data for training and evaluation.\nWe fine-tune the pretrained VQA models in 15 epochs, and the \\(\\lambda \\)  in Eq\u00a0(REF ) is set to 0.00001.\nAll codes are implemented using Pytorch 0.4.1 and are available in public https://github.com/LeeDoYup/Anomaly_Detection_VQA.\n"]}
{"id": "2006.00986", "categories": "cs.LG q-bio.MN q-bio.QM stat.ML", "paragraphs": ["We use 128 samples per mini-batch for all experiments. Since the data set \\(X=\\lbrace X_\\mathrm {ZINC}, X_\\mathrm {FDA}\\rbrace \\)  is highly unbalanced with \\(|X_\\mathrm {ZINC}| \\gg |X_\\mathrm {FDA}|\\) , we use a weighted random sampler to add more examples from the FDA-approved drugs during training. Based on our empirical results, we find that upsample \\({x} \\in X_\\mathrm {FDA}\\)  to \\(20\\%\\)  in each mini-batch is sufficient for learning the drug hierarchy.\n", "Meanwhile, we fine-tune two hyperparameters: (1) \\(\\gamma \\) , the weight of the soft local ranking loss, and (2) \\(K\\) , number of randomly sampled negative examples for each anchor data point. Both of them affect the model's capability of learning hierarchy. We perform ablation studies of these two hyperparameters and find that using \\(\\gamma = 11\\)  and \\(K=11\\)  are good options in our experiments.\n"]}
{"id": "2010.08265", "categories": "cs.CL cs.LG", "paragraphs": ["We replicate the model configuration (embed=512, ffn=1024, head=4) as the baseline in (Wu et al., 2019). In addition, the batch size of 8192, attention dropout of 0.1, and relu dropout of 0.1 is used as suggested by [7]. When training the standard Transformer from scratch, we follow the inverse_sqrt learning rate schedule with learning rate of 0.0015 and warmup of 8k. To speed up convergence of FDMs (e.g. MT, LayerDrop), all of them are finetuned from the pre-trained baseline with learning rate of 0.0005 and warmup of 4k.\n"]}
{"id": "2010.02778", "categories": "cs.CV cs.LG", "paragraphs": ["In this section, we present our algorithm to learn the model parameters \\(\\lbrace \\mathbf {B}_1,\\dots , \\mathbf {B}_m\\rbrace \\)  and \\(\\lbrace \\mathbf {P}^t\\rbrace _{t=1}^{c_{out}}\\)  from the training data. Without loss of generality, let us consider to optimize the model parameters for one layer. Assume \\(\\lbrace \\mathbf {X}_{input}, \\mathbf {Y}_{output}\\rbrace \\)  is a mini-batch of inputs and targets for a given convolution layer. Therefore, the objective for optimizing \\(\\lbrace \\mathbf {B}_1,\\dots , \\mathbf {B}_m\\rbrace \\)  and \\(\\lbrace \\mathbf {P}^t\\rbrace _{t=1}^{c_{out}}\\)  will be\n\\(\\begin{split}\\min & \\sum _{t=1}^{c_{out}}\\Vert \\mathbf {Y}_{output}^{t} - \\sum _{i=1}^{k}\\text{Conv}(\\mathbf {X}_{input(i)}, \\sum _{j=1}^{m}\\mathbf {P}_{ji}^{t}\\mathbf {B}_j)\\Vert ^2 \\\\s.t & \\ \\Vert \\mathbf {P}_{(:,i)}^{t}\\Vert _0 = 1 \\\\& \\ \\mathbf {B}_{ij} \\in \\lbrace -1, 1\\rbrace .\\end{split}\\) \n", "Proposition 1 \nSuppose \\(\\mathbf {X}_{input(i)} \\in \\mathbb {R}^{w_{in} \\times h_{in} \\times s}\\) , \\(\\lbrace \\mathbf {B}_1,\\dots , \\mathbf {B}_m\\rbrace \\)  is a set of \\(m\\)  low-dimensional binary filters where each \\(\\mathbf {B}_i \\in \\mathbb {R}^{d \\times d \\times s}\\)  and \\(\\mathbf {P}_{(:,i)}^{t}\\)  is the \\(i\\) -th column in \\(\\mathbf {P}^t\\)  which is a length-\\(m\\)  sparse vector with only one non-zero element. Then, \\(\\emph {Conv}(\\mathbf {X}_{input(i)}, \\sum _{j=1}^{m}\\mathbf {P}_{ji}^{t}\\mathbf {B}_j)\\)  is equivalent to \\(\\sum _{j=1}^{m}\\mathbf {P}_{ji}^{t}\\emph {Conv}(\\mathbf {X}_{input(i)}, \\mathbf {B}_j)\\) .\n\n{FIGURE}", "where sign() is the element-wise sign function which return 1 if the element is larger or equal than zero and return \\(-1\\)  otherwise. Similarly, sparse indicator matrices \\(\\lbrace \\mathbf {P}^{t}\\rbrace _{t=1}^{c_{out}}\\)  can be obtained by\n\\(\\mathbf {P}^t_{ji} ={\\left\\lbrace \\begin{array}{ll}\\ \\mathbf {Q}^t_{ji} \\ \\ \\ \\ \\text{if $j = \\text{argmax}\\ (|\\mathbf {Q}^t_{(:,i)}|)$} \\\\\\ 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{otherwise}\\\\\\end{array}\\right.}\\) \n", "Specifically, let us use \\(r\\)  to denote a full-precision weight and it is a proxy for a binary weight \\(b\\) . Therefore,\n\\(b ={\\left\\lbrace \\begin{array}{ll}\\ 1 \\ \\ \\ \\ \\text{if $r \\ge 0$,} \\\\\\ -1 \\ \\ \\ \\ \\text{otherwise.}\\\\\\end{array}\\right.}\\) \n", "(REF ) is not a differentiable function, STE will just simply estimate its gradient as sign function is not present. That is \\(\\frac{\\partial b}{\\partial r} = 1\\) . In practice, we also employ the gradient clipping as in [13]. Then, the gradient for the sign function is\n\\(\\frac{\\partial b}{\\partial r} = 1_{|r|\\le 1}.\\) \n"]}
{"id": "2008.12969", "categories": "cs.CV cs.LG cs.RO", "paragraphs": ["At training time we perform the following procedure to simulate the output of a perception stack, i.e., to simulate noise.\nWe use two truncated Normal distributions (values between 0 and 1) as shown in Fig.\u00a0REF  (\u201cPerception Noise\u201d) to simulate the confidence score generated by a generic perception stack, and approximate the perception system's confidence estimate on its failure modes:\n\\(c_j \\sim \\mathcal {N}_{trunc}(\\mu _j, \\sigma _j), j \\in [FP, TP]\\) \n"]}
{"id": "2011.12850", "categories": "cs.CV", "paragraphs": ["Our model is implemented using Pytorch 1.1. We train our model on a ThinkStation P920 workstation with one NVIDIA GTX 1080Ti, and use Adam as a training optimization strategy with the initial learning rate 3e-4. Besides, the super convergence training strategy is employed to boost the training processing and the maximum learning rate is set to 6e-4.\n"]}
{"id": "2011.00395", "categories": "cs.CV", "paragraphs": ["For training, Adam [36] is used for optimization. The learning rate of our model is set to \\(2\\times 10^{-4}\\)  at first. To restrain the slightly larger fluctuation at the beginning of the training process, it is set to \\(2\\times 10^{-5}\\)  at the first 10 epochs as a learning rate warmup strategy. The learning rate drops 10 times once the validation accuracy does not increase (over a large patience 100). Mini-batch with 128 batch size is used to train our model. The dense block configuration is set to (8, 6, 4), where in the first, second and third dense block, 8, 6 and 4 dense layers are used, respectively. This keeps a relatively similar number of neurons in each dense block. The growth rate is set to 48.\n"]}
{"id": "2007.00295", "categories": "cs.LG stat.ML", "paragraphs": ["All BPNNs trained in this section were composed of 5 BPNN-D layers followed by a BPNN-B layer and were trained to predict the natural logarithm of the number of satisfying solutions to an input formula in CNF form. This is accomplished by converting the CNF formula into a factor graph whose partition function is the number of satisfying solutions to the input formula. We evaluated the performance of our BPNN using benchmarks from (), with ground truth model counts obtained\nusing DSharp\u00a0(). The benchmarks fall into 7 categories, including network QMR problems (Quick Medical Reference)\u00a0(),\nnetwork grid problems, and bit-blasted versions of satisfiability modulo theories library (SMTLIB) benchmarks\u00a0(). Each category contains 14 to 105 problems allocated for training and validation. See the appendix for additional details on training, the dataset, and our use of minimal independent support variable sets.\n"]}
{"id": "2003.06711", "categories": "cs.CV cs.LG cs.SD", "paragraphs": ["On the DFDC Dataset, we trained our models with a batch size of 128 for 500 epochs. Due to the significantly smaller size of the DF-TIMIT dataset, we used a batch size of 32 and trained it for 100 epochs. We used Adam optimizer with a learning rate of \\(0.01\\) . All our results were generated on an NVIDIA GeForce GTX1080 Ti GPU.\n"]}
{"id": "2004.04696", "categories": "cs.CL", "paragraphs": ["To separate training and validation data, we set aside a fixed ratio of records in such a way that there is no \u201cleak\u201d between the datasets (i.e., train and validation records that share the same source). We use 10% of the data for validation for years 2017 and 2018, and 5% for year 2019. We report results for the models that yield the highest Kendall Tau across all records on validation data. The weights associated to each pretraining task (see our Modeling section) are set with grid search, using the train/validation setup of WMT 2017.\n"]}
{"id": "2004.04919", "categories": "cs.LG stat.ML", "paragraphs": ["\nStack Epochs: 20. Batch size: 256. Optimizer: Adam with learning rate \\(0.001\\)\n\nAuto Epochs: 5. Batch size: 128. Optimizer: Adam with learning rate \\(0.001\\)\n\nC_E Epochs: 210. Batch size: 256. Optimizer: Adam with learning rate starting at \\(0.0001\\) , decreasing to \\(0.00001\\)  and \\(0.000008\\)  respectively after 126 and 168 epochs\n\nLuring Epochs: 210. Batch size: 256. Optimizer: Adam with learning rate starting at \\(0.0001\\) , decreasing to \\(0.00001\\)  and \\(0.000008\\)  respectively after 126 and 168 epochs. Dropout is used\n\n", "\nStack Epochs: 200. Batch size: 32. Optimizer: Adam with learning rate starting at \\(0.1\\) , decreasing to \\(0.01\\)  and \\(0.001\\)  respectively after 80 and 120 epochs\n\nC_E Epochs: 216. Batch size: 256. Optimizer: Adam with learning rate starting at \\(0.00001\\) , decreasing to \\(0.000005\\)  and \\(0.0000008\\)  respectively after 154 and 185 epochs. Dropout is used\n\nLuring Epochs: 216. Batch size: 256. Optimizer: Adam with learning rate starting at \\(0.00001\\) , decreasing to \\(0.000005\\)  and \\(0.0000008\\)  respectively after 154 and 185 epochs. Dropout is used\n\n"]}
{"id": "2007.08340", "categories": "cs.CV", "paragraphs": ["We use the dataset of 80k images and the pseudo ground truth described in Section REF  for training. It contains 20k images from four categories, where each category includes images with one, two, three and four or more persons. We split the dataset into 77k training and 3k validation images. When downsampling the images to sizes 80x60 and 64x48, we use bicubic interpolation. To generate pseudo ground truth, we use a threshold of 0.7 in the person-detector stage and then select the skeleton if at least 4 keypoints are detected with a score greater than 0.35.\nWe use PyTorch deep learning framework in our experiments. The depth images are normalized in the range [0, 255] and we train our networks using the stochastic gradient descent optimizer with a momentum of 0.9. The initial learning rate is set to 0.001 with a step decay of 0.1 after 12k iterations and each model is trained for 32k iterations with a batch size of 12. We use the pre-trained weights from the authors of RTPose to initialize the pose-estimator networks. Note that these weights were originally obtained using the color images from the COCO dataset. For the layers that have been modified in the pose-estimation network and contain a larger number of channels (e.g. to accommodate S1 and S2), we repeated the same weights and perturbed them by a small random number. The weights of the super-resolution network are initialized using orthogonal initialization [13].\n"]}
{"id": "2006.14978", "categories": "cs.LG stat.ML", "paragraphs": ["where \\(\\displaystyle \\mathbf {R}_n=r_{n}+\\gamma V(s_{n+1})\\)  and\n\\(\\displaystyle r_n =10 \\times l_n \\cdot w_n \\cdot h_n/(L \\cdot W \\cdot H) \\)  is our reward\nfunction which indicates the space utilization. When the current item is not placeable, its reward is zero and the packing sequence ends.\nHere, \\(\\gamma \\in [0,1]\\)  is the discount factor and we set \\(\\gamma \\)  as 1 so that \\(\\displaystyle \\mathbf {R}_n\\)  can directly present how much utilization can agent obtain from \\(\\displaystyle s_n\\)  on.\nThe output of critic network \\(\\displaystyle V(s_{n})\\)  would give a state value prediction of \\(\\displaystyle s_n\\)  and help the training of actor network which outputs a possibility matrix of the next move. This probability is scaled based on \\(\\mathbf {M}_n\\)  \u2014 if the move is infeasible, the possibility will be multiplied by a penalty factor of \\(0.001\\) . Afterwards, a softmax operation is adopted to output the final action distribution.\nNote that, the infeasibility penalty could be absent in test with the help of \\(E_{inf}\\)  and our method can still work well in this situation.\n{FIGURE}{FIGURE}"]}
{"id": "2006.14953", "categories": "cs.CL cs.AI cs.IT math.IT", "paragraphs": ["We examine here the effect of the training parameters on learners' biases. As previously, we only consider the Count-or-Memorization task with \\(l=40\\) , the Add-or-Multiply task with \\(l=20\\) , the Hierarchical-or-Linear task with \\(d=4\\)  and the Composition-or-Memorization task with \\(M=36\\) . We experiment with the architectures detailed in the main text; however, we use 20 different random seeds instead of 100.\n"]}
{"id": "2010.03918", "categories": "cs.LG stat.ML", "paragraphs": ["In this section we present additional experiments where we vary various parameters of the planted model.\nRecall that we consider the following parameters as default: \\(n=1000\\) , \\(k=4\\) , \\(\\epsilon =0.75\\) , \\(|\\mathcal {T}| = |\\mathcal {Q}|= n(\\ln n)^4\\)  and \\(F_{in}=\\mathcal {N}\\left(\\sqrt{2}\\sigma \\Phi ^{-1}\\left( \\frac{1+\\delta }{2} \\right),\\sigma ^2\\right),F_{out}=\\mathcal {N}\\left(0,\\sigma ^2\\right)\\)  with \\(\\sigma = 0.1\\)  and \\(\\delta = 0.5\\) .\n", "Intrinsic noise \\(\\delta \\) , second row in Figure\u00a0REF .\nUsing \\(|\\mathcal {Q}|,|\\mathcal {T}|=n(\\ln n)^3\\) , we see that, for both SPUR and known \\(k\\) , REF  and REF  exactly recover the clusters even when the intrinsic noise is high, that is \\(\\delta = 0.4\\) . REF  and REF  can only make random predictions in this case.\nWhen the number of observations increases to \\(n(\\ln n)^4\\) , REF  and REF  exactly recover the clusters even for values of \\(\\delta \\)  that are as small as \\(0.25\\) . In this case, REF  still predicts random clusters, while REF  is able to recover the clusters when the intrinsic noise is sufficiently small, that is \\(\\delta \\ge 0.6\\) .\n"]}
{"id": "2008.04047", "categories": "cs.CV cs.AI cs.RO", "paragraphs": ["In this evaluation, the models are compared on the whole preview dataset.\nThe dataset is split into 77 training sequences (2982 frames) and 9 held-out testing sequences (358 frames).\nAll networks are trained over 100 epochs with Stochastic Gradient Descent (SGD) and a batch size of 8, with learning rate of \\(10^{-7}\\)  for OFT and \\(10^{-3}\\)  for our network and VED.\nThe grid in OFT is set to \\(100\\times 55\\)  (in meters) with resolution of 1m per pixel.\n"]}
{"id": "2006.06494", "categories": "cs.LG cs.NE cs.SD eess.AS stat.ML", "paragraphs": ["We apply two architectural modifications to the standard implementation: we reduce the channel number of the very first layer to 1 (since we use single-channel magnitude spectrograms) and we vary the number of output neurons to match the classes to the task.\n", "\nWe first down-sample all audio data to 16KHz sampling rate.\n\nThen we zero-pad/segment all sounds in order to have data-vectors of the same length for each task. We segment the audio as follows:\n\nIn the word recognition target task, we use 1-second sound samples as provided in the GSC.\nFor the orthogonal noise classification task, we first generate \u00a020 hours of noisy speech from MS-SNSD and then we extract 1-second fragments with no overlap.\nFor emotion recognition, we extract 1-seconds fragments from IEMOCAP.\n\nIn the speech emotion recognition target task, we use 4-seconds sound samples from IEMOCAP.\nFor the orthogonal task of word recognition, we extract segments containing only one wordWe use https://github.com/bepierre/SpeechVGG for this. from Librispeech and then zero-pad them to 4-seconds.\n\nIn the sound goodness recognition target and the orthogonal instrument recognition task, we use 6-second sounds, applying zero-padding to both Nsynth and Good-Sounds sounds.\n\n\nOnly for GSC, we add noise to the segmented speech sounds at 3 different levels: no noise, low noise\n(-40 to -20 dBfs) and high noise (0 to -10 dBfs).\nThe noise sounds are from the MS-SNSD datasets.\nLike for MS-SNSD we use the MS-SNSD codehttps://github.com/microsoft/MS-SNSD to perform this operation.\n\nNext we compute the Short-Time-Fourier-Transform (STFT) using 16 ms sliding windows with 50% overlap, applying a Hamming window and discarding the phase information.\n\nFinally, we normalize the magnitude spectra of each dataset to zero mean and unit standard deviation, based on the training set's mean and standard deviation.\n\n", "We perform all neural network trainings and pre-trainings with the same parameters.\nWe use a learning rate of 0.0005, a batch size of 13 and the ADAM optimizer .\nWe apply dropout at 50% but neither \\(L_1\\)  nor \\(L_2\\)  regularization.\nWe randomly initialize the weights of all networks, except in the case of weight initialization from a pre-trained network (for WI and dual AT).\nWe train for a maximum of 50 epochs and apply early stopping by testing at the validation loss improvement with a patience of 5 epochs.\nWe divide every dataset using subsets of approximately 70% of the data for training, 20% for validation and 10% for the test set.\nAll of the above settings are kept constant for all datasets in all configurations: non-transfer, transfer, anti-transfer/dual anti-transfer and also for all pre-trainings.\n"]}
{"id": "2011.12672", "categories": "cs.LG cs.CV", "paragraphs": ["Coherently with other works, we evaluate both the AlexNet\u00a0[23] and the more recent ResNet-18\u00a0[16] architecture.\nBefore training each network, we initialize them with pre-trained weights on ImageNet and fine-tune the last fully-connected layer on the dataset of interest for 20 epochs.\nTo train AlexNet\u00a0[23], we use SGD as optimizer with momentum \\(0.95\\)  and L2 regularization on network weights with weight decay \\(5\\times 10^{-5}\\) .\nThe initial learning rate is \\(10^{-3}\\) , exponentially decayed with decay rate \\(0.95\\) .\nResNet-18 is trained with Adam\u00a0[21] and weight decay \\(10^{-6}\\) . The initial learning rate is \\(10^{-4}\\) .\nCoherently with previous works\u00a0([7], [6], [38]), we also compute gradients through the mean and standard deviation computation for the batch normalization layers.\nAll the input images are normalized according to the statistics computed on ImageNet.\nAt training time, data augmentation is performed by first resizing the input image to 256, then randomly cropping to \\(224 \\times 224\\)  for ResNet-18 and \\(227 \\times 227\\)  for AlexNet; finally, a random horizontal flip is performed.\nEvery training batch is composed of 16 samples per domain for ResNet-18 and 6 for AlexNet.\n", "All the models are implemented in Tensorflow 2.0\u00a0([0]).\nWe initialize both AlexNet and ResNet-18 using the publicly available Caffe weights pre-trained on ImageNet, after carefully converting them.ResNet-18 and AlexNet ImageNet weights available at https://github.com/heuritech/convnets-keras and https://github.com/cvjena/cnn-models.\n"]}
{"id": "2011.02947", "categories": "cs.CL", "paragraphs": ["We train \\({\\rm CODER_{ENG}}\\)  initialized from PubMedBERT for the English version with 100K training steps.\nWe also train cross-lingual version \\({\\rm CODER_{ALL}}\\)  initialized from mBERT with 1M training steps.\nTerms are encoded by \\(\\rm [CLS]\\)  representation.\nA batch-size of \\(k=128\\)  relation triplets and gradient accumulation steps of 8 are used for training.\nIn each mini-batch, we set the count of repeat triplets \\(m=8\\) .\nThe maximal sequence length is 32 since the terms are short.\nWe use AdamW [33] as the optimizer with a linear warm-up in the first 10000 steps to a peak of 2e-5 learning rate that decayed to zero linearly.\nThe parameters \\(\\alpha =\\alpha ^{rel},\\beta =\\beta ^{rel},\\lambda =\\lambda ^{rel},\\epsilon =\\epsilon ^{rel}\\)  of MS-loss are set to 2, 50, 0.5, 0.1 respectively.\nThe weight \\(\\mu \\)  is set to 1 to balance term-term loss and term-relation-term loss.\n", "For feature-based, the learning rate is set at 1e-3 for all kinds of embeddings.\nFor fine-tuning, the learning rate is set at 1e-3 for word embeddings and concept embeddings and 2e-5 for contextual embeddings.\nBatch-size is 512 for feature-based contextual embeddings, word embedding, and concept embeddings.\nFor fine-tuned contextual embeddings, batch-size is 96.\nContextual embeddings use \\(\\mathbf {e}\\)  to represent terms.\nAll models are trained for 50 epochs.\n"]}
{"id": "2009.11961", "categories": "cs.LG eess.SP", "paragraphs": ["The model is trained using the Adam optimizer with default tensorflow 2.0 settings and initial learning rate of 0.001 for 20 epochs. The learning rate is annealed by a factor of 2 every 2 epochs starting at epoch 15. One epoch consists of 50 batches of size 256 and the model takes the history of 12 points (12 month; \\(w=12\\) ) and predicts 12 points (12 month; \\(H=12\\) ) ahead in one shot. Each training batch is assembled using weighted stratified sampling over time-series IDs. First, 256 time-series IDs are sampled with replacement and the probability of sampling a given time series is proportional to the length of the time-series. Second, the split time point is chosen uniformly at random for each of the time-series ids sampled in the previous step.\n", "Due to the stochastic nature of N-BEATS, all results reported for this model take averages over 100 trials. In each trial we build an ensemble of 64 models bootstrapped from the set of 1024 trained models.\n"]}
{"id": "2007.11814", "categories": "cs.CV", "paragraphs": ["Recall that the objective of ZSL is to correctly assign a test image to its label. This is a typcial classification problem. For a training sample \\(x_i\\) , Let \\(y_i = \\lbrace y_i^1, y_i^2, ..., y_i^{\\vert \\mathcal {Y}_s \\vert } \\rbrace  \\in \\lbrace 0, 1\\rbrace \\)  denote the one-hot encoding of the ground truth label and \\(p_i = \\lbrace p_i^1, p_i^2, ..., p_i^{\\vert \\mathcal {Y}_s \\vert } \\rbrace \\)  denote the compatibility scores of \\(x_i\\)  (Equ.\u00a0REF ). That is, \\(p_i^j = F(x_i, y_j; W)\\) . The model parameters \\(W\\)  are learned by minimizing the cross entropy loss:\n\\(\\mathcal {L} = - \\sum _{i=1}^{N}\\sum _{j=1}^{\\vert \\mathcal {Y}_s \\vert } y_i^j \\log (p_i^j) + (1-y_i^j) \\log (1-p_i^j).\\) \n"]}
{"id": "2010.06432", "categories": "cs.CL cs.AI cs.LG", "paragraphs": ["The parameters configuration of the binary classification tasks, namely, stance classification and evidence detection, was:\nmaximum sequence length of 128, batch size of 32, dropout rate of \\(0.1\\)  and learning rate of 5e-5. Each model was fine-tuned over 10 epochs, using a cross-entropy loss function.\nThe regression model for argument quality prediction, similar to the one used by [10], used\na maximum sequence length of 100, a batch size of 32, a dropout rate of \\(0.1\\)  and a learning rate of 2e-5.\nEach model was fine-tuned over 3 epochs, using a mean-squared-error loss function.\nIn all cases, the model from the last epoch was selected for evaluation.\n"]}
{"id": "2004.07945", "categories": "cs.CV", "paragraphs": ["where \\(\\Theta \\)  and \\(\\Phi \\)  are the parameters of the Detector CNN \\(g(\\cdot )\\)  and the\nNormalizer \\(f(\\cdot )\\) , respectively; \\(\\ell (\\cdot ; y)\\)  is the negative log-likelihood loss\nfunction. \\(I_j\\)  and \\(y_j\\)  are training image regions and their labels: We\nuse image regions extracted from the Phos dataset\u00a0([37]),\nincluding the\nimages shown in Fig.\u00a0REF , the labels are either background or\nthe index of the object contained in the corresponding image region. We use\nPhos for our purpose because it is made of various objects under different\nillumination conditions, with 9 images captured under various strengths of\nuniform illumination and 6 images under non-uniform illumination from various\ndirections. In practice, we use Theano\u00a0([2]) to\noptimize Eq.\u00a0(REF ).\n{FIGURE}{FIGURE}"]}
{"id": "2010.03662", "categories": "cs.CL", "paragraphs": ["We employ the Adam optimizer with initial learning rate \\(\\eta = 2\\mathrm {e}{-5}\\) , fine-tune for at most 5 epochs, and use early-stopping to select the best model. We use a batch size of 32 for experiments that do not use contrastive training and a batch size of 16 for those using contrastive training to establish a fair comparison.\n"]}
{"id": "2010.14701", "categories": "cs.LG cs.CL cs.CV", "paragraphs": ["The transformers used for language and multimodal modeling have fully connected layers of size \\(4 d_{\\rm model}\\)  and attention layers of size \\(d_{\\rm model}\\) , in the notation of [8], [0]. For math, image, and video modeling we scale the FC layers to \\(d_{\\rm model}\\)  and the attention layers to \\(d_{\\rm model}/4\\) . We use an aspect ratio \\(d_{\\rm model}/n_{\\rm layer} \\approx 10\\)  for math, images, and videos as we find that this is approximately optimal, meaning that these domains prefer much deeper models as compared to language [8], where the optimal aspect ratio \\(\\sim 100\\) . Thus our math, image, and video models are essentially identical, differing only in context length. For math alone we used a weight decay [11] of \\(0.05\\) . We provide more detailed hyperparameter settings in appendix .\n"]}
{"id": "2010.14678", "categories": "cs.CL", "paragraphs": ["In our experiments, for the rule-based, feature-based and language-model-based models, i.e., NOA, ADE, UAD, BIOADI and LNCRF, we use the same hyper parameters and features reported in the original papers. For the deep learning models, i.e., LSTM-CRF, BEM, DECBAE and GAD, we fine tune the hyper parameters using the performance on the development set for each task. More specifically, we find the following hyper parameters for the deep learning models: 50 samples per each mini-batch; 200 hidden dimensions for all feed forward and BiLSTM layers; dropout rate 0.2; two layers of BiLSTM and GCN; and Adam optimizer with learning rate 0.3. Note that for pre-trained word embeddings we use the uncased version of BERT\\(_{base}\\)  [8] with 768 dimensions.\n{FIGURE}"]}
{"id": "2009.08276", "categories": "cs.CV", "paragraphs": ["The software setup consists of Kubuntu 18.04, Python, Keras, Tensorflow, CUDA and other miscellaneous software required. We train using single GPU accross different machines with a similar configuration: CPU Intel i9-9820X, memory 64GB, 2 x GPU RTX 2080Ti. We performed a batch of experiments on both Iteration1 and Iteration2 where we handled the different considerations as follows:\n"]}
{"id": "2004.13167", "categories": "cs.LG q-bio.QM stat.ML", "paragraphs": ["Models were trained for 180 thousand parameter updates using 32 NVIDIA V100 GPUs, a batch size of 16,384, and the Adam optimizer (\\(\\alpha = 2 \\cdot 10^{-4}\\) , \\(\\beta _1 = 0.99\\) , \\(\\beta _2 = 0.999\\) ). We evaluated training progress using a held-out 5% subset of the training data as a validation set.\n"]}
{"id": "2003.04780", "categories": "cs.CV", "paragraphs": ["The training process and experiments are conducted on a NVIDIA TitanX GPU. The network is trained with the Adam optimizer with the learning rate 1e-4 and the batch size of 16. Data augmentation is applied mainly for image rotation because the vehicle seldom makes a turn. In semi-supervised learning loss, the regularization weight \\(\\lambda \\)  is usually less than 1, such as \\(0.4\\) . When converting the cost map into discrete labels, we set the thresholds \\(\\alpha _1\\)  and \\(\\alpha _2\\)  to 0.5 for an SUV. Actually, they can be changed based on the through capacity of a vehicle.\n"]}
{"id": "2008.09655", "categories": "cs.CV cs.GR cs.LG", "paragraphs": ["Our final models follows original StyleGAN training schedule. We alternate between two phases: a resolution transition phase for 600k samples then a stabilization phase for 600k samples. After final reslution is reached we train model until the number of batches reaches 450k.\nThe proportion of pairwise discriminator changes linearly from 0.5 to 0.1 during the resolution transition phase.\nWe use crops instead of generated frames when update pairwise discriminator with probabily 0.5.\nFor inference we used accumulated exponential moving average with \\(\\alpha =0.999\\)  to generate samples.\nOur final model was trained using Adam optimizer with parameters \\(\\beta _1=0, \\beta _2=0.99\\) .\n"]}
{"id": "2012.13103", "categories": "cs.LG cs.CV", "paragraphs": ["We perform all experiments on a desktop PC using a single GeForce RTX 2080 Ti GPU and 12-core Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz. We implemented training and certification in PyTorch and used Gurobi 9.0 as a MILP solver.\n"]}
{"id": "2012.13113", "categories": "cs.CV", "paragraphs": ["Training on CIFAR-10/100. We adopt a batch size of 128, and a learning rate (LR) is initially set to 0.1 and then decayed by 10 at both the 80-th and 120-th epochs among the total 160 epochs, as in [8].\n", "Training on ImageNet. We adopt a batch size of 256, and the LR is initially set to 0.1 and then decayed by 10 every 30 epochs among the total 90 epochs, following [8].\n", "Training on WikiText-103.\nWe train the basic transformer [41] on WikiText-103 consisting of 100M tokens and a vocabulary of around 260K. We use a dropout rate of 0.1, and the Adam optimizer with \\(\\beta _1=0.9\\) , \\(\\beta _2=0.98\\)  and \\(\\epsilon =10^{-9}\\) .\nEach training batch contains a set of 1024 tokens with a sequence length of 256.\nWe train the model for a total of 50,000 steps, following [54].\n"]}
{"id": "2011.09820", "categories": "cs.LG cs.NE", "paragraphs": ["By following DARTS [19], a half of the standard training set is used for training a model and the other half for validation. A small network of 8 cells is trained via the FGSM-based adversarial training method [29] in Eq. (REF ) with the batch size as 64 and initial channels as 16 for 50 epochs. Following the setting of [29], the perturbation of the FGSM adversary is randomly initialized from the uniform distribution in \\([-\\epsilon ,\\epsilon ]\\) , where \\(\\epsilon = 2/255\\) . The attack step size \\(\\xi \\)  is set to \\(1.25\\epsilon \\) .\nThe SGD optimizer with the momentum \\(0.9\\)  and the weight decay \\(3\\times 10^{-4}\\)  is used. The proposed method is implemented in PyTorch 0.3.1 and all the experiments are conducted in Tesla V100S GPUs with 32G CUDA memory.\n"]}
{"id": "2012.07808", "categories": "cs.CL", "paragraphs": ["Across models, we set all hidden dimensions to 256, the dropout rate\nto 0.1, and batch size to 16. We used the subword tokenizer of BERT\n[10], which has a 30k token vocabulary trained using\nWordPiece [50]. For RT, we follow\n[49] and add a generic label for movie titles during\ntraining which we replace with the original title during inference.\nWe used the Adam optimizer [24] with a learning rate\nof \\(3e-4\\) , \\(l_2\\)  constraint of 3, and warmup of 8,000\u00a0steps. We also\nused dropout [45] after every non-linear\nfunction. For each dataset, we additionally tuned the number of\naspects, regularization parameter \\(\\lambda \\) , Dirichlet parameters\n\\(\\alpha _a\\)  and \\(\\alpha _s\\) , label smoothing parameter \\(\\delta \\) , and\nbeam search size on the development set. We performed early stopping\nbased on the token-level accuracy of the model, again on the\ndevelopment set. Our model was trained on a single GeForce GTX 1080Ti\nGPU and is implemented using PyTorch.Our code can be\ndownloaded from https://github.com/rktamplayo/PlanSum. A more detailed model\nconfiguration is described in the Appendix.\n"]}
{"id": "2011.06833", "categories": "cs.LG", "paragraphs": ["As NN-MC needs an initial training phase, we use an initial set of 50 samples for all datasets except letters. For letters, since the number of classes is higher and the dataset is more complex, we let the initial set be 150 samples. The instances are chosen randomly from the training set. To speed up training, we limit the datasets to the total training size of \\(N=1050\\)  (and \\(N=8000\\)  for letters), including the initial set. After the initial clean data batch, noisy data arrives in batches of 50 instances. For the classifier with the UCI datasets we use SVM.\n", "To synthesize the crowd labels, we evaluated NN-MC using the following parameters: number of workers \\(K \\in \\lbrace 6,8,10\\rbrace \\) , empty proportion \\(e \\in \\lbrace 0.1,0.2,0.3\\rbrace \\) , and noise rate \\(\\varepsilon \\in \\lbrace 0.4, 0.6, 0.8\\rbrace \\)  with all four noise patterns mentioned above. Note that the empty proportion indicates the proportion of missing labels for each worker. Moreover, the mentioned noise rates \\(\\varepsilon \\)  are the average of the noise rates of all the workers. The noise rates of each worker are randomly selected in the range of 10% to 90% with respect to the average of \\(\\varepsilon \\) .\n", "Our algorithm queries the true label of the \\(r=5\\)  most informative noisy samples per batch via the oracle. We repeat each experiment 50 times and report the average final accuracy computed on the test set.\n"]}
{"id": "2011.06868", "categories": "cs.CL cs.LG", "paragraphs": ["All models adopt the base Transformer architecture\u00a0[54] with\u00a0\\(d_{\\text{model}}=512\\) ,\u00a0\\(d_{\\text{hidden}}=2048\\) ,\u00a0\\(n_{\\text{heads}}=8\\) ,\u00a0\\(n_{\\text{layers}}=6\\) , and\u00a0\\(p_{\\text{dropout}} = 0.3\\) . For En-De and Ro-En, the source and target embeddings are tied with the output layer weights\u00a0[43], [38].\nWe add dropout to embeddings\u00a0(0.1) and label smoothing\u00a0(0.1). AR models are trained with the Adam optimizer\u00a0[27] with a batch size of\u00a04096 tokens. We checkpoint models every\u00a01000 updates. The initial learning rate is\u00a00.0002, and it is reduced by\u00a030% after\u00a04 checkpoints without validation perplexity improvement. Training stops after\u00a020 checkpoints without improvement. All NAR models are trained using Adam \u00a0[27] with initial learning rate of\u00a00.0005 and a batch size of 64,800 tokens for maximum 300,000 steps.Our preliminary experiments and prior work show that NAR models require larger training batches than AR models. We select the best checkpoint based on validation BLEU\u00a0[41]. All models are trained on 8 NVIDIA V100 Tensor Core GPUs.\n"]}
{"id": "2012.06971", "categories": "cs.CL", "paragraphs": ["We train models on public Chinese female corpus [15], which includes 10-hour professional speech and 10000 sentences.\n500 sentences are used for validation and other sentences are used for training.\nWe down-sample the speech to 16k Hz sampling frequency,\nThe tacotron 2 part in our model is trained with vanilla setups except setting frequency to match our speech.\nThe learning rate is fixed to \\( 10^{-3} \\)  and the loss weight of NML is 0.05.\n"]}
{"id": "2007.12140", "categories": "cs.CV", "paragraphs": ["The SceneFlow dataset consists of 3 components (Flyingthings, Driving and Monkaa) and comes with a predefined train and test split with ground truth for all examples. Following the standard practice with this dataset we use the predefined train and test split for all experiments. We also used only FlyingThings part of the dataset, as Driving and Monkaa don't have corresponding TEST sets and including them into training hurts accuracy for both Sceneflow and when it's used to pre-train for Middlebury. When all 35k images are used to train a model, the PSM EPE of XL model is \\(0.41\\)  on \"finalpass\".\nWe considered random crops of \\(320 \\times 960\\)  and a batch size of 8, and a maximum disparity of 320. We trained for \\(1.42\\) M iterations using the Adam optimizer, starting from a learning rate of \\(4e^{-4}\\) , dropping it to \\(1e^{-4}\\) , then to \\(4e^{-5}\\) , then to \\(1e^{-5}\\)  after 1M, \\(1.3\\) M, \\(1.4\\) M iterations respectively.\nThe general robust loss for SceneFlow experiments was applied with, \\(\\alpha = 0.9, c = 0.1\\) . For all other experiments, \\(\\alpha = 0.8, c = 0.5\\) .\n", "The training set for the real world ETH3D stereo dataset [44] contains just a few stereo pairs, so additional data is needed to avoid overfitting. For the benchmark submission we trained the network on all 394 images from both KITTI datasets, as well as all half and quarter resolution training images from Middlebury dataset V3 [42] and training images from ETH3D dataset. We used the same training parameters as for KITTI submission and stopped training after 115k iterations, which was picked using 4 fold cross-validation on ETH3D training set. Note that there is no additional training, pre-training, finetuning.\n", "Similarly, the Middlebury dataset [42] contains a limited training set. To avoid overfitting, we pre-trained the model on SceneFlow's FlyingThings TRAIN set with data augmentation, then fine-tuned on the 23 Middlebury14-perfectH training images, while keeping all data augmentations on. Specifically, we used a HITNet Large model with initialization at 6 scales (M=5), pre-trained it for 445k iterations, using batch size of 8 and random crops of \\(512 \\times 960\\) .\nWe initialize the learning rate to \\(4e^{-4}\\) , then gradually drop it to \\(1e^{-4}\\) , \\(4e^{-5}\\)  and \\(1e^{-5}\\)  after 300K, 400K, 435K iterations respectively. Finally, we fine-tuned the model for 5K iterations at \\(1e^{-5}\\)  learning rate. These parameters were selected by using 4-fold cross validation on Middlebury training set.\n{FIGURE}"]}
{"id": "2007.12146", "categories": "cs.CV", "paragraphs": ["All the 6-layer models have 96.6 million parameters and the 4-layer models have 82.4 million parameters. We train our models using Adam optimizer\u00a0[17] with a linear warmup and with a learning rate of 1e-4 and a staircase learning rate schedule, where we multiply the learning rate by 0.1 at 14000 and at 19000 iterations. We train for 36.1K total iterations (100 epochs) on 2 NVIDIA Titan XP GPUs for 12 hours and use a batch-size of 96 and \\(d = 768\\)  as dimensionality for encoding all multi-modal features. We use the PyTorch [33] deep-learning framework for all the experiments.\n"]}
{"id": "2010.12148", "categories": "cs.CL cs.LG", "paragraphs": ["Following the previous practice, we pre-train ERNIE-Gram in base size (\\(L=12, H=768,\\)  \\(A=12\\) , Total Parameters=110M)We donate the number of layers as \\(L\\) , the hidden size as \\(H\\)  and the number of self-attention heads as \\(A\\) ., and set the length of the sequence in each batch up to 512 tokens. We use Adam\u00a0[16] with \\(\\beta _1=0.9,\\beta _2=0.999,\\epsilon =10^{-6}\\)  for optimizing. For pre-training on base-scale English corpora, the batch size is set to 256 sequences, the peak learning rate is \\(1e\\) -4 for 1M training steps, which are the same settings as BERT. As for the pre-training on large-scale English corpora (underway), the batch size is 5112 sequences, the peak learning rate is \\(4e\\) -4 for 500K steps. For pre-training on Chinese corpora, the batch size is 256 sequences, the peak learning rate is \\(1e\\) -4. All the pre-training hyper-parameters are supplemented in the Appendix\u00a0.\n"]}
{"id": "2012.11662", "categories": "cs.LG cs.RO", "paragraphs": ["ARS: For all environments \\(\\alpha = .02\\) , \\(\\sigma = .025\\) , \\(N=50\\) , \\(b=20\\) .\n"]}
{"id": "2001.05647", "categories": "cs.LG eess.IV", "paragraphs": ["A multi-layer perceptron (MLP) 6105-16-2 (corresponding to 6105 nodes for the input (first) layer, 16 nodes for the hidden layer, and 2 nodes for the output layer) was used for classification. The outputs of the MLPs were the probability of the given input being classified as each class. We used cross-entropy as the objective function. We performed 5-fold cross-validation (subject-wise splitting), and each entry of the input vectors was normalized by training set mean and standard deviation (std) within each site. As we performed overlapping truncation for data augmentation in data processing, we used the majority voting method to evaluate the final classification performance. For example, we augmented \\(m\\)  input instances for a single subject, and if more than \\(m/2\\)  instances were classified as ASD, then we assigned 'ASD' label to the subject. Adam optimization was applied with initial learning rate 1e-5 and reduced by 1/2 for every 20 epochs and stopped at the 50th epoch. In each epoch, we performed local updates multiple times instead of once based on communication pace \\(\\tau \\) . We set the total steps of each epoch as 60, and the batch size of each site was the number of training data over 60.\n", "Then, we investigated adding the randomization mechanism on shared weights to protect the data from inversion attack, such as inferring data from model weights, given local model weights. Here we tested the Gaussian and Laplace mechanism, which corresponded to L2 and L1 sensitivity. Institutions may want to specify the level of privacy they want to preserve, which would be reflected in the noise levels. For the Gaussian mechanism experiment, we generated Gaussian noise \\(\\varepsilon _n\\sim N(0,\\alpha \\sigma )\\)  adding to local model weights, where \\(\\sigma \\)  was the standard deviation of the local model weights and \\(\\alpha \\)  was the noise level. We varied \\(\\alpha \\)  from 0.001 to 1. For the Laplace mechanism experiment, we generated Laplace noise \\(\\varepsilon _n\\sim Lap(\\alpha \\sigma /\\sqrt{2})\\)  adding to local model weights, where \\(\\alpha \\)  was the scale parameter, and \\(\\sigma \\)  was the standard deviation of the local model weights. We varied \\(\\alpha \\)  from 0.001 to 1. As the results in Figure REF  and Figure REF  show, there was a trade-off between model performance and noise level (privacy-preserving level). When the noise level was too high (\\(\\alpha = 1\\)  in our setup), corresponding to high privacy-preserving levels, the models failed in the classification task.\n{FIGURE}{FIGURE}"]}
{"id": "2012.03322", "categories": "cs.CV cs.AI", "paragraphs": ["Each Auto-Encoder has been trained for 90 epochs using the MSE loss function and Adam optimiser with a learning rate of lr=0.001, \\(\\beta _1\\) =0.9 and \\(\\beta _2\\) =0.999 [27] and a batch size of 100.\n{FIGURE}", "Also, since the Alexnet model takes as input images of size 224\\(\\times \\) 224 and the images of the datasets used in this work are of size 64\\(\\times \\) 64, we pad the images with zeros all around until we get the desired size.\n"]}
{"id": "2004.11886", "categories": "cs.CL", "paragraphs": ["All of our training settings for machine translation are in line with\u00a0[50]. We use a dropout of 0.3 for both the WMT and IWSLT datasets and linearly scale down the dropout ratio when shrinking the dimension of the embeddings for the WMT datasets. Same as\u00a0[50], we apply Adam optimizer and a cosine learning rate schedule\u00a0[20], [30] for the WMT models, where the learning rate is first linearly warm up from \\(10^{-7}\\)  to \\(10^{-3}\\)  followed by a cosine annealing with a single cycle. For IWSLT De-En, we use inverse square root learning rate scheduling\u00a0[46] with the linear warm-up. We use the same training settings for summarization. For the language modeling task, the training settings are in line with\u00a0[1]. We decrease the dropout ratio for the FFN layer by half in our\u00a0Lite Transformer due to the flattened layer.\n"]}
{"id": "2005.05740", "categories": "cs.CV", "paragraphs": ["\\(PK\\) -style batches is employed in the experiments. For reasonable comparison, the\nbatch size is set to 128 to match TriNet\u00a0\u00a0[13] by setting\n\\(P\\)  to 32 and \\(K\\)  to 4, thus a batch contains 32 identities and 4 images for\neach identity. All images are resized to \\((128, 64)\\)  and we only use random\nhorizontal flips for data augmentation, with the flips probability at \\(50\\%\\) .\n", "Adam\u00a0[29] is chosen as the optimizer for both feature extractor\nand discriminator network, with base learning rate at \\(2\\cdot 10^{-4}\\)  and\n\\(3\\cdot 10^{-4}\\) , respectively, with a same weight decay factor of\n\\(5\\cdot 10^{-4}\\) . All other hyper-parameters of the optimizers are default in\nPytorch. The number of epochs is set to 600, and the learning rate will decay at 200\nepochs and 400 epochs with a decay factor of \\(0.1\\) .\n", "Features are normalized when computing \\(\\mathcal {L}_{atl}\\)  and\n\\(\\mathcal {L}_{cam}\\) , but not normalized for \\(\\mathcal {L}_{reg}\\)  and original\ntriplet loss \\(\\mathcal {L}_{ori}\\) . It is observed that Euclidean-margin can reach\nits best performance at \\(0.8\\) , so the Euclidean-margin is set to \\(0.8\\)  in all\nthe experiments. The angular-margin is set to \\(0.05\\)  on DukeMTMC-ReID, while\n\\(0.08\\)  for the other two datasets when the best accuracy is reached. In\naddition, the weight \\(w_1\\)  is set at \\(10^{-3}\\)  in loss \\(\\mathcal {L}_{acn}\\)  in\nall experiments. Because the dimensions are different, the weight \\(w_2\\)  is set\nat \\(5 \\cdot 10^{-4}\\)  and \\(10^{-4}\\)  for Euclidean-distance-based triplet loss and\nATL, respectively.\n"]}
{"id": "0709.3013", "categories": "cs.CV", "paragraphs": ["where \\(E[.]\\)  is the expectation operator related to the probability distribution \\(p(\\Phi \\mid \\mathcal {A}_\\nu )\\) . We note that the multinomial distribution does not show a clear maximum because of the too few examples provided by the user compared to the large number \\(r\\)  of values \\(\\phi _l^j; l=1,...,r\\) . This justifies the use of the MMSE estimator rather than the maximum a posteriori estimator.\n"]}
{"id": "1209.5111", "categories": "cs.CV cs.NE", "paragraphs": ["b = choice(0,\u00a0(uniform(2,10)),\u00a0a) }\n\nspecifies a joint distribution in which \\(a\\)  is distributed normally with mean 0 and variance 1,\nand \\(b\\)  takes either value 0, or \\(a\\) , or a value drawn uniformly between 2 and 10.\nThere are three hyper parameters at play here, shown in bold: the value of \\(a\\) , the value of the choice, and the value of the uniform.\n"]}
{"id": "1202.1568", "categories": "cs.CL", "paragraphs": ["Motivated by Assumption 4, the parameters \\(\\mu _y=(Z|Y=y), y\\in C\\)  are determined by running multidimensional scaling (MDS) or Kernel PCA on the empirical versions of \\(\\lbrace (X|Y=y), y\\in C\\rbrace \\)  (replace expectation with train set average).\n", "\nThe covariance matrices \\(\\Sigma _y\\)  of the Gaussians \\(Z|Y=y\\) , \\(y=1,\\ldots ,C\\)  may be estimated by computing the empirical variance of \\(z\\)  values simulated from \\(p_{\\hat{\\theta }}(Z|X^{(i)})\\) , for all \\(i\\)  such that \\(Y^{(i)}=y\\) . Alternatively, the samples may be replaced with the most likely values\n\\(\\left\\lbrace _z p_{\\hat{\\theta }}(Z|X^{(i)}): i=1,\\ldots ,n\\right\\rbrace .\\) \n"]}
