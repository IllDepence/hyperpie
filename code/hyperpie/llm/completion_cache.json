{"7cd11117d1d8841cf72cef5d77835da0": {"517fd7ce81544868f3ad12c632fa85d0": {"timestamp": "20230601_102256", "params": {"model": "text-davinci-003", "max_tokens": 512, "temperature": 0.0, "top_p": 1, "n": 1, "logprobs": 0, "echo": false}, "paragraph": {"annotator_id": "tsa", "document_id": "2005.00512", "paragraph_index": 65, "text": "We divide our 438 annotated documents into training (70%), validation (30%) and test set (30%).\r\nThe base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state. We use a dropout of 0.2 after BiLSTM embeddings. All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers. For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax. We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights. We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score. All our models were trained using 48Gb Quadro RTX 8000 GPUs. The multitask model takes approximately 3 hrs to train.\r\n", "annotation": {"entities": {"a6": {"id": "a6", "type": "a", "subtype": null, "surface_forms": [{"id": "#fb9fec7c-fff7-44a7-85cc-41f77f52c364", "surface_form": "model", "start": 136, "end": 141}, {"id": "#5628e381-9e76-4263-9125-4acaf18f0cb3", "surface_form": "model", "start": 603, "end": 608}]}, "a14": {"id": "a14", "type": "a", "subtype": null, "surface_forms": [{"id": "#1c77af68-2d6e-4db4-9501-57ac38f5287a", "surface_form": "BiLSTM", "start": 176, "end": 182}, {"id": "#3f44cc99-e718-4886-8302-a8b011bfdb26", "surface_form": "BiLSTM", "start": 238, "end": 244}]}, "v5": {"id": "v5", "type": "v", "subtype": "n", "surface_forms": [{"id": "#32269e63-2aa5-4be7-8673-56fda6e6458e", "surface_form": "128", "start": 188, "end": 191}]}, "p1": {"id": "p1", "type": "p", "subtype": null, "surface_forms": [{"id": "#d14e7fbd-e8c5-4b7e-ba1d-f38b8ddd895e", "surface_form": "hidden state", "start": 194, "end": 206}, {"id": "#4c0ef44a-0235-4bf3-b500-2ebeb87ac97e", "surface_form": "hidden state", "start": 551, "end": 563}]}, "p2": {"id": "p2", "type": "p", "subtype": null, "surface_forms": [{"id": "#3677f201-e8b6-47c6-98bf-d940c71285d7", "surface_form": "dropout", "start": 217, "end": 224}, {"id": "#77e61c37-b503-4085-9fc5-f28b8380ab12", "surface_form": "dropout", "start": 371, "end": 378}]}, "v6": {"id": "v6", "type": "v", "subtype": "n", "surface_forms": [{"id": "#48ffc6bb-b356-4f4f-b03c-bd408cccc046", "surface_form": "0.2", "start": 228, "end": 231}]}, "a18": {"id": "a18", "type": "a", "subtype": null, "surface_forms": [{"id": "#82322f99-efc5-4991-b895-e06d06a5442e", "surface_form": "feedforward networks", "start": 261, "end": 281}, {"id": "#a02000ac-a1d0-40d9-8d84-b554328da4a1", "surface_form": "feedforward layer", "start": 522, "end": 539}]}, "v7": {"id": "v7", "type": "v", "subtype": "n", "surface_forms": [{"id": "#1dc004fa-d05b-4486-a88b-8e37515cddb4", "surface_form": "two", "start": 298, "end": 301}]}, "p3": {"id": "p3", "type": "p", "subtype": null, "surface_forms": [{"id": "#2e27d38a-f82e-42cc-a15d-9d1294a95f5f", "surface_form": "hidden layers", "start": 302, "end": 315}, {"id": "#5b6e9c44-2d22-4447-9c6b-f317afcc8cc9", "surface_form": "dimension", "start": 325, "end": 334}]}, "v8": {"id": "v8", "type": "v", "subtype": "n", "surface_forms": [{"id": "#71e9cfe6-128b-4af0-96cc-85aae4d176e7", "surface_form": "128", "start": 335, "end": 338}]}, "v9": {"id": "v9", "type": "v", "subtype": "o", "surface_forms": [{"id": "#a95b47a5-80ac-4588-9379-288486c3c0c6", "surface_form": "gelu", "start": 344, "end": 348}]}, "p4": {"id": "p4", "type": "p", "subtype": null, "surface_forms": [{"id": "#5edd9f78-5718-4998-b0ea-b97f6402a818", "surface_form": "activation", "start": 349, "end": 359}]}, "v11": {"id": "v11", "type": "v", "subtype": "n", "surface_forms": [{"id": "#4f351626-b59d-4e71-a061-b65f4e637c03", "surface_form": "128", "start": 545, "end": 548}]}, "v12": {"id": "v12", "type": "v", "subtype": "n", "surface_forms": [{"id": "#ccf1a670-2b62-44aa-8560-1ef916c44ecc", "surface_form": "30", "start": 613, "end": 615}]}, "p5": {"id": "p5", "type": "p", "subtype": null, "surface_forms": [{"id": "#8b428ec9-5ccb-4b0f-9307-2fb6bca904ac", "surface_form": "epochs", "start": 616, "end": 622}]}, "a22": {"id": "a22", "type": "a", "subtype": null, "surface_forms": [{"id": "#09100685-2022-4f8f-b5e4-bd9a3a669e77", "surface_form": "Adam", "start": 629, "end": 633}, {"id": "#ad6dea75-a336-4564-957b-fb5f22e38834", "surface_form": "early stopping", "start": 730, "end": 744}]}, "v13": {"id": "v13", "type": "v", "subtype": "n", "surface_forms": [{"id": "#299b15ad-953a-4664-b9c2-10a330e0a51d", "surface_form": "1e-3", "start": 649, "end": 653}]}, "p6": {"id": "p6", "type": "p", "subtype": null, "surface_forms": [{"id": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "surface_form": "learning rate", "start": 657, "end": 670}]}, "c1": {"id": "c1", "type": "c", "subtype": null, "surface_forms": [{"id": "#186b0fcb-4905-4782-b328-f0a9d139e50b", "surface_form": "for all non BERT weights", "start": 671, "end": 695}]}, "v14": {"id": "v14", "type": "v", "subtype": "n", "surface_forms": [{"id": "#27ade662-e3eb-4a71-bfa7-e5161909dbde", "surface_form": "2e-5", "start": 700, "end": 704}]}, "c2": {"id": "c2", "type": "c", "subtype": null, "surface_forms": [{"id": "#95b12990-5e20-4df2-aba1-ca98d916bbe9", "surface_form": "for BERT weights", "start": 705, "end": 721}]}, "p8": {"id": "p8", "type": "p", "subtype": null, "surface_forms": [{"id": "#5581acf1-60aa-420c-88df-2e2abcf2aaef", "surface_form": "patience value", "start": 752, "end": 766}]}, "v15": {"id": "v15", "type": "v", "subtype": "n", "surface_forms": [{"id": "#3faea6b8-f987-43bf-92f0-b81c15973ee7", "surface_form": "7", "start": 770, "end": 771}]}, "c3": {"id": "c3", "type": "c", "subtype": null, "surface_forms": [{"id": "#d78c347f-4a30-484e-bd35-b77c1f156360", "surface_form": "on the validation", "start": 772, "end": 789}]}}, "relations": {"r23": {"id": "r23", "source": "v5", "target": "p1", "evidences": [{"id": "#06db44b0-ddb1-469d-a7b8-e6690c06811e", "source_surface_form": "#32269e63-2aa5-4be7-8673-56fda6e6458e", "target_surface_form": "#d14e7fbd-e8c5-4b7e-ba1d-f38b8ddd895e", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state.", "start": 97, "end": 208}]}, "r0": {"id": "r0", "source": "p1", "target": "a14", "evidences": [{"id": "#708d50d7-1c98-4543-abea-59c5536be8e8", "source_surface_form": "#d14e7fbd-e8c5-4b7e-ba1d-f38b8ddd895e", "target_surface_form": "#1c77af68-2d6e-4db4-9501-57ac38f5287a", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state.", "start": 97, "end": 208}]}, "r1": {"id": "r1", "source": "v6", "target": "p2", "evidences": [{"id": "#4acd1b39-650e-44e9-b226-47994fe4462a", "source_surface_form": "#48ffc6bb-b356-4f4f-b03c-bd408cccc046", "target_surface_form": "#3677f201-e8b6-47c6-98bf-d940c71285d7", "evidence_sentence": "We use a dropout of 0.2 after BiLSTM embeddings.", "start": 209, "end": 257}]}, "r2": {"id": "r2", "source": "v7", "target": "p3", "evidences": [{"id": "#264d74a8-d8b9-40eb-a71b-915886b79739", "source_surface_form": "#1dc004fa-d05b-4486-a88b-8e37515cddb4", "target_surface_form": "#2e27d38a-f82e-42cc-a15d-9d1294a95f5f", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r3": {"id": "r3", "source": "p3", "target": "a18", "evidences": [{"id": "#56c4fd32-f245-4f62-a1d2-3b543ace0833", "source_surface_form": "#2e27d38a-f82e-42cc-a15d-9d1294a95f5f", "target_surface_form": "#82322f99-efc5-4991-b895-e06d06a5442e", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r6": {"id": "r6", "source": "v8", "target": "p3", "evidences": [{"id": "#0cf7b217-68d0-475a-a459-66358e62ca37", "source_surface_form": "#71e9cfe6-128b-4af0-96cc-85aae4d176e7", "target_surface_form": "#5b6e9c44-2d22-4447-9c6b-f317afcc8cc9", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}, {"id": "#81fda297-20ec-407b-a7cd-62fe00c717fc", "source_surface_form": "#c8974ec6-0e80-4c4b-a038-50c7416d7ad6", "target_surface_form": "#77e61c37-b503-4085-9fc5-f28b8380ab12", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r5": {"id": "r5", "source": "p3", "target": "a18", "evidences": [{"id": "#9a5bf55b-dee1-4e0f-8c32-6ee9b21a7852", "source_surface_form": "#5b6e9c44-2d22-4447-9c6b-f317afcc8cc9", "target_surface_form": "#82322f99-efc5-4991-b895-e06d06a5442e", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r7": {"id": "r7", "source": "p2", "target": "a18", "evidences": [{"id": "#07c8d48e-739c-4b77-9868-74d2f43e2ca3", "source_surface_form": "#77e61c37-b503-4085-9fc5-f28b8380ab12", "target_surface_form": "#82322f99-efc5-4991-b895-e06d06a5442e", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r8": {"id": "r8", "source": "v9", "target": "p4", "evidences": [{"id": "#a968b22b-1c6c-4f08-8aca-bb9cc37500db", "source_surface_form": "#a95b47a5-80ac-4588-9379-288486c3c0c6", "target_surface_form": "#5edd9f78-5718-4998-b0ea-b97f6402a818", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r9": {"id": "r9", "source": "p4", "target": "a18", "evidences": [{"id": "#6aced56a-ed26-4354-a8de-14dd1bcc039f", "source_surface_form": "#5edd9f78-5718-4998-b0ea-b97f6402a818", "target_surface_form": "#82322f99-efc5-4991-b895-e06d06a5442e", "evidence_sentence": "All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers.", "start": 258, "end": 402}]}, "r10": {"id": "r10", "source": "v11", "target": "p1", "evidences": [{"id": "#7255e268-ed4c-4c85-9b1b-7b318dee916e", "source_surface_form": "#4f351626-b59d-4e71-a061-b65f4e637c03", "target_surface_form": "#4c0ef44a-0235-4bf3-b500-2ebeb87ac97e", "evidence_sentence": "For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax.", "start": 403, "end": 590}]}, "r11": {"id": "r11", "source": "p1", "target": "a18", "evidences": [{"id": "#be435edd-7ae6-463d-b16c-d15f6fdc7fba", "source_surface_form": "#4c0ef44a-0235-4bf3-b500-2ebeb87ac97e", "target_surface_form": "#a02000ac-a1d0-40d9-8d84-b554328da4a1", "evidence_sentence": "For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax.", "start": 403, "end": 590}]}, "r12": {"id": "r12", "source": "v12", "target": "p5", "evidences": [{"id": "#99f684c4-7805-455f-91b3-affc7c903cdf", "source_surface_form": "#ccf1a670-2b62-44aa-8560-1ef916c44ecc", "target_surface_form": "#8b428ec9-5ccb-4b0f-9307-2fb6bca904ac", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r13": {"id": "r13", "source": "c1", "target": "v13", "evidences": [{"id": "#1f307d9b-bc98-41ee-ac4d-26ace98e8b81", "source_surface_form": "#186b0fcb-4905-4782-b328-f0a9d139e50b", "target_surface_form": "#299b15ad-953a-4664-b9c2-10a330e0a51d", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r14": {"id": "r14", "source": "v13", "target": "p6", "evidences": [{"id": "#fb6a13c7-168a-45db-af49-fbd72c586f22", "source_surface_form": "#299b15ad-953a-4664-b9c2-10a330e0a51d", "target_surface_form": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r15": {"id": "r15", "source": "p6", "target": "a22", "evidences": [{"id": "#c0c79ad0-5e19-4386-b37d-70941edddf4f", "source_surface_form": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "target_surface_form": "#09100685-2022-4f8f-b5e4-bd9a3a669e77", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r16": {"id": "r16", "source": "c2", "target": "v14", "evidences": [{"id": "#81530c42-1957-4256-b4e5-8df6be3ff126", "source_surface_form": "#95b12990-5e20-4df2-aba1-ca98d916bbe9", "target_surface_form": "#27ade662-e3eb-4a71-bfa7-e5161909dbde", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r17": {"id": "r17", "source": "v14", "target": "p6", "evidences": [{"id": "#38308cae-d23f-4819-aadc-de7a09b2ca05", "source_surface_form": "#27ade662-e3eb-4a71-bfa7-e5161909dbde", "target_surface_form": "#b4b24aa7-9daf-4dd1-b14b-504b89c65784", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}, "r18": {"id": "r18", "source": "v15", "target": "p8", "evidences": [{"id": "#0806a9b4-de19-46af-9db6-8ff46e31a11a", "source_surface_form": "#3faea6b8-f987-43bf-92f0-b81c15973ee7", "target_surface_form": "#5581acf1-60aa-420c-88df-2e2abcf2aaef", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}]}, "r19": {"id": "r19", "source": "p8", "target": "a22", "evidences": [{"id": "#ae84153d-0564-4945-9ce9-d2d6083a82f5", "source_surface_form": "#5581acf1-60aa-420c-88df-2e2abcf2aaef", "target_surface_form": "#ad6dea75-a336-4564-957b-fb5f22e38834", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}]}, "r20": {"id": "r20", "source": "c3", "target": "v15", "evidences": [{"id": "#d07071d6-7c2f-4449-9a09-15d70457056b", "source_surface_form": "#d78c347f-4a30-484e-bd35-b77c1f156360", "target_surface_form": "#3faea6b8-f987-43bf-92f0-b81c15973ee7", "evidence_sentence": "We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score.", "start": 724, "end": 830}]}, "r21": {"id": "r21", "source": "p2", "target": "a6", "evidences": [{"id": "#c6c9b671-6633-443d-8bed-9e0a79e58c30", "source_surface_form": "#3677f201-e8b6-47c6-98bf-d940c71285d7", "target_surface_form": "#fb9fec7c-fff7-44a7-85cc-41f77f52c364", "evidence_sentence": "The base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state. We use a dropout of 0.2 after BiLSTM embeddings.", "start": 97, "end": 257}]}, "r22": {"id": "r22", "source": "p5", "target": "a6", "evidences": [{"id": "#422aa41a-b031-4a26-86ca-6378d77fc8e2", "source_surface_form": "#8b428ec9-5ccb-4b0f-9307-2fb6bca904ac", "target_surface_form": "#5628e381-9e76-4263-9125-4acaf18f0cb3", "evidence_sentence": "We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights.", "start": 591, "end": 723}]}}}, "annotation_raw": null}, "prompt": "In the context of machine learning and related fields, what (if any) are the entities (datasets, models, methods, loss functions, regularization techniques) mentioned in the LaTeX Input Text below? What (if any) are their parameters and values?\n\n[LaTeX Input Text start]\nWe divide our 438 annotated documents into training (70%), validation (30%) and test set (30%).\r\nThe base document representation of our model is formed by SciBERT-base [4] and BiLSTM with 128-d hidden state. We use a dropout of 0.2 after BiLSTM embeddings. All feedforward networks are composed of two hidden layers, each of dimension 128 with gelu activation and with a dropout of 0.2 between layers. For additive attention layer in span representation, we collapse the token embeddings to scalars by passing through the feedforward layer with 128-d hidden state and performing a softmax. We train our model for 30 epochs using Adam optimizer with 1e-3 as learning rate for all non BERT weights and 2e-5 for BERT weights. We use early stopping with a patience value of 7 on the validation set using relation extraction F1 score. All our models were trained using 48Gb Quadro RTX 8000 GPUs. The multitask model takes approximately 3 hrs to train.\r\n\n[LaTeX Input Text end]\n\nAnswer in the following YAML format.\n\nFormat:\n---\n- text_contains_entities: true/false\n- entities (datasets, models, methods, loss functions, regularization techniques):\n    - entity<N>:\n        name: <entity name>\n        type: <entity type>\n        has_parameters: true/false\n        parameters:\n            - parameter<N>:\n                name: <parameter name>\n                value: <parameter value>/null\n                context: <value context>/null\n...\n\nOnly produce output in the YAML format specified above. Output no additional text.\n\nOutput:\n", "completion": {"id": "cmpl-7MXs88DJxAxaTZuclNccWQE0I90Dk", "object": "text_completion", "created": 1685607776, "model": "text-davinci-003", "choices": [{"text": "---\n- text_contains_entities: true\n- entities (datasets, models, methods, loss functions, regularization techniques):\n    - entity1:\n        name: SciBERT-base\n        type: model\n        has_parameters: false\n        parameters: null\n    - entity2:\n        name: BiLSTM\n        type: model\n        has_parameters: true\n        parameters:\n            - parameter1:\n                name: hidden state\n                value: 128-d\n                context: null\n    - entity3:\n        name: dropout\n        type: regularization technique\n        has_parameters: true\n        parameters:\n            - parameter1:\n                name: after BiLSTM embeddings\n                value: 0.2\n                context: null\n            - parameter2:\n                name: between layers\n                value: 0.2\n                context: null\n    - entity4:\n        name: feedforward networks\n        type: model\n        has_parameters: true\n        parameters:\n            - parameter1:\n                name: hidden layers\n                value: 128-d\n                context: null\n            - parameter2:\n                name: activation\n                value: gelu\n                context: null\n    - entity5:\n        name: additive attention layer\n        type: method\n        has_parameters: true\n        parameters:\n            - parameter1:\n                name: token embeddings\n                value: scalars\n                context: null\n            - parameter2:\n                name: feedforward layer\n                value: 128-d hidden state\n                context: null\n    - entity6:\n        name: Adam optimizer\n        type: method\n        has_parameters: true\n        parameters:\n            - parameter1:\n                name: learning rate\n                value: 1e-3\n                context: non BERT weights\n            - parameter2:\n                name: learning rate\n                value: 2e-5\n                context: BERT weights\n    - entity7:\n        name: early stopping\n        type: method\n        has_parameters: true\n        parameters:\n            - parameter1:\n                name: patience value\n                value: 7\n                context: validation set", "index": 0, "logprobs": {"tokens": ["---", "\n", "-", " text", "_", "cont", "ains", "_", "ent", "ities", ":", " true", "\n", "-", " entities", " (", "dat", "as", "ets", ",", " models", ",", " methods", ",", " loss", " functions", ",", " regular", "ization", " techniques", "):", "\n", "   ", " -", " entity", "1", ":", "\n", "       ", " name", ":", " Sci", "BER", "T", "-", "base", "\n", "       ", " type", ":", " model", "\n", "       ", " has", "_", "param", "eters", ":", " false", "\n", "       ", " parameters", ":", " null", "\n", "   ", " -", " entity", "2", ":", "\n", "       ", " name", ":", " Bi", "L", "ST", "M", "\n", "       ", " type", ":", " model", "\n", "       ", " has", "_", "param", "eters", ":", " true", "\n", "       ", " parameters", ":", "\n", "           ", " -", " parameter", "1", ":", "\n", "               ", " name", ":", " hidden", " state", "\n", "               ", " value", ":", " 128", "-", "d", "\n", "               ", " context", ":", " null", "\n", "   ", " -", " entity", "3", ":", "\n", "       ", " name", ":", " drop", "out", "\n", "       ", " type", ":", " regular", "ization", " technique", "\n", "       ", " has", "_", "param", "eters", ":", " true", "\n", "       ", " parameters", ":", "\n", "           ", " -", " parameter", "1", ":", "\n", "               ", " name", ":", " after", " Bi", "L", "ST", "M", " embed", "d", "ings", "\n", "               ", " value", ":", " 0", ".", "2", "\n", "               ", " context", ":", " null", "\n", "           ", " -", " parameter", "2", ":", "\n", "               ", " name", ":", " between", " layers", "\n", "               ", " value", ":", " 0", ".", "2", "\n", "               ", " context", ":", " null", "\n", "   ", " -", " entity", "4", ":", "\n", "       ", " name", ":", " feed", "forward", " networks", "\n", "       ", " type", ":", " model", "\n", "       ", " has", "_", "param", "eters", ":", " true", "\n", "       ", " parameters", ":", "\n", "           ", " -", " parameter", "1", ":", "\n", "               ", " name", ":", " hidden", " layers", "\n", "               ", " value", ":", " 128", "-", "d", "\n", "               ", " context", ":", " null", "\n", "           ", " -", " parameter", "2", ":", "\n", "               ", " name", ":", " activation", "\n", "               ", " value", ":", " gel", "u", "\n", "               ", " context", ":", " null", "\n", "   ", " -", " entity", "5", ":", "\n", "       ", " name", ":", " additive", " attention", " layer", "\n", "       ", " type", ":", " method", "\n", "       ", " has", "_", "param", "eters", ":", " true", "\n", "       ", " parameters", ":", "\n", "           ", " -", " parameter", "1", ":", "\n", "               ", " name", ":", " token", " embed", "d", "ings", "\n", "               ", " value", ":", " scal", "ars", "\n", "               ", " context", ":", " null", "\n", "           ", " -", " parameter", "2", ":", "\n", "               ", " name", ":", " feed", "forward", " layer", "\n", "               ", " value", ":", " 128", "-", "d", " hidden", " state", "\n", "               ", " context", ":", " null", "\n", "   ", " -", " entity", "6", ":", "\n", "       ", " name", ":", " Adam", " optim", "izer", "\n", "       ", " type", ":", " method", "\n", "       ", " has", "_", "param", "eters", ":", " true", "\n", "       ", " parameters", ":", "\n", "           ", " -", " parameter", "1", ":", "\n", "               ", " name", ":", " learning", " rate", "\n", "               ", " value", ":", " 1", "e", "-", "3", "\n", "               ", " context", ":", " non", " B", "ERT", " weights", "\n", "           ", " -", " parameter", "2", ":", "\n", "               ", " name", ":", " learning", " rate", "\n", "               ", " value", ":", " 2", "e", "-", "5", "\n", "               ", " context", ":", " B", "ERT", " weights", "\n", "   ", " -", " entity", "7", ":", "\n", "       ", " name", ":", " early", " stopping", "\n", "       ", " type", ":", " method", "\n", "       ", " has", "_", "param", "eters", ":", " true", "\n", "       ", " parameters", ":", "\n", "           ", " -", " parameter", "1", ":", "\n", "               ", " name", ":", " patience", " value", "\n", "               ", " value", ":", " 7", "\n", "               ", " context", ":", " validation", " set"], "token_logprobs": [-0.017545206, -0.0056321174, -0.020461664, -0.000470523, -4.1320116e-05, -7.517727e-05, -5.037998e-05, -1.5805701e-05, -5.4314798e-05, -0.0003477465, -9.592052e-05, -0.0011938312, -0.0046918984, -4.573365e-05, -0.00012751458, -0.022401374, -9.675685e-05, -3.2906587e-06, -2.5344274e-05, -8.00667e-05, -6.194322e-05, -1.0084622e-05, -1.7356348e-05, -1.1039229e-05, -4.7399757e-05, -7.815842e-05, -1.4616429e-05, -9.806815e-05, -2.28437e-05, -0.00046110176, -0.00026755192, -0.0053390493, -0.006841569, -1.2708151e-05, -0.0005583038, -0.007131596, -0.00040129185, -0.017133838, -0.001464798, -6.9573514e-05, -3.488097e-05, -0.015403785, -0.00056868145, -4.9187893e-05, -0.0041360743, -0.0040444443, -0.049870346, -9.592052e-05, -8.5358415e-06, -1.9503042e-05, -0.3663344, -0.0070629236, -0.00014790209, -0.00014873935, -5.2885232e-05, -2.1051823e-05, -0.0007534595, -3.01145e-05, -0.026620377, -0.004508067, -0.26525986, -0.00013562171, -4.5374152e-05, -0.11761563, -0.021383766, -0.0014487397, -7.4639065e-06, -0.00037493455, -8.078007e-05, -0.00012453248, -0.024764676, -0.00052281993, -3.4403198e-05, -2.045765e-05, -0.0020526398, -0.00018248011, -0.00010188283, -0.00041935995, -0.017586516, -3.8103353e-05, -1.3306066e-05, -1.163433e-05, -0.32089177, -0.0035430577, -3.5717305e-05, -4.0843282e-05, -3.7743856e-05, -2.2245786e-05, -0.00020167646, -4.883026e-05, -0.0038737194, -0.0029469815, -6.170386e-05, -5.7532503e-05, -4.8353428e-05, -0.12306795, -0.0020251647, -8.352281e-05, -0.0012037387, -0.0003575263, -0.00055126764, -0.024172807, -0.001170855, -2.4032974e-05, -3.678925e-05, -0.14029129, -0.036754783, -0.012045966, -7.2672e-05, -6.7427754e-05, -3.333313e-05, -0.0010966838, -0.40067205, -0.00055365276, -0.022395888, -0.0007318678, -7.553489e-05, -4.799487e-05, -0.018619094, -0.013416816, -0.14312819, -3.529076e-06, -0.00036181876, -0.00016554873, -0.00013443055, -0.028881606, -0.00038674276, -4.036551e-05, -4.1678675e-05, -0.62363887, -0.0014152542, -0.005635895, -2.6060465e-05, -1.5805701e-05, -5.037998e-05, -0.0014403837, -0.00047833633, -0.012907629, -0.0027603628, -3.3093773e-05, -5.8247755e-05, -5.3837026e-05, -2.4272329e-05, -0.00012250594, -2.6060465e-05, -0.00089579565, -0.0016188136, -0.00010092916, -0.00010784233, -5.8010275e-05, -0.03523316, -0.00046384355, -3.8103353e-05, -0.0014506498, -0.0049550096, -0.00044857734, -0.012327736, -0.00058406877, -3.4287736e-05, -1.9382902e-05, -1.1488556, -0.002527348, -0.00036337037, -0.000104267005, -0.00012119652, -0.026971249, -0.008961859, -0.0007775012, -0.013319561, -9.628002e-05, -0.00010283744, -2.5344274e-05, -0.00016316361, -1.0920954e-05, -4.9427246e-05, -0.0046321955, -0.00020155631, -8.447461e-05, -4.406191e-05, -0.0035590879, -0.0059090266, -0.0041307495, -2.6298881e-05, -0.00084622175, -0.0005321858, -0.00013490739, -0.033767797, -0.0023523988, -8.0899285e-05, -0.00015708302, -0.0057298304, -0.023261381, -0.0060228934, -6.325358e-05, -4.2153635e-05, -7.8752586e-05, -0.0001191709, -1.3422466e-05, -2.0815281e-05, -0.0033965777, -4.6206733e-05, -9.198663e-05, -5.2525735e-05, -0.004861448, -0.012357666, -0.0008209852, -7.1062755e-06, -0.00040832703, -0.00032681835, -9.24644e-05, -0.015234137, -0.0004885883, -5.4194657e-05, -2.7371756e-05, -0.20536575, -0.0015714729, -0.6217313, -0.012872792, -2.4032974e-05, -2.8682118e-05, -2.8445576e-05, -0.15852775, -0.0032021592, -3.4048382e-05, -7.708367e-05, -6.746778e-06, -2.2958224e-05, -0.00035788392, -3.869565e-05, -0.0010036626, -0.0015917101, -0.00010343254, -8.662224e-05, -5.7057543e-05, -0.023141392, -0.00044249676, -1.1514188e-05, -0.0010209638, -0.00014539869, -0.00022218504, -0.012137006, -0.00072518643, -3.2140102e-05, -1.7595703e-05, -0.238624, -0.21368822, -0.047715355, -5.0500123e-05, -3.8577375e-05, -1.3661821e-05, -0.74605477, -0.22633618, -0.0010936997, -0.035201423, -0.0001486192, -8.388231e-05, -3.690939e-05, -0.3274231, -0.003975682, -0.00067496306, -6.031526e-06, -0.00090116286, -0.0005110145, -0.00034416933, -0.008208998, -0.0005040948, -9.675685e-05, -2.7371756e-05, -0.14723092, -0.02102584, -5.5745302e-05, -5.312365e-05, -1.306671e-05, -0.018945057, -3.595572e-05, -0.013271659, -0.00022838576, -6.7307614e-05, -6.110595e-05, -0.0004584773, -0.006584383, -0.27869183, -6.2699432e-06, -0.0003985491, -0.00040594288, -0.00011225586, -0.007486562, -0.00085755315, -5.0856816e-05, -2.6538237e-05, -0.6437967, -0.0009889238, -0.0102743395, -0.042639405, -4.1200903e-05, -1.2351458e-05, -5.1215375e-05, -0.20825985, -0.0018913309, -2.6538237e-05, -9.806815e-05, -3.7983213e-05, -4.489638e-05, -0.0023889036, -4.2631407e-05, -0.07280215, -0.00104143, -6.7069195e-05, -0.00013395278, -4.382537e-05, -0.019208906, -0.00030845366, -3.7266083e-05, -0.0013655339, -0.00017210613, -0.00026850656, -0.011778333, -0.00079807744, -4.811501e-05, -1.5687427e-05, -0.19640197, -0.00023321231, -0.0048836106, -0.0003686146, -0.035725247, -9.103483e-05, -6.396884e-05, -1.9263687e-05, -0.22362554, -0.058163673, -0.012407934, -8.864972e-05, -6.516092e-05, -5.550595e-05, -0.12895666, -0.0031301675, -0.21446209, -8.4157e-06, -0.00071272027, -0.00020751769, -0.00013657632, -0.0073630414, -0.0005878262, -6.48061e-05, -5.467149e-05, -0.07607183, -0.00130173, -0.0049193073, -0.029682552, -0.00011928824, -7.064452e-05, -1.8073477e-05, -0.0024039005, -0.0002376249, -4.6325007e-05, -0.50055254, -0.0007038932, -0.0020363335, -5.7652644e-05, -7.005035e-05, -8.650302e-05, -0.00050290365, -0.0079110265, -0.11982853, -8.4157e-06, -0.00034428947, -0.0003803027, -0.00012310104, -0.004768794, -0.00021157267, -4.2153635e-05, -1.7236207e-05, -0.0036232146, -0.052260682, -0.0007209531, -0.0051426943, -3.8577375e-05, -1.1159372e-05, -3.0473995e-05, -0.10809566, -0.0011850595, -2.987608e-05, -6.373136e-05, -3.225932e-05, -2.0218295e-05, -0.00011058788, -4.2511267e-05, -0.0011238332, -0.0008171667, -6.790459e-05, -6.265847e-05, -6.3134365e-05, -0.014433363, -0.0002376249, -1.5094201e-05, -0.0008138261, -0.00011571198, -0.0001856978, -0.008740204, -0.0008850565, -3.4762696e-05, -8.653179e-06, -0.023919003, -0.0017606757, -0.07418127, -0.0001101101, -4.0246297e-05, -2.4866502e-05, -0.06577884, -0.00031262785, -0.00022963606, -0.00012465262, -0.40996912, -0.00010212031, -5.490991e-05, -2.045765e-05, -0.84687364, -0.030352402, -2.3360508e-06, -0.0018075499, -0.011979246, -0.0001532674, -1.8190814e-05, -0.0005232977, -0.00037887125, -0.00018999024, -0.02142694, -0.00060870085, -5.3363005e-05, -4.2153635e-05, -0.0013475663, -0.00012918444, -0.009594203, -7.136165e-05, -3.595572e-05, -4.9187893e-05, -0.00028639348, -7.350647e-05, -0.00012942286, -0.00022743117, -0.0064100437, -4.716134e-05, -6.563869e-05, -6.885639e-05, -0.002672866, -1.8190814e-05, -0.00027876132, -0.0068329275, -0.0021837514, -9.251094e-06, -0.00031978224, -0.00020107947, -0.00025014277, -0.008504891, -0.00024501586, -5.8367896e-05, -2.7728449e-05, -0.06544482, -0.0012529118, -0.00532233, -2.8682118e-05, -1.1398727e-05, -3.8457234e-05, -0.18257973, -0.0010061669, -3.9055147e-05, -6.6949986e-05, -1.6282536e-05, -3.7743856e-05, -0.00081370876, -4.394177e-05, -0.00029414485, -0.0008909025, -8.960433e-05, -8.244993e-05, -6.396884e-05, -0.015026456, -0.00035860104, -9.012677e-06, -0.00087037886, -0.00017961723, -0.00023619535, -0.005306571, -0.00068862323, -4.763911e-05, -1.0323977e-05, -0.21940106, -0.24328457, -0.004850307, -0.000114638155, -4.2153635e-05, -1.8788729e-05, -0.00034738888, -0.00515396, -0.00016304628, -6.0633804e-05, -5.9085025e-05, -0.43005657, -0.0003506094], "top_logprobs": null, "text_offset": [1799, 1802, 1803, 1804, 1809, 1810, 1814, 1818, 1819, 1822, 1827, 1828, 1833, 1834, 1835, 1844, 1846, 1849, 1851, 1854, 1855, 1862, 1863, 1871, 1872, 1877, 1887, 1888, 1896, 1903, 1914, 1916, 1917, 1920, 1922, 1929, 1930, 1931, 1932, 1939, 1944, 1945, 1949, 1952, 1953, 1954, 1958, 1959, 1966, 1971, 1972, 1978, 1979, 1986, 1990, 1991, 1996, 2001, 2002, 2008, 2009, 2016, 2027, 2028, 2033, 2034, 2037, 2039, 2046, 2047, 2048, 2049, 2056, 2061, 2062, 2065, 2066, 2068, 2069, 2070, 2077, 2082, 2083, 2089, 2090, 2097, 2101, 2102, 2107, 2112, 2113, 2118, 2119, 2126, 2137, 2138, 2139, 2150, 2152, 2162, 2163, 2164, 2165, 2180, 2185, 2186, 2193, 2199, 2200, 2215, 2221, 2222, 2226, 2227, 2228, 2229, 2244, 2252, 2253, 2258, 2259, 2262, 2264, 2271, 2272, 2273, 2274, 2281, 2286, 2287, 2292, 2295, 2296, 2303, 2308, 2309, 2317, 2324, 2334, 2335, 2342, 2346, 2347, 2352, 2357, 2358, 2363, 2364, 2371, 2382, 2383, 2384, 2395, 2397, 2407, 2408, 2409, 2410, 2425, 2430, 2431, 2437, 2440, 2441, 2443, 2444, 2450, 2451, 2455, 2456, 2471, 2477, 2478, 2480, 2481, 2482, 2483, 2498, 2506, 2507, 2512, 2513, 2524, 2526, 2536, 2537, 2538, 2539, 2554, 2559, 2560, 2568, 2575, 2576, 2591, 2597, 2598, 2600, 2601, 2602, 2603, 2618, 2626, 2627, 2632, 2633, 2636, 2638, 2645, 2646, 2647, 2648, 2655, 2660, 2661, 2666, 2673, 2682, 2683, 2690, 2695, 2696, 2702, 2703, 2710, 2714, 2715, 2720, 2725, 2726, 2731, 2732, 2739, 2750, 2751, 2752, 2763, 2765, 2775, 2776, 2777, 2778, 2793, 2798, 2799, 2806, 2813, 2814, 2829, 2835, 2836, 2840, 2841, 2842, 2843, 2858, 2866, 2867, 2872, 2873, 2884, 2886, 2896, 2897, 2898, 2899, 2914, 2919, 2920, 2931, 2932, 2947, 2953, 2954, 2958, 2959, 2960, 2975, 2983, 2984, 2989, 2990, 2993, 2995, 3002, 3003, 3004, 3005, 3012, 3017, 3018, 3027, 3037, 3043, 3044, 3051, 3056, 3057, 3064, 3065, 3072, 3076, 3077, 3082, 3087, 3088, 3093, 3094, 3101, 3112, 3113, 3114, 3125, 3127, 3137, 3138, 3139, 3140, 3155, 3160, 3161, 3167, 3173, 3174, 3178, 3179, 3194, 3200, 3201, 3206, 3209, 3210, 3225, 3233, 3234, 3239, 3240, 3251, 3253, 3263, 3264, 3265, 3266, 3281, 3286, 3287, 3292, 3299, 3305, 3306, 3321, 3327, 3328, 3332, 3333, 3334, 3341, 3347, 3348, 3363, 3371, 3372, 3377, 3378, 3381, 3383, 3390, 3391, 3392, 3393, 3400, 3405, 3406, 3411, 3417, 3421, 3422, 3429, 3434, 3435, 3442, 3443, 3450, 3454, 3455, 3460, 3465, 3466, 3471, 3472, 3479, 3490, 3491, 3492, 3503, 3505, 3515, 3516, 3517, 3518, 3533, 3538, 3539, 3548, 3553, 3554, 3569, 3575, 3576, 3578, 3579, 3580, 3581, 3582, 3597, 3605, 3606, 3610, 3612, 3615, 3623, 3624, 3635, 3637, 3647, 3648, 3649, 3650, 3665, 3670, 3671, 3680, 3685, 3686, 3701, 3707, 3708, 3710, 3711, 3712, 3713, 3714, 3729, 3737, 3738, 3740, 3743, 3751, 3752, 3755, 3757, 3764, 3765, 3766, 3767, 3774, 3779, 3780, 3786, 3795, 3796, 3803, 3808, 3809, 3816, 3817, 3824, 3828, 3829, 3834, 3839, 3840, 3845, 3846, 3853, 3864, 3865, 3866, 3877, 3879, 3889, 3890, 3891, 3892, 3907, 3912, 3913, 3922, 3928, 3929, 3944, 3950, 3951, 3953, 3954, 3969, 3977, 3978, 3989]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 445, "completion_tokens": 499, "total_tokens": 944}}}}}